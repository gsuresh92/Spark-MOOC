 
Piazza is a Q&A; platform designed to get you great answers from classmates and instructors fast. We've put together this list of tips you might find handy as you get started:

1.  Ask questions!

The best way to get answers is to ask questions! Ask questions on Piazza rather than emailing your teaching staff so everyone can benefit from the response (and so you can get answers from classmates who are up as late as you are).

2.  Edit questions and answers wiki-style.

Think of Piazza as a Q&A; wiki for your class. Every question has just a single students' answer that students can edit collectively (and a single instructors’ answer for instructors).

3.  Add a followup to comment or ask further questions.

To comment on or ask further questions about a post, start a followup discussion. Mark it resolved when the issue has been addressed, and add any relevant information back into the Q&A; above.

4.  Go anonymous.

Shy? No problem. You can always opt to post or edit anonymously.

5.  Tag your posts.

It's far more convenient to find all posts about your Homework 3 or Midterm 1 when the posts are tagged. Type a “#” before a key word to tag. Click a blue tag in a post or the question feed to filter for all posts that share that tag.

6.  Format code and equations.

Adding a code snippet? Click the pre or tt button in the question editor to add pre-formatted or inline teletype text. 
Mathematical equation? Click the Fx button to access the LaTeX editor to build a nicely formatted equation.

7.  View and download class details and resources.

Click the Course Page button in your top bar to access the class syllabus, staff contact information, office hours details, and course resources—all in one place!


Contact the Piazza Team anytime with questions or comments at team@piazza.com. We love feedback! The teaching staff has posted a new homework resource.Title: Software environment setup assignmenthttps://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/courseware/d1f293d0cb53466dbb5c0cd81f55b45b/920d3370060540c8b21d56f05c64bdda/ Due date: Jun 6, 2015 (00:00 UTC time!)You can view it on the course page: https://piazza.com/edx_berkeley/summer2015/cs1001x/resources https://piazza.com/edx_berkeley/summer2015/cs1001x/resources
 Hello, i was wondering if it was possible to have early access to the last 2 weeks material and exercises?quizzes. I'll be off at the end of June, but would still like to complete this course. Hi, The 'Vagrant for Windows' video actually shows how to install Virtual Box on Windows not 'Vagrant for Windows'.

Thanks,
Shahul Hi,

I wanted to watch video in the train.

Is it possible to download them?

Thanks The video under this section is the same as the one in 'Virtualbox for Windows' if not could it be taken into account for next session. When I run Vagrant Up, I get the error "HTTP server doesn't seem to support byte ranges. Cannot resume."

I tried restarting, and get the same error. Also tried Vagrant Destroy/Reload then Up, but still the same error. Could it be the Server is just busy?

Arnold Intro video has been buffering for the past 10 minutes as I write this note -- is anyone else having the same issue? I am access edx.org from Hong Kong. http://awards.acm.org/doctoral_dissertation/ Hi!

I prefer printing lecture slides and then draw on them.
Your slides are almost "printer friendly": no colored header/futter, clear text. Big thanks for that!
But all of them have black rectangular that has no meaning.. placeholder for video?

Will it be possible to have slides without black rectangular?

Thank you Hello, fellow data scientists!

LinkedIN group here:  http://www.linkedin.com/grp/home?gid=8316702

Also, I reciprocate LinkedIN endorsements with my fellow MOOC cohorts who make it to the end of each class. So if you wanna swap "Apache Spark" endorsements in about a month, connect with me:  http://www.linkedin.com/in/freemanjd

Also, put your own LinkedIN info here and on the group so others can find you...

I wish everybody the very best of luck in the class.

Best,
J D Freeman Hello TAs, Fellow Classmates,

Does anyone know of an easy way to get the environment set up using Hyper-V, a virtual run-time engine from Windows, rather than VirtualBox. I know that Vagrant supports Hyper-V for Vagrant-ready images. 

I can use VirtualBox, but I use hyper extensively. You cannot have access to both enabled. I want to avoid cycling my setting for this class versus the rest of my work. I'm on a university connection and I'm pulling down the VM from within Vagrant at 116k/s which is just insanely slow.  Is anyone else experiencing this? Or is there way to allow Vagrant to pull in data from multiple TCP sources?  

This is excruciating.... Can not install Vagrant on system with windows 7

error message: Installation directory must be on a local drive.
note I use the default directory and I tried also a different directory = same error.

Any help?  Hi, 

I am getting an installation error after issuing " vagrant up " command on Ubuntu 14.04. Can someone please help me on this ?



Thanks,
Sudarshan Vagrant for windows video is still not fixed .
Please help. I have VMware virtual machine installed with ubuntu? Can i use that? while trying to install virtual machine on my mac os x 10.10.3. I am getting the following error.

vagrant upBringing machine 'default' up with 'virtualbox' provider...==> default: Box 'sparkmooc/base' could not be found. Attempting to find and install...    default: Box Provider: virtualbox    default: Box Version: >= 0==> default: Adding box 'sparkmooc/base' (v0) for provider: virtualbox    default: Downloading: https://atlas.hashicorp.com/sparkmooc/baseThe box failed to unpackage properly. Please verify that the boxfile you're trying to add is not corrupted and try again. Theoutput from attempting to unpackage (if any):bsdtar: Error opening archive: Unrecognized archive format Hi 
this is great chance 
Do you have a web to support. Is there a method to run a Spark Cluster on a single machine (like a laptop) ?  I've found one solution from the AMP lab (https://amplab.cs.berkeley.edu/got-a-minute-spin-up-a-spark-cluster-on-your-laptop-with-docker/) but that one is too old ( version 0.7 was used, the newest  is 1.3 ). 

Thanks! Lecture videos 1 and 2 are hardly half a minute duration. Are there more videos for this week or that is all for this week? Hi there,

I have VirtualBox 4.3.18 on Mac and there don't seem to be any newer updates available. The installation instructions suggest that 4.3.28  needs to be installed.

Is that correct?

Regards,
Sandeep My laptop is quite under powered, however I do have a fairly beefy linux server.

I've already set up ipython but struggling a bit with pyspark

Anybody tried this?Any ideas?

 Hi there,


I've installed VirtualBox and Vagrant (on Windows 8.1), no problems whatsoever.

Started "vagrant up" from the console, downloading of the VM started, but as the time remaining was way beyond 5 hours I decided to cancel it.

Since then it is not possible to resume the process. I deleted the temporary files created by vagrant in the target directory, but this didn't help. Every time I create a fresh directory where I put only the Vagrant file, starting "vagrant up" spits the following error:

...==> sparkvm: Box download is resuming from prior download progress
    sparkvm: Progress: 0% (Rate: 0/s, Estimated time remaining: --:--:--):--)
An error occurred while downloading the remote file. The error
message, if any, is reproduced below. Please fix this error and try
again.

HTTP server doesn't seem to support byte ranges. Cannot resume.

It seems that vagrant tried to resume the download somehow, ever since I deleted the temporary files.

Any help?
  .?any idea just curious, I'm downloading it now, but it would be nice to have a requirements.txt file or similar for the course. my laptop is old and the hard drive is very full, want to make sure I have enough space & that I'm not wasting it on things I might already have. thanks!  When I type "vagrant up" to install the VM on OS X I'm getting this error - any thoughts?

/opt/vagrant/bin/../embedded/gems/gems/vagrant-1.7.2/lib/vagrant/pre-rubygems.rb:31: warning: Insecure world writable dir /usr/local in PATH, mode 040777
/opt/vagrant/embedded/gems/gems/bundler-1.7.11/lib/bundler/runtime.rb:222: warning: Insecure world writable dir /usr/local in PATH, mode 040777
Bringing machine 'sparkvm' up with 'virtualbox' provider...
==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install...
    sparkvm: Box Provider: virtualbox
    sparkvm: Box Version: >= 0
The box 'sparkmooc/base' could not be found or
could not be accessed in the remote catalog. If this is a private
box on HashiCorp's Atlas, please verify you're logged in via
`vagrant login`. Also, please double-check the name. The expanded
URL and error message are shown below:

URL: ["https://atlas.hashicorp.com/sparkmooc/base"]
Error: SSL certificate problem: unable to get local issuer certificate
More details here: http://curl.haxx.se/docs/sslcerts.html

curl performs SSL certificate verification by default, using a "bundle"
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn't adequate, you can specify an alternate file
 using the --cacert option.
If this HTTPS server uses a certificate signed by a CA represented in
 the bundle, the certificate verification probably failed due to a
 problem with the certificate (it might be expired, or the name might
 not match the domain name in the URL).
If you'd like to turn off curl's verification of the certificate, use
 the -k (or --insecure) option. This is how you download videos in this lecture:
https://github.com/shk3/edx-downloader
If you run this script now, you'll only get first week's lectures. Once next week's lectures are up, you can run it again and it'll only download the new ones. I tried doing a 'Vagrant destroy' followed by a 'vagrant up' but the issue persists.. How do I resolve and complete the setup?
________________________________________________________________________________
| ~/vagrant @ Goutams-MacBook-Air (goutamkumarbiswas) 
| => vagrant up
Bringing machine 'sparkvm' up with 'virtualbox' provider...
==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install...
    sparkvm: Box Provider: virtualbox
    sparkvm: Box Version: >= 0
==> sparkvm: Loading metadata for box 'sparkmooc/base'
    sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base
==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7) for provider: virtualbox
    sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7/providers/virtualbox.box
==> sparkvm: Box download is resuming from prior download progress
An error occurred while downloading the remote file. The error
message, if any, is reproduced below. Please fix this error and try
again.

HTTP server doesn't seem to support byte ranges. Cannot resume.
________________________________________________________________________________ Because of security concerns I can't  install virtualbox in my working place.Is there an alternative?Either in my computer or a host provider in the web or something else?Thanks,David Hello.
I had some problems with Vagrant because the charset was incompatible, this because the username has a special char ( é) to solve this just create a new user without any special char and login with it. That solved my problem.
Good day. Hi

An error occurred while downloading the remote file sparkmooc/base

Thanks for your help

Regards  Hello, I'm getting the following error:
Help please.
 Hello everybody,

I would like to know if there is a torrent version available for the virtual.box file.
The download on my side takes roughly 4 hours, but my internet connection is rather shaky with occasional disconnects.


Already disconnected once while downloading, resulted in the HTTP error commonly seen on the board. Had to delete all temp files and start over again.

Best regards We have moved the VM host to S3 - which hopefully will be much more pleasant. Please be patient.

The Vagrant image is hosted on S3 so the low bandwidth many of you are encountering may be a result of either many students simultaneously downloading the VM image or peering issues between your ISP and Amazon. Using my ISP, I saw similar slow download rates and then the speed eventually increased.

If your download fails or you manually interrupt it, you should be able to resume it because the new hosting environment supports resuming partial downloads (byte ranges).

If you encounter Unrecognized archive format errors you will have to delete the partial download with the following command:
vagrant box remove sparkmooc/base
You can then use:
vagrant up 
to redownload the VM image.

#pin Vagrant downloads is taking upto 5 hours .
Please make a torrent or zip for all the required files needed and upload it so that it doesn't take such time to download and install.
Thank you. Hi, 

There are two videos that appear in the Android app but that doesn't appear in the web version at https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/courseware/d1f293d0cb53466dbb5c0cd81f55b45b/fe9a95cc542d4c30b855e632663c4797/. The videos are "USA 2012 Presidential Election" and "Faceboon Lexicon"

This looks like an error, could somebody confirm that this is ok?

Thanks in advance, greetings, 

Juan 
 I'm running OpenSUSE 13.2 x64 with vagrant 1.7.2 and VirtualBox 4.3.28 and I did manage to start the download with "vagrant up" but it crashed after 10 minutes. Then I removed the temp files and restarted the process, but I got the following error:
Bringing machine 'sparkvm' up with 'virtualbox' provider...
==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install...
    sparkvm: Box Provider: virtualbox
    sparkvm: Box Version: >= 0
The box 'sparkmooc/base' could not be found or
could not be accessed in the remote catalog. If this is a private
box on HashiCorp's Atlas, please verify you're logged in via
`vagrant login`. Also, please double-check the name. The expanded
URL and error message are shown below:

URL: ["https://atlas.hashicorp.com/sparkmooc/base"]
Error: Failed connect to atlas.hashicorp.com:443; Connection refused

PS: I've used vagrant before in other MOOC courses with OpenSUSE and had no issues with it. It seems that it's something particular to this setup.
 I have installed both Vagrant and Oracle virtual box. And as instructed make a directory marco in c->Users then inside myvagrant, also put the file 'vagrantfile' in folder myvagrant.

And on giving command 'vagrant up' in prompt show error whose picture I have posted below
 Lets build a social connection of similar kind people  and continue this learning and take it to more advance level in future . 
join our facebook group 
https://www.facebook.com/groups/1659409384270903/ I unzipped the "Vagrantfile" and pasted it into the directory. Then in command line I wrote user/vagrant up. Almost after 1 hour progress is only 10%. Is it ok? What should I do? Hi there,
I'm experiencing timeout during download...
(I think a lot of people are trying to set up their software environment at this time)
Is it possible to configure the server to support resume in case of timeout?
That's the message I get...


An error occurred while downloading the remote file. The error
message, if any, is reproduced below. Please fix this error and try
again.

transfer closed with 612426865 bytes remaining to read
and if I issue again
vagrant up
I got this...
HTTP server doesn't seem to support byte ranges. Cannot resume.
And I think that's a server configuration...can you please support it?
I'm using vagrant 1.7.2 on ubuntu 14.4 64 bit
Cheers,
Seba I had a great experience with Piazza when I was taking a Nanodegree from Udacity.  I was a little sad when they stopped using it.  It is a great platform to really connect with and get to know other students.  Great choice, instructors! Hi all,

We have moved the VM to a new host.
Please try again.

Thanks

FAQ: https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/wiki/BerkeleyX.CS100.1x.1T2015/vm-download-faq/


#pin All other parts pass but part 1a & 1b fail.

The video on submitting the notebook also shows errors for part 1a & 1b. See screen capture immediately below:


So I submitted the error prone notebook to the grader and it passed!
Compare with hash (2a)
----------------------
All tests passed
Compare lists (2b)
------------------
All tests passed
-- 2 cases passed (100.0%) --

This is the course vm installation.

Output for 1a) & 1b)
4999950000
714264285


122395 I get this error when I try vagrant up

 Machine booted and ready :-) Hi when I run vagrant up, the download works fine, but it gets to "waiting for machine to boot" it starts to repeatedly generating the following error:

sparkvm: Warning: Connection timeout. Retrying...

I'm using windows 8 

thanks
 Where do I submit the first assignment?
I haven't found the link yet.

I found it, and submitted.
It's in the last item of the "Setting up the Course Software Environment" section.
The auto grader is taking a few minutes to grade.

All tests passed :-)

Compare with hash (2a)
----------------------
All tests passed
Compare lists (2b)
------------------
All tests passed
-- 2 cases passed (100.0%) --




 I am glad to meet everyone and taking this class,welcome to leave your social information here and let classmate finding you.
My Linkedin profile is https://www.linkedin.com/in/sunsocool ,welcome to join my connection. If you use QQ you can find a QQ group by the number: 289315034. Someone help pls, I finally got everything installed. I can even connect to the VM using ssh. But I can't get localhost:8001 working, it keeps saying webpage not available. I searched online and found something about port forwarding, I tried that and it says forwarding port during vagrant up but It's still not working when I try to connect on that port. The install of VirtualBox seemed to go fine, and I did Run as Admin, but when I try to open VB I get the following - 


I was able to open the app at the completion of the install, but also got a different error when I tried checking for updates as shown in the video.

More info... I can open VirualBox if I Run as Admin, but when I do Check for Updates, the error I get is "The network operation failed with the following error: SSL authentication failed.".

I did an uninstall & re-install and got the same results. The install seems to go fine, then when I open VirtualBox at the end of the install and Check for Updates, I get the following - 


Following this I click Cancel All. From that point on when I try to open VirtualBox I get the first error shown above.

Apparently we have firewall issues with being able to check for updates. I continued with the setup and VirtualBox worked. I had other problems with Vagrant, but VirtualBox worked. Thanks
 Is there any way I can get access to all the required features provided by the virtual machine image without installing a virtual machine?

I have most of the libraries and other software (iPython, python, Java etc) on my linux machine and therefore, would prefer installing the remaining libraries and not go through the Virtual Machine setup.

Thanks. I can't play the videos to setup the programming environment, hopefully I have prior experience with VirtualBox and Vagrant so I could easily go to last step, download notebook, run cells and submit the file.

I'm running Ubuntu 14.04 and using Chromium 41.0.2272.76 I am testing the my installation (looks to me it went fine).

I get Ubuntu 14.x prompt

sparkvm login:

what is Linux  username & password ?

thanks Hello,

I have problem with downloading file (ISP problem), but I solved it trough proxy (I downloaded it manually without vagrant)
Now, I don't know to which file hierarchy I should move file virtualbox.box...

 I am getting a failure to bring up vagrant on Windows 7 environment.

Following is the error:



Anyone else getting this error and any pointers to resolve it? The vagrant version is 1.7.2. Thank you. I would be very glad if some of you could help me with this problem:

Thanks!

 Apparently there are no subtitles, at least not as a button/right column combo on edX. Are they not ready yet or there is no plan to release subtitles?
I only use them sometimes but it's good to have them. For the article in HBR in lesson 2, I'm getting a 500 server error. Is this a restricted article or have we crashed the HBR servers :-)

https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/ar/1

 I already have Virtual Box installed on my computer. Its version number is 4.3.26.
But in the Courseware section it is clearly stated that version should be 4.3.28 or later.
Whenever I try to update it, it prompts that virtual box is up to date (4.3.26).
But website shows the latest version to be 4.3.28. How should I update it ?

If I uninstall it and download and install new version, will it automatically load my previously used images.

Should I use the version I already have ? Would it affect my Virtual Machine performance ?

Thanks. This course might be different, but edX courses usually work best with the Chrome browser, so I'm hoping someone associated with this course can tell me how to fix problems I'm having with Chrome and/or my Google account or point me to where I can find a solution.

I've used and liked Chrome in the past, but I've had problems with it ever since I installed version 43.0.2357.81 m -- or ran into malware around the same time. Chrome says "Your preferences can not be read". Resetting and saving my preferences has no effect. Removing and reinstalling Chrome seems to have no effect. Chrome also keeps asking me whether I want to enable the Application Launcher for Drive, and whichever answer I give seems to have no effect. Removing and reinstalling Drive seems to have no effect. An error message I encountered in the past from Chrome said that my Google account settings had been corrupted by another application. Can someone tell me how to reset my Google account settings, other than my previous failed attempts to do so?

I'm using Opera to post this note. My system is Windows Vista on an antique desktop. Did anyone have issues with libvirt? I've received the following error on Fedora 22 with vagrant:

The box you're attempting to add doesn't support the provider you requested. Please find an alternate box or use an alternate provider. Double-check your requested provider to verify you didn't simply misspell it. If you're adding a box from HashiCorp's Atlas, make sure the box is released. Name: sparkmooc/base Address: https://atlas.hashicorp.com/sparkmooc/base Requested provider: [:libvirt]



Commands issued are shown in the screen-shot, any assistance is greatly appreciated.

Shane

 I encounter a strange error.
After several attemts I was able to run vagrant up
and http://localhost:8001, but if I open VirtualBox GUI
I see that 'sparkvm' marked as Powered Off and after
I try to open settings for this VM, window with error
message "Failed to assign the machine to the session" shows up.

Some help would be very helpful.

Thanks in advance! Please assist:

Bringing machine 'sparkvm' up with 'virtualbox' provider...
==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install...
    sparkvm: Box Provider: virtualbox
    sparkvm: Box Version: >= 0
==> sparkvm: Loading metadata for box 'sparkmooc/base'
    sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base
==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox
    sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virt
ualbox.box
An error occurred while downloading the remote file. The error                                            
message, if any, is reproduced below. Please fix this error and try
again.
 
Failed writing body (1404 != 16384)
 Hi,

could you tell me what is the last day to pay for getting the certificate?

Where is it written?

Best regards

Michele Rizzi 

 Hi All,
sparkvm is not progressing while giving the command "vagrant up". It shows following:

Bringing machine 'sparkvm' up with 'virtualbox' provider...
==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box sparkvm: Progress: 0% (Rate: 7d/s, Estimated time remaining: 0:59:37)34:54)

My machine contains Windows 7: 32 bit.
Please help me to solve this problem. 

Thanks

 Where do i find first assignment....I am unable to find ir I am not not able to see any quiz after lecture 1 and 2. Where to find them? During vagrant up, i get the following
Error: Could not resolve host: (nil); Host not found

here is the detailed error message

Bringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0The box 'sparkmooc/base' could not be found orcould not be accessed in the remote catalog. If this is a privatebox on HashiCorp's Atlas, please verify you're logged in via`vagrant login`. Also, please double-check the name. The expandedURL and error message are shown below:
URL: ["https://atlas.hashicorp.com/sparkmooc/base"]Error: Could not resolve host: (nil); Host not found Hi,

I am using windows 7 and when i try to run the vagrant up command from the cmd, i am getting the following error

 Hi,
Has anyone succeeded in installing VirtualBox on Windows 10 Enterprise Insider Preview Build 10074?
If so, I'd appreciate a how-to tip or work-around.
Brgds, Hi all.

So, going through the videos now.  The video quality is not very good so far, especially for charts and the professor's little talking head insert.  I've tried toggling the "HD" option both ways but it seems to make no difference.  (running Windows 7)

Is this just my experience or are others having similar quality issue?

Thanks!

-Chris
  There is a major issue with this processor and kernel bugs.  It will produce the following error when attempting to run vagrant up:

Error: error:0307A071:bignum routines:BN_rand_range:too many iterations

I solved the issue by downloading a new Curl file here and replacing the Curl in the C:\HashiCorp\Vagrant\embedded\bin directory.

 The transcripts/subtitles for the videos "USA 2012 Presidential Election" and "Facebook Lexicon" all seem to be the transcripts for the Google Flu Trends video See bold text, any ideas? using OSX.

macbookpro-2:vagrant nm$ curl -O https://github.com/spark-mooc/mooc-setup/archive/master.zip
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   126    0   126    0     0    194      0 --:--:-- --:--:-- --:--:--   194
macbookpro-2:vagrant nm$ ls
master.zip
macbookpro-2:vagrant nm$ unzip master.zip 
Archive:  master.zip
  End-of-central-directory signature not found.  Either this file is not
  a zipfile, or it constitutes one disk of a multi-part archive.  In the
  latter case the central directory and zipfile comment will be found on
  the last disk(s) of this archive.
unzip:  cannot find zipfile directory in one of master.zip or
        master.zip.zip, and cannot find master.zip.ZIP, period. The Windows "run as administrator" trick doesn't work to actually install Vagrant on a thumb drive, it still insists on local hard drive

This is a problem for me as I use public computers that do not allow installation to local drives, I have installed Octave, r, python etc. successfully on thumb drives to complete coursework in other classes

I hope there is a solution to this, otherwise the most disadvantaged learners will be excluded from this course
 Screen_Shot_20150602_at_7.06.26_AM.png
Hi, everyone:
It seems like my network cannot work very well with s3 amazon sites.
Can I directly use the link in prompt do download package.box file myself?
Please help, thanks a lot

 I have tried several versions of virtual box (starting with 4.28) according to online threads, but I still hit the error below while trying to start the VM through Vagrant. Has anyone come across this?

==> sparkvm: Checking if box 'sparkmooc/base' is up to date...
==> sparkvm: Clearing any previously set forwarded ports...
==> sparkvm: Clearing any previously set network interfaces...
==> sparkvm: Preparing network interfaces based on configuration...
    sparkvm: Adapter 1: nat
==> sparkvm: Forwarding ports...
    sparkvm: 8001 => 8001 (adapter 1)
    sparkvm: 4040 => 4040 (adapter 1)
    sparkvm: 22 => 2222 (adapter 1)
==> sparkvm: Booting VM...
==> sparkvm: Waiting for machine to boot. This may take a few minutes...
The guest machine entered an invalid state while waiting for it
to boot. Valid states are 'starting, running'. The machine is in the
'aborted' state. Please verify everything is configured
properly and try again.

If the provider you're using has a GUI that comes with it,
it is often helpful to open that and watch the machine, since the
GUI often has more helpful error messages than Vagrant can retrieve.
For example, if you're using VirtualBox, run `vagrant up` while the
VirtualBox GUI is open.
 I am trying to download as a python file, but when I click Python(.py) to download as, I get a html file that looks  like this 'lab0_student.py.html'. How do I fix this? All,

I (among full time job,ect) am a TA for MongoDB. As such I have vagrant and Virtualbox installed and have seen and troubleshooted everything. I install course image using vagrant. Vagrant status shows everything fine and have SSH'd into the VM. However when I look at VirtualBox, the VM is not listed!! At all. I have 6-10 other images and they are all there. Does anyone have an idea? There are Two VBoxSVC processes running which is not correct. One of the processes (for this course VM had the flag embedded passed to the process. Was this intentional or am I just having a bad hair day?

thanks in advance,
Mike clovis hi all,

any idea how we can add any new package to VM, i wanted to use pep8_magic and it needs pep8


cheers I must be missing something .. I have looked around the edx site to find how to submit assignment and can't find it. 

Where is this fabled 'autograder' hiding at?

Thanks! I'm having trouble with vagrant, and I find no answers here or elsewhere (eg., stackoverflow). I bet that I would have no trouble installing whatever version of Spark and Python, etc. you are using in the virtual environment. Would you please provide us with a list of the software and versions thereof that get installed automatically, so that those of us who wish to can try to provide the needed software ourselves? (Caveat emptor, and all that.)

Thanks. I followed the instructions in the video regarding the download and installation of the vm in osx, everything went well until I tried to copy what I have downloaded to the custom directory. It gave me this thing:

ChenYijings-MacBook-Air:myvagrant ClaudiaChen$ cp ../Download/mooc-setup-master.zip
usage: cp [-R [-H | -L | -P]] [-fi | -n] [-apvX] source_file target_file
       cp [-R [-H | -L | -P]] [-fi | -n] [-apvX] source_file ... target_directory

I don't really understand what it means and every effort I have made to copy this file failed. I checked the downloads and it appeared to be somehow automatically unzipped and existed as a folder. The web browser I use is Safari, do I need to download another one like Google Chrome in order to complete this? We will release each week's lecture video moduless and lab exercise on Saturdays at 16:00 UTC. Each week's lab exercise will be due the following Friday at 00:00 UTC.

As scheduled today, the releases and deadlines do not allow students in Asia to use their Saturday at all. For those like me who have a family and work during the week, that means that I have to do everything on the Sunday.

Can you please move the deadline to the following Saturday 16:00 UTC, to offer a complete week to everyone, regardless of the continent they live in ? Am I right in assuming that last week of this course will run concurrently with next course in this xseries , CS190?
if so, I am wondering if this was intentional 
I happend to have Evernote open when viewing Lecture 1. I think a few of you might find these useful in order to read beyond the mandatory course material; in coffeebreaks or the ride in the underground.



http://www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/
 
https://hbr.org/2012/10/big-data-the-management-revolution&cm_sp=Article-_-Links-_-Top%20of%20Page%20Recirculation



http://www.economist.com/node/15579717


http://arxiv.org/abs/1401.4208

http://arxiv.org/pdf/1401.4208v1.pdf


http://www.symmetrymagazine.org/article/august-2012/particle-physics-tames-big-data

http://www.economist.com/node/16349358

http://gorbi.irb.hr/en/method/growth-of-sequence-databases/

http://en.wikipedia.org/wiki/FasTrak

http://traffic.berkeley.edu


#pin For those of you who want to set up the environment on a VPS (e.g. AWS), here are the steps. I was able to install this on my Koding.com VM (runs Ubuntu 14.04), which is hosted by AWS. I made the instructions easy by having you all start in your user home directory.

1) Open up the terminal
2) Go into your home directory, by typing in
cd
3) Enable multiverse and update repo
sudo apt-add-repository multiversesudo apt-get update
4) Install virtualbox
sudo apt-get install virtualbox
5) Download vagrant
wget https://dl.bintray.com/mitchellh/vagrant/vagrant_1.7.2_x86_64.deb
6) Install vagrant
sudo dpkg -i vagrant_1.7.2_x86_64.deb
7) Clone git repo
git clone https://github.com/spark-mooc/mooc-setup
8) Go into the repo folder
cd mooc-setup
9) Install vagrant image
vagrant up
 Getting the following message when tried to install the VM Image. The error message occurs even when I disable the firewall


 Hello,

I have VMWare already installed on my Mac. Just wondering if I really need to install Virtual Box? Since I never used Virtual Box so I am not sure if there's anything in this course that will need to be done via Virtual Box? 

Thanks, Hi,

this question is not related to this course, but I`ll be thankful if someone could help me.

I configured a Spark Cluster using 4 VM into my workstation. I can run pyspark into parallel mode using '--master spark://SERVER:7077'

I also installed Anaconda into these 4 VMs and configured IPython notebook into master node, using this recipe: 
http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/

But, when I tried to run multiple IPython Notebooks into cluster, the second notebook does not get any resource from cluster util I kill the kernel of the first notebook.
I also noticed that the same happens without using IPython Notebooks and running only pyspark --master spark://server:7077 from shell. The second pyspark console does not get any worker until I kill the first console.
If I run the consoles with --master local[*], I can run multiple consoles, but this way all the consoles will use only one machine.

Is there any way to get multiple consoles/notebooks running into cluster?

Thanks,
Vitor.
  Academic question: interested if anyone has a recipe to run/debug pyspark scripts in eclipse. Have spent just a bit of time investigating this and seems most advanced usage is to add the pyspark libraries to eclipse to enable editing with code-completion, then use spark-submit to run the script. (Script is submitted to a Java program so perhaps pydev is not really an option). Will someone who knows tell me how Spark compares to HDF5? So far I know nothing about Spark and only a little about HDF5. When my PERL code for analyzing Congressional campaign contributions began running into out-of-memory failures -- I was looking at 146 candidates' reports over a two-year period -- a Cornell professor neighbor recommended using HDF5. This led me to learn Python because Python has better interfaces to HDF5 than PERL. So how does Spark compare to HDF5?

.I might have to quit this course because I have an antique computer (2005) and OS (Vista), but my computer has plenty of disk space, so maybe I can get by. If it is 100% completed for the setup, can we show it as grey bar like we did for the quizzes or else show as green bar as completion. It always makes me nervous to see a red bar in the progress. :)


Thanks,
Michael Dear Sir,

I already have a virtual box image of Ubuntu where I have installed Apache Spark. Can I work with the instance which I already have instead of downloading the image given by the course?

Regards,
Sudhindra vagrant up - gives missing interpolation argument error. 
I had tried to re-installing VirtualBox and also Vagrant two times but still the same error everytime.
Any help is much appreciated. Thx


 I can see the version on the command prompt
 can we do progamming assignments in java instead of python ? All, 
May I know the root password of sparkvm box?  Instead of using VirtualBox I used VMware Fusion Pro 7. 

I've purchased commercial license of vagrant plugins for vmware Fusion already. I can start the box with no issues. 

However, I'd like to install vmware tool to get better network performance (according the vagrant doc).  In order to do this I need root password.

Please help. TIA!
 I am getting "Access is denied" message. Any idea how to get around this? Thanks. As said on the tittle, the Docker option is much more easier and non intrusive that the vagrant one.
I'm already using for other deployments and tests: https://github.com/sequenceiq/docker-spark I ran vagrant up command which ran without error and also have http://localhost:8001/tree running successfully. However, I am not seeing VM image saved in VirtualBox VM folder or VirtualBox manager. 

 Dear Professor and TAs,

I read from the course info as follows,

"The course is graded on the following scale:
75-100: A grade55-74: B grade45-54: C gradeBelow 45: non-passing grade

The course software setup assignment is due June 6, 2015 at 00:00 UTC. Each week's lab exercise will be due the following Friday at 00:00 UTC. We encourage you to start software setup and each exercise as early as possible. There is an automatic 3-day grace period for submission deadlines. After the grace period, there is a 20% penalty for late submissions."

My questions as subject are as follows:
does quiz count into the grading points?is 00:00 UTC Friday the beginning time or end time of Friday?does the 3 day grace period apply to each individual assignment or is it an accumulative time for the whole assignments?
Thanks!
 Is this course specific to python? I am a Java developer and wish to learn it. Anyway i can learn it? I followed the steps and I can access the installation through the web UI. Is it possible to log into the VM to explore the content? What's the user/password?

Thanks! The download interrupted at 74% and gave this error:
SSL read: error: 1408F119:SSL routines: SSL3_GET_RECORD: decryption failed or bad record mac, errno 0

Please help. I'm on windows 8 :(, so when I run vagrant up,
I got 'Access is denied'. Help plz!! Thx in advance! :) After submitting lab0 to the autograder I get a red cross and the following comment:

Compare with hash (2a)
----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 39, in main
    "collapsed": false
NameError: global name 'false' is not defined

All tests passed
Compare lists (2b)
------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 1 cases passed (50.0%) --Can I just move on or do I need to go back and correct something ? while i am trying to download https://github.com/spark-mooc/mooc-setup/archive/master.zip from the above link the folder is getting downloaded not the zip file its causing problems while i am trying to config using the terminal in mac os while i am trying to download from https://github.com/spark-mooc/mooc-setup/archive/master.zip the entire unzipped folder is getting downloaded not the zip file.This is causing me problems in setup process "Download and Install Virtual machine" Hi there. The videos mention that there are quizzes for lecture 1 and lecture 2 but I don't see any. I've watched each of the video segments in each lecture.

Am I missing something?

Best,
Nosh The 'Progress' page indicates there are five questions scattered throughout Lecture 2 - however, I can only find the last four. This could be a possible bug on the part of the 'Progress' page.These are the ones I can find:
What are some impediments to collaboration?Why is Extract-Transform-Load critical?Why is curating/filtering data a key model component?Why is overcoming assumptions hard?

Is there in fact a fifth question, and, if so, where can I find it? Thanks. I have used coursera for long time so when I try to download the videos from edx i felt confused and it is not easy way to download. After downloading the names of files are cryptic not easy ordered. What is the purpose of the Save button?
 got the following error on vagrant up
Failed to mount folders in Linux guest. This is usually becausethe "vboxsf" file system is not available. Please verify thatthe guest additions are properly installed in the guest andcan work properly. The command attempted was:
mount -t vboxsf -o uid=`id -u vagrant`,gid=`getent group vagrant | cut -d: -f3` vagrant /vagrantmount -t vboxsf -o uid=`id -u vagrant`,gid=`id -g vagrant` vagrant /vagrant
The error output from the last command was:
stdin: is not a tty/sbin/mount.vboxsf: mounting failed with the error: No such device

followed official site help as well as various ubuntu forum / stackoverflow but nothing helped.
I made sure vagrant, vbox and everything is latest. 

also based on conversation in vagrant (https://github.com/mitchellh/vagrant/issues/1657) I've ran the following and didn't help...

$ sudo apt-get update -y
$ sudo apt-get install build-essential linux-headers-`uname -r` dkms -yReading package lists... DoneBuilding dependency treeReading state information... Donebuild-essential is already the newest version.dkms is already the newest version.
$ sudo /etc/init.d/vboxadd setupRemoving existing VirtualBox DKMS kernel modules ...done.Removing existing VirtualBox non-DKMS kernel modules ...done.Building the VirtualBox Guest Additions kernel modulesThe headers for the current running kernel were not found. If the followingmodule compilation fails then this could be the reason.
Building the main Guest Additions module ...fail!(Look at /var/log/vboxadd-install.log to find out what went wrong)Doing non-kernel setup of the Guest Additions ...done.vagrant@sparkvm:~$ sudo /etc/init.d/vboxadd setupRemoving existing VirtualBox DKMS kernel modules ...done.Removing existing VirtualBox non-DKMS kernel modules ...done.Building the VirtualBox Guest Additions kernel modulesThe headers for the current running kernel were not found. If the followingmodule compilation fails then this could be the reason.
Building the main Guest Additions module ...fail!(Look at /var/log/vboxadd-install.log to find out what went wrong)Doing non-kernel setup of the Guest Additions ...done.

vagrant reload after that still endup with same message :(
any advice would be appreciated I have experience of spark programming using java api but not python. Also I don't have much experience with python. I have done a simple project with spark. Page rank implementation and TFIDF implementation.
Will this class be hard if I don't know python?   I see from the Virtualbox manager that the virtual size of the storage is 40gb.  Will we actually need all of that room eventually? I'm on a laptop and don't have that much storage space on my hard drive...

Thanks! any recommendations on books to go along with the course, or simply as supplements - both for this and the next course in the series
o'reilly has two books in particular around spark/datascience, and the authors include committers and databricks folks

I find that, I really enjoy and get the most out of MOOCs when there is an accompanying text to either follow along or explore at your own leisure, and use
for long term reference.

any suggestion appreciated!
 I am getting the following error while running "vagrant up":

C:/software/Vagarant/embedded/gems/gems/childprocess-0.5.5/lib/childprocess/windows/handle.rb:12:in `open': Access is denied. (5) (ChildProcess::Error)

Could you please guide which file I should get admin access ?


Thanks
 I have set up the apache spark 1.3.1 from official apache site on my Laptop having Ubuntu 14.04. And its working fine with both pySpark and spark-shell.
So do i need to download and use this VM any more?? Because Spark binaries have everything in it,to carry out any work. 
Please clarify me? Hello, can I get to know the exact versions of all software that will be required during this course?I already had VirtualBox, and I succeeded in installing vagrant, but I have an internet connection which only allows up to 512 kbps, due to which I cannot download the VM file in satisfactorily sufficient time by issuing the command "vagrant up". I am currently running Windows 8.1 and would really like to take this course, which is why I want to know whether there is another way of doing this without a Virtual machine (I can bear installing different software separately) but I have to do it on Windows only, not Ubuntu.
Can I have a list of the main software I need? I will be really thankful.
(Like I just found out how to install Jupyter notebooks with an existing Portable Python installation here: http://www.walkingrandomly.com/?p=5734, hence if I have the whole list I will be able to install the rest without difficulty too.)
Thanks again. in test 
Part 4: Check MathJax Formulas
 I expect to see something gets typed on the screen,
 the instruction is saying: " You should see a formula on the line below this one:"
But I see nothing new.

Also the accompanying video  shows nothing new on its screen.

is this test running correctly? If i joined as free , can i get the certificate for completion of course ? I followed up the instruction as it is an checked with some of the other post as well. 

I tried uninstalling and installing the dependencies but of no worth. I am using Windows 8 64 bit Machine.

While this should have been easy to do, I do not seems to figure out the exact reason. 

Can somebody help me out here. 

Bringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0The box 'sparkmooc/base' could not be found orcould not be accessed in the remote catalog. If this is a privatebox on HashiCorp's Atlas, please verify you're logged in via`vagrant login`. Also, please double-check the name. The expandedURL and error message are shown below: I installed the most recent version (VirtualBox-4.3.28-100309-Win) on my Windows 7, 64 bit computer (I installed it in Administrator mode). However, when I start it, the program does show itself despite the fact that I see it running at the Windows Task Manager. I also tried to run in Compatibility mode, but none worked.

Any suggestions? Thanks! A few pointers to Fedora users...
also need to install a libvirt binary, so...

sudo dnf install -y vagrant-libvirt vagrant

to get the spark image, do the following

vagrant init sparkmooc/base; vagrant up --provider virtualbox Lecture 2 talks about "hacking skills" and "Domain expertise". Did I miss something in one of the lectures? I didn't see these terms being defined anywhere.  Thanks. Hi,

encounter this error when typing "vagrant up" in the command prompt. Can anyone assist?C:\Users\elchong>cd myvagrant
C:\Users\elchong\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0The box 'sparkmooc/base' could not be found orcould not be accessed in the remote catalog. If this is a privatebox on HashiCorp's Atlas, please verify you're logged in via`vagrant login`. Also, please double-check the name. The expandedURL and error message are shown below:
URL: ["https://atlas.hashicorp.com/sparkmooc/base"]Error: Failed connect to atlas.hashicorp.com:443; No error
C:\Users\elchong\myvagrant> I tried to run "vagrant up" while my Oracle VirtualBox was running(both are up to date), and I got the following - 


The guest machine entered an invalid state while waiting for it
to boot. Valid states are 'starting, running'. The machine is in the
'poweroff' state. Please verify everything is configured
properly and try again.

If the provider you're using has a GUI that comes with it,
it is often helpful to open that and watch the machine, since the
GUI often has more helpful error messages than Vagrant can retrieve.
For example, if you're using VirtualBox, run `vagrant up` while the
VirtualBox GUI is open.

And then I tried to run the sparkvm icon that had successfully installed on the VirtualMachine app. However, as I clicked on it, it gave me an alert box, saying 

Kernel driver not installed (rc=-1908)Make sure the kernel module has been loaded successfully.

I googled the above and it told me to re-install VirtualBox. But I had to reinstall VirtualBox twice before it actually worked for me, and could actually come up.

Does anyone have any advice?

Thank you in advance! I have some problems with Vagrant, but I have IPython installed and I can install Spark.

Have we need vagrant or we can do the job with IPython and Spark ? C:\Users\hp\Myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and intall... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0The box 'sparkmooc/base' could not be found orcould not be accessed in the remote catalog. If this is a privatebox on HashiCorp's Atlas, please verify you're logged in via`vagrant login`. Also, please double-check the name. The expandedURL and error message are shown below:
URL: ["https://atlas.hashicorp.com/sparkmooc/base"]Error: SSL certificate problem: self signed certificate in certificate chainMore details here: http://curl.haxx.se/docs/sslcerts.html
curl performs SSL certificate verification by default, using a "bundle" of Certificate Authority (CA) public keys (CA certs). If the default bundle file isn't adequate, you can specify an alternate file using the --cacert option.If this HTTPS server uses a certificate signed by a CA represented in the bundle, the certificate verification probably failed due to a problem with the certificate (it might be expired, or the name might not match the domain name in the URL).If you'd like to turn off curl's verification of the certificate, use the -k (or --insecure) option.
C:\Users\hp\Myvagrant>
 I am working on Linux Mint 17 (Ubuntu 14.04) and I have successfully installed the latest versions of VirtualBox and Vagrant.  Vagrant seems to have downloaded the VM OK as well, but it just hangs during the start-up process at this step:

==> sparkvm: Waiting for machine to boot. This may take a few minutes...    sparkvm: SSH address: 127.0.0.1:2222    sparkvm: SSH username: vagrant    sparkvm: SSH auth method: private key    sparkvm: Warning: Connection timeout. Retrying...    sparkvm: Warning: Connection timeout. Retrying...

Eventually it times out.  I have run "vagrant halt" and "vagrant up" to see if it can re-start cleanly, but I get the same problem.

Is there any way to fix this SSH problem?

Alternatively, is it possible to use the VirtualBox VM without Vagrant?  I've used VirtualBox VMs on other courses in the past, so right now Vagrant just seems to be introducing unnecessary problems.

I have several colleagues who are keen to take this course on the same platform (Linux Mint), so we would all appreciate any help here.  Otherwise we will have to drop out of the course before it's even started.

Thanks for any help you can provide.

Chris Hi,

I'm trying to run through the lab0_student.ipynb notebook in Jupyter but I'm not seeing any output when executing the cells. I also don't have the option to download the notebook as a Python file (there are other options e.g. PDF, txt, HTML). There is an icon on the top right of the screen indicating 'No connection to Kernel' next to where it says 'Python 2'.  I try the options to restart and reconnect to the kernel but it doesn't help. I've also tried destroying and re-upping the VM and in different browsers but to no avail. Do I have some kind of firewall/networking issue with the VM? I'm on Windows 8.1.

Thanks,
Adam I appear to be having problems with my VM. The first test may have run correctly (there's nothing to say), but the following two numbers are displayed unaccompanied by text.
4999950000
714264285
The second test failed spectacularly, throwing out around 60 lines of text, starting with: 
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-2-5453bfa2d105> in <module>()
      6 
      7 rawData = sc.textFile(fileName)
----> 8 shakespeareCount = rawData.count()
      9 
     10 print shakespeareCount

Where do I go from here? Hello :)
I'm working since 2010 in machine learning field. What I do it is building machine learning models to identify churn clients in the future (for example). 
But until today I never work with massive datasets /big data, but looking ahead I think that is fundamental understand about this topic.

Because I never worked with Spark / Hadoop, this course it is for me or this course it is for advanced professional? 

Thks
Regards
Ricardo
 Hi All,

I've installed suggested versions of vagrant and virtualbox successfully. I've been running virtualbox and command prompt using option "run as administrator". However, while running command vagrant up, it shows the following error:

C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/config.rb:83:in `block in missing_interpolation_argument_handler': missing interpolation argument :vboxmanage in "Vagrant detected that VirtualBox appears installed on your system,\nbut calls to detect the version are returning empty. This is often\nindicative of installation issues with VirtualBox. Please verify\nthat VirtualBox is properly installed. As a final verification,\nplease run the following command manually and verify a version is\noutputted:\n\n%{vboxmanage} --version" ({:_key=>:virtualbox_version_empty, :_namespace=>"vagrant.errors"} given) (I18n::MissingInterpolationArgument)        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/interpolate/ruby.rb:29:in `call'        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/interpolate/ruby.rb:29:in `block in interpolate_hash'        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/interpolate/ruby.rb:21:in `gsub'        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/interpolate/ruby.rb:21:in `interpolate_hash'        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/interpolate/ruby.rb:17:in `interpolate'        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/backend/base.rb:153:in `interpolate'        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/backend/base.rb:41:in `translate'        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n.rb:157:in `block in translate'        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n.rb:153:in `catch'        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n.rb:153:in `translate'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/errors.rb:103:in `translate_error'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/errors.rb:72:in `initialize'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/driver/meta.rb:155:in `exception'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/driver/meta.rb:155:in `raise'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/driver/meta.rb:155:in `block in read_version'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/retryable.rb:17:in `retryable'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/driver/meta.rb:140:in `read_version'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/driver/meta.rb:38:in `initialize'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/provider.rb:11:in `new'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/provider.rb:11:in `usable?'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/environment.rb:378:in `block in default_provider'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/environment.rb:377:in `each'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/environment.rb:377:in `default_provider'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/plugin/v2/command.rb:165:in `block in with_target_vms'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/plugin/v2/command.rb:201:in `call'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/plugin/v2/command.rb:201:in `block in with_target_vms'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/plugin/v2/command.rb:200:in `map'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/plugin/v2/command.rb:200:in `with_target_vms'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/commands/ssh/command.rb:41:in `execute'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/cli.rb:42:in `execute'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/environment.rb:301:in `cli'        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/bin/vagrant:174:in `<main>' 
Looking for any suggestions to resolve this issue.

virtualbox.PNG
vagrantupscreenerror.PNG

Regards,
Neeraj It was an easy download. Though the download time was approx 245 minutes on a 50 MBps connection.
If you follow the instructions provided by the instructor I think you should not have any problems apart from the download time. 
A few points that might help others I guess are
1. If you are a beginner to Spark and Big data analysis then just stick to the VM setup that this course is providing. Once you have gained a fine understanding then dive into installing it on your personal computer
2. A 64 bit OS is better than 32-bit 
3. A prior understanding of statistics, python and R should help you in gaining the maximum out of this course.
4. I did not reboot my computer after Vagrant installed and yet I was successful in completing the lab assignment. So in case you did not reboot your computer, dont worry about it
5. And finally when attempting the lab assignment read the instructions carefully because it will save you a lot of hassle otherwise. It is all clearly stated in the assignment. Just follow the guidelines and you will be good to go.

Hope this helps.
Cheers,
Ashish I am using windows 7 32 bit os. When I try to run vagrant up in the cmd prompt it gives the following error:
C:\Users\Pc\Macros\myvagrant>vagrant up'vagrant' is not recognized as an internal or external command,operable program or batch file. Hi All,

Installing VirtulBox (with a downloaded .deb file from the site) on Ubuntu 14.04 will mess up your computer. It actually deleted xterm, unity and software center from my computer.
There is a solution here:
http://ubuntuforums.org/showthread.php?t=2227131

The repository version seems to work fine.

Best regards,
Aleksandar How's this course for someone who has no background in data analysis nor in programming? good for someone wanting to begin? I have successfully installed vagrant.Here, is the trace of my output.

vagrant upBringing machine 'default' up with 'virtualbox' provider...==> default: Clearing any previously set forwarded ports...==> default: Clearing any previously set network interfaces...==> default: Preparing network interfaces based on configuration...    default: Adapter 1: nat==> default: Forwarding ports...    default: 22 => 2222 (adapter 1)==> default: Booting VM...==> default: Waiting for machine to boot. This may take a few minutes...    default: SSH address: 127.0.0.1:2222    default: SSH username: vagrant    default: SSH auth method: private key    default: Warning: Connection timeout. Retrying...==> default: Machine booted and ready!==> default: Checking for guest additions in VM...==> default: Mounting shared folders...    default: /vagrant => /Users/satsaxen/Documents/myvagrant==> default: Machine already provisioned. Run `vagrant provision` or use the `--provision`==> default: to force provisioning. Provisioners marked to run always will still run.

But, when i am trying to visit 'http://127.0.0.1:2222/' or 'http://localhost:2222' . It is giving me the following error on the web page

The service is not running on port 8001. So, when i am trying to access http://localhost.com:8001. Its saying its unable to connect. 

SSH-2.0-OpenSSH_6.6.1p1 Ubuntu-2ubuntu2
Protocol mismatch. I am currently installing the software on an old netbook but plan to have the configuration over a repaired desktop, over the weekend, after the deadline. Do I need to submit the first assignment again after installation on the new machine? Hello:
I wonder how to download slides of class?
thanks Hello:
I wonder how much experience in python is needed for this course? The article opens by implying "that Amazon has put so many brick-and-mortar bookstores out of business" due to its ability to use Big Data. I think this is quite a far fetched explanation and there is a far simpler theory to explain this phenomena.

This actually is a typical case of "correlation does not imply causation"...:
There is no good reason why Amazon might not be putting bookstores out of business due to far simpler facts: they pay nearly no taxes at all on their revenue, they pay their employees far less money, and they make those employees work much harder. Finally, Amazon is probably the only book retailer that can live with making a huge deficit year after year and still get investors to throw money at it like mad.

Therefore, and following Occam's razor, it is much more likely that Amazon is winning out because it can use those facts to sell books at far lower prices than any retail can than to their ability to access Big Data. Internet connection is very slow in my region. So I downloaded the box package via an IDM. I have Vagrantfile and I have the box package downloaded (from IDM) . How to proceed further? I knew ruby ! But nor Python. Can  I continue in ruby or Python Is Compulsory ? win7 64bit
VirtualBox-4.3.28-100309-Win 
vagrant_1.7.2
After typing "vagrant up" in cmd, it comes to this, and the virtual machine "sparkvm" is always "Powered Off", I don't know what's going on. Could someone please give me some help?
 

  When I run 'vagrant up' in the command prompt, it shows an error,
c:\users\marco\myvagrant>vagrant up'vagrant' is not recognized as an internal or external command,operable program or batch file.
 
Tried running as an administrator but didn't work.
Help Hi

My notebook.py that I upload on the assignment page passes all the test cases, but I cannot "submit" - I only have Check and Save buttons? Hi all, welcome to join our Chinese study group, the qq group number is 424077, you can use either English or Chinese to discuss your theoretical or practical issues, when you apply please fill up your affiliation and occupation/publication(preferred) in the request message, e.g., Massachusetts Institute of Technology+Mathematical Logic & Foundations Ph.D. 1st year, thanks.

------
I just created a WeChat Group for this class. Hope we can learn while making good friends. You may scan the following QR code to join. 
-- added by Yifei Huang (黄一飞), PhD candidate in Economics at Caltech
 

my  laptop can't detect the ethernet device, so it does't work in  the  NAT module .  Hi all,
I am using libvirt instead of oracle vm because I am using libvirt for other purpose as well. I have to do `vagrant mutate` to get it working. No problem with that.  Vagrant up was successful except my machine name is slightly different but hostname is still the same.

~/MOOC/edx/apache_spark/mooc-setup-master$ vagrant ssh sparkvmWelcome to Ubuntu 14.04.2 LTS (GNU/Linux 3.13.0-53-generic i686) * Documentation:  https://help.ubuntu.com/  System information as of Tue Jun  2 12:59:05 UTC 2015  System load:  0.64              Processes:           124  Usage of /:   4.9% of 39.34GB   Users logged in:     0  Memory usage: 10%               IP address for eth0: 192.168.121.129  Swap usage:   0%  Graph this data and manage this system at:    https://landscape.canonical.com/  Get cloud support with Ubuntu Advantage Cloud Guest:    http://www.ubuntu.com/business/services/cloudLast login: Tue Jun  2 12:48:14 2015 from 192.168.121.1vagrant@sparkvm:~$ 

However, on my "Test Spark functionality" I have 2 fail initial test case. Not sure, if other having the same issue.
 (1a) Parallelize, filter, and reduce 
NameError                                 Traceback (most recent call last)
<ipython-input-2-f7aa330f6984> in <module>()
      1 # Check that Spark is working
----> 2 largeRange = sc.parallelize(xrange(100000))
      3 reduceTest = largeRange.reduce(lambda a, b: a + b)
      4 filterReduceTest = largeRange.filter(lambda x: x % 7 == 0).sum()
      5 

NameError: name 'sc' is not defined
Not sure if others are having same problem. 

(1b) Loading a text file 
NameError                                 Traceback (most recent call last)
<ipython-input-3-5453bfa2d105> in <module>()
      5 fileName = os.path.join(baseDir, inputPath)
      6 
----> 7 rawData = sc.textFile(fileName)
      8 shakespeareCount = rawData.count()
      9 

NameError: name 'sc' is not defined

 

so  http://localhost:8001 and some else can't open   Hello,

I am glad to be enrolled to this course. My queries are:

If i have previous set-up of Spark on my linux machine. Is this sufficient to carry out this course. Or should i follow the usual procedures that is described in the chapter ?If any spark set-up is ok. What else I should take care  of to be on the same level of the other set-up recommended by your course ?

I am running Spark on the Linux Mint 

Thx
Roshan
 


I have attached the screen shot , i am using  MAC and os version is  10.9.4 
OS X Mavericks
 Has anyone successfully used Amazon to host our course VM and all its required software?  Has anyone yet constructed a disk image for our course, that works on Amazon?

My main Linux workstation just is not able to hold all the required software and required OS version.  I am not about to buy a whole new machine today and set that all up.  Amazon.com seems like the next logical thing to use.  

I don't want to go first, unless I have to of course. Do Kubuntu and any other customized Debian Linux environments meet the requirements for set up?
 Is it suppose to take time for the command vagrant up to run. It is still trying to download sparkmooc/base and it is taking hours Hi all,

After installing VirtualBox, Vagrant, and the reboot, I got the blue screen on my Windows 7.
I spent hours troubleshooting, recovery, etc,, and now I kind of isolated the problem. The only time it works, is when I recover windows and install VirtualBox without the USB and networking drivers. 

Is anyone having the same problem? I am assuming I will need the networking feature of VirtualBox in this course?

Thanks
S Not sure how many other people have been having problems with Vagrant, but it didn't work for me until I added some code to the Vagrantfile (as suggested by a fellow student:  see here).  I still don't know why it didn't work before, but I don't really have time to mess around with Vagrant configuration right now.

However, I've used VirtualBox VMs before on other courses, and it was pretty straightforward, so I wonder if we actually need Vagrant here at all.  I know Vagrant is one of the cool new devops tools, but it doesn't seem to be adding any benefits here, and it's certainly causing problems for some of us at least.
Why not get rid of Vagrant and just use a plain VM in VirtualBox? When I submit notebook it just says all tests passed how do we check grades whether I have been graded or not  Hello! When I run vagrant up, after all the downloads, I get this message

C:\Users\erlan.m\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Clearing any previously set forwarded ports...==> sparkvm: Clearing any previously set network interfaces...==> sparkvm: Preparing network interfaces based on configuration... sparkvm: Adapter 1: nat==> sparkvm: Forwarding ports... sparkvm: 8001 => 8001 (adapter 1) sparkvm: 4040 => 4040 (adapter 1) sparkvm: 22 => 2222 (adapter 1)==> sparkvm: Booting VM...==> sparkvm: Waiting for machine to boot. This may take a few minutes...The guest machine entered an invalid state while waiting for itto boot. Valid states are 'starting, running'. The machine is in the'poweroff' state. Please verify everything is configuredproperly and try again.
If the provider you're using has a GUI that comes with it,it is often helpful to open that and watch the machine, since theGUI often has more helpful error messages than Vagrant can retrieve.For example, if you're using VirtualBox, run `vagrant up` while theVirtualBox GUI is open.

I have OracleVM Virtual Box Manager running and get an error code E_FAIL (0x80004005) when I try to start sparkvm from GUI.

Thank you

 hi all,

I can't up vagrant on my windows 8.

Can we help me ? All,
First of all. I have no issues using VirtualBox+Vagrant. I got them. 

However, since I did have VMWare Fusion Pro and I did purchased vagrant plugin license, I'd like to give a try using VMWare Fusion Pro + Vagrant.  However, it's very hard so far. I still didn't get iPython notebook up and running. I did created VMWare box and installed ipython though. Here are so far what I've done: 
1. Change Vagrantfile :
cat Vagrantfile# -*- mode: ruby -*-# vi: set ft=ruby :
ipythonPort = 8888 # Ipython port to forward (also set in IPython notebook config)
Vagrant.configure(2) do |config| config.ssh.insert_key = true config.vm.define "sparkvm" do |master| #master.vm.box = "sparkmooc/base" master.vm.box = "hashicorp/precise64" master.vm.network :forwarded_port, host: ipythonPort, guest: ipythonPort, auto_correct: true # IPython port (set in notebook config) master.vm.network :forwarded_port, host: 4040, guest: 4040, auto_correct: true # Spark UI (Driver) master.vm.hostname = "sparkvm2" master.vm.usable_port_range = 4040..4090
#master.vm.provider :virtualbox do |v| # v.name = master.vm.hostname.to_s #end master.vm.provider :vmware_fusion do |v| v.name = master.vm.hostname.to_s end endend
2. Install Anaconda

3. Start up python-notebook using ipython
Here's the help doc: http://ipython.org/install.html

However, I could not open web page by http://localhost:8888 (8888 was the default port number when starting notebook web application)


If someone successfully setup box using VMWare, please share the steps in detail. TIA!




 I downloaded the VM but got:

 ./box-disk1.vmdk: Write failed

and friends.

That, because vagrant puts images in ~/.vagrant.d

This caused a fill on that partition.

I symlinked .~/vagrant.d to another disk and it went fine.

After I also moved ~/VirtualBox\ VMs VirtualBox\ VM where there were a couple of sparkvm accumulating (killing the old 0.7.0 versions in the process).

Not to troll, but docker may have been easier and less resource intensive. Lectures video not works for me - getting error "No playable video sources found." i have this error while having the vagrant up VM download..what should i do that it doesn't happen again? retry?
 Neither of the two links works. What to do? Everything went perfectly the first attempt with the installation of VirtualBox and the accompanying VM. Not! It took me a day of trying various combinations and permutations to get it going, but that is to be expected for a relatively new (set of) technologies labeled Spark. For whatever reason, I finally had "luck" with installing default directories for all course software on a USB-connected SSD. As the installation notes imply, many issues (all in my case) are tied to permissions of some sort. As a request to the powers-that-be, please add some detail to the description of the course install package (what each piece is doing, how Vagrant helps, how the setup simulates real-life Spark usage). Thanks! Wayne Every thing is working fine but uploading the file taking so much of the time Hi, I encountered this error message

Error Message: Error writing to file: C:HashCorp\Vagrant\embedded\bin\file.exe. Verify that you have acess to that

while I was trying to install Vagrant. Anyone know how to solve this? Thanks C:\Users\marco\myvagrant>dirVolume in drive C is OSDiskVolume Serial Number is 64F0-288F
Directory of C:\Users\marco\myvagrant
06/02/2015 08:44 AM <DIR> .06/02/2015 08:44 AM <DIR> ..06/02/2015 08:58 AM <DIR> .vagrant05/29/2015 09:29 PM <DIR> mooc-setup-master06/02/2015 08:35 AM 3,695 mooc-setup-master.zip05/29/2015 09:29 PM 722 Vagrantfile2 File(s) 4,417 bytes4 Dir(s) 23,451,828,224 bytes free
 
C:\Users\marco\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install...sparkvm: Box Provider: virtualboxsparkvm: Box Version: >= 0The box 'sparkmooc/base' could not be found orcould not be accessed in the remote catalog. If this is a privatebox on HashiCorp's Atlas, please verify you're logged in via`vagrant login`. Also, please double-check the name. The expandedURL and error message are shown below:
URL: ["https://atlas.hashicorp.com/sparkmooc/base"]Error: SSL certificate problem: self signed certificate in certificate chainMore details here: http://curl.haxx.se/docs/sslcerts.html
curl performs SSL certificate verification by default, using a "bundle"of Certificate Authority (CA) public keys (CA certs). If the defaultbundle file isn't adequate, you can specify an alternate fileusing the --cacert option.If this HTTPS server uses a certificate signed by a CA represented inthe bundle, the certificate verification probably failed due to aproblem with the certificate (it might be expired, or the name mightnot match the domain name in the URL).If you'd like to turn off curl's verification of the certificate, usethe -k (or --insecure) option.
 
  Hi,

I retry do download, erase the directories few times and I have this error  ,,

What I should do !!!!

Thanks, Yvon



Please, answer the question how to fix this issue, I can't go further !!! Is it possible to use Jupytor to use spark Scala in a notebook? Is that functionality already included in the VM? If not, can it be installed? How?

No matter if the course is grading in Python, just would be nice to be able to experiment around with scala as well.

It seems possible to run spark Scala from the terminal:

1. Browse to:  http://localhost:8001
2. Choose "New" "Terminal on the right upper browser window corner
3. within the terminal, startup the spark scala REPL with:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/bin/spark-shell 

4. within the spark shell, execute for example:
 sc.textFile("/home/vagrant/data/cs100/lab1/shakespeare.txt").count
 When I issue command "vagrant up" after some lines I get the following: The guest machine entered an invalid state while waiting for it to boot.  The valid states are starting, running. The machine is in the "poweroff" state. What does it mean? What to do? 
-------------------
Hi,  Please help to resolve the vagrant issue.

I am using Ubuntu 14 

@homeUbi:~/myvagrant$ vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...There are errors in the configuration of this machine. Please fixthe following errors and try again:
SSH:* The following settings shouldn't exist: insert_key
vm:* The box 'sparkmooc/base' could not be found. 

Anyone knows how to solve this. Thanks a lot

 I am on windows7 x64 with virtualbox 4.3.28 , vagrant 1.7.2 (all latest versions)

I successfully installed the virtual machine (see install log bellow) and can login via ssh ("vagrant ssh" with cygwin). A "telnet localhost 8001" does not return any connection error. The "sparkvm" virtual machine state is correctly updated in the virtual box GUI as I start or stop it.

However I am unable to connect to the notebook server at http://localhost:8001.

To fix the problem I have tried to:
- add port 8001 over TCP as exception in my firewall => no success
- change the ipythonPort param. in the Vagrantfile file in case port 8001 was already taken and restart the VM => no success
- launched "vagrant up" as admin => no success

The install log warns about a "guest addition" version issue but I doubt this is the root of the problem.

Thanks for any suggestion. I tried different tips I found in piazza.  Neither of them worked out.  I uninstalled and installed VirtualBox and Vangrant several times. I still get the sequence of problems.  Please indicate if there are another configuration steps I am missing.

1. Reference Memory



2. If I do not click Ok, then Warning: Connection timeout. Retrying ...  continues forever.

3. After clicking Ok:



4. And finally on VM VirtualBox:

 For those of you who want to set up on AWS, I have a public AMI created.

1) From the AWS Console Dashboard, click on 'EC2'
2) Click 'Launch Instance' button
3) In the Community AMIs tab, search for 'spark_mooc'
4) Select image spark_mooc - ami-6bba5600
5) Choose t2.small or t2.medium instance type
6) Click 'Review and Launch' button
7) Scroll down to Security Groups and click 'Edit security groups'   *** very insecure *** 
8) Click 'Add Rule'; for Port Range, enter 8001; for Source, select Anywhere; Click 'Review and Launch'
9) Click 'Launch' and it'll ask you for a connection key. If you have an existing key, you can use it, or create a new key. Make sure you download the key after it is created!
10) After instance is launched, terminal/SSH into your VM using your auth key. Log in as 'ubuntu' (AWS will tell you to connect as 'root', but you want to connect as 'ubuntu'). You can find the public IP of your instance under the 'Public IP' label of your instance. I used a mock 123.123.1.1 in the example below. Replace yourkey.pem with the key that you created.
ssh -i yourkey.pem ubuntu@123.123.1.1
11) Once connected, type
cd sparkmooc
12) Type
vagrant up

 

Anyone knows about this? Thanks Have anyone worked with something like the nvidia devbox or some other kind of GPU accelerated hardware to run BigData/Machine Learning or Deep Learning Jobs?
https://developer.nvidia.com/devbox it get this error repeatedly in the middle of download tried 4 times always get this error at various stages of download can any one help 

error: 00000000:lib<0>:func<0>:reason<0>,errno 10054

 Vagrant successfully downloaded the box, Oracle VB Manager shows 'Sparkvm running' but the Vagrant output on terminal says
"Warning. Connection timeout. Retrying.." about 10 times and then
"Warning: Remote connection disconnect. Retrying.." 10 times.

Finally "Timed out while waiting for the machine to boot. If your box appears to be booting properly, you may want to increase config.vm.boot_timeout" Hello guys,
I am using windows 7 32bit PC  Please help me on this While setting up the VM I get the error 


[sparkvm] Local stat file not found: F:/ovm_stuff/vagrant_home/sparkmooc/base.stat

syck has been removed, psych is used instead

F:/ovm_stuff/vagrant_home/gems/gems/vagrant-box-updater-0.0.3/lib/vagrant-box-updater/util/common.rb:14:in `initialize': No such file or directory - F:/ovm_stuff/vagrant_home/sparkmooc/base.stat (Errno::ENOENT)


I have Windows 7 64 bit OS. The complete trace is pasted here at http://pastebin.com/f3r7MTA3

Could it be because the installation is attempted across different disk drives? Any resolutions Hi,

I have completed week 1 with quizzes. do i need for next week or i can start week 2 now.

Regards,
Ashish Jain
https://in.linkedin.com/pub/ashish-jain/5/412/21 I already have a dual boot Win7/Ubuntu on my main drive, I do have another drive a terabite drive that I have yet to install OSs and do a partition. 

Can I just use my dual boot Win7/Ubuntu without virtual box? C:\Users\Pahadi\myvagrant>vagrant up 
Bringing machine 'sparkvm' up with 'virtualbox' provider... ==> 
sparkvm: Box 'sparkmooc/base' could not be found. 
Attempting to find and ins tall... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0
The box 'sparkmooc/base' could not be found or could not be accessed in the remote catalog. 
If this is a private box on HashiCorp's Atlas, please verify you're logged in via `vagrant login`. Also, please double-check the name.
 The expanded URL and error message are shown below: I have encountered the following errors on running vagrant up command in cmd. I run a Windows 10 machine and virtualization is on in BIOS.  

In the Virtual Box, I got the following errors:


and


Please help me out on how to rectify these errors.
Thank you. Where on the site do I go to submit homework? Thank you. I get the following error following vagrant up command.

Any help to resolve appreciated

 Quizzes just from the first week make 12% of the final score, there's 5 weeks, so the total value of quizzes is 60%? edit: or is it 4 weeks?
Seems very high, it's a passing grade as far as I understand.
Looking forward to practical exercises. I uploaded the file lab0_student.ipynb to the VM via the jupyter browser and when I try to run it, it gives me a blank page  Hi,

After typing in the Vagrant up command in the cmd, I got the following screen, I hope that is right



After this when I try to start the Instance on the Virtual Box, i am getting the following error:



Also, i noticed that the virtual box is powering on, once i halt the vagrant. This is very strange. Please help.




 On edX this course is listed as a part of an XSeries along with a Machine Learning course.

So, there is an XSeries certificate for those who complete both of these courses? What does it look like? I can't find any images, only verified certificates for separate courses on edX. Yesterday  I got the setup working with the mozilla browser.

Today, I have vagrant running BUT it does not allow the connection
127.0.0.1:8081 or localhost:8081 on any browser.
I am running windows 7 and installed the files provided in the zip file.

Any suggestions?

Ali  In contrast to many of the other posts here, I just wanted to say I had absolutely no problems install the VM and getting everything up and running.
In case the instructors are interested, I am running Windows 10, build 10074 64-bit, with 8 G RAM and an Intel Core i7 processor.

I also created 2 batch files for "Vagrant Up" and "Vagrant Halt" which should be helpful in remembering to shut it down properly.
I am a Network Admin by day.  I've taken a Python class to get ready for this and I've dome some previous coding. This class looks like it may be a little over my head, but I'm hoping to be able to keep up. I'm getting the following error below, but I can get to the URL via Chrome without a problem:

C:\Personal\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0The box 'sparkmooc/base' could not be found orcould not be accessed in the remote catalog. If this is a privatebox on HashiCorp's Atlas, please verify you're logged in via`vagrant login`. Also, please double-check the name. The expandedURL and error message are shown below:
URL: ["https://atlas.hashicorp.com/sparkmooc/base"]Error: Could not resolve host: (nil); Host not found Maybe it's just me, but could I ask the editors to increase the sound volume of the video?  I need to max my volume to listen to his voice.  
Thank you.
 Has anyone used Microsoft's Azure Platform or Amazon AWS to deploy the environment required for this course on the cloud?

Would love to test it and document it into a little wiki page within this week.
Available and trying on my own for now. IMPEDIMENTS TO COLLABORATION

this quiz I cannot find a proper answer. It is always wrong. Is There something wrong with this specific quiz?
 unable to understand the following..
1. hacking skill
2. danger zone All downloaded OK but then I got a problem with VirtualBox. Any ideas anyone?On Oracle VM VirtualBox Manager the sparkvm is 'Powered off'.When I press 'start' on VirtualBox I get the error message:"Failed to open a session for the virtual machine sparkvm.
The virtual machine 'sparkvm' has terminated unexpectedly during startup with exit code 1.Result Code: NS_ERROR_FAILURE (0x80004005)Component: MachineInterface: IMachine {22781af3-1c96-4126-9edf-67a020e0e858}"

In the terminal I get the following error message:"The guest machine entered an invalid state while waiting for itto boot. Valid states are 'starting, running'. The machine is in the'poweroff' state. Please verify everything is configuredproperly and try again.
If the provider you're using has a GUI that comes with it,it is often helpful to open that and watch the machine, since theGUI often has more helpful error messages than Vagrant can retrieve.For example, if you're using VirtualBox, run `vagrant up` while theVirtualBox GUI is open." Dr Joseph

You stated that most Data Scientist only compete in Kaggle competitions for bragging rights related to algorithms.  However, is it really the algorithms?  There are ~ 50 useful Machine learning algorithms and in my experience choosing the correct one is the least time consuming and dare I say least important part of the process.  Wouldn't you say that data wrangling is equally or more important?  By far the most important part is feature creation and feature selection, specifically feature creation.

Our team has been able to drastically improve precision/recall through newly created features. 

Kindly, resolve the issue When running "vagrant up" in directory containing the vagrant file, I get following error message:

  Hello,
during the installation ("vagrant up") I am getting error: 

Stderr: 0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100%Interpreting C:\Users\tata\.vagrant.d\boxes\sparkmooc-VAGRANTSLASH-base\0.0.7.1\virtualbox\box.ovf...OK.0%...Progress state: VBOX_E_IPRT_ERRORVBoxManage.exe: error: Appliance import failedVBoxManage.exe: error: Could not create the directory 'T:\\sparkvmbase_1433277357221_23941' (VERR_PATH_NOT_FOUND)VBoxManage.exe: error: Details: code VBOX_E_IPRT_ERROR (0x80bb0005), component Appliance, interface IApplianceVBoxManage.exe: error: Context: "int __cdecl handleImportAppliance(struct HandlerArg *)" at line 779 of file VBoxManageAppliance.cpp

VirtualBox + cmd window run as admin, intallations of VirtualBox and Vagrant by videos.
I am using Windows 7.
Any suggestions?
Thanks
Elemer I'm on a Mac running OS X 10.9.5. I get the following error:

ed-keiths-mbp% ./Vagrantfile up./Vagrantfile: line 4: ipythonPort: command not found./Vagrantfile: line 6: syntax error near unexpected token `2'./Vagrantfile: line 6: `Vagrant.configure(2) do |config|'

I have used VirtualBox a fair bit, but this is is the first I have heard of vagrant.

I have ipython installed (but see nothing in the set up insteructions the say I need it.) I have Parallels VM on my mac. I can install Vagrant on Parallels. Would that be sufficient or do I need to install Virtual Box? The survey forgot an important thing.  I installed the course software on an Amazon Machine Instance.  I was not able to install the software on my regular workstation.  The questions in the survey were all assuming I was installing the software on just one computer, where in reality I am using two computers:  My home workstation is merely being used to display and interact with the course's x software remotely.  The Amazon AMI is being used to actually run the course's software.  

The survey asked about RAM and free disk space, but I have literally two computers, one at home and one at Amazon.  

In the responses to the survey, I replied using the specs for the workstation I have at home, so maybe you will make the course software better suited to a typical home workstation in the future. 
I have already created a login account in:
https://atlas.hashicorp.com/sparkmooc/boxes/base I've uploaded my notebook. It said all tests passed when I click check but the text just checked part 2, see below auto grader response. Shows green check mark, but when I click save it says you must check your answers first. Which I've tried 4 times. Am I done? or am I missing something?

Compare with hash (2a)
----------------------
All tests passed
Compare lists (2b)
------------------
All tests passed
-- 2 cases passed (100.0%) --
 Hi folks,

After I typing "vagrant up" in the console, a lot of these messages "Warning: Authentication failure. Retrying..." are displayed.

Anyone has a solution to stop these warnings?

: Warning: Authentication failure. Retrying...
: Warning: Authentication failure. Retrying...
: Warning: Authentication failure. Retrying...
: Warning: Authentication failure. Retrying...


Tks, 
Luis Everything completed in the lab0_student assignment, except loading a txt file. This gave the following error trace:

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-9-5453bfa2d105> in <module>()
      6 
      7 rawData = sc.textFile(fileName)
----> 8 shakespeareCount = rawData.count()
      9 
     10 print shakespeareCount

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/vagrant/data/cs100/data/cs100/lab1/shakespeare.txt
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:203)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:57)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1512)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:813)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:374)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)It looks to me that it cant find the file for some reason? I used a VM before but when I started it a window was opened. But, with vagrant don't. Why?

When the Jupyter is open, is it conect with some server? Are My notebooks storage in some site?

ps: sorry my english... I would like to know more about this theorem, like why all these three can't happen simultaneously, the lecture doesn't seem to provide sufficient explanations.

Thank you so much! :) I am unable to play last video named "Where does the big data come from". It says connection to the server is not established. HI I added the full output. 

==> sparkvm: Attempting graceful shutdown of VM...six:edx-spark akshay$vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Clearing any previously set forwarded ports...==> sparkvm: Clearing any previously set network interfaces...==> sparkvm: Preparing network interfaces based on configuration... sparkvm: Adapter 1: nat==> sparkvm: Forwarding ports... sparkvm: 8001 => 8001 (adapter 1) sparkvm: 4040 => 4040 (adapter 1) sparkvm: 22 => 2222 (adapter 1)==> sparkvm: Booting VM...==> sparkvm: Waiting for machine to boot. This may take a few minutes... sparkvm: SSH address: 127.0.0.1:2222 sparkvm: SSH username: vagrant sparkvm: SSH auth method: private key sparkvm: Warning: Connection timeout. Retrying...==> sparkvm: Machine booted and ready!GuestAdditions versions on your host (4.3.28) and guest (4.3.10) do not match.stdin: is not a ttyReading package lists...Building dependency tree...Reading state information...dkms is already the newest version.linux-headers-3.13.0-53-generic is already the newest version.0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.Copy iso file /Applications/VirtualBox.app/Contents/MacOS/VBoxGuestAdditions.iso into the box /tmp/VBoxGuestAdditions.isostdin: is not a ttymount: block device /tmp/VBoxGuestAdditions.iso is write-protected, mounting read-onlyInstalling Virtualbox Guest Additions 4.3.28 - guest version is 4.3.10stdin: is not a ttyVerifying archive integrity... All good.Uncompressing VirtualBox 4.3.28 Guest Additions for Linux............VirtualBox Guest Additions installerRemoving installed version 4.3.28 of VirtualBox Guest Additions...Copying additional installer modules ...Installing additional modules ...Removing existing VirtualBox DKMS kernel modules ...done.Removing existing VirtualBox non-DKMS kernel modules ...done.Building the VirtualBox Guest Additions kernel modulesThe headers for the current running kernel were not found. If the followingmodule compilation fails then this could be the reason.
Building the main Guest Additions module ...fail!(Look at /var/log/vboxadd-install.log to find out what went wrong)Doing non-kernel setup of the Guest Additions ...done.Installing the Window System driversCould not find the X.Org or XFree86 Window System, skipping.An error occurred during installation of VirtualBox Guest Additions 4.3.28. Some functionality may not work as intended.In most cases it is OK that the "Window System drivers" installation failed.stdin: is not a tty==> sparkvm: Checking for guest additions in VM...==> sparkvm: Setting hostname...==> sparkvm: Mounting shared folders... sparkvm: /vagrant => /Users/akshay/Courses/edx-spark
Hi Everyone,

   I am getting the below error. I have tried updating all of the vagrant plugins as well as virtual box 

  
stdin: is not a tty==> sparkvm: Checking for guest additions in VM...==> sparkvm: Setting hostname...==> sparkvm: Mounting shared folders... sparkvm: /vagrant => /Users/example/Courses/edx-sparkFailed to mount folders in Linux guest. This is usually becausethe "vboxsf" file system is not available. Please verify thatthe guest additions are properly installed in the guest andcan work properly. The command attempted was:
mount -t vboxsf -o uid=`id -u vagrant`,gid=`getent group vagrant | cut -d: -f3` vagrant /vagrantmount -t vboxsf -o uid=`id -u vagrant`,gid=`id -g vagrant` vagrant /vagrant
The error output from the last command was:
stdin: is not a tty/sbin/mount.vboxsf: mounting failed with the error: No such device
six:edx-spark 

Thanks!


 Is the first lab (0) as straight forward as downloading the ipython notebook as a .py script and submitting it? Or am I missing something. Thanks.

Richard Im trying to download over my work network which is proxied. Is there another way I can install this vm than by having vagrant install it. I can probably download sparkmooc/base using http but not by using terminal - Im getting the following error.

➜ myvagrant vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0The box 'sparkmooc/base' could not be found orcould not be accessed in the remote catalog. If this is a privatebox on HashiCorp's Atlas, please verify you're logged in via`vagrant login`. Also, please double-check the name. The expandedURL and error message are shown below:
URL: ["https://atlas.hashicorp.com/sparkmooc/base"]Error:➜ myvagrant Hi, I opened a terminal on Jupyter and tried to execute

$cd $SPARK_HOME
$./bin/pyspark

and got the following error:

[I 00:24:47.637 NotebookApp] Using MathJax from CDN: https://cdn.mathjax.org/mathjax/latest/MathJax.js            
[W 00:24:47.722 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryptio
n. This is not recommended.                                                                                       
[W 00:24:47.725 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using authentic
ation. This is highly insecure and not recommended.                                                               
[I 00:24:47.730 NotebookApp] The port 8001 is already in use, trying another random port.                         
[I 00:24:47.746 NotebookApp] Serving notebooks from local directory: /usr/local/bin/spark-1.3.1-bin-hadoop2.6     
[I 00:24:47.749 NotebookApp] 0 active kernels                                                                     
[I 00:24:47.750 NotebookApp] The IPython Notebook is running at: http://[all ip addresses on your system]:8002/   
[I 00:24:47.753 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmati
on).

How to proceed? Can someone explain (or point to web for answer), what is virtual box and vagrant and why it needs to be started/halted and why do we upload files to it and download .py files.
I visited Apache spark site and read about it on Wikipedia. It seems like a Distributed R (because of mlib).
But I am not sure where the tools downloaded fit into the picture.

I finished week 1 lectures/quiz/exercises and my progress shows 20%.
It seems those 20% was for making an effort, as there was no limit on number of attempts, and eventually I got all right.

thanks Getting the following error after some progress :

SSL read: error:00000000:lib(0):func(0):reason(0), errno 10054
 When I run "vagrant up" or "vagrant init" I get the following message.

A valid license is required to run the Vagrant VMwareprovider. Please visit http://www.vagrantup.com to purchasea license. Once you purchase a license, you can install itusing `vagrant plugin license`.Vagrant failed to initialize at a very early stage:
The plugins failed to load properly. The error message given isshown below.


I've been Googling for an hour and I have found no link to "purchase" a license for Vagrant. Did I miss a step?
 Hello,

I have completed week one, eagerly waiting for week 2 videos. Can anyone outline the Key differences between Apache Spark and Apache Storm based on their limitations, field of use, where to use which one, etc. Just wondering because I'm learning this alongside some other stuff I am doing and it would be useful to me. I was reading the documentation of Spark and see the Examples section

https://spark.apache.org/examples.html

Is it possible to run these examples like we did in the class this week? 
Hi All, 

In the lab0_student.ipynb file, I double clicked the formula, then I get some latex-like syntax, how can I convert back to text view, can anyone tell me, thanks.







Regards,
Kevin 
Source Vagrant up is not working...........what to do?

  Thank you for providing excellent course material. Happy to enroll the course and completed two lecture. Waiting for the rest!!!! Just in case anyone interest, here is the spark 1.4 nightly build http://bit.ly/sparknight
thanks to P. Wendell and the spark team.

disclaimer: this is nightly build for experiment, shall not be used for the course

Cheers,
Michael Hi,

I have the following problem:


vagrant and vb is installed as admin, I've also tried
 vagrant box add sparkmooc/base --insecure 
but didn't work. how can i access quizzess for Lecture 1: Introduction to Big Data and Data Science. i have gone through the complete section but i have not come across the quiz portion. As mentioned by M. Liu and the FAQ page: https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/wiki/BerkeleyX.CS100.1x.1T2015/All you need is: The course VM provides Spark 1.3.1 running in Jupyter notebooks via IPython 3.1.0 and Python 2.7.6. Spark is deployed in standalone mode.A little bit effort to find all the software online and in github.The class data files and test library are on GitHub:https://github.com/spark-mooc/cs100-datahttps://github.com/spark-mooc/test_helperAnd a setup to link Spark and IPython, as instructed in http://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/It works on my Windows 7 32 bit.No more VM and Vagrant. Yay.....Max Note from instructors:Note that in general it can be hard for the autograder to deal with subtle differences in platforms. E.g. sometimes results are returned in a different order, or with different precision in numeric outputs, and this can depend on many aspects of the installation. Of course it is great to see students learning about different ways to set Spark up. But be aware that your lab results may not match the tests provided with each lab, and may not receive credit.Unfortunately we don't have the staff to support non-standard installations. Hello,

I am planning to install red hat linux on my machine . is it ok for learning spark ?

Regards,
Amita Hi,

I would like to know minimum no of attempts to answer the quiz/exercises.


Regards
Naveen Will Windows XP meet the OS requirement for the course? Thanks Hi,

 I am installing in windows 2008 R2 server, i found the following error while installing command "vagrant up".

Kindly do the need full.

 Hi,
Will we be able to access the course material, the course website, after the course is over?
Thanks Hi. I encounter some errors below. C:\Users\elchong>cd myvagrant
C:\Users\elchong\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box sparkvm: Progress: 0% (Rate: 18d/s, Estimated time remaining: 2:03:44)7:28)An error occurred while downloading the remote file. The errormessage, if any, is reproduced below. Please fix this error and tryagain.
SSL read: error:00000000:lib(0):func(0):reason(0), errno 10054 Virtual machine has been set up successfully. However, when accessing the site "http://127.0.0.1:2222/", was informed of the following:
SSH-2.0-OpenSSH_6.6.1p1 Ubuntu-8
Protocol mismatch.
Checked that the "8001" must be replaced by "2222" for the particular configuration, hence the difference. Cannot access Jupyter webpage. Need help on resolving issue.

Thank you. as soon as i issued the command vagrant up 
it showed me
"Vagrant failed to initialize at a very early stage:
  The directory Vagrant will use to store local environment-specific state is not acccessible. I installed virtual box as the video said and used "Run as administrator", but still encounter the error

 Hi,
I already have Hyper V configured, so I cannot use any other solution, so I was wondering if the vm works with Hyper V. If not, is there any way to manually build the image ? As captioned. After vagranting up, still cannot access Jupyter webpage after entering the local host site via the URL address: 127.0.0.1:2222.

Webpage now reads the following:
SSH-2.0-OpenSSH_6.6.1p1 Ubuntu-8

Tried accessing Jupyter using three different browsers: Safari, Firefox, and Chrome. Nothing worked.

Maybe it is still protocol mismatch?

Need help resolving issue.

Thanks. Hi,

Host not found error is given when running the cmd vagrant up, please find the screenshot.

 Hi All,

Thanks for the course so far. I noticed that Windows 8.1 64-bit is not supported as indicated by https://www.virtualbox.org/ticket/13187 .

Thus, if you cannot run the vigrant VM, you might want to try deleting your VirtualBox and downloading the (last supported) version 4.3.12: http://download.virtualbox.org/virtualbox/4.3.12/VirtualBox-4.3.12-93733-Win.exe 


Best
Martin

PS: please highlight if this helped you! Just a fun site to understand correlation does not imply causation
http://www.tylervigen.com/spurious-correlations I think this is a nice example of it:
http://www.tylervigen.com/spurious-correlations I am getting the following error on Ubuntu

Bringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box
Bringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.boxThe box failed to unpackage properly. Please verify that the boxfile you're trying to add is not corrupted and try again. Theoutput from attempting to unpackage (if any):
bsdtar: Error opening archive: Unrecognized archive format can i follow this course on pentium R system I downloaded the image without problem but it failed to start up as below


When I tried to start it up from Virtualbox GUI, the follwoing error message show up:




I have checked in BIOS that Intel virtualization is turn on .

Besides, I also tried it with my other AMD desktop, the same symptom is still there.

Does any body have idea on how to work this around?

Thanks in advance!

Alan When I am submitting the file .
It is showing me 
Your answers have been saved but not graded. Click 'Check' to grade them. Hi all, I am experiencing the following problem while running vagrant up on command window. Does anyone have encountered this issue or have an idea on how to solve it?
Thanks a lot!

 Hi
Once I do this first time & it's a long time, for subsequent reboots of Spark will it also take so long?
Thanks i'm using Win7 sp1. installed VB and Vagrant ok.
BUT...
When starting my VM, I've got :"VirtualBox - Error In supR3HardenedWinReSpawn".Ntcreatefile(\device\VBoxDrvStub) failed : 0xc0000034STATUS_OBJECT_NAME_NOT_FOUND (0 retries)Drivers is probably stuck stopping/starting. Try 'sc.exe query vboxdrv' to get more information about it's state. Rebooting may actually help. (rc=101).Make sure the kernel module has been loaded successfully.C:\Program Files\Oracle\VirtualBox>sc query vboxdrvSERVICE_NAME: vboxdrv TYPE : 1 KERNEL_DRIVER STATE : 1 STOPPED WIN32_EXIT_CODE : 2 (0x2) SERVICE_EXIT_CODE : 0 (0x0) CHECKPOINT : 0x0 WAIT_HINT : 0x0 What are name and password ? Microsoft Windows [Version 6.3.9600](c) 2013 Microsoft Corporation. All rights reserved.
C:\Users\Karthik>cd myvagrant
C:\Users\Karthik\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box==> sparkvm: Box download is resuming from prior download progress sparkvm: Progress: 51% (Rate: 0/s, Estimated time remaining: 0:51:06)46))An error occurred while downloading the remote file. The errormessage, if any, is reproduced below. Please fix this error and tryagain.
SSL read: error:00000000:lib(0):func(0):reason(0), errno 10054
C:\Users\Karthik\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box==> sparkvm: Box download is resuming from prior download progress sparkvm: Progress: 51% (Rate: 0/s, Estimated time remaining: 0:52:41)8)))An error occurred while downloading the remote file. The errormessage, if any, is reproduced below. Please fix this error and tryagain.
SSL read: error:00000000:lib(0):func(0):reason(0), errno 10054
C:\Users\Karthik\myvagrant> To the instructors:

This course has a very tight homework  submission schedule - just one week after the release. Many of us (myself included) are not full-time students, and it is hard to find time for the homework during some very busy weeks. I for example will be traveling for 2 weeks, and I would not have access to the computer with the VM installed.

Would it be possible to release the lectures and labs in advance, so there is a chance for people like myself to do the homework on time?

Or alternatively extend the deadlines by 1-2 weeks?

Thank you for the wonderful course!

My colleague brings up a great question. It will be super helpful if you can release the courseware (esp. the assignments) for each week sooner. This allows us to balance our work and family commitments aong with the class. Additionally, I preassume lot of us have finished assignment 1 since it was easy and supplemented with accurate instructions by our TA.
 
I echo the thoughts and sentiments of the class -- Thank you for putting together a wonderful course. 

 How can I get confirmation that my submission is graded? Where to submit assignment for week 1 as I see we have only upload student.py which basically validates if the setup was correct or not When I am testing my lab0_student, the error appeare. In reality most of NO SQL storage engines created to have much simple implementation (limited budget, missing schema, consistency issue, document approach vs relational) of Queuing e.g. retrieval of values by key. e.g. Redis

Spark Project actually putting effort to provide SQL access to various NO SQL / Incomplete SQL storage engines.

My point: Generalisation on NO SQL query languages are Rich flexible is simply incorrect. Hello,
I do have an Oracle VM 4.3.20 already installed and I am using a pre configured virtual Machine set up for some other course. Do I need to uninstall this VM version as the Lecture 1 notes tells to use version 4.3.28 or later. 
While I try to install 4.3.28 in addition to the version 4.3.20 I have it shows a warning that it modifies certain files that is being used by other VM 4.3.20. Does this cause any issue to my other pre configured Virtual Machine set up that I am using under Virtual Box 4.3.20?


Thanks hi all, 
i am unable to make an virtual image of vagrant because of this error below and i dont  understand this error.
can any one tell me how to resolve it.
thanks
 I have an existing installation of VMWare's VMPlayer and other VM images. How can I take advantage of that for this course? After I pass command vargant up in cmd, i get the below but the VM is still turned off and I am unable to turn it on manually:

I get the following error when I try to turn it on manually:

Failed to open a session for the virtual machine sparkvm.

The VM session was aborted.
Result Code:E_FAIL (0x80004005)Component:SessionMachineInterface:ISession {12f4dcdb-12b2-4ec1-b7cd-ddd9f6c5bf4d}

tried the entire process again and got this:

Failed to open a session for the virtual machine sparkvm.

The VM session was aborted.
Result Code:E_FAIL (0x80004005)Component:SessionMachineInterface:ISession {12f4dcdb-12b2-4ec1-b7cd-ddd9f6c5bf4d}



 Hi

when I do vagrant up, it installs for a while, then comes with the error
SSL read: error:00000000:lib(0):func(0):reason(0), errno 60

How can I solve this?

Thanks
Best
 Hi, please someone could help me?I encountered this issue in the window prompt and also at the beginning in the VirtualBox's help.


 I work on Ubuntu and have a single node spark setup on it. Do I still need to install VM?
Can we also use scala to develop in this training. On running `vagrant up`, I get the following error:

The box you're attempting to add doesn't support the provideryou requested. Please find an alternate box or use an alternateprovider. Double-check your requested provider to verify you didn'tsimply misspell it.
If you're adding a box from HashiCorp's Atlas, make sure the box isreleased.
Name: sparkmooc/baseAddress: https://atlas.hashicorp.com/sparkmooc/baseRequested provider: [:parallels]

I have both VirtualBox and Parallels installed. I have not modified the default Vagrantfile which specifies the provider as VirtualBox
Vagrant.configure(2) do |config|
  config.ssh.insert_key = true
  config.vm.define "sparkvm" do |master|
    master.vm.box = "sparkmooc/base"
    master.vm.box_download_insecure = true
    master.vm.boot_timeout = 900
    master.vm.network :forwarded_port, host: ipythonPort, guest: ipythonPort, auto_correct: true   # IPython port (set in notebook config)
    master.vm.network :forwarded_port, host: 4040, guest: 4040, auto_correct: true                 # Spark UI (Driver)
    master.vm.hostname = "sparkvm"
    master.vm.usable_port_range = 4040..4090

    master.vm.provider :virtualbox do |v|
      v.name = master.vm.hostname.to_s
    end
  end
end
Does anyone know what's happening here? Hi everyone,
if you click on the link on this page you'll get an error. Don't worry, there is only a misprint: there's https://... instead of http://...
So the correct link is
http://www.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-28.pdf
Enjoy In the lecture 2 Anthony shows a venn diagram of Hacking skills, Math & statistic knowledge and domain expertise. I do not quite understand the "Hacking skills" term and how it relates to the content of the lecture. Can someone help me? Do you like R or RStudio along with your spark and python? If you are like me, you do.

The following Amazon machine instance includes a very nice R environment.  It works with our course's software too (vagrant, virtualbox, etc), which I know works, because that's what I am currently running our course software on.  So you can have spark and python and Rstudio all at once.  

I have written lots of software in R and python so I love having both.  Soon I will have written some spark software too!!!  

What else could anyone possibly want to solve small and large stats and data science problems on???

Community AMI that I used: ami-628c8a0a

Where I learned about this community AMI: http://www.louisaslett.com/RStudio_AMI/

Software pre-installed: R, Rstudio, Dropbox, RMySql, Latex, GIT


Hope this helps. Hi,

Are we able to complete this course on multiple computers since our work/assignments are done online? I currently have it installed on my work computer (PC) and at home (MAC). I would like to work on the assignments on during my breaks and continue where I left off on my home computer.  

Is this possible? Or do I have to copy my work from one computer to the other?

Thanks in advance! Hi
I am not sure how to run the MathJax formula part of the lab0 assignment (part 4 and above). I clicked that cell and did shift+enter and it does not return anything. What is the expected output ?
Thanks
 It's difficult to set up Vagrant VM on my work computer.

Is there a way to run the virtual machine in the cloud? I have a free AWS Tier and would love to set it up on AWS and access the VM from anywhere.  I gave correct answers to the quizzes after multiple incorrect attempts. Will it be given credits ?  Hi

When executing http://localhost:8001, opening my first notebook. However am unable to get to save file as .py. there are no such option. I can only download to ipynb,pdf,xml and other two option. There is no way to download .py file. Can some one help me on how to get .py file? I ran the python code I think OK. Some numbers were outputted and a graph. On My Mac I did 'Save as...' from Safari and had to save as a .webarchive file. Is this what I need to submit? Apologies if a silly question but I'm hindered by an extremely slow connection and subsequent inability to watch the videos. Thanks! I got this error, try the Python commend but got 0 extracted. How do I fix this? Hi,

I am getting an error in "vagrant up" command while downloading the vm. Kindly look into the below error message and help me in getting resolve it. 

C:\Users\kugautam\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box sparkvm: Progress: 0% (Rate: 0curl:/s, Estimated time remaining: --:--:--)An error occurred while downloading the remote file. The errormessage, if any, is reproduced below. Please fix this error and tryagain.
Failed connect to s3-us-west-2.amazonaws.com:443; No error

Thanks,
Gautam Yesterday I had the same problem that a lot of people are having where I was unable to download the VM image.

I am on a corporate network that uses a proxy server in order to access the network.

I set the environment variable HTTPS_PROXY to the IP address of our proxy server and then I was able to download the image without any further problems.

This might work for other users that are behind a proxy server:

The specific statement I used (windows) was:

set HTTPS_PROXY=https://206.xxx.xxx.xxx:xxx

Obviously you would put in your own IP address of your proxy server along with the port# if one is used.

Hope this manages to help others out :) One of the issues i faced was low disk space in my C drive on Windows.

By default Vagrant downloads the box into user's home directory in folder ~/.vagrant.d.

You can change it to another path by setting VAGRANT_HOME environment variable.

Also when you give "vagrant up" command it tries to import the vm to the default path given in your virtual box which is $HOME/VirtualBox VMs/. You can change this in the preferences menu.

The error i got because of low disk space is:

VMDK: cannot write allocated data block in 'C:\Users\...\VirtualBox VMs\sparkvmbase_..../box-disk1.vmdk' (VERR_DISK_FULL)

 Hi Can I go for paid certificate after finishing course or do I need to buy certificate now at the starting of course? Please guide. Remember the warning message one sees when installing VirtualBox? Something like "this will temporarily modify your internet settings"

Well, looks like the change wasn't temporary. I am not able to access the internet from my laptop anymore.

There were many posts in the internet on this, but being a beginner it was a difficult to glean the action items from the jargon.

Any help will be much appreciated.

Thanks,
Ben For lab0_student.ipynb I am so grateful to the staff for providing the amazing opportunity to take this course free of charge! You rock ;) It seems scores are not reflecting for quizzes in lecture 1 and lecture 2. When viewed in Progress bar those are coming as 0/1 for all. Is there any glitch? Hey ,
I am new here. Please somebody help me to understand how does it works.
Thanks  To be honest,  I haven't really started reading any of the material except for lectures 1 and 2 and I saw that Apache Spark requires Python 2.x. But is there a newer version of Apache Spark that supports Python 3.x? Also, I have worked with website scraping and accumulating data would that be considered  a form of Data Acquisition under Data Sciences? Is there a repository with data files? E.g., with 'shakespeare.txt' from the lab0.

I have a running Spark on my own machine and, of course, I don't want to bother with the virtual one.

Thanks.
 Hi everyone,

The installation was rolling back before finishing and I got below error massages. Has anyone seen this issue and could share some information on how to fix it? Thanks!




 Is there an ical export for deadlines for the course so we can add it into our calendars? I have gone through the Lecture 1 , But i did not  come accross the quizes PLEASE, can somebody help me? I am stuck at the "vagrant up" step. I would appreciate you if you provide some explanation or a detailed instruction how to solve this problem.

Thank you in advance.

P.S. Previously I successfully installed the VM VirtualBox and then the vagrant_1.2.7 as was demonstrated in a video instruction.

My problem is below:


!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Due to the instructor's answer "The problem is most likely the unicode characters in your directory name. Please try using a directory that does not include unicode characters."
I tried to do "vagrant up" in another directories and received the same problem.
Can you provide some examples to your answer?
Thanks.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!







 Hi,
Once i start vagrant up , after 73% finished , it is suddenly stopped downloading . Can some one assist

Please see the below :

The directory Vagrant will use to store local environment-specificstate is not accessible. The directory specified as the local datadirectory must be both readable and writable for the user that isrunning Vagrant.
Local data directory: c:/Users/marco/myvagrant/.vagrant
c:\Users\marco\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and itall... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/verons/0.0.7.1/providers/virtualbox.box sparkvm: Progress: 73% (Rate: 0/s, Estimated time remaining: 0:57:15)57)) I completed lab0, went to the autograder page, uploaded my .py file and checked it successfully.  I then clicked 'Save', at which point it told me "Your answers have been saved but not graded. Click 'Check' to grade them."  Clicking 'Check' now tells me "You did not select any files to submit".  Did clicking 'Save' undo my submission?  Could I have clicked 'Save' first and later come back and checked my saved submission?  If so, then it would make more sense to put the 'Save' button to the left of the 'Check' button.  At least there should be an explanation of the 'Save' button on the page. Can some one correct me on Notebook submission?
does this mean downloading and installing Virtual box?
 Hello, my vagrant setup is up and running without any issues. However I am curious if there is setup instruction on how to install everything from scratch on Ubuntu, including Spark, Ipython, as well as any required library and tools. Just wanted to get an idea of what's going on under the hood. I already have an Ubuntu 14.04 VM and would like to try installing everything on top of it. I understand I can google individual pieces but love to see an overall installation guide. I know in the courseware it says, the supported OS is 
64-bit (preferred) Mac OS X 10.9.5 or later
But I am wondering if it is an absolute prerequisite or just recommendation. I have OS 10.7.5 

 Can some one correct me on Notebook submission?
does this mean downloading and installing Virtual box? Vagrant up command not recognized by windows- have followed all instructions (downloaded mooc file etc) and oracle vm loaded ok on my windows machine..?

 After downloading the Virtual box for windows 7 system. I'm finding an issue when i start with the installation. The error i get is as below

:"Installation failed!Error The system cannot find the file specified ".

 When I click on run after entering into a particular section, and run the code, then nothing happens, just * appears in between [ ] like [*]
Please have a look at the picture attached

 Hi Everyone.
I do not want to click tens of buttons to get Spark up. Instead, I went to the CMD prompt (from my 64-bit WIndoes 8.1 machine) and typed:
> copy con: up.bat
> vagrant up
> Ctrl-C
> copy con: halt.bat
> vagrant halt
> Ctrl-C
> exit

(">" stands for the prompt, which in my case has the path name to my course directory)

I checked my WIndows File Explorer, and the two batch files are there in the window. double-clicking on them does the expected.

Hope this helps. 

 when I run vagrant up I get the following issue
(virtual box was installed as and runs as Administrator Vagrant was installed as administrator and vagrant up is run in a command shell opened as administrator.) screen image below as well.

Any help would be appreciated.

Abie

error on vagrant up is -> 

VirtualBox error:
VBoxManage.exe: error: Could not rename the directory 'C:\Users\abie\VirtualBoxVMs\sparkvmbase_1433373329751_14037' to 'C:\Users\abie\VirtualBox VMs\sparkvm' to save the settings file (VERR_ALREADY_EXISTS)VBoxManage.exe: error: Details: code E_FAIL (0x80004005), component SessionMachine, interface IMachine, callee IUnknownVBoxManage.exe: error: Context: "SaveSettings()" at line 2788 of file VBoxManageModifyVM.cpp
 

 Are there any meetup groups in Mountain View or Sunnyvale? The virtual machine comes configured with 2GB of memory and a single core CPU. Is it OK to change the settings of the virtual machine to use more memory or more than on core? After all Spark is supposed to be about parallel computing.

Also, is it wrong to start/stop the VM from the VirtualBox window? There are diversified languages and tools.

What are some key programming languages a data scientist needs to know?
Likewise what are some key tools - a must? Will there be any extra credit assignments during this course? Also pardon if this this has been asked and answered, I tried to search out the info before I posted.Thanks kindly. Hi,

I downloaded and installed VirtualBox, Vagrant as specified in the setup instructions. But I am getting below error on Windows 7 machine

The guest machine entered an invalid state while waiting for itto boot. Valid states are 'starting, running'. The machine is in the'poweroff' state. Please verify everything is configuredproperly and try again.

I tried to open it directly from the VirtualBox GUI but getting a different error

Failed to open a session for the virtual machine sparkvm.

The virtual machine 'sparkvm' has terminated unexpectedly during startup with exit code 1 (0x1). More details may be available in 'C:\Users\Ram\VirtualBox VMs\sparkvm\Logs\VBoxStartup.log'.
Result Code:E_FAIL (0x80004005)Component:MachineInterface:IMachine {480cf695-2d8d-4256-9c7c-cce4184fa048}


I enabled the virtualization as well and installed as administrator. 
Open the command prompt also as administrator.
Please let me know how to resolve the issue.


Below is the error from the log file

Error opening VBoxDrvStub: STATUS_OBJECT_NAME_NOT_FOUNDe28.5d0: supR3HardenedWinReadErrorInfoDevice: NtCreateFile -> 0xc0000034e28.5d0: Error -101 in supR3HardenedWinReSpawn! (enmWhat=3)e28.5d0: NtCreateFile(\Device\VBoxDrvStub) failed: 0xc0000034 STATUS_OBJECT_NAME_NOT_FOUND (0 retries)
Driver is probably stuck stopping/starting. Try 'sc.exe query vboxdrv' to get more information about its state. Rebooting may actually help.1688.1490: supR3HardenedWinCheckChild: enmRequest=2 rc=-101 enmWhat=3 supR3HardenedWinReSpawn: NtCreateFile(\Device\VBoxDrvStub) failed: 0xc0000034 STATUS_OBJECT_NAME_NOT_FOUND (0 retries)
Driver is probably stuck stopping/starting. Try 'sc.exe query vboxdrv' to get more information about its state. Rebooting may actually help.1688.1490: Error -101 in supR3HardenedWinReSpawn! (enmWhat=3)1688.1490: NtCreateFile(\Device\VBoxDrvStub) failed: 0xc0000034 STATUS_OBJECT_NAME_NOT_FOUND (0 retries)
Driver is probably stuck stopping/starting. Try 'sc.exe query vboxdrv' to get more information about its state. 

I tried rebooting several times as well

Thanks What web scraping tools should we familiarize ourselves with?
Thanks lab0_student.ipynb is running.  Do I need to shutdown the currently running Jupyter processes?
Are there any prerequisites before -> vagrant halt 
The Sparkvm is not starting - it stays in "aborted" state in the virtual box.
The error code below is displayed in the DOS window.

Result Code: VBOX_E_VM_ERROR (0x80BB0003)
Component: Machine


What does this error mean? What are some things to try to resolve this? There seems to be no sound from minute 4:52 to 9:10 in the video Hi everyone,

I have install ipython 3.1.0. While running lab0_student.ipnb I got error message.
Unreadable Notebook: Unsupported JSON nbformat version 4 (supported version: 3)

Please free to suggest me idea to solve this problem.

 Using Spark on one computer with more than one core as we are doing in this course, is this a way for us to use multiple cores on our computer.  I have 2 cores and 2 virtual cores, (maybe 4 cores) enabled through BIOS. What types of projects  can be done ?
 E:\Edx\Spark\MyVagrant>vagrant box remove sparkmooc/baseThe box you requested to be removed could not be found. Noboxes named 'sparkmooc/base' could be found.
E:\Edx\Spark\MyVagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box==> sparkvm: Box download is resuming from prior download progress sparkvm: Progress: 0% (Rate: 7d/s, Estimated time remaining: 0:16:59)4)))An error occurred while downloading the remote file. The errormessage, if any, is reproduced below. Please fix this error and tryagain.
SSL read: error:00000000:lib(0):func(0):reason(0), errno 10054 Hi,

I currently do not have sufficient enough wifi to set up Vagrant in a reasonable time. However, from Saturday June 7th onwards, I will have adequate wifi. So I am wondering if the three day grace period for assignments includes the software setup (due on June 6th)?

Thanks for the help. Hello,

I have submitted the first notebook and click on check but it keeps running and i could get any results even aftter 15 min. What should i do. i had to use my 2 attempts because of this just to make sure that submission i did is right.


Thanks
 I got the following error while trying to start the sparkvm using 'vagrant up'



I tried to start it manually and got the error below.

 First of all, let me say that I'm usually that guy with all the unusual setup problems.  I don't know why - maybe aliens replaced my phalanges with magnets or something.

But so far, my experience getting set up here in this course has been the exception.  Knock on wood!  Everything has worked, well, the first time. so far.  I would like to credit EdX for being so thorough.

Unfortunately, a LOT of students are clearly having a more difficult time getting set up, and I'm afraid that this may be par for the course when working with (1) many (2) new (3) complex technologies that are just expected to cooperate outside of laboratory-conditions testing and in our real-world lives.  I'm not Microsoft's biggest fan, but having "one throat to choke" has had its advantages.  I would encourage frustrated students to walk away from their first choice of hardware / operating system and find another.  In my experience, this solves 99% of the problems I have faced as an IT consultant with 17 years of experience.

EdX/Instructors: I don't have much to offer you in the short term, but next time you should consider having fewer moving parts.  Consider putting Spark directly on the VM and getting rid of Vagrant.  Not that Vagrant is bad, it's just one thing that can potentially go wrong - as evidenced by the feedback I'm seeing now.

Take care,

BCB Video lectures don't show up in Chrome.

Works fine in Firefox.

Doesn't work in Chrome 43. Adblocks are off. Hi, Could anyone please let me know, what is the problem. Below is the screen dump of the error 

error.png

Thanks for the help in advance!

 Anthony and team,Please don't develop drinks bag problem during or after this course. I am getting the following error after I give "vagrant up" command.


Kindly, provide a solution.Thanks in advance.
 
Could someone please explain me the connection between vagrant and the iPython notebook.

Is the iPython notebook installed as part of the virtual machine image?

How am I able to run it on my local web browser when the iPython is part of the image?

I have a Hadoop instance working on my local machine. Is it possible for the spark virtual machine to interact with the HDFS of the Hadoop on my machine?

Please let me know if I got it wrong.

Thank you. Happy learning.  inconsistent with the course video 'RUNNING YOUR FIRST NOTEBOOK' I'm a Geogia Tech Onllne Master Of Science student in my spare time, and wanted to say a few good things about this course so far.

We also use Piazza for discussions, the largest course there was about 500 students, (well, it is not free, it's for full credit). This is the first time I see 4500 students in Piazza on the same course. 

What I'd like to say is 

Wow, that's a lot of students, I'm so happy Spark gets so much attention! I have to say that edx's platform is great, they use Udacity for GATech but Edx is as good at least. and much preferred over coursera (I'm taking a few courses there as well)  Logistics and instructions - this was pretty smooth compared to the average I'm used to from other MooCs, don't be alarmed, EVERYONE has setup issues the first few weeks. Vagrant is actually a very good idea for VM management. I had issues too, but in the end everyone gets the setup done. Remember this course is FREE (for most students who are not ID verified) and that the Instructors / TAs need to handle 4500 people. Please remember to be polite and courteous, most of them are grad students and some of them do it out of passion and willing to help others (trust me they don't do it for the money) The start so far is great, seems like we have a very high quality course ahead of us!

Good luck everyone after installing vagrant, when I type "vagrant up" in the command window, it says -"'vagrant' is not recognized as an internal or external command, operable program or batch file" I see the setup progress bar in maroon color (100% complete) on the course progress page, while the quiz progress bars are in different shade.

Does this need some corrective action on my part or is this as per the page design?  I feel as I made a mistake on trying to install VM.

I have the files but I dont know how to change the settings in a way that allows me to start anything.

Help needed. Thanks Hi all,

If anyone has issues with vagrant after a kernel update on Fedora, namely: 



Then a quick:

$ sudo /etc/init.d/vboxdrv setup
$ sudo /etc/init.d/vboxdrv start

...fixes things. 

I only wish I could help out the Windows users a bit more! Just have a habit of browsing this Q&A forum since the launch of this class.
It's comforting to see the instructor everywhere, answering as many questions as possible.
This is only the first week, all we students are expected to do is installations , but his activity do boost my and hopefully your motivation
Big DATA is big, and it's time consuming as well. But I can say that I get the social support here! 
Stay resilient, all! Its stuck at this from a long tym! what should i do Luckily, I'm done with the setup now, thanks to an Udacity class on databases, which uses more or less the same configuration.
Now, preparing for the next week, I have watched tons of Apache Spark on Youtube, plus some from Data Bricks and the lab at UC Berkeley. Saw some functional programming in their code, 
Though I adore Python and have been using it since 2012, this is uncharted territories for everyone. Any suggestions from the Instructor on things to come? Hi, I have a Win machine and I already have VirtualBox 4.3.26 r98988. When I select the pull down for "check for upgrades", it states this is the latest version. I knowill 4.3.28 is the latest.  I do not want to reinstall since I'm using 4.3.26 for another application. Please advice.

Thanks,

Vik   I have tried downloading 5-6 times, but it fails everytime. I am not using any proxy or anything. I have tried a normal as well as a high speed internet connection but it still didn't work. I have google these two terms, however, I only get their normal meanings in daily life which seem not relative to computer science. So, what does the professor refer when he said these two words? I have already setup the VMware, do I still need to download and install the VirtualBox and Vagrant automatic VM configuration? Hello,
Has somebody experienced this :

If you're adding a box from HashiCorp's Atlas, make sure the box isreleased.Name: sparkmooc/baseAddress: https://atlas.hashicorp.com/sparkmooc/baseRequested provider: [:libvirt]
Thanks for any help Thank u QAQ ’sparkmooc/base‘ download time is too long, can we download by other ways?
Thank you:-) 
How do I resolve it....??Its already late Tried to set up vagrant thrice but unable to do it. It takes a long time. It comes to 60-65% and as I leave my computer unattended it goes on 'stand by mode' and then stops. Gives eror "SSLread:error:000000:lib<0>:func(0):reason(0), errno 10054.

Any suggestions I had a few issues w/ some of the statements made in Lecture 2 about the current state of data acquisition & analysis.

"traditional" data acquisition & storage vs. "big data" acquisition & storage: "In data science, data is cheap..." This statement is highly misleading b/c it assumes that "data science" only uses transactional data, and that transactional data is only stored in non-RDBMS environments, which simply isn't true. Many organizations still rely on relational databases, & still employ people who are doing highly complex & advanced analyses based on those data. So is the advanced work they're doing in computing & statistics not "data science" b/c it's not stored in a cloud-based cluster? and as far as cost... price out what it takes to get tweets, facebook posts, or credit card transactions from vendors. It is NOT cheap. 

Additionally, my understanding is that "data science" is not about the size of data, but what you DO with data. And in this case, depending on the problem that you have, losing values will affect your results. Maybe not one or two records, or even 1,000 records (say your average dataset is >=1M). But if you're talking about vulnurable populations, or very specific segments of your consumer base, i would be very interested to know what, why, & how those records were lost. While it may not matter from a statistical standpoint (in some respects), it is dangerous & lazy to say you don't care about losing a few records. Losing records is bad data management, it could be indicative of a larger problem with your warehousing environment or ETL/cleansing process, & it could end up seriously skewing your results depending on your population of interest.

Also, the lecture seems to disparage the development of theory-based models & as pointless or old or passe. Although i'm in the private sector & we don't use a whole lot of underlying theory, in fact, we rely on underlying theory behind all machine learning algorithms. Theory-based models can be extremely useful for understanding omplex relationships, etc. and although we are focused getting the job done in the private sector, we ultimately rely on folks who make theory-based models & practice "traditional machine learning". When Google or Facebook create a custom algorithm, it is patented & kept locked up tight. When academic research is published, it is freely available to all (assuming you pay for the publication subscription. many institutions, public & private, are now focusing on making theoretical results more openly available). Many of the custom data science algorithms are at least in part derived from exploratory work produced in the ivy fortress. In reality, "traditional machine learning"/academic modeling & practice-driven modeling are necessary & very beneficial for one another.

And the four archetypes of "data science" roles -- businessperson, programmer... & that's it? Aren't you forgetting the actual data scientists/statisticians/quantitative analysts? This seems to suggest that programmers do the modeling & the businesspeople consume the results. Isn't true "data science" a collaborative process, where you have experts in each of those three circles of the Venn diagrams working together to create new & innovative products/analyses?

My point in this pointless rant is not to argue over RDB vs. unstructured warehouses, or quibble over whether losing records is important & other such benign details; but to point out that the instructor's definitions of what data science is & who does data science, is extremely narrow & misleading. I understand the course's purpose is to introduce us to Apache Spark, & that's why i'm taking the course. However it might be better to just cut out the slides trying to define data science, or seriously revise them. And let's not kid ourselves, "data science" is just some buzzterm that marketing ppl came up w/ so they could distill complex processes & disciplines into something that other marketing ppl would be able to comprehend.

Have a great day! :) Hi,

In lab file I see Python 2 code. I understand Python 3 well. Is knowledge of python 2 necessary?

Thanks,
Ayush What's your take on that. I placed the myvagrant file and folder under User/Username/myvagrant. It installed the sparkmooc/base. Vagrant up is not working from terminal on my mac. I get this.

Couldn't open file /Users/Primehelper/base
Primehelpers-MacBook-Pro:~ Primehelper$ vagrant up
Bringing machine 'default' up with 'virtualbox' provider...
==> default: Box 'base' could not be found. Attempting to find and install...
    default: Box Provider: virtualbox
    default: Box Version: >= 0
==> default: Adding box 'base' (v0) for provider: virtualbox
    default: Downloading: base
An error occurred while downloading the remote file. The error
message, if any, is reproduced below. Please fix this error and try
again. Hi. I am currently hospitalized right now. (Surgery, since 1st June)
I don't think I can get out before the deadline of setup homework.
Is there anything I can do?
I would like to take this course as a verified track also. (But I haven't done yet since I'm in hospital )
Would this affect my grade?l I've successfully installed VirtualBox, etc, and apparently it's up and running.  However, the machine it's set up on is a remote host, NOT a desktop, it does not have a monitor attached to it currently, and even it it did, it's not really usable as a desktop.

What are my configuration options here for using it? EDIT3: sparkvm is running in VBox so I think I am clear here.
========================================================================================================
EDIT2: installed packages 'dkms' and 'kernel-devel'

Now when I run 'vagrant up' it scrolls through a few lines, last one (for now) is
Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box

and now I am making Progress on a download.

Does this sound like I will be able to proceed after the download completes? 


==================================================================================================

EDIT: was able to set virtualbox as default provider.  but now when running 'vagrant up' the following error occurs

VirtualBox is complaining that the kernel module is not loaded. Pleaserun `VBoxManage --version` or open the VirtualBox GUI to see the errormessage which should contain instructions on how to fix this error.

Opened VBox gui but don't see an error message

=============================================================================
ORIGINAL:

No idea what this means.  I do see where it references provider libvirt, is that indicative of a problem? thx

Vagrant ver 1.7.2-5.fc21.1 
Vbox  4.3.26

Full error message here

$ vagrant upBringing machine 'sparkvm' up with 'libvirt' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: libvirt sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/baseThe box you're attempting to add doesn't support the provideryou requested. Please find an alternate box or use an alternateprovider. Double-check your requested provider to verify you didn'tsimply misspell it.
If you're adding a box from HashiCorp's Atlas, make sure the box isreleased.
Name: sparkmooc/baseAddress: https://atlas.hashicorp.com/sparkmooc/baseRequested provider: [:libvirt] 1. Download this file (https://github.com/spark-mooc/mooc-setup/archive/master.zip) to thecustom directory and unzip it.

2. Download the image from " https://s3-us-west-2.amazonaws.com/spark-mooc-vm/package.box ", You can use your download Managers or browser to download it or even resume when you what.
3. place the downloaded package.box inside the custom directory

4. Edit the vagrantfile in the custome directory, replace the line master.vm.box = "sparkmooc/base" with master.vm.box = "package.box" .

5. run the command vagrant up remaining in that directory.

6. finished
 EDIT: Nvm. Just found out that it won't work on Windows 8 with Hyper-V enabled. Disabled Hyper-V and it worked.


Hi,

I've been getting the time out issue no matter what I do. i tried the different remedies in the forum.

Reboot. Use GUI first. Nope, doesn't work for me even after waiting for several minutes.,

Insert this code into the vagrant file. Nope, doesn't work too.
config.vm.provider :virtualbox do |vb|
    vb.gui = true
  end

Help anyone? Thanks.


 
i got this error twice. once when i downloaded the image from vagrant up command , second time after downloading through downloading manager and editing vagrant file..

sparkvm: Progress: 100% (Rate: 17.3M/s, Estimated time remaining: --:--:--)
The box failed to unpackage properly. Please verify that the box file you're trying to add is not corrupted and try again. The output from attempting to unpackage (if any):

p.s : my operating system is win xp sp3/32bit Wanna know the working of Vagrant and what exactly it is. Also explain what we have done so far in installments.
 I have a huge amount of sports (and other) data available through my GitHub. See https://github.com/octonion.

The scraper/spider scripts are included (usually Ruby or Python), and the repos usually have schema and loader scripts, but they're presuming you have Linux and PostgreSQL installed. There's also plenty of analytical code, typically written in R.

Sports covered are baseball, football (American), hockey, lacrosse, softball, field hockey, soccer/football, volleyball, tennis - even Scripps Spelling Bee data.

-octonion I just installed configured the system after watching all the videos. Everything was fine until I issued the command "vagrant up". I'm constantly receiving the message "sparkvm: Warning: Connection timeout. Retrying..." or "sparkvm: Warning: Remote connection disconnect. Retrying...". I have Hyper-V and VMWare installed on my system and I stopped those services too but this error is still there. How could I resolve this issue? The vm is running inside the VirtualBox. Thanks for your great courses about Apache Spark on edx, I'm proud to be one of your "students" !

I have a quick question and hope you will be able to answer it fast, I can imagine how busy you must be all year long!

I'm working on a p2p  educational platform, monitoring student's assiduity, assignment, grading, motivational and emotional factors, allowing online courses to improve retention rates, and efficiency, that's why I'm attending apache spark courses, to be able to work on our expert system that will use data mining to collect information, improve algorithm, and adapt its own efficiency.

I have a partnership with Qarnot computing, a french company that converts CPU in free heat! Clever...

But to determine my business model, I need to figure out, how much CPU data mining uses...

Do you have any ideas, any examples, any figures about the CPU need for data mining (examples you experiences or so...) ?

Your answer would be more than welcome...

All my best,

Raphael
 can any one tell me how to upload quizzes other than checking the set up? and where do i find them? Hello,

I am unable to get the VM running. 'sparkvm' is always in powered off state.
Sequence followed:
1. Openend Virtual Box
2. Issued 'vagrant up' from cmd prompt.

Please refer to the snapshot below for details.



Thanks! Can you please clarify if Internet connectivity is disrupted before vagrant halt command. Would this be causing any issue. What should be done next, try Halting or do Vagrant Up again?

Thanks,
Saurabh Hi,
can somebody explain me please where the advantage of using vagrant is? I have an issue with no path of vagrant in win8. Googling -> solved. I have an issue with not ASCII user name in win8 -> Googling and solved. And now  after half hour downloading and information that sparkwm successfully added box I have an error message by importing base. Again prolem with encoding...I've thougth this is Big Data and Spark course and not vagrant administration course..
I appriciate the idea of use predefined image but vagrant I will never use again...
Thanks Hi I downloaded virtualbox and vagrant (I think). I'm in the third video of the setup videos and I go to c:/HashiCorp/Vagrant and doubleclick the file which is 2078 kb big and the command propmt appears for 3 seconds and then crashes. Am I doing something wrong?
 
Pl. help, i'm having hard time, getting the vagrant up and running.  The option "Windows XP (Service Pack 3)" is not available in "Oracle VM VirtualBox" 64 bits.
It is s not possible to select this option.
Why do you suggest to run a new program like Virtual Box in an unsupported platform?  https://www.youtube.com/watch?v=qfEpA99Bjrs
It's fun how he chose to save himself on fire, because everything else's on the CLOUD! Hi,I know that the vmware provider is not officialy supported for the Lab, but i had to much headach running VM on virtualbox in the past, so since i'm on Vmware Fusion for about 1 year now. I had to try to make the Ubuntu Sparkmooc VM play nice with VMware Fusion.So without futher ado, here is the complete step, browken down to the very little details ;-)**************************** Sparkmooc on Fusion ************************vmware_fusion offer better performance and overall stability than virtualbox, plus the sharefolder fonctionnality always work seamlessly in Fusion compared to virtualbox ( my own opinion ).pre-requisite :- vagrant : 1.7.2- vmware fusion : 7 or later ( i haven't tested with vmware fusion 5 or 6 since i don't have them on my machine)- Mac OSX: Yosemite1/ install the vagrant-vmware-fusionopen the terminal and type :
   vagrant plugin install vagrant-vmware-fusion

2/ Install the plugin license :Of course if you already have a license corresponding to your version of the plugin you can skip this step and go to step 3.The plugin can be installed as describe above but will not work if you don't buy the license from Vagrant website (hashicorp.com).Once you've downloaded the license.lic file, open a terminal and type :
   vagrant plugin license vagrant-vmware-fusion /path-to-the-license-on-your-disk/license.lic  
3/ Download the vagrant box manually:Open a terminal and use the "wget" command to download the official virtualbox.box provided for the mooc . Note however that you can olso download it with your regular web browser, you just have to copy and past the url below in your browser adress field.In a terminal type:( replace your_name with your actual userlogin name )
   cd /Users/your_name/myvagrant/
   mkdir temp
   wget -O temp/virtualbox.box  https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7/providers/virtualbox.box
Depending on your internet connection, this might take a while, the file is aroung 600Mo after all, you will get a file named :virtualbox.boxassuming you did as suggested above, go on and extract the virtualbox.box file ( it's just an archive ) by typing the following command:Create a box directory
   mkdir box   
extract the file
   tar -xzvf temp/virtualbox.box -C box  
list the content of box folder after extraction
   ls -la box/  
You should see this output:box.ovf (file), box-disk1.vmdk (file), include (folder),Vagrantfile (file).The first file of interest here is box.ovf with is an international standard file format for virtual machine, even if, some company have their own proprietary ( vmware has vmx for exemple) , they all can read this format. so we are going to import it in vmware fusion.4/ Launch Vmware Fusion GUI and start the VM :Select the "File" menu, then "importation" , you have to tell Fusion where the file is located so click the "Choose File" and navigate to the /Users/your_name/myvagrant/box then select box.ovf and chose Open then click continue. Fusion will ask you for a name and a location where you wanna store the Virtual machine.for exemple :name : sparkmooclocation : /Users/your_name/myvagrant/boxThen click continue. know that it will failed first because vmware fusion uses a very strict version of ovf file format while virtualbox is a bit more loose, so just click retry and Fusion will import it successuly by relaxing a bit the verification check.click finish or whatever ... by now the vm should be starting up, if not just press play and start it.we're going to log in the vm to customize it a bit , don't worry nothing fancy, but we need to install VMwareTools.5/ Uninstall Virtualbox-GuestAdditions.All vm vendors have some additionals packages you can install into the vm, to ease usability and ensure better support in terms of : screen resolution, keyboard, performance and all kind of additionnals fonctionality that will make running the vm a smooth experience . This VM has some of those VirtualBox guests Additionnals package installed. Since we wanna use Vmware fusion we will install VmwareTools instead, which is the equivalent of virtualbox guests additions, but for VMware Product familly.First remove virtualbox guests addition. Log into the VM with user: vagrant password: vagrant.Since you are log in the vm, on the console type :
   sudo apt-get remove virtualbox-*  
An then
   sudo apt-get purge virtualbox-*  
6/ Install VMwareTools guests additionsStill in the ubuntu console VM type :I dont know at this point if there is any thing wrong with updating the os or installing git. Personally i haven't experience any thing causing trouble after going through the notebook etc ...But remember you have a backup of virtualbox.box on your local disk in the /Users/your_name/myvagrant/temp folder so anytime if anything goes wrong revert back to it if needed.
   sudo apt-get update  
   sudo apt-get upgrade  
We will need those for installing VMwareTools, so go on
   sudo apt-get install p7zip-full  
   sudo apt-get install wget   
   sudo apt-get install zip
   sudo apt-get install git  
I needed to install this version of the kernel because VMwaretools didn't work well with the one installed by default. Go on
   sudo apt-get install linux-image-3.16.0-31-generic  
   sudo apt-get install linux-headers-3.16.0-31-generic  
At this point, restart the Ubuntu VM box with the command :

   sudo reboot now


when the vm is up again login into with user: vagrant and password : vagrant
Just to be sure check that we're actually running on kernel 3.16 by typing in the console :
   uname -r  
you should see this output : 3.16.0-31-genericNow let's Go and install those VMwareTools :
   cd /home/vagrant  
   git clone https://github.com/rasa/vmware-tools-patches.git 
   cd vmware-tools-patches  
 This is going to download VMwareTools( here 7.1.1 refer to the version of your vmware fusion ;-))
   ./download-tools.sh 7.1.1  
   ./untar-and-patch.sh  
And finaly to install vmwaretools
   sudo ./compiles.sh  
That's it you now have vmwaretools installed correctly on Ubuntu 14.04 same as the original virtualbox VM but for VMware FusionJust to be Sure verify that the PySpark_notebook is running by typing :
   ps -ef | grep python  
You should see the spark_notebook.py process and notebook --profile=pyspark process running.Shut the Ubuntu VM ( don't just suspend it , shut it down) type : sudo init 0 //(it's a zero)Or by selecting Virtual Machine in the Fusion Menu, then "Stop" or "Shutdown" in the dropdown menu.7/ Package it up and Create Vagrant BoxAccording to the Vagrant official documentation https://docs.vagrantup.com/v2/vmware/boxes.html. The important files that composed a vagrant box for the VMware Provider are the 5 files listed below + one json file : nvram, vmsd, vmx, vmxf, and vmdk ( needed by VMware Fusion VM)metadata.json ( needed by vagrant )Open a terminal on your Mac and type :
   cd /Users/your_name/myvagrant  
Create the metedata.json file expected by vagrant, by typing:
   touch box/box.vmwarevm/metadata.json  
( box.vmwarevm is actually the format used by Fusion to package VM, and yes it's in fact a folder)Open the file, type:
   vim box/box.vmwarevm/metadata.json  
( if you wish you can create this file with a simple text editor of your choosing)And put the following content in there :
    {

      "provider": "vmware_fusion"  
 
    }  
Close the file.Now edit the Vagrantfile that came with the zip file mooc-setup-master.zip that you downloaded earlier from edx.course website.It should be located in /Users/your_name/myvagrant/Inside that vagrant file change the sections:
   master.vm.provider :virtualbox  
and replace by this
   master.vm.provider :vmare_fusion  
Then change the section :
   v.name  
by this :
   v.vmx['displayName']    
saved and close the fileAccording to Vagrant official documentation https://docs.vagrantup.com/v2/vmware/boxes.html you can optimize the box a little bit before packaging it up like so, in a terminal type and from the /Users/your_name/myvagrant folder, type :
/Applications/VMware\ Fusion.app/Contents/Library/vmware-vdiskmanager -d box/box.vmwarevm/box-disk1.vmdk  
Ignore the message "VixDiskLib: Invalid configuration file parameter. Failed to read configuration file" it's not helping, in fact it has to do with some internal SSL-Verify that this command line tools does, but nothing impacting the real process of defragmenting the VM disk ( box-disk1.vmdk ).Again on your terminal type:
/Applications/VMware\ Fusion.app/Contents/Library/vmware-vdiskmanager -k box/box.vmwarevm/box-disk1.vmdk  
( notice the -k switch)Now is time to package the whole thing as an vagrant archive ( .box )Assuming you're on the terminal in the /Users/your_name/myvagrant folder, create the archive by typing:
   tar -cvzf sparkmoocFusion.box -C box/box.vmwarevm {metadata.json,nvram,box.vmx,box-disk1.vmdk,box.vmsd,box.vmxf}  
You'll get sparkmoocFusion.box in the current directoryYou now have a fully compatible and optimize version of the sparkmooc Ubuntu Vagrant VirtualBox but running with the VMware Fusion Vagrant provider.What you can maybe do now is, typing those vagrant command,in the terminal:
   vagrant box add sparkmooc/base sparkmoocFusion.box  
   vagrant box list  
(you should see something like in return : sparkmooc/base (vmware_fusion, 0))
   vagrant up  
( wait for the VM Box to boot ) here is the output of vagrant up command : /************************************************************************************** Bringing machine 'sparkvm' up with 'vmware_fusion' provider...==> sparkvm: Cloning VMware VM: 'sparkmooc/base'. This can take some time...==> sparkvm: Verifying vmnet devices are healthy...==> sparkvm: Preparing network adapters...==> sparkvm: Starting the VMware VM...==> sparkvm: Waiting for machine to boot. This may take a few minutes... sparkvm: SSH address: 192.168.157.203:22 sparkvm: SSH username: vagrant sparkvm: SSH auth method: private key sparkvm:  sparkvm: Vagrant insecure key detected. Vagrant will automatically replace sparkvm: this with a newly generated keypair for better security. sparkvm:  sparkvm: Inserting generated public key within guest... sparkvm: Removing insecure key from the guest if its present... sparkvm: Key inserted! Disconnecting and reconnecting using new SSH key...==> sparkvm: Machine booted and ready!==> sparkvm: Forwarding ports... sparkvm: -- 8001 => 8001 sparkvm: -- 4040 => 4040 sparkvm: -- 22 => 2222==> sparkvm: Setting hostname...==> sparkvm: Configuring network adapters within the VM...==> sparkvm: Waiting for HGFS kernel module to load...==> sparkvm: Enabling and configuring shared folders... sparkvm: -- /Users/roks/DataBricks/EdxBerkley/Sparkmooc/myvagrant: /vagrant /**************************************************************************************
   vagrant status  
( here is the output of status command ) : /****************************************************************** Current machine states:sparkvm running (vmware_fusion)The VM is running. To stop this VM, you can run `vagrant halt` to shut it down, or you can run `vagrant suspend` to simply suspend the virtual machine. In either case, to restart it again, run `vagrant up`. /******************************************************************Maybe this can be helpfull to any one out there, for whome using VirtualBox is not an option.Et Voila les amis.Enjoy !
 The vagrant machine includes Java 7, which does not support lambda functions. This makes using Java with Spark a little harder and a lot more verbose.

Also it seems that the SDK is not installed.

I know that this course will use Python but I wonder why the VM does not include Java 8.

Is it because Scala needs Java 7?  Would Scala work with Java 8 VM?

Just curious. I tried to search it at different places however, I am unable to find it. I can see that the due date to submit first Python notebook is 6th of June. Would be great if I get to know the same about lecture 1 and 2. Thank you. hi guys change to the custom directory in dos

plz help 


i am sorry its slove My mac has limited resource, and I don't want to use virtual machine. Can I install all the softwares needed in this course in os 10.9? Hi,

Can you please share the versions of various packages installed on the VM?
If you have raw unsupported documentation for setting up the VM it will be even better.
I intend to use the setup even after the course, also understanding the setup would be essential if we were to use these outside of the course. Here is a post by Werner Vogels (Amazon's CTO and incidentally one of the guy behind AWS)   http://www.allthingsdistributed.com/2008/12/eventually_consistent.html
and the little brother of that post:
http://www.allthingsdistributed.com/2010/02/strong_consistency_simpledb.html
The reference for distributed database (we could say big brother to those 2 posts pointed out by the first link, notice it's from berkeley... just like this class):
http://www.cs.berkeley.edu/~brewer/cs262b-2004/PODC-keynote.pdf

These gave me a very nice understanding of the constraint behind distributed data and the different trade-offs between the ACID (traditional databases) and BASE (distributed bound by the CAP theorem) models.

Hope this can be interesting to some. 
Here is how I got stuck in setting up the sparkvm. Is there any one who has the same problem? How can I solve it?
Thank you!! 

plz help While installing VirtualBox 4.3.28 on Windows 7, I get the message "Could not write value to key \Software\Classes\progId_VirtualBox.Shell.vbox\shell\open\command."  I'm running with administrator privilege, so have as much access as can be had.  When I look at the registry, I see the key in HKEY_LOCAL_MACHINE.  I have no idea what the appropriate value would be so I don't know if I can set it manually or not.  Has anyone seen this, or know what the value should be?
 As everything is already setup for us in a VM, I'm a bit curious about the architecture we would want to setup this in a "live", "production" environment.

I've looked (although very quickly) a bit on the web (and with the only somewhat correct search engine available to me in China). I've found 2 schemas:
The first using mongoDB as the data backend.The second using hadoop HDFS.
I am more used to "traditional" relational/object databases, but have enough understanding of mongoDB to sort of see what it could bring. On the other hand I have no knowledge (yet, I hope to have time to update myself) on Hadoop HDFS.

So here are my questions:
What is the current architecture of the application stack we have in the VM?What is the data back-end used (if any)? (and possibly, why this choice)What is the recommended architecture to make it scalable? (let's say I have enough money to have 1 VMs, but want to be able to go up to thousands of VMs as, for example, the business grows)

I realize while writing these questions that it might be addressed in the class, but the syllabus seems to focus more on the  data science/analysis part... 
I also realize going through the administration documentation of spark might answer those question, but as I am working full-time I would appreciate any pointer/ shortcut. :) As you see from the screen shots below,virtual machine is running.
But its not appearing in the virtual box.

Could you please help.






 Vagrant up command is not recognized in mac.Please find below

total 8 -rwxr-xr-x@ 1 xxxxxx staff 798 Jun 2 22:48 Vagrantfile drwxr-xr-x@ 5 xxxx staff 170 Jun 4 23:24 mooc-setup-master HareeshMini:myvagrant xxxxx$ pwd /Users/xxxxxx/myvagrant HareeshMini:myvagrant $ vagrant up -bash: vagrant: command not found HareeshMini:myvagrant $ If anybody has a previous version of vagrant installed on OSX and upgraded, while also having the vagrant-windows plugin installed, you may see the following error when you try to run:

$ vagrant up --provide=virtualbox
Vagrant failed to initialize at a very early stage:
The plugins failed to load properly. The error message given isshown below.
uninitialized constant Vagrant::Action::Builtin::MixinSyncedFolders

This is caused by the vagrant-windows plugin, which is no longer needed in newer versions of vagrant and uninstalling it resolves the issue:

$ vagrant plugin uninstall vagrant-windowsUninstalling the 'vagrant-windows' plugin...

$ vagrant up --provide=virtualbox
Bringing machine 'sparkvm' up with 'virtualbox' provider... I have installed Virtualbox and Vagrant on Mac
But when I try to run vagrant up --provider=virtualbox
I get this
The box 'sparkmooc/base2' could not be found or
could not be accessed in the remote catalog. If this is a private
box on HashiCorp's Atlas, please verify you're logged in via
`vagrant login`. Also, please double-check the name. The expanded
URL and error message are shown below:

URL: ["https://atlas.hashicorp.com/sparkmooc/base2"]
Error: Couldn't resolve host 'atlas.hashicorp.com'


Kindly help me out I am interested to setup our programming environment on cloud (precisely a virtual machine on Microsoft Azure Cloud)

From my intuition I would have to install ubuntu on a virtual machine and connect through remote desktop connection, but anything futher than that I a grey zone for me.

Here we had a custom made vm image ready which we downloaded and just started working with it, but I have no idea how to make it work on cloud. Would be more that happy to receive help.

Note: For all those who would ask that why do I need to setup the environment on cloud its simple -> Its scalable and most of the times at enterprise level we might have to do that only. And yes I fancy it :p

Anybody interested in such use cases can contact me, would love to work in collaboration :) Not sure if I did something during the set up, but I I have VM on my com and for some reason when I follow the cmd vagrant halt, I get back that I dont even have a Defuat VM machine. It says

"VM not created, Moving on..."

However, my VM manager says its running.

What is the reason for this? Play nicely now children: https://snap.stanford.edu/data/ Hi,

I am new to world of VMs and Vagrants. But followed the lecture video and installed Vagrant and VitrualBox. I did upload the lab0 result with no issues. when i run the VirtaulBox, i am able to see an emulator coming up and able to log in successfully. so what is next?

do i have to install anything on the sparkvm?

I am bit confused based on all these discussion happening in the forum.

Appreciate your response. Hi,

I am getting error while executing the command "vagrant up" in windows command prompt.Please find the error as mentioned below.

 I only see the video from the professor but could not find the quizzes. In assignment/ lab 1 there's a "question" which mentioned proper viewing of a math formula.I know there's no code to run for that two question. I just can't see the formulas properly.I tried to "run" that part (although I don't think that help) by the run button as well as shift+enter, doesn't work.I submitted the assignment and got full mark but I just can't view the Math formula, can anyone help? Hello guys,

I already have Ubuntu 14.04 running on VMWare Fusion. In it, I've already installed Spark 1.3.1, IPython Notebook and I can now interact with Spark from IPython Notebook.

When I run the notebook for the assignment, 1a passes, but most of the rest fails. From the error traces, it seems to me like there are scripts and other files in the provided virtual machine that the assignment notebook depends on.

For example, running 1b fails with the error:
Input path does not exist: file: .../data/cs100/lab1/shakespeare.txt


and running 2a produces this error: 
ImportError: No module named test_helper
 
among others.

Is there anyone who has had the same experience? I'd love any suggestions on how I can make these work on my custom virtual environment?

Thanks in advance.

 I having tons of issus with the setup for the past 3 days. I notice from looking here that most people have spark in their VM machine when running vagrant. I dont have that at all and I'm wondering as to why. I ran the first notebook without any error. However, I'm receiving the following error message upon submitting the downloaded lab0_student.py. Have I missed anything here?Compare with hash (2a)
----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 37, in main
    "execution_count": null,
NameError: global name 'null' is not defined

All tests passed
Compare lists (2b)
------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 1 cases passed (50.0%) -- Hi All

 I am not able to find master.zip file at the location of "Downloading and installing the virtual machine". Can anybody help me to define where is it located

-Regards
Pravin
 I have been trying to install the spark vm for nearly 7 times and each time it end up with this error
what is this error and how to resolve it, I am using ubuntu 14.04
 
Why? Connection refused? What should I do to solve this problem? VM successfully installed, Vagrant seemed ok but look at my prompt command errors, any help? 
Thanks!

Microsoft Windows [version 6.3.9600](c) 2013 Microsoft Corporation. Tous droits réservés.
C:\Users\RMoraglia>cd myvagrant
C:\Users\RMoraglia\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box sparkvm: Progress: 100% (Rate: 515k/s, Estimated time remaining: --:--:--)==> sparkvm: Successfully added box 'sparkmooc/base' (v0.0.7.1) for 'virtualbox'!==> sparkvm: Importing base box 'sparkmooc/base'...==> sparkvm: Matching MAC address for NAT networking...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Setting the name of the VM: sparkvmThe name of your virtual machine couldn't be set because VirtualBoxis reporting another VM with that name already exists. Most of thetime, this is because of an error with VirtualBox not cleaning upproperly. To fix this, verify that no VMs with that name do exist(by opening the VirtualBox GUI). If they don't, then look at thefolder in the error message from VirtualBox below and remove itif there isn't any information you need in there.
VirtualBox error:
VBoxManage.exe: error: Could not rename the directory 'C:\Users\RMoraglia\VirtualBox VMs\sparkvmbase_1433496332258_58189' to 'C:\Users\RMoraglia\VirtualBox VMs\sparkvm' to save the settings file (VERR_ALREADY_EXISTS)VBoxManage.exe: error: Details: code E_FAIL (0x80004005), component SessionMachine, interface IMachine, callee IUnknownVBoxManage.exe: error: Context: "SaveSettings()" at line 2788 of file VBoxManageModifyVM.cpp
C:\Users\RMoraglia\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Importing base box 'sparkmooc/base'...==> sparkvm: Matching MAC address for NAT networking...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Setting the name of the VM: sparkvmThe name of your virtual machine couldn't be set because VirtualBoxis reporting another VM with that name already exists. Most of thetime, this is because of an error with VirtualBox not cleaning upproperly. To fix this, verify that no VMs with that name do exist(by opening the VirtualBox GUI). If they don't, then look at thefolder in the error message from VirtualBox below and remove itif there isn't any information you need in there.
VirtualBox error:
VBoxManage.exe: error: Could not rename the directory 'C:\Users\RMoraglia\VirtualBox VMs\sparkvmbase_1433496388954_16737' to 'C:\Users\RMoraglia\VirtualBox VMs\sparkvm' to save the settings file (VERR_ALREADY_EXISTS)VBoxManage.exe: error: Details: code E_FAIL (0x80004005), component SessionMachine, interface IMachine, callee IUnknownVBoxManage.exe: error: Context: "SaveSettings()" at line 2788 of file VBoxManageModifyVM.cpp
C:\Users\RMoraglia\myvagrant> after insalling vm it runs and asks for login and password  what should be given it says hint "Num lock is on" DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO interface: detail: 8001 => 8001 (adapter 1)
 INFO interface: detail:     sparkvm: 8001 => 8001 (adapter 1)
    sparkvm: 8001 => 8001 (adapter 1)
 INFO interface: detail: 4040 => 4040 (adapter 1)
 INFO interface: detail:     sparkvm: 4040 => 4040 (adapter 1)
    sparkvm: 4040 => 4040 (adapter 1)
 INFO interface: detail: 22 => 2222 (adapter 1)
 INFO interface: detail:     sparkvm: 22 => 2222 (adapter 1)
    sparkvm: 22 => 2222 (adapter 1)
 INFO subprocess: Starting process: ["C:\\Program Files\\Oracle\\VirtualBox\\VBoxManage.exe", "modifyvm", "cd496676-4f85-4e15-b87a-44ee9a87e9da", "--natpf1", "tcp8001,tcp,,8001,,8001", "--natpf1", "tcp4040,tcp,,4040,,4040", "--natpf1", "ssh,tcp,127.0.0.1,2222,,22"]
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO warden: Calling IN action: #
 INFO warden: Calling IN action: #
 INFO subprocess: Starting process: ["C:\\Program Files\\Oracle\\VirtualBox\\VBoxManage.exe", "modifyvm", "cd496676-4f85-4e15-b87a-44ee9a87e9da", "--rtcuseutc", "on"]
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO sanedefaults: Automatically figuring out whether to enable/disable NAT DNS proxy...
 INFO subprocess: Starting process: ["C:\\Program Files\\Oracle\\VirtualBox\\VBoxManage.exe", "modifyvm", "cd496676-4f85-4e15-b87a-44ee9a87e9da", "--natdnsproxy1", "on"]
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO warden: Calling IN action: #
 INFO warden: Calling IN action: #
 INFO interface: info: Booting VM...
 INFO interface: info: ==> sparkvm: Booting VM...
==> sparkvm: Booting VM...
 INFO subprocess: Starting process: ["C:\\Program Files\\Oracle\\VirtualBox\\VBoxManage.exe", "startvm", "cd496676-4f85-4e15-b87a-44ee9a87e9da", "--type", "headless"]
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: Waiting for VM "cd496676-4f85-4e15-b87a-44ee9a87e9da" to power on...

VM "cd496676-4f85-4e15-b87a-44ee9a87e9da" has been successfully started.
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 31998
DEBUG subprocess: Exit status: 1
 INFO warden: Calling IN action: #
 INFO warden: Calling IN action: #
 INFO interface: output: Waiting for machine to boot. This may take a few minutes...
 INFO subprocess: Starting process: ["C:\\Program Files\\Oracle\\VirtualBox\\VBoxManage.exe", "showvminfo", "cd496676-4f85-4e15-b87a-44ee9a87e9da", "--machinereadable"]
 INFO interface: output: ==> sparkvm: Waiting for machine to boot. This may take a few minutes...
DEBUG subprocess: Selecting on IO
==> sparkvm: Waiting for machine to boot. This may take a few minutes...
DEBUG subprocess: stdout: name="sparkvm"

groups="/"

ostype="Ubuntu (32 bit)"

UUID="cd496676-4f85-4e15-b87a-44ee9a87e9da"

CfgFile="C:\\Users\\user\\VirtualBox VMs\\sparkvm\\sparkvm.vbox"

SnapFldr="C:\\Users\\user\\VirtualBox VMs\\sparkvm\\Snapshots"

LogFldr="C:\\Users\\user\\VirtualBox VMs\\sparkvm\\Logs"

hardwareuuid="cd496676-4f85-4e15-b87a-44ee9a87e9da"

memory=1468

pagefusion="off"

vram=12

cpuexecutioncap=100

hpet="off"

chipset="piix3"

firmware="BIOS"

cpus=1

pae="on"

longmode="off"

synthcpu="off"

bootmenu="messageandmenu"

boot1="disk"

boot2="none"

boot3="none"

boot4="none"

acpi="on"

ioapic="off"

biossystemtimeoffset=0

rtcuseutc="on"

hwvirtex="on"

nestedpaging="on"

largepages="off"

vtxvpid="on"

vtxux="on"

VMState="poweroff"

VMStateChangeTime="2015-05-30T08:09:30.000000000"

monitorcount=1

accelerate3d="off"

accelerate2dvideo="off"

teleporterenabled="off"

teleporterport=0

teleporteraddress=""

teleporterpassword=""

tracing-enabled="off"

tracing-allow-vm-access="off"

tracing-config=""

autostart-enabled="off"

autostart-delay=0

defaultfrontend=""

storagecontrollername0="SATAController"

storagecontrollertype0="IntelAhci"

storagecontrollerinstance0="0"

storagecontrollermaxportcount0="30"

storagecontrollerportcount0="1"

storagecontrollerbootable0="on"

"SATAController-0-0"="C:\Users\user\VirtualBox VMs\sparkvm\box-disk1.vmdk"

"SATAController-ImageUUID-0-0"="cd633bd6-5317-47ca-a319-07804e73b058"

natnet1="nat"

macaddress1="0800272244C6"

cableconnected1="on"

nic1="nat"

nictype1="82540EM"

nicspeed1="0"

mtu="0"

sockSnd="64"

sockRcv="64"

tcpWndSnd="64"

tcpWndRcv="64"

Forwarding(0)="ssh,tcp,127.0.0.1,2222,,22"

Forwarding(1)="tcp4040,tcp,,4040,,4040"

Forwarding(2)="tcp8001,tcp,,8001,,8001"

nic2="none"

nic3="none"

nic4="none"

nic5="none"

nic6="none"

nic7="none"

nic8="none"

hidpointing="ps2mouse"

hidkeyboard="ps2kbd"

uart1="off"

uart2="off"

lpt1="off"

lpt2="off"

audio="none"

clipboard="bidirectional"

draganddrop="disabled"

SessionType="headless"

vrde="off"

usb="off"

ehci="off"

SharedFolderNameMachineMapping1="vagrant"

SharedFolderPathMachineMapping1="C:/Users/user/myvagrant"

VRDEActiveConnection="off"

VRDEClients=0

vcpenabled="off"

vcpscreens=0

vcpfile="C:\Users\user\VirtualBox VMs\sparkvm\sparkvm.webm"

vcpwidth=1024

vcpheight=768

vcprate=512

vcpfps=25

GuestMemoryBalloon=0
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 31999
DEBUG subprocess: Exit status: 0
ERROR warden: Error occurred: The guest machine entered an invalid state while waiting for it
to boot. Valid states are 'starting, running'. The machine is in the
'poweroff' state. Please verify everything is configured
properly and try again.

If the provider you're using has a GUI that comes with it,
it is often helpful to open that and watch the machine, since the
GUI often has more helpful error messages than Vagrant can retrieve.
For example, if you're using VirtualBox, run `vagrant up` while the
VirtualBox GUI is open.
 INFO warden: Beginning recovery process...
 INFO warden: Calling recover: #
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO environment: Released process lock: fpcollision
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
ERROR warden: Error occurred: The guest machine entered an invalid state while waiting for it
to boot. Valid states are 'starting, running'. The machine is in the
'poweroff' state. Please verify everything is configured
properly and try again.

If the provider you're using has a GUI that comes with it,
it is often helpful to open that and watch the machine, since the
GUI often has more helpful error messages than Vagrant can retrieve.
For example, if you're using VirtualBox, run `vagrant up` while the
VirtualBox GUI is open.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
ERROR warden: Error occurred: The guest machine entered an invalid state while waiting for it
to boot. Valid states are 'starting, running'. The machine is in the
'poweroff' state. Please verify everything is configured
properly and try again. can we do this setup in ubuntu 14 .04 without this vm As weird as it can sounds, and as you can see on the pic at the bottom of this message, my oracle VM virtual box displays the sparkvm off, BUT my vagrant is up, weird?

I copied the text in the prompt command box as well.

Thanks for your help.




Microsoft Windows [version 6.3.9600](c) 2013 Microsoft Corporation. Tous droits réservés.
C:\Users\RMoraglia>cd myvagrant
C:\Users\RMoraglia\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Clearing any previously set forwarded ports...==> sparkvm: Clearing any previously set network interfaces...==> sparkvm: Preparing network interfaces based on configuration... sparkvm: Adapter 1: nat==> sparkvm: Forwarding ports... sparkvm: 8001 => 8001 (adapter 1) sparkvm: 4040 => 4040 (adapter 1) sparkvm: 22 => 2222 (adapter 1)==> sparkvm: Booting VM...==> sparkvm: Waiting for machine to boot. This may take a few minutes... sparkvm: SSH address: 127.0.0.1:2222 sparkvm: SSH username: vagrant sparkvm: SSH auth method: private key sparkvm: Warning: Connection timeout. Retrying...==> sparkvm: Machine booted and ready!==> sparkvm: Checking for guest additions in VM...==> sparkvm: Setting hostname...==> sparkvm: Mounting shared folders... sparkvm: /vagrant => C:/Users/RMoraglia/myvagrant==> sparkvm: Machine already provisioned. Run `vagrant provision` or use the `--provision`==> sparkvm: to force provisioning. Provisioners marked to run always will still run.
C:\Users\RMoraglia\myvagrant>
 Can somebody please list the steps to be performed(commands in cmd) after downloading the virtualbox.box externally to get my vigrant and vm up and running. Could one install Python 3.4 in the VM? or do you have a VM with Python 3.4, please? Since Python 2.7 is a bit old ...

Thanks a lot. when  ever i try to access http://127.0.0.1:8001  
it shows unable to connect, i use ubuntu 14.04

how to acess it i tried the url after installing the vm
i was able to open the shakespeare.txt file in the vm but i was not able to connect to  http://127.0.01:8001
it always shows problem loading page
  cannt open sparkkvm by vagrant command, successfully open by virtualBox GUI and Jupyter works normally.
I remember after installing the sparkkvm successfully, faild by enter command "vagrant up" in "myvagrant" dir.

afterwards, I type the vagrant command (detailed  vagrant command I forgot) "remove" sparkkvm, but sparkkvm still can work(open) by virtualBox GUI and Jupyter works normally.

Now, when I open the virtualBox(only open virtualBox, dont run sparkkvm), type the comand "vagrant up", it shows that(below):
 I finished the doing two quizzes and I would like to do the Ipython again ( I really did not pay attention to what I was doing) .

For the life of me, I can not find the instructions for the test that, I run inside IPython and then transfer it's results to the AutoGrader.

Can someone help me find my missing quiz? Which one should I download for windows 7- 32 bit?

 After seeing all the posts/solutions, and still getting the "Power off" status,I am posting my problem.
Kindly see the screen shots.



In the command line interface, logged in as Admin, I see the following"
C:\Users\D.K.verma\Desktop\marco>vagrant upBringing machine 'default' up with 'virtualbox' provider...==> default: Box 'base' could not be found. Attempting to find and install...    default: Box Provider: virtualbox    default: Box Version: >= 0==> default: Adding box 'base' (v0) for provider: virtualbox    default: Downloading: base    default:An error occurred while downloading the remote file. The errormessage, if any, is reproduced below. Please fix this error and tryagain.
Couldn't open file /Users/D.K.verma/Desktop/marco/base
C:\Users\D.K.verma\Desktop\marco>"
Kindly suggest.Thanks in advance.

Amit Hi

I have been reading the posts referring to the uploading of Lab0, and they refer to an "autograder page", where is this located? I have completed the assignment, saved to a .py file and now just need to upload

Many thanks

 I got this error when I first fire "vagrant up --provider=virtualbox":

Bringing machine 'sparkvm' up with 'virtualbox' provider...
==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and ins
tall...
    sparkvm: Box Provider: virtualbox
    sparkvm: Box Version: >= 0
The box 'sparkmooc/base' could not be found or
could not be accessed in the remote catalog. If this is a private
box on HashiCorp's Atlas, please verify you're logged in via
`vagrant login`. Also, please double-check the name. The expanded
URL and error message are shown below:

URL: ["https://atlas.hashicorp.com/sparkmooc/base"]
Error: Failed connect to atlas.hashicorp.com:443; No error
What must I do? Hi,

If I start sparkvm directly from VirtualBox, it asks me for a username and password. 

Can someone please help me out with the credentials?


Thanks
Prashant I see that the first quiz was submitting your lab setup and test.  The 2nd lecture shows quiz under it, but when I got to the last video, there was no quiz or anything to submit.  Am I missing anything?
Now looking at the progress bar, it seems I only submitted the lab, so I am missing both quizzes, How do you access those? "Each week's lab exercise will be due the following Friday at 00:00 UTC."

Does this mean midnight Friday -> Saturday or midnight Thursday - Friday?

If the former: Perhaps label it as "Friday at 23:59" to avoid confusions. Time zone differences are confusing as it is!

Wbr
Mattias Hi, is it possible to know how many students registered for this course ?
looking at the forum attendance, it seems to be a big number. That's great !  downloaded virtualbox and vagrant and spent the next 8 hours trying to remove all the adware. will not be continuing with any course. Hi everyone,

I think I have completed all week 1 assignments but is there a quick index page that just lets me know I didn't miss anything?

Thanks,

Brian i just can't see sparkvm on Virtualbox even if everything was successful Ubuntu. Cant find any problem. Did any one faced same problem? The software download process, first lecture, quizzes went smoothly. I'm using mac yosemite. I've run my notebook successfully. When I try to save it as a .py file as instructed, Safari saves it as lab0_student.py.html

Should I just remove the ".html" extension and try to submit to the autograder? Does Chrome or Firefox work better?

TIA

Randy Hawkins In the following tutorial link:

Step #1 says:

"Create a custom directory (e.g., for windows users c:\users\marco\myvagrant or for Mac/Linux users /home/marco/myvagrant)"

For Mac the actual home directory is /Users/marco/myvagrant , with marco subsituted for your computer name.

You can just easily type in your terminal:  "mkdir ~/myvagrant" and it will do it automatically.   Hi I would like to go go for verified certificate I was wondering if passport is valid proof of identity. Please guide. Thanks. Did anybody get the point for the following quiz? It is the what is data science lecture which is the second lecture of lecture 2. Thanks!
Data Science Danger Zone
(1 point possible) Hi,

we should have a wiki for posting most informative articles around, like the one telling about different free data sources.

Regards,
Gourav Just to fulfill my curiosity. As databricks offering a cloud version of spark and notebook environment. Why not create a single cluster to share with all the students? To have everyone register and become a user? Like the good old mainframe days, everyone gets an user account and connect from terminal (in today it would be browser)Thanks I am running the vagrant command from the directory where I have the Vagrantfile.

$ ~/myvagrant/mooc-setup-master $  vagrant up --provider=virtualbox
Bringing machine 'sparkvm' up with 'virtualbox' provider...
==> sparkvm: Checking if box 'sparkmooc/base' is up to date...
==> sparkvm: Clearing any previously set forwarded ports...
==> sparkvm: Clearing any previously set network interfaces...
==> sparkvm: Preparing network interfaces based on configuration...
    sparkvm: Adapter 1: nat
==> sparkvm: Forwarding ports...
    sparkvm: 8001 => 8001 (adapter 1)
    sparkvm: 4040 => 4040 (adapter 1)
    sparkvm: 22 => 2222 (adapter 1)
==> sparkvm: Booting VM...
==> sparkvm: Waiting for machine to boot. This may take a few minutes...
    sparkvm: SSH address: 127.0.0.1:2222
    sparkvm: SSH username: vagrant
    sparkvm: SSH auth method: private key
    sparkvm: Warning: Connection timeout. Retrying...
==> sparkvm: Machine booted and ready!
==> sparkvm: Checking for guest additions in VM...
==> sparkvm: Setting hostname...
==> sparkvm: Mounting shared folders...
    sparkvm: /vagrant => /Users/gchak207/myvagrant/mooc-setup-master
==> sparkvm: Machine already provisioned. Run `vagrant provision` or use the `--provision`
==> sparkvm: to force provisioning. Provisioners marked to run always will still run.

$ ~/myvagrant/mooc-setup-master $  ls -altr
total 40
-rwxr-xr-x@ 1 gchak207  staff  8321 Jun  2 22:48 lab0_student.ipynb
-rwxr-xr-x@ 1 gchak207  staff   798 Jun  2 22:48 Vagrantfile
-rwxr-xr-x@ 1 gchak207  staff    59 Jun  2 22:48 README.md
drwxr-xr-x  3 gchak207  staff   102 Jun  5 11:53 ..
drwxr-xr-x+ 3 gchak207  staff   102 Jun  5 11:54 .vagrant
drwxr-xr-x@ 6 gchak207  staff   204 Jun  5 11:54 .

$ ~/myvagrant/mooc-setup-master $  ls .vagrant/machines/sparkvm/virtualbox/
action_provision  action_set_name   id                index_uuid        private_key       synced_folders 


When I start virtual box application, t seems to be running the VM. What am I doing wrong ?

 There have been instances of this problem reported by others too but i am yet to find a solution. After downloading vagrant is running as told by 'vagrant status'. Also, localhost:8001/tree is visible but why is it not listing in the virtual box.
 Are tools like FluentD or Logstash ETL tools?

I use them with ElasticSearch or Splunk for exploring logs... ===> Below is the log when I ran "vagrant up". 

E:\myvagrant>vagrant up
Bringing machine 'sparkvm' up with 'virtualbox' provider...
==> sparkvm: Checking if box 'sparkmooc/base' is up to date...
==> sparkvm: Clearing any previously set forwarded ports...
==> sparkvm: Clearing any previously set network interfaces...
==> sparkvm: Preparing network interfaces based on configuration...
    sparkvm: Adapter 1: nat
==> sparkvm: Forwarding ports...
    sparkvm: 8001 => 8001 (adapter 1)
    sparkvm: 4040 => 4040 (adapter 1)
    sparkvm: 22 => 2222 (adapter 1)
==> sparkvm: Booting VM...
==> sparkvm: Waiting for machine to boot. This may take a few minutes...
The guest machine entered an invalid state while waiting for it
to boot. Valid states are 'starting, running'. The machine is in the
'poweroff' state. Please verify everything is configured
properly and try again.
 
If the provider you're using has a GUI that comes with it,
it is often helpful to open that and watch the machine, since the
GUI often has more helpful error messages than Vagrant can retrieve.
For example, if you're using VirtualBox, run `vagrant up` while the
VirtualBox GUI is open.

===> Then I tried to start sparkvm from VirtualBox and got the following error message:
Failed to open a session for the virtual machine sparkvm.
Unable to load R3 module C:\Program Files\Oracle\VirtualBox/VBoxDD.DLL (VBoxDD): GetLastError=1790 (VERR_UNRESOLVED_ERROR).

Result Code: 

E_FAIL (0x80004005)

Component: 

Console

Interface: 

IConsole {8ab7c520-2442-4b66-8d74-4ff1e195d2b6}

 
 Hi:

I have windows laptop used for work.  I have already installed VMPlayer hosting Linux as my work station.  I installed Virtual box and vagrant up installing SparkVM and submitted my assignment yesterday.   I shutdown my laptop.   I restarted my laptop today.  I couldn't get back in my VMPlayer.   I have to work.   I might have to vagrant destroy and find another box to accommodate.  The problem is that I will travel for work with this laptop.   I need to make sure I still am able to do my homework.  Any other solution?

Thanks. Hi, I am a bit curious, since there are other "real time distributed computation system" with the much more recent one's like "Apache Storm" and the old hog like "Mapreduce" why haven't we've utilized these ?. Is it because Apache Spark is a "data parallel, batch processing engine" if so why not a "task parallel continuous computational engine" like Apache storm" ? i run vagrant correctly but when i shutdown my pc i could not run it again and i found the error message :
VBoxManage.exe: error: Could not rename the directory 'C:\Users\sallam\VirtualBox VMs\sparkvmbase_1433513411577_17680' to 'C:\Users\sallam\VirtualBox VMs\sparkvm' to save the settings file (VERR_ALREADY_EXISTS)VBoxManage.exe: error: Details: code E_FAIL (0x80004005), component SessionMachine, interface IMachine, callee IUnknownVBoxManage.exe: error: Context: "SaveSettings()" at line 2788 of file VBoxManageModifyVM.cpp
 As i saw in the lecture, Cloud Big Data services should be cheaper than on-premise Big Data.

What is the best hardware for that? it is better to have a bunch of old-cheap servers in a cluster or top hardware for better performance?
What kind of servers are more use, by example, on EC2?

I guess that in Big Data doesnt matter to have latency in results, not like on real-time web applications.

also, is it Power8 architecture better architecture for Big Data?
 When I run vagrant up, it seems to be downloading to my main drive (the / mount on Linux), but unfortunately I don't have enough space there. I need it to download to a different location with suitable free space.

Is this something I can control? Just to check if the problem of vm not being listed in virtual box could be overcome by halting and starting the vagrant. I did a vagrant halt and then re-started the virtual box and then did vagrant up, and voila its downloading the entire stuff again. Say 90min of download again... Why so? Hello all

when is the last date before which we can pay for verified certificate track ?
 my system is window7 X64. I have downloaded the latest version of virtual box and vagrant. After install I got the fellowing problem.
I have shut down the window firewall. But it still didn't work
Anyone knew how to solve the problem 

Bringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box sparkvm: Progress: 100% (Rate: 1028k/s, Estimated time remaining: --:--:--)==> sparkvm: Successfully added box 'sparkmooc/base' (v0.0.7.1) for 'virtualbox'!==> sparkvm: Importing base box 'sparkmooc/base'...==> sparkvm: Matching MAC address for NAT networking...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Setting the name of the VM: sparkvm==> sparkvm: Clearing any previously set network interfaces...==> sparkvm: Preparing network interfaces based on configuration... sparkvm: Adapter 1: nat==> sparkvm: Forwarding ports... sparkvm: 8001 => 8001 (adapter 1) sparkvm: 4040 => 4040 (adapter 1) sparkvm: 22 => 2222 (adapter 1)==> sparkvm: Booting VM...==> sparkvm: Waiting for machine to boot. This may take a few minutes... sparkvm: SSH address: 127.0.0.1:2222 sparkvm: SSH username: vagrant sparkvm: SSH auth method: private key sparkvm: Warning: Connection refused. Retrying...


 The Progress tab displays status of each activity. My progress for Lecture 1, Lecture 2 and Optional is correct. For Setting up the Course Software Environment thou, I see 0 complete which is incorrect. I have installed both VirtualBox, Vigrant and the VM successfully. I watched parts of those videos as well. How does % complete work for this activity?
 I completed the setup and the quizzes but the progress page shows my grade as 20% and some missing grades for labs. Is that expected? Answer is @410 As example for lecture 1 and 2, what is deadline if any?
Thanks,
 Hello,
From the example notebook I have changed the range from 100,000 to 10,000,000 to see how spark utilizes the CPU.  On my host machine i have installed htop (a linux utility that shows the utilization of each core, including hyperthreading). To my surprise i saw that only one core is utilized.

Then I remembered that maybe virtualbox had been configured to run only on one CPU. The vagrantfile does not have any configuration regarding to cpu.

I wonder if we had installed the same setup on real hardware, would all the CPU's be utilized?

PS. There is a spark_notebook.py in virtual machine home directory, which sets the  pyspark configuration with stuff like executors and etc. which I don't fully understand at the moment. This file is used as a boot time shell script which starts up both the python notebook environment and the spark.  Dear instructors and comrades,
Please, I need your help! I could not finish the last step of installation process. After typing "vagrant up" command I received the answer(image below) which look like the list with paths to the Compatibility Errors. I want to follow this course but I don't know how to fix this problem. I guess the problem is in my OS (Windows 7, x64). Perhaps there is another vagrant setup for windows or you know how to help me.
I would appreciate you for any help, advices, propositions etc.

 http://mobile.reuters.com/article/idUSKBN0OL0BG20150605?irpc=932 I have unzipped the master zip file but when I select the lab0 notebook for uploading nothing happens.  The jupyter page shows the same files as before.  Here is a screen capture.
 The first week has been great. I've finished with 0 issues in the material and the Env. installation.
Also, the optional reading material are very intresting, Although the articles tend to be very long.
the only comment I have is that the week has been very light and took almost no time.
I wait for and hope that the next 4 weeks will be more challenging and more full of new information and knowledge.
Thanks for the awesome course.
 Hi,

I would like to know if a quiz has a deadline, say Sunday ?

What I know from the course info is that I can repeat the videos and quizzes as many times as I want.

But I see a small clock icon beside each lecture and its quiz, that makes me think of the deadline for quiz.

Any clarification will be great.

Thank you.

 I got everything set up and I submitted the file to the autograder but it keeps saying "You submitted as an iPython notebook, please save as .py and try again."

I navigated to download as..Python (.py) in Jupyter and got a python file (.py) out. I verified that it is valid python in my text editor. But yet the grader will not accept it. What should I do? When I write command vagrant up this error comes up:

The guest machine entered an invalid state while waiting for it
to boot. Valid states are 'starting, running'. The machine is in the
'poweroff' state. Please verify everything is configured
properly and try again.

If the provider you're using has a GUI that comes with it,
it is often helpful to open that and watch the machine, since the
GUI often has more helpful error messages than Vagrant can retrieve.
For example, if you're using VirtualBox, run `vagrant up` while the
VirtualBox GUI is open.

Failed to open a session for the virtual machine sparkvm.

Failed to load VMMR0.r0 (VERR_SUPLIB_WORLD_WRITABLE).
Result Code:NS_ERROR_FAILURE (0x80004005)Component:ConsoleInterface:IConsole {db7ab4ca-2a3f-4183-9243-c1208da92392}

My virtual box is up to date and open in the process and there was no error downloading the image but the image seen as powered off in GUI.. I am using MAC OS X Yosemite 10.10.3 
 I downloaded the file as a .py file and clicked on Choose File, Open, and Check as instructed. The Choose File text said explicitly that I had submitted a .py file. I keep being told that I have submitted a py notebook file. Please help.   Can Vagrant run on VMWare player - why not - Instructions specifically asked for VirtualBox?

If you are issuing the "vagrant up" there is no need to start the VirtualBox right? Because we have travel plans - it will help to manage the time better.
Thanks I am interested in seeing Architectural diagrams relating Spark assignments and/or Spark in general. Thanks Hi Folks,

I am new to the Piazza overflow forums :). While filling the Databricks Cloud Access questionnaire for selected students of this class (??!!) I am trying to figure out my Piazza access code I have no idea. I tried the Activation Code or my email id to login to Piazza it didn't work! Please help me out in finding the "access code" to get a Spark on Cloud (Thunder!!)

Happy Weekend! HI all,
I believe it would help some people to be able to customize vagrant to install and use custom directories for vm setup as default state is pretty messy (at least for me):
1.I created custom directory in V:\MyVagrant and did all the necessary setup as dictated on the web.
2. Unfortunately, vagrant up --provider=virtualbox created c:\Users\xxx\.vagrantd where it started downloading the VM ..since I do not have enough space on C drive, I did several cycles of tests before I found out where the it was really downloading and therefore why it crashed it the first place.
3. So I needed to check the vagrant website and find out, that vagrant reads VAGRANT_HOME env variable to set the working directory (defaults to home directory\.vagrantd). After this I was able to finally go through install process but find out that actual VM was installed in V:\sparkvm, not in vagrant directory. Oh man.

I will not go and relocate the vm, but I dont like how the process goes in terms of placement of various stuff. Please at least document it.

On the other hand, process itself gone smoothly and setup task is already submitted :o)  If we fill out the form, win the lottery, and get free access to Databricks Cloud, how long can we keep this free access? I know only a little about Apache Spark so far, but I anticipate using it for my own programming projects after the course is over, so I want to learn how to do things with the tools that will be available to me for an indefinite period. Here is one piece of news appear in our San Francisco local paper. A programmer plotted the Google shuttle bus stop and new restaurants permit overlay on a map. It saw a overlapping pattern. Bingo the conclusion is Google bus brings affluence to a neighborhood. The newspaper wrote a story about this. This is actually a criticism to Google causing a social problem of rising prices.



http://www.sfgate.com/bayarea/article/Where-tech-buses-roam-affluence-follows-5217788.php

So I ran another analysis to overlap Google bus stop with violent crime report. Sure enough there is a pattern. So is Google bus causing violent crime to happen? No actually I deliberately selected the crime data from years precede the bus. So maybe it is crimes that cause Google to launch shuttle bus???



What happens is both maps are simply showing a pre-existing urban pattern. The dense urban core has more of everything, restaurants, bus stop, crime, are all concentrated there.

The paper do not do anything with my rebuttal. So I posted it in my own blog.
http://tungwaiyip.info/blog/2014/02/09/tech_bus_stop_relate_to_violent_crime  Hi,
I get error messages as generated by the python code while running part 1. all other parts work fine. How do i make the first part run correctly? http://gizmodo.com/use-google-searches-to-figure-out-how-racist-your-neigh-1709200937?utm_source=recirculation&utm_medium=recirculation&utm_campaign=fridayPM The form emailed to participants for opting into the Databricks Cloud offer has an error in the timezone input field - it only accepts whole hour offsets.

My timezone is UTC +5:30, but I was unable to input that, because the form would only accept 5. So "5" is what I used in my submission, and then a moment later I realized that "6" would work better for me because (geographically speaking) I live very close to the UTC +6:00 timezone. So I clicked on the "Edit your submission" link, changed timezone to "6", and submitted again.

It was only later that I realized the last line of the mail had a draconian policy on duplicate submissions. 

I am hoping that the powers that be will have the good sense to just accept the last entry of any duplicate submissions. 

 It finished downloading and it says The executable 'bsdtar' Vagrant is trying to run was not found in the %PATH% variable.This is an error. Verify that software is installed on path.

Here is error image : http://i.imgur.com/NibsHT8.png  After downloading and installing both vagrant and VM virtual box i faced this error when i was trying to configure them: what is the access code for this piazza? 
Hi,

Which command prompt should I use for installing virtual machine? And this warning appeared? I never used command prompt before. Please help me with access code or please let me know how do I get it? Thanks. i think using virtual box is really easy compare to vargant. Is it not possible to provide virtual box ?

Thanks
 I am in China, I don't have a good internet connection with Amazon service, it always failed with some errors when I run 'vagrant up'. Is there any other way to get that vm image and run it?  

Hi,

Which command prompt should I use for installing virtual machine? And this warning appeared? I never used command prompt before.

OS System:


 I've got an estimated slow download of vagrant; over 6 hours since the last hour. Is it possible to do this in part? Because I probably can't wait for 6 hours now and some setting up deadline was coming up soon. Is it possible to stop and continue where I left off later on? I noticed that it was expected that we use OS Window 7 or later. I have run Oracle VM on Vista before so I am not sure if I will have any problem working with the assignment? Is the any alternative for  running Apache Spark without a VM for this class? I have downloaded package.box using download manager, from http://bnrg.cs.berkeley.edu/~adj/package.box.
How can I setup vm using this downloaded file?
thanks It seems I am encountering a known vagrant bug, that prevents me from using vagrant in combination with virtualbox.
See following link for details: https://github.com/mitchellh/vagrant/issues/5309
I receive an error when trying the "vagrant up"  command, mentioning vagrant cannot determine the recommended name of the VM (see screenshot).
The error message suggest reinstalling virtualbox. Which I tried (multiple times) but to no avail.
I am using Windows8.1, and the latest versions of virtualbox, vagrant and Vagrantfile.

Please advise.




 I would like to enroll now, but i see that the first deadline has just passed today.
There is some kind of grace period, so i want to understand, can i enroll for verified certificate now, or i can't since i have missed the first deadline. (I do not want to throw 50 dollars out of the window)

Thanks,
Anatolii I am able to successfully run my Spark VM and open the localhost from the web browser, however the code is not running when I press the play button as it is shown in the video. What is the problem? I though that there might be a glitch in the virtual machine so I reinstalled it after uninstalling the older one but the problem still persists. The code is not running. :( What to do about it? So i was watching lecture 1 and i watched that video but i can't understand with the help of the data how can the campaign predict the results? I noticed in the course syllabus
that in week 5 we will have only
lab 4 without any video lectures.

Please, say, is it actually the case or 
lecture from week 4 about machine learning
should be in week 5 and just mistakenly was placed
in week's 4 material?

Thanks in advance for clarification! Hi All 
i am trying to run lab 1 
the page says that i am not connected to the Kernel , 
i have the vm up and running according to the initial set up 

please help Hi,

I am unable to download the virtual machine. seeing the below error continuously even though the virtual box is running in administrator mode..
Can anyone please help me in installing the virtual machine?




 OS: os x yosemite 10.10.1
virtual box 4.3.4

Followed video instructions, but when executing "vagrant up" I get stuck in the following line:
$ cd myvagrant/$ vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Clearing any previously set forwarded ports...==> sparkvm: Clearing any previously set network interfaces...==> sparkvm: Preparing network interfaces based on configuration...    sparkvm: Adapter 1: nat==> sparkvm: Forwarding ports...    sparkvm: 8001 => 8001 (adapter 1)    sparkvm: 4040 => 4040 (adapter 1)    sparkvm: 22 => 2222 (adapter 1)==> sparkvm: Booting VM...

Any help? Hi,

I was running the "vagrant up" command and I'm getting the following issue. I can see this is because my directory name is "Users\ma-" which creating the issue. Can anybody suggests me a work-around this problem.


Thanks & Regards Hi Everyone,

While installing the virtual machine, I am seeing the below error. Can anyone tell me how to fix it?

Note:- I am installing vm in E drive - E:\madhusudhan
 When I load for the first time lab0_student I get this error;

Failed to retrieve MathJax from 'https://cdn.mathjax.org/mathjax/latest/MathJax.js'


Math/LaTeX rendering will be disabled.
If you have administrative access to the notebook server and a working internet connection, you can install a local copy of MathJax for offline use with the following command on the server at a Python or IPython prompt:
>>> from IPython.external import mathjax; mathjax.install_mathjax()
This will try to install MathJax into the IPython source directory.
If IPython is installed to a location that requires administrative privileges to write, you will need to make this call as an administrator, via 'sudo'.
When you start the notebook server, you can instruct it to disable MathJax support altogether:
$ ipython notebook --no-mathjax
which will prevent this dialog from appearing.

 i already got VMware installed on my PC can i use it on the place of oracle Virtual BOx
 Hello Everyone,

After enterning the vagrant up --provider=virtualbox command, I am seeing the below error in command prompt


And when i tried to power-on the sparkvm from the virtual box, i am seeing the below error. Can you please help me in this...
 After installing sparkvm through virtualbox and vagrant, I can log in the sparkvm (I know we can just use notebook for the course, but I'm curious to see what's in the vm. ). However, I can't change the resolution of the screen and the screen size is so small. Can someone help me to increase the screen resolution?

I tried sudo apt-get install virtualbox-guest-dkms, but the package is already installed. Hi ,

Could anyone help me?
When I am selecting the  option - 2 & 4 and checking , its showing wrong answer,though this is the correct answer . 

Is this issue happening for all?

Thanks,
Ashis Hi, my Ubuntu Spark VM doesn't show up in my Virtual Box Manager.

I got ipython up in my browser, ran all tests and submitted my assignment successfully.

Here are the details from my "Vagrant Up" command, after install:




Here are the details from my  vagrant up --provider=virtualbox  command i.e. install VM:



Virtual Box Manager (Spark Ubuntu VM not shown)



See no Spark Ubuntu VM ;(

many thanks!

TJ


 Hi, 
just has been released the week 2 lectures and the youtube video does not work, is just keep loading ... Anybody has this issue ? is a way to be fixed ?


Thank you, 
Sergiu B Hi Friends,

Unable to complete the installation. Request your inputs to get this issue fixed asap. getting the below error.


 Hi, I tried to download the video about "SPARK KEY-VALUE RDDS" but I couldn't find the link to download it.

I tried to do that using the browsers Iceweasel 31.7.0 and chromium 43.0.2357.65 on Ubuntu Jessie (8.0), but I can't see the link on both browsers.
Here is the print screen of the page I see.

Tanks in advance.

[]s,
Jackson

 Hi All,

Where/How do we access Lab ? 

Do we have the Lab for 1st Week , I could find only few video and quizzes.

Thanks in advance.

Vikas In some of the videos from lecture 3 was said that
it is much faster to use memory (Spark) than disk I/O (Hadoop).
Of course I agree with this, but I have a question: Does Spark
suffer from the network overhead as Hadoop does or it designed
so that all the data needed for processing appears to be local?

As I understand, only disk was replaced to memory but network
overhead remains similar to the Hadoop's one. Am I right? In Lecture 3, I suggest putting the MR quiz question after the MR lecture.

The very first question question on big data tools seems that it should have checkboxes instead of radio buttons, as well (many of those options seem to be true). Hi People,

when i entered the command "vagrant up" in the command prompt i am getting the below message

C:\HashiCorp\Vagrant>vagrant upBringing machine 'default' up with 'virtualbox' provider...==> default: Box 'base' could not be found. Attempting to find and install...    default: Box Provider: virtualbox    default: Box Version: >= 0==> default: Adding box 'base' (v0) for provider: virtualbox    default: Downloading: base    default:An error occurred while downloading the remote file. The errormessage, if any, is reproduced below. Please fix this error and tryagain.Couldn't open file /HashiCorp/Vagrant/baseC:\HashiCorp\Vagrant>
Plz help...

Thanks & Regards Appears to be in the wrong place (before the lecture, not after).

Also, would be better for those of us who want to follow along to make it clear that that is a PySpark shell in the lecture, not a spark-shell. I'm sorry if this seems stupid but when does the course end?
The exercises and labs can be submitted after their due date with 20% penalty until the course ends but what is this date? Is it July 3?... I've got a question pertaining to the how the parallelize function works under the hood. Say we've got a large file on the same machine as the driver and we want to count the words in that file, similar to the example discussed in the lecture.I understand that the file isn't actually partitioned until the first action, however, I've got two questions pertaining to when that first action is invoked.My first question is, how does the text file makes its way to the workers? Is its the driver responsibility to send the paritions to the workers, or does the driver tell workers where to find the file and instruct them to go get it? Does this behavior change if we use a distributed file system like hdfs?My second question is, how does spark know to partition the file on a word boundary? Say we're calling flat map, using "split" on white space, counting the words, then calling collect, before finally printing all of the words and their counts. (I'm going to assume the data set fits on the driver machine.)The naive approach, might be to just use file size, then divide it by the number of requested partitions. That doesn't seem right though. If a single word spans one of the dividing lines between two partitions, our workers might end counting portions of our word. I'm imagining one worker would count the part of the word that landed before the boundary, and another worker would count the second portion. To illustrate my example, say the word "behave" lands on a partition's dividing line (in my naive approach). "be" could end up in worker A, and "have" could end up in worker B. The correct answer would be to count the word "behave" as appearing one time, but the naive approach would count "be" appearing once and "have" appearing once. My hunch is spark is smarter than that. Would it be possible explain how that works in more detail? followed instruction for installation. started VirtualBox as admin. But failed the start vagrant. not sure what the error means. Thanks in advance.

Microsoft Windows [Version 6.1.7601]Copyright (c) 2009 Microsoft Corporation.  All rights reserved.
C:\Users\Yan>cd ../macro
C:\Users\macro>dir Volume in drive C has no label. Volume Serial Number is CA5D-3C54
 Directory of C:\Users\macro
06/01/2015  09:15 PM    <DIR>          .06/01/2015  09:15 PM    <DIR>          ..06/01/2015  09:16 PM    <DIR>          myvagrant               0 File(s)              0 bytes               3 Dir(s)  397,078,036,480 bytes free
C:\Users\macro>cd myvagrant
C:\Users\macro\myvagrant>vagrant upVagrant failed to initialize at a very early stage:
The directory Vagrant will use to store local environment-specificstate is not accessible. The directory specified as the local datadirectory must be both readable and writable for the user that isrunning Vagrant.
Local data directory: C:/Users/macro/myvagrant/.vagrant
 On page 7 of the handouts, it discusses several master parameters. I noticed that YARN is missing, which could be used to schedule jobs as well. Was it deliberately left out? I am not getting the option to save the file as a python (.py) file.
What to do? When I try to submit lab1.py I get this error :

There might be some problems in your code submission. Please contact with TAs for further support if you could not figure out the problems.
I tried to load a fresh notebook 3 times but without succss

Note: the 3 submissions are counted ( I have now 12 submission left ) although there are problems.

 (RE: Spark Accumulator video)

In the video, there is a call "blankLines += 1" inside "extractCallSigns()". What happens when I do something like "blankLines = rand()"? Will the driver take the last value set? I'm getting 927631 using split(' '). Any recommendations how to accomplish this task?
All other tests are OK. When Spark creates an RDD based on a file in HDFS, are there any optimizations done to attempt to collocate executors with the data they are processing?  When i type "Vagrant Up in

Bringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install...    sparkvm: Box Provider: virtualbox    sparkvm: Box Version: >= 0The box 'sparkmooc/base' could not be found orcould not be accessed in the remote catalog. If this is a privatebox on HashiCorp's Atlas, please verify you're logged in via`vagrant login`. Also, please double-check the name. The expandedURL and error message are shown below:URL: ["https://atlas.hashicorp.com/sparkmooc/base"]Error: Failed connect to atlas.hashicorp.com:443; No error Hello,

Is someone else having issues with the shakespeareWordCount test?
The rest of the lab is working correctly, but this test fails, I think because the flatMap function ignores the empty values, so no need to apply the filter blank values afterwards.
Should I not use the flatMap function?


Thanks,
Bogdan weird, since I can run the code fine, and I can do this for lab 0. I loved the first sessions and installation steps. It was pretty smooth without any trouble. I enjoyed the quizzes of the first week sessions. I am enjoying the course.I would suggest to put quizzes after each sessions. It's really helpful

Regards,
Somanath Nanda Hi,

I went thru the instructions to download and run the sparkvm and bring it up using vagrant. Everything is running fine and I am able to see the sparkvm registered and running inside Oracle Virtual Box.  I am also able to submit the lab0_student.py file, just fine.

However, I cannot see Ubuntu desktop !! Just out of curiosity, I am wondering why this is so ?  The other VMs which I run using Oracle Virtual Box (like Cloudera VM, Hortonworks VM etc), come up and show the Ubuntu's or Red Had Linux's desktop UI, from which I can open a Linux Terminal, Internet Browser, File Explorer etc.

However in the case of this sparkvm, although it is running fine and I can see it registered and running in Oracle Virtual Box, still I cannot see Ubuntu desktop !!  Is there a reason behind this ?

Thanks,
Gagan Singh I am not getting any output when i click on play button and in download as menu there is no .py option  










I'm trying to load the text file in part 4c of the lab, and I receive an error, as below.  Any clues?















---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-114-cbd912244396> in <module>()
      8                   .textFile(fileName, 8)
      9                   .map(removePunctuation))
---> 10 print '\n'.join(shakespeareRDD
     11                 .zipWithIndex()  # to (line, lineNum)
     12                 .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in zipWithIndex(self)
   1985         """
   1986         starts = [0]
-> 1987         if self.getNumPartitions() > 1:
   1988             nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
   1989             for i in range(len(nums) - 1):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in getNumPartitions(self)
    319         2
    320         """
--> 321         return self._jrdd.partitions().size()
    322 
    323     def filter(self, f):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _jrdd(self)
   2286         command = (self.func, profiler, self._prev_jrdd_deserializer,
   2287                    self._jrdd_deserializer)
-> 2288         pickled_cmd, bvars, env, includes = _prepare_for_python_RDD(self.ctx, command, self)
   2289         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(),
   2290                                              bytearray(pickled_cmd),

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _prepare_for_python_RDD(sc, command, obj)
   2204     # the serialized command will be compressed by broadcast
   2205     ser = CloudPickleSerializer()
-> 2206     pickled_command = ser.dumps(command)
   2207     if len(pickled_command) > (1 << 20):  # 1M
   2208         broadcast = sc.broadcast(pickled_command)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py in dumps(self, obj)
    409 
    410     def dumps(self, obj):
--> 411         return cloudpickle.dumps(obj, 2)
    412 
    413 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dumps(obj, protocol)
    814 
    815     cp = CloudPickler(file,protocol)
--> 816     cp.dump(obj)
    817 
    818     #print 'cloud dumped', str(obj), str(cp.modules)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dump(self, obj)
    131         self.inject_addons()
    132         try:
--> 133             return pickle.Pickler.dump(self, obj)
    134         except RuntimeError, e:
    135             if 'recursion' in e.args[0]:

/usr/lib/python2.7/pickle.pyc in dump(self, obj)
    222         if self.proto >= 2:
    223             self.write(PROTO + chr(self.proto))
--> 224         self.save(obj)
    225         self.write(STOP)
    226 

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    560         write(MARK)
    561         for element in obj:
--> 562             save(element)
    563 
    564         if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    634                 write(APPENDS)
    635             elif n:
--> 636                 save(tmp[0])
    637                 write(APPEND)
    638             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    679                 for k, v in tmp:
    680                     save(k)
--> 681                     save(v)
    682                 write(SETITEMS)
    683             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    304             reduce = getattr(obj, "__reduce_ex__", None)
    305             if reduce:
--> 306                 rv = reduce(self.proto)
    307             else:
    308                 reduce = getattr(obj, "__reduce__", None)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in __getnewargs__(self)
    242         # This method is called when attempting to pickle SparkContext, which is always an error:
    243         raise Exception(
--> 244             "It appears that you are attempting to reference SparkContext from a broadcast "
    245             "variable, action, or transforamtion. SparkContext can only be used on the driver, "
    246             "not in code that it run on workers. For more information, see SPARK-5063."

Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.



 When i run the vagrant up and then go to my localhost:8001...it shows a blank page with the home in the title..it doesn't show the jupyter web ui I like to watch the videos at 1.5x and find that I have to set it every time since it defaults to 1x when I start a new video.  The notebook appears to look at a relative pathname for the text file, and I moved my notebooks into a "notebooks/" directory.

I suggest changing the notebook to use absolute paths, such as:
fileName = '/home/vagrant/data/cs100/lab1/shakespeare.txt' In lecture 4 there are few examples with sc.textFile("some filename", 4).

How to compute best partitions number fit to available cores/memory/data size in my cluster?

In pySpark source code: https://github.com/apache/spark/blob/e5054605994b8777e629c02fcbf8a5a6cbd0b0fe/python/pyspark/context.py#L407 default is None. Additional research shows me that almost everywhere in PySpark there are None, exclude PageRank example: https://github.com/apache/spark/blob/04e44b37cc04f62fbf9e08c7076349e0a4d12ea8/examples/src/main/python/pagerank.py#L61 (there are 1) Since, internet is discontinuous in my location , every time internet goes i have to restart the vagrant and it says "resuming from prior download " but starts from 0% again . I have even updated the master file even then problem is continuing. This is the 6th time i am trying , If this time it doesn't work I think I am going to leave this course all because of this vagrant. 

Please help I am not able to get as the piazza code is not valid.

Any pointers or hits to solve it ?
Thanks the question is:

Which of the following is not a property of Spark Actions?

  They cause Spark to execute the recipe to transform the source data    They are the primary mechanism for getting results out of Spark    They are lazily evaluated They are lazily evaluated - correct    The results are returned to the driverAren't Spark actions lazily evaluated? I don't understand why that is "not a property of Spark Actions"
 Hi,
Kyle Foster from Lab41 recently blogged on Using Docker to Build an IPython-driven Spark Deployment if you where interested in using Docker instead of Vagrant. What is the access code of this course for piazza?

Thanks, EDIT: Sorry, figured this out on my own. Missing parenthesis but I don't seem to be able to delete the question. I faced an issue with lab1 in the last task. All previous are pass. Some words are counted correctly, but some are not:
My results:
the: 27361and: 26028i: 20621to: 19108of: 17447a: 14592you: 13324my: 12481in: 10908that: 10846is: 9071not: 8347with: 7763for: 7548       -- should be word "me"it: 7437
Who faced with the same issue and how to resolve it? In Lecture 3, Spark Map Reduce Differences, Professor Joseph says: "Spark also supports lazy evaluation of the lineage graph, which leads to reduced wait states and better pipelining."  What is the lineage graph and what does this mean?

Thank you. Hi,

I am not able to pass the test corresponding to:

# TEST Words from lines (4d)
Test.assertEquals(shakespeareWordCount, 928908, 'incorrect value for shakespeareWordCount')
Can anyone confirm that this is indeed the correct count? It seems like some people are using the "lab1" label/folder for lab0 (checking that the setup is working) and using the "lab2" label/folder for Lab 1 due next Saturday.

Can people please use the "setup" label/folder for lab0 and the "lab1" label/folder for lab1?  Thanks!  Aren't word counts actually a bad example of the need for map/reduce? No matter how big the initial data set is, the size of the final result will be determined by the number of different words in that initial data set, and if the initial data set is all in English -- of any other particular language -- then there are only a few million words in that language. Hi,

I was wondering if you had references for the numbers that were quoted in the Big Data Examples slide.

Facebook's daily log: 60TB
1,000 genomes project: 200TB
Google Web index: 10+ PB
 My one humble request if there is anyway to download virtual machine file seperatly and add with vagrant offline. I means pre download virtual machine and then configure with vagrant. If possible then please let me know the process.
 
I have tried 3 times to download this way and all times it stopped in between so I could not complete this process and it is really very costly when using mobile internet data.  :(
thanks..



 There is something wrong with this quiz name IMPEDIMENTS TO COLLABORATION in the first week .That's the last week and second last question in the quiz. I tried to answer it correctly but is still showing wrong answers

Please help

Thanks Hi,

my code for lab1 passes every test properly and doesn't seem to have any performance issue but autograder replies:

There might be some problems in your code submission. Please contact with TAs for further support if you could not figure out the problems. I don't understand what the point of 4D is.

With two functions, I can already create an RDD where each element is a non-empty string. This actually yields the count that is expected in Part 4e directly, skipping whatever count is expected in 4D.  

Can someone provide some more examples of what exactly is being counted in 4D, because it sure isn't non-empty words.  Is it words and inter-word spaces?  Maybe we're not supposed to be using "len" in 4D, but rather filtering on some other criterion?

It would be nice if private-to-instructor questions were allowed.  I would like to post my code snippet, but obviously don't want to do that to the whole class.  I'm pretty confident that I know what I'm doing and am just confused by the intent of 4D, since all subsequent parts of my code passed all of the tests, and it's only 4D that I can't get the right count for... Like your table analogy with Databases and Data science. However having a little trouble with some of the interpretation.  Since I believe we can perform data science on not just massive data but smaller data sets.

Would it be correct to say and refine the comparison to show we are comparing relational database technologies ( & maybe data mining-methods) with big data technologies (...and data science-methods), where data mining (descriptive stats, exploration/viz, clustering, anomaly detection, etc.)  is a subset of data science., and relational databases are a subset of all database/big data technologies at scale in terms of scope?

Thanks

TJ If you, like me, are curious and alter some of the test scripts outside the <FIX ME> blocks, change them back before submitting. I broke various things that then needed to be rebuilt (especially the plurals).

Just a note to be cautious We're in the process of dealing with some great questions around question 4 of Lab 1.  We're trying to sort out all the aspects of it now.  Thanks for the great feedback!

One thing to note is that in python regular expressions, \w matches word characters, which are defined as [a-zA-Z0-9_].
Please note that in problem 4b of lab1, the requirement is 


Only spaces, letters, and numbers should be retained.  Other characters should  be eliminated. (e.g. it's becomes its)


So in this case we could not use \w since underscore(_) is included in \w for python, and the autograder checks will fail to match.
#pin
 Hi,

Can anybody tell me what is sc? it is not initialized in the notebook.

largeRange = sc.parallelize(xrange(1000))

Thanks in advance. When I submit lab1 I get the following feedback but in the lab it passes the test for the correct value. Please could you explain why this is marked as a failed test.

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect value for shakespeareWordCount Just in case anybody have the similar issue I as encountered: when I ran "vagrant up" for the first time, I got a network connection error. To solve this problem I have to start up a vpn connection, which is often necessary to access "foreign" websites or web services from Mainland China due to the infamous Great FireWall (GFW)  maintained by its governor.

The error was like below:

root@Debian:/home/exoool/Downloads/mooc-setup-master# vagrant up
/opt/vagrant/bin/../embedded/gems/gems/vagrant-1.7.2/lib/vagrant/pre-rubygems.rb:31: warning: Insecure world writable dir /opt in PATH, mode 040777
/opt/vagrant/embedded/gems/gems/bundler-1.7.11/lib/bundler/runtime.rb:222: warning: Insecure world writable dir /opt in PATH, mode 040777
Bringing machine 'sparkvm' up with 'virtualbox' provider...
==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install...
    sparkvm: Box Provider: virtualbox
    sparkvm: Box Version: >= 0
==> sparkvm: Loading metadata for box 'sparkmooc/base'
    sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base
==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox
    sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box
An error occurred while downloading the remote file. The error
message, if any, is reproduced below. Please fix this error and try
again.

Unknown SSL protocol error in connection to s3-us-west-2.amazonaws.com:443 
 Will the course cover creating a SparkContext and setting the necessary parameters? I'll need a SparkContext for using Spark in my own applications, so I'd appreciate some guidance and examples, but if the course won't do this then I'll try to figure it out from the manuals. Just a quick note about the video comparing Spark with MapReduce/Hadoop.  Dr. Joseph says you can only use Java with Hadoop which is not accurate.  You can use Hadoop streaming to execute your mappers/combiners/reducers in any language you want.

Great lectures otherwise! Hi - It has been my view that the "scientific method - deductive" traditionally used in the scientific research, has somewhat transformed and migrated into what we call "data science - more inductive i.e. many/millions of experiments" enabled by open source big data technologies, and commodity hardware for clustering, etc. which are used and expanded to other areas of data analysis  such as in business (business analytics), politics, banking, social media, etc.   Would you agree with this?   Is this  somewhat related to what is being presented in this slide?

Also I think in data science we have to have some understanding like with "physics based models"/cause and effect (i.e. the domain knowledge) to have a good idea as to whether the model or inference "makes sense", and not fall in the trap of being enticed by an "insightful" or  "elegant" model.   (i.e. correlation instead of causation), so we don't fall into the trap of the "Danger Zone" (from the data science venn diagram).

Thanks

TJ

 I installed virtual box and vagrant as stated and I restarted my laptop and since then I am not able to access internet. I even uninstall both applications but still no success. I'm using Windows 8.1 and it shows that my Ethernet is enabled and my modem is working fine. But when try browsers to use internet I'm unable to do so. please help.. This may sound terribly ignorant to anyone who's learned programming in the modern era, as opposed to using first edition Kernighan & Ritchie, but... I don't think I've ever heard the term "closure" as it's being used in the lecture. I can kind of infer the meaning, from the examples used, but it would have aided understanding to have had a brief explanation of the term up front.

Some digging revealed a couple of helpful explanations:
http://www.lua.org/pil/6.1.html
http://www.artima.com/intv/closures2.html

 I am using win7 64bit and tried to use spark 1.3.1 standalone with ipython notebook. Following the tutorial found on Google, I created ipython profile and successfully executed pyspark with default configuration.

However, the execution directly on my computer is SLOWER than the one provided by this course in the vm, every job has a significantly longer duration. I am curious about HOW THE VM ENVIRONMENT WAS SET UP. Do I need set some parameters about cpu or memory to have a higher efficiency? Thanks! I have completed Lecture1 and 2 but I couldn't find the Quiz questions. Where is the link for quizzes?

 I am Java programmer. Can I submit labs in Java? I'm having some error submitting lab1, when I hit "Check" gives me the follwoing error:

There might be some problems in your code submission. Please contact with TAs for further support if you could not figure out the problems.

When I run the IPython notebook, all the tests pass and everything is correct, but I am not getting any love from the autograder.  Any help is appreciated, thanks! Hi!  It looks like there is no video lecture on RDDs topic (slide 7 - 10 from lecture note, Week2lec4.pdf). Can anyone please let me know how to see deadline of the course . I missed my first deadline :(

Also can anyone please let me know weightage of different labs and submission too.

Thank you.
 In Lecture 4, Resilient Distributed Datasets, at 1:12, Professor Joseph says "Each element of the RDD is going to be aligned from the input file."  What does this mean?  What is an element of the RDD and what does aligned mean?

Thank you. The the last sentence in the last paragraph in section 5(a) reads:

Reducing locally at paritions makes reduce() very efficient.

paritions -> partitions My first submission to lab 1 went through correctly and all but one test passed. I fixed the problem and tried to upload the submission where all tests are passing in my notebook, but I see the following message. I checked the differences between my first and second submission in a diff tool and there were only differences in one function that I changed and cell numbers which were auto generated. I also tried to make the function change directly in the python file from the first submission and still got the same message. Any help would be appreciated.
There might be some problems in your code submission. Please contact with TAs for further support if you could not figure out the problems.


 Where can I download our instructor's lecture notes On the 
"SPARK AND MAP REDUCE DIFFERENCES" 
page of lecture three, there is mention of "serializing the results." What does this mean? I do not think this is covered in lecture 3. If I am wrong, then I apologize.

Thanks I used split in 4(d) and it seems it will automatically remove all empty element (I will get 882996 directly). How to keep the empty element using split? Thank you The lectures state that one should "always be careful when using collect() to ensure that the results will fit in the driver's memory"

Conceptually, this is easy to understand. But what if I have a case where the results are sort of big, but not that big. Is there a way to check what my driver's memory is? Or any way to get a vague idea about this?

Just curious, thanks. I use the word "system" because I don't know the proper term.  Is there a "big picture" of what the different components are and how the different components of a Spark system fit together?  Slide 5 from Lecture 4 says that "A Spark program is two programs: a driver program and a workers program."  How many driver machines and worker machines are there in a Spark system?   Are the number of such machines and their organization user-configurable, and if so, how do we configure them?  How do the different types of machines interact?  Besides drivers and workers, are there other types of machines in a Spark system?   Again, a "big picture" would be helpful.

Thank you. Why spark has lower overhead of starting a job as compared to MapRedAnd why is it less expensive for shuffles? Is it in-memory shuffle?Who does the shuffle is there a shuffle service as well? Nothing serious, just thought this is fun quote after some of brain difficultyweek 2 - map reduce and Python hates me as much as I hate 'emEnjoy the dowloading is too slow what should i do I get this error when running spark_tutorial_student as well as in lab01. lab0 works fine

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-1-8acce61ffdda> in <module>()
      1 # Display the type of the Spark Context sc
----> 2 type(sc)

NameError: name 'sc' is not defined
 Hi Everyone,

While running the first notebook, I am seeing the below error. Can anyone tell me how to resolve this...
 C:\Users\USER\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Importing base box 'sparkmooc/base'...==> sparkvm: Matching MAC address for NAT networking...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Setting the name of the VM: sparkvmThe name of your virtual machine couldn't be set because VirtualBoxis reporting another VM with that name already exists. Most of thetime, this is because of an error with VirtualBox not cleaning upproperly. To fix this, verify that no VMs with that name do exist(by opening the VirtualBox GUI). If they don't, then look at thefolder in the error message from VirtualBox below and remove itif there isn't any information you need in there.
VirtualBox error:
VBoxManage.exe: error: Could not rename the directory 'C:\Users\USER\VirtualBoxVMs\sparkvmbase_1433651292800_28744' to 'C:\Users\USER\VirtualBox VMs\sparkvm' to save the settings file (VERR_ALREADY_EXISTS)VBoxManage.exe: error: Details: code E_FAIL (0x80004005), component SessionMachine, interface IMachine, callee IUnknownVBoxManage.exe: error: Context: "SaveSettings()" at line 2788 of file VBoxManageModifyVM.cpp
C:\Users\USER\myvagrant>vagrant halt
C:\Users\USER\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Setting the name of the VM: sparkvmThe name of your virtual machine couldn't be set because VirtualBoxis reporting another VM with that name already exists. Most of thetime, this is because of an error with VirtualBox not cleaning upproperly. To fix this, verify that no VMs with that name do exist(by opening the VirtualBox GUI). If they don't, then look at thefolder in the error message from VirtualBox below and remove itif there isn't any information you need in there.
VirtualBox error:
VBoxManage.exe: error: Could not rename the directory 'C:\Users\USER\VirtualBoxVMs\sparkvmbase_1433651292800_28744' to 'C:\Users\USER\VirtualBox VMs\sparkvm' to save the settings file (VERR_ALREADY_EXISTS)VBoxManage.exe: error: Details: code E_FAIL (0x80004005), component SessionMachine, interface IMachine, callee IUnknownVBoxManage.exe: error: Context: "SaveSettings()" at line 2788 of file VBoxManageModifyVM.cpp
C:\Users\USER\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Setting the name of the VM: sparkvmThere was an error while executing `VBoxManage`, a CLI used by Vagrantfor controlling VirtualBox. The command and stderr is shown below.
Command: ["modifyvm", "650f855e-9842-4b72-8646-a3c5d5adcde7", "--name", "sparkvm"]
Stderr: VBoxManage.exe: error: Could not rename the directory 'C:\Users\USER\VirtualBox VMs\sparkvmbase_1433651292800_28744' to 'C:\Users\USER\VirtualBox VMs\sparkvm' to save the settings file (VERR_FILE_NOT_FOUND)VBoxManage.exe: error: Details: code E_FAIL (0x80004005), component SessionMachine, interface IMachine, callee IUnknownVBoxManage.exe: error: Context: "SaveSettings()" at line 2788 of file VBoxManageModifyVM.cpp
C:\Users\USER\myvagrant> Microsoft Windows [Version 6.2.9200](c) 2012 Microsoft Corporation. All rights reserved.
C:\Windows\system32>cd C:\Users\User\myvagrant
C:\Users\USER\myvagrant>vagrant destroy sparkvm: Are you sure you want to destroy the 'sparkvm' VM? [y/N] y==> sparkvm: Destroying VM and associated drives...There was an error while executing `VBoxManage`, a CLI used by Vagrantfor controlling VirtualBox. The command and stderr is shown below.
Command: ["unregistervm", "650f855e-9842-4b72-8646-a3c5d5adcde7", "--delete"]
Stderr: 0%...10%...20%...30%...Progress state: VBOX_E_FILE_ERRORVBoxManage.exe: error: Machine delete failedVBoxManage.exe: error: Could not delete the medium storage unit 'C:\Users\USER\VirtualBox VMs\sparkvmbase_1433651292800_28744\box-disk1.vmdk'.VBoxManage.exe: error: VD: error VERR_PATH_NOT_FOUND opening image file 'C:\Users\USER\VirtualBox VMs\sparkvmbase_1433651292800_28744\box-disk1.vmdk' (VERR_PATH_NOT_FOUND)VBoxManage.exe: error: Details: code VBOX_E_FILE_ERROR (0x80bb0004), component Medium, interface IMediumVBoxManage.exe: error: Context: "int __cdecl handleUnregisterVM(struct HandlerArg *)" at line 166 of file VBoxManageMisc.cpp
C:\Users\USER\myvagrant> Hi Everyone,
 
While running the first notebook, I am seeing the below error. Can anyone tell me how to resolve this...
 cant we do lecture 2 today?anyone pls tell me



 I have already tried http://localhost:8001/ and http://127.0.0.1:8001/, but it shows unable to access.
I check whether there is the conflict. I find it's the system conflicts the 80 port. I turned off the IIS. Then the virtual machine didn't work. I modified the RegEdit:HKEY_LOCAL_MACHINE/SYSTEM/CurrentControlSet/Services/HTTPset Start as 0, then I couldn't run the virtual machine. It showed "Remote Connection refused...Retrying... "infinitely.
I changed it as the original state.

But If I type http://localhost, it shows the picture below(If I open the SII):

Thanks! Do you guys find these videos useful? My sense is that one will have to study outside the videos to get a real understanding. I actually found the tutorial - https://raw.githubusercontent.com/spark-mooc/mooc-setup/master/spark_tutorial_student.ipynb easier to understand than the videos.  Hi guys,

Currently I need to do some transposing work for a project yet I am stuck in the middle :/
Soo here is the case:

Say If I have the rdd whose format is like this: ('Beat1',('Count1.1','Time1')),('Beat1',('Count1.2','Time2')), ('Beat1',('Count1.3','Time3')), ('Beat2',('Count2.1','Time1')),('Beat2',('Count2.2','Time2')), ('Beat2',('Count2.3','Time3')), ...

And I would like to transfer to this format:
Beat1Beat2Beat32014count1.1count2.1count3.12013count1.2count2.2count3.22012count1.3count2.3count3.3
 
Could you please give any hits or suggestions of how to do the cross table? It's easier to do in sql ;'( Thanks!  This is an article i have come across:

http://gizmodo.com/use-google-searches-to-figure-out-how-racist-your-neigh-1709200937?utm_campaign=socialflow_gizmodo_facebook&utm_source=gizmodo_facebook&utm_medium=socialflow

As people in the comments section have pointed out, the map seems to correlate an awful lot to population density in the USA. Are there criticisms/conclusions that can be made? in Windows xp when trying to add the box u get the error "BOX FAILED TO UNPACKAGE PROPERLY"

The problem is because of bsdtar.exe file gets corrupted. 
The location for bsdtar.exe file is .\Vagrant1\embedded\gnuwin32\bin 

SOLUTION
replace bsdtar.exe file with file i have attached.its updated file. If you have any issues plz mail me to tnp@sitam.co.in
new_bsdtarfile.zip What happens if the driver program says rdd.cache() when rdd is too large to fit in available memory on the driver program's machine? Does Spark just raise an error, or does it put as much of rdd in memory as it can and put the rest on disk? I am interested in enrolling for introduction to spark course enrollment, looking at the course it looks like it was started in June 1 , so am thinking can I pursue the course as of now ? Please help.

Thanks

Santosh I am not looking for any direct answers to assignment , but it will great if someone can give me a hint on : 
since the elements of the RDD are pairs, we need a custom sort function that sorts using the value part of the pair , how to write this custom sort function , i do not have background in python.  'PipelinedRDD' object has no attribute 'split',when I use split(PipelinedRDD) The lecture 4 video on 'Transformations' puzzled me for a bit. I think the slides are correct
comments = lines.filter(isComment)
However the spoken explanation suggests that comments are removed from the RDD. That is unfortunate wording:
the transformation does not change the original RDD, but produces a new one (immutability)the filter function discards some entries, yes, but it actually keeps the entries for which the passed function returns true (as seen on the slide) Lecture 3 tab  SPARK AND MAP REDUCE DIFFERENCES

Why the answer  It sends less data over the network is not marked as correct ?

Note: as seen in the video, to perform the same task of sorting 100Tb  in less time to the answer with much less #cores and the same network is, imo, the same as to say that much less data was moved in the network.  
  
thank you. There is no option to change the visibility or delete a question. Any suggestion? Hello,
While going through the spark course, I have a few questions
1. What would be the benchmarks that one should keep in mind when determining that the business use case is applicable for big data? For e.g. would 1TB input data processing per day be the right number? This question is in context of batch processing jobs. 
2. Are there any guidelines/best practices on how to integrate big data technologies into traditional enterprise application architecture? Taking an example of one of the enterprise architectures involving a relational database like Oracle where most of the business logic is implemented. 

Any real world examples depicting the details would be helpful.  What are the files I needed to copy for running spark vm in another machine. The second machine also running oracle virtual box and vagrant. I don't want want to download it again

Thanks at first, sorry for my poor english.

I did Lab 1 homework and download .py file as I did Setup(and it was successfully graded.) but Lab 1 didnt as you see above.
Did I miss something? Who did experience the same thing? I dunno if it's mentioned in the lectures but I was wondering do the keys have to be hashable? I ask this because in Python when creating a dictionary it's just a hashmap and your keys must be hashable. Is the underlying construct for the RDDs key, value the same? what utility need if i want to setup env.  instead  using vagrant VM (<600MB) ?
spark , jupyter , python , ... ?
it seems to be good that students know , how to setup environment and what utility needs (not just using a provided VM ). When  I enter vagrant up command the following shows up . Can any one please help me.
A Vagrant environment or target machine is required to run this command. Run `vagrant init` to create a new Vagrant environment. Or, get an ID of a target machine from `vagrant global-status` to run this command on. A final option is to change to a directory with a Vagrantfile and to try again.
Thanks in advance # Just run this code
import os.path
baseDir = os.path.join('data')
inputPath = os.path.join('cs100', 'lab1', 'shakespeare.txt')
fileName = os.path.join(baseDir, inputPath)

shakespeareRDD = (sc
                  .textFile(fileName, 8)
                  .map(removePunctuation))
print '\n'.join(shakespeareRDD
                .zipWithIndex()  # to (line, lineNum)
                .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'
                .take(15))


And I have the following error

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-41-cbd912244396> in <module>()
      8                   .textFile(fileName, 8)
      9                   .map(removePunctuation))
---> 10 print '\n'.join(shakespeareRDD
     11                 .zipWithIndex()  # to (line, lineNum)
     12                 .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in zipWithIndex(self)
   1986         starts = [0]
   1987         if self.getNumPartitions() > 1:
-> 1988             nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
   1989             for i in range(len(nums) - 1):
   1990                 starts.append(starts[-1] + nums[i])

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 1 times, most recent failure: Lost task 0.0 in stage 35.0 (TID 120, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <lambda>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <genexpr>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "<ipython-input-40-15f05bc8c785>", line 18, in removePunctuation
TypeError: translate() takes exactly one argument (2 given)

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


Should I download the Shakespeare and put it into somewhere, or 'just run this code'?  I think that the example on using Map Reduce for Sorting is wrong..or at least misfortunate in the explanation.
As we have split the document in pieces.. we have appearances of the same word in different machines in the first processing step (Map), hence the Reduce step can not take in consideration the total  number of appearances to send the words to one machine or the other. At most can take the partial number what I guess it´s not very useful as the result will depend on the distribution.
Can somebody clarify this? Does the distinct() transformation have some guarantee on the order of elements returned, e.g. that each element be placed in its first occurrence?

For example, in the example given in lecture 4, does
rdd2.distinct()
have to perform
RDD: [1, 4, 2, 2, 3] -> [1, 4, 2, 3]
or can we also get e.g.
RDD: [1, 4, 2, 2, 3] -> [4, 1, 3, 2]
 I would prefer to simply run the notebook through "ipython notebook" rather than vagrant, like I run all my other notebooks or those I download from GitHub . what is the difference? why vagrant? Just a note on the subtitles: the command "sc.parallelize" is transcribed as "sc.paralyze", which would really not be the greatest functionality ;)
 I don't understand the instructions at all. Can someone please clarify? Thanks in advance.
 Can someone tell me how to perform the sum of the values, I've trying many ways and noone worked.

wordCountsGrouped = wordsGrouped.groupByKey().map(lambda (k, v): (k, sum(v))

I tried this one, but it doesn't work.

 I passed the test 4f offline while I was running in my computer, but when I uploaded the PY file to autograder, it reported that

TestFailure: incorrect value for top15WordsAndCounts

I have no idea what should I fix, because in my computer


# TEST Count the words (4f)
Test.assertEquals(top15WordsAndCounts,
                  [(u'the', 27361), (u'and', 26028), (u'i', 20681), (u'to', 19150), (u'of', 17463),
                   (u'a', 14593), (u'you', 13615), (u'my', 12481), (u'in', 10956), (u'that', 10890),
                   (u'is', 9134), (u'not', 8497), (u'with', 7771), (u'me', 7769), (u'it', 7678)],
                  'incorrect value for top15WordsAndCounts')
1 test passed.
 In the 'Counting words in really big documents', I do not quite understand the difference between aggregating all of the results (and then outputting it to one machine)  and the divide-and-conquer approach. Doesn't the final large result still have to be outputted to one machine in both approaches? The only difference I could make out is that the same machines that perform the map can also perform the reduce operations. Downloading the pdf slides for lecture 4, they seem to be missing the 3 parts of the video lectures before the "Summary" one.
Does this happen to everybody or just to me?
 Hi!

Do you have to answer all questions in a lab for the automarker script to work?  I've not finished the last question, but thought I would load up the python script to get some marks:

I get the following:

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 423, in main
    Test.assertEquals(top15WordsAndCounts,
NameError: global name 'top15WordsAndCounts' is not defined

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not definedetc... When executing this code I cannot see the cached RDD in the storage section of the Spark Web UI (I did the Shift refresh).

Is it expected?

Or else should I restart/refresh something else? Hi,
I have followed the instruction as per video and information on edX

https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/courseware/d1f293d0cb53466dbb5c0cd81f55b45b/920d3370060540c8b21d56f05c64bdda/

After installing vagrant and VM , i still have issues.I am using Windows 8 and on AMD processor.I tried searching on forum but didnt helped.Need urgent help on this.Error is as follows

C:\Users\macro\myvagrant>vagrant up --provider virtualboxBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to fintall... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0The box 'sparkmooc/base' could not be found orcould not be accessed in the remote catalog. If this is a privatebox on HashiCorp's Atlas, please verify you're logged in via`vagrant login`. Also, please double-check the name. The expandedURL and error message are shown below:
URL: ["https://atlas.hashicorp.com/sparkmooc/base"]Error: error:0307A071:bignum routines:BN_rand_range:too many iterations In the Accumulators example, I don't understand what line of code will cause the workers to execute.

I see the line callsigns = file.flatMap(extractCallSigns)but that's just creating the transformation, which will be evaluated lazily. Then the next line prints blankLines.value, but callsigns itself has not been used in any context that would require the flatMap to be evaluated.Is Spark clever enough to inspect the code of extractCallSigns, and determine that when we invoke blankLines.value it will require the flatMap to be executed?Thanks! I wonder why are you discriminating Scala developers? I looked at the videos and they are all about python! I understand that python is popular and good for newbies and you want to attract more people, but in the same time you push down Scala developers, many of whom were attracted to Spark because it is native Scala alternative to other bigdata libs that are usually written in an ugly-Java way. I know Python but in the same time I understand that it is bad idea for me to learn Spark with python, not because Scala is a better language (that is true) but mostly because Scala is native for Spark and most of libs written for Spark are written in Scala. And I bet that many who happened to learn Spark in python will have switch to Scala in the Future. Hi I am doing 1f exercise where my output looks correct but testcase fails I dont know why

when I run In [24] it gives the following which is correct and testcase should pass with the following output please guide

[('rat', 2), ('elephant', 1), ('cat', 2)]
 The virtual machine is not running .  I finished Lectures 1 and 2 and can't really see where the quizzes are.
What am I missing?
I use Safari on Mac 10.9.5.
Could it be the add blocker?
Thanks. Hi all, 
For part 4b, I tried reading the document, but this is all I can get :(   Help..
return re.sub(r',|!|_', '', text, flags=re.IGNORECASE)print removePunctuation('Hi, you!')print removePunctuation(' No under_score!')
Hi you No underscore
 I installed Vagrant and box. When I type command vagrant up the command prompt shows that VM is starting and then has started but in Virtualbox application status of machine does not change from "powered off" to "running". What must have gone wrong?? Hi, I am not being able to install the VM in Windows. I have already installed Oracle's VM and downloaded vagrant. I am getting the following error on the DOS command line:  

C:\Users\phadjini\myvagrant>vagrant up --provider=virtualboxBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install...    sparkvm: Box Provider: virtualbox    sparkvm: Box Version: >= 0The box 'sparkmooc/base' could not be found orcould not be accessed in the remote catalog. If this is a privatebox on HashiCorp's Atlas, please verify you're logged in via`vagrant login`. Also, please double-check the name. The expandedURL and error message are shown below:URL: ["https://atlas.hashicorp.com/sparkmooc/base"]Error: Failed connect to atlas.hashicorp.com:443; No error Hello,

Can someone explain me the process of downloading the notebook of lab 01, in my Jupyter web UI there is only the lab0 notebook.

Thank you I put the following code that apparently is correct:

shakespeareWordsRDD = shakespeareRDD.<FILL_IN>

but the test reports that shakespeareWordsRDD is OK but shakespeareWordCount fails (Not OK) and it's rare to me. Does anyone have the same problem?, Does anyone see anything wrong?

 I want to do more practice .can you suggest more problems ? What is the user name and password that should be used for logging into the given Spark VM available through VirtualBox. This is not required for doing the lab. However, it is needed for logging into the spark vm directly.

Thanks! Lab 1 talks about the reasons groupByKey isn't the best approach, namely lots of data movement and lists could be too large to hold in memory.

Wouldn't another problem with groupByKey be that it would make it more resource intensive to recover from a failed node? Since groupByKey requires data from every parent partition to create child partitions, then Spark would have to recreate all the parents in order to recover a single child (basically rerunning the entire pipeline up until that point). Whereas reduceByKey only has one parent per child, so it would be easy to recover from a failed node.

Am I understanding this correctly?  Does anyone else have this problem or know what I am doing wrong?

The filteredRDD.is_cached attribute is 'true' however I do not see the cached RDD in the "Storage" section of the Spark web UI:
    http://localhost:4040/storage/
This page only shows a header and does not show any RDD's.

I can see that it is in memory as:
    print filteredRDD.toDebugString()
returns the following:
(8) My Filtered RDD PythonRDD[6] at collect at <ipython-input-28-2e6525e1a0c2>:23 [Memory Serialized 1x Replicated]
 |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:392 [Memory Serialized 1x Replicated] I am not getting in the exercise 3b.Can you explain it ?   pluralRDD = wordsRDD.map(<FILL IN>)print pluralRDD.collect()

What should come at <FILL IN> ? Having passed the tests locally up to question 3a (included), I decided to do a test submission. Autograder returned the following:

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 263
    .map()
         ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

Seems that the Test module from test_helper is not imported.

Any ideas?
 Hello, I made two batch files to start and halt vagrant, to make things simpler.
in myvagrant folder I used the old DOS command "copy con"
So I did it this way:
go to myvagrant folder using Command prompt
then type
copy con on.bat
vagrant up
and then  Ctrl+Z

Then again
copy con off.bat
vagrant halt
and then Ctrl+Z

You will find these new two batch files and you can make shortcuts on the desktop.
I hope this will be of any help.

. Lecture 4, Spark Key-Value RDDS ends with the caution that groupByKey() may cause a lot of data movement. It gives the example of an RDD with a million pairs that have the key 1. Would the same be true with using sortByKey()?

If we wanted to verify the above example, is it possible to look at the file system on a worker node and see the portion of an RDD on that node? A good read I felt: https://medium.com/deep-learning-101/algorithms-of-the-mind-10eb13f61fc4 please can any one tell how do i test the answers locally in the vm I am have allot of trouble setting up the software.  I went to the site https://atlas.hashicorp.com/search and found the command to run in the Mac terminal to install the virtual machine in virtual box.  

I am now having trouble with the using the my chrome to connect to Apache spark.  When I ran virtual box it wants a login sparkvmbase.  What is the default login?

Also, I can't 1.0.0.127:8001 is unresponsive.
 
Can you extend the deadlines to allow more time to work out these issues.

  Hi,

I was listening to Broadcast variable session and in that the term "task" has been used quite often.
For example:
Now one closure per worker is sent with every task, and there's no 
communication between workers. And any changes that you make to the 
global variables that are at the workers are not sent to the driver
or to other workers.
What is "task" here w.r.t Spark architecture and Driver/Worker etc?

Thanks in Advance. besides, I am not sure how useful the 'show answer' is when it only shows up after you got the right answer. I am having a problem. I finished the first week's setup assignment(with OPEN AS ADMIN command and install vbox as well). Then,  for this week's assignment, I opened the command without opening as admin and typed vagrant up but in a wrong directory. I was confused so I tried to reinstall everything. I deleted vagrant, vbox, and .vagrant, and sparkvm directory as well at Virtual VMs folder. I also did vagrant destroy. I reinstalled everything AS ADMIN but I couldn't see "sparkvm" at the VMbox UI so I double-clicked "sparkvm" at C:\Users\Daniel\VirtualBox VMs\sparkvm then I can see sparkvm at the UI but... I cannot make it run. Here is the screen shot and error message.
 

 
 
 
 
The full error message is 
Failed to open a session for the virtual machine sparkvm.
 
Failed to assign the machine to the session (E_FAIL).
Result Code:VBOX_E_VM_ERROR (0x80BB0003)Component:MachineInterface:IMachine {480cf695-2d8d-4256-9c7c-cce4184fa048}Callee RC:E_FAIL (0x80004005)
 
 
Seriously, this is not the first time I am facing so many errors using VM. I tried to learn Hadoop as well but gave up because of problems of VM. I really want to finish this class. Please somebody help me... Everyday, I am receiving Piazza email which I take time to read religiously because it gives an opportunity to browse what classmates are experiencing. It is a pity that many of the posts are a result of frustration of vagrant setup.
I , personally, did not have any issue with vagrant but since I see that many classmates cannot progress because of environment setup, I would like to recommend that, in the next round of this "wonderful" lesson, machines running in the cloud can be used so that students can access the environment on line, in the browser.
This is a suggestion which surely involves cost, but please think about it as course organizers to find an easy way to facilitate learning. I guess most of students were not attracted by the system engineering of setting up environment but by the use of spark. I would have given up, if I had tried vagrant setup for two weeks without success.
Hope this suggestion help. Happy learning to all & Cheers!
 

I got 88296 words, different from the  answer. What's wrong?


# TODO: Replace <FILL IN> with appropriate code
shakespeareWordsRDD = shakespeareRDD.<FILL IN> Please do not post answers
shakespeareWordCount = shakespeareWordsRDD.count()
print shakespeareWordsRDD.top(5)
print shakespeareWordCount




[u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds']
882996







 How do I delete a draft in piazza after it's been automatically saved? After this course is over, is there a version of Spark that I can download, install, and run without use of Jupyter and/or VirtualBox? In trying to do the tutorial for Lab 1, I've been plagued by "unresponsive Web page" failures; I certainly don't want that for my own applications.

Is use of VirtualBox necessary, or will this course just use it to show us how Spark works with multiple machines for students who have only one machine?

Don't all the layers -- Python interpreters inside Java Virtual Machines (inside browsers?) inside virtual machines inside a process running on an actual machine -- make things inefficient? Or are these inefficiencies insignificant compared to disk (and memory?) delays? Hello!
I've got an error using the provided code (baseDir = os.path.join('data') inputPath = os.path.join('cs100', 'lab1', 'shakespeare.txt').
I tried to find where the "shakespeare.txt' file was, but I didn't find it in my setup files, so I've downdoaded it from Gutemberg and tweek the os.path.
It works, and the result is ok for the less common words, but ko for the total count (
1 test failed. incorrect value for shakespeareWordCount
1 test passed.
I suppose the file I've downloaded does not completely match the file used for preparing the lab.
Where can I find the "right" file?
Thank you!
Josep Hey all- I am getting a very long traceback error when trying to run the code to load the Shakespeare Text File. I didn't alter the code or anything. I got all of the previous exercises correct, so it is weird that I am getting an error when trying to load the file. Has anyone else run into this issue?

Thanks! Can anyone explain how re.sub works? I really have no idea of how to work it out. I understand that there are multiple, varied definitions of 'data science.'  But the definition presented in lecture 2 is just wrong. 
The definition presented in lecture 2 states that Data Science is, by definition, working with big data. This is incorrect.  There is tons of work done by practicing data scientists that uses small or medium data.  
Similarly, the statement "In data science, we store [data] using NoSQL data stores" is wrong.  Yes, in many cases, data scientists will be using NoSQL or MPP data stores.  But data scientists will almost always be using SQL data stores as well. 
I'm only making this point because there are probably many aspiring data scientists taking this class.  As someone who has hired and will be hiring more data scientists in the future, I look for demonstrated competence with small/medium data, with SQL, with R/Python before I even think about big data skills.  
You have to walk before you can run, and there are vast expanses of 'data science' that are best covered by walking. 
 

 I'm using a Chromebook and would like to know if I can successfully setup the Course Software Environment (vagrant, virtualbox, spark, etc.) on this device. Or do I need to purchase a new system? 

(I used Crouton to run Linux on top of Chromium, but I'm not sure this is a sufficient workaround)

I appreciate any help you can offer! Thanks in advance! The grouping operators are very hard to see in my browser Jupyter view (using Chrome).  I don't see any way to modify the colors.  I am mildly red-green colorblind, and parens, brackets, and braces are nearly invisible until the line is committed.

Is there any solution for this? In the lecture, the instructor advised running `local[k]` ideally the amount of cores available on a cpu.

However, I want to distinguish between a process being multiprocess, where each process can run on a separate core using shared memory, versus multithreaded, many threads running under the same process.

In the case of being multithreaded, it is merely an abstraction provided by the operating system and doesn't actually afford any more parallelism in cpu bound tasks.

Was wondering if running local[k] is indeed only multithreaded? Hi,

I'm stuck with the lab0 :  When I run the ipython cells, they are marked as runinng but execution never finishes.

I'm confused because the setup is very simple, and I don't get what I could have missed ??
What can I look at to understand the problem ?

Mathieu In the key-value transformation examples in lecture 4, the following example is provided:
>>> rdd = sc.parallelize([(1,2), (3,4), (3,6)])>>> rdd.reduceByKey(lambda a, b: a + b)RDD: [(1,2), (3,4), (3,6)] → [(1,2), (3,10)] 
The instructor explains that the (3, 10) pair is created by adding the 4 and the 6.  Shouldn't the first part of that pair then be 6 from adding the two 3s?  I'm really unclear on reduceByKey.  Why are the 4 and the 6 the only parts of the input that get added? Here is my code:

CODE REMOVED 

However, this obtains a word count of 883007 (despite the correct top words), where the answer wants: 927631 or 928908.It could be a bug in the removePunctuation code - but that passes the test examples at least.


Nevermind - the problem was caused by splitting on more than just spaces. I just played around with the punctuation function some hours to figure out, why I got an error from the 4d tests:

# TEST Words from lines (4d)Test.assertEquals(shakespeareWordCount, 928908, 'incorrect value for shakespeareWordCount')Test.assertEquals(shakespeareWordsRDD.top(5), [u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds'], 'incorrect value for shakespeareWordsRDD')

1 test failed. incorrect value for shakespeareWordCount
1 test passed.

The strange thing is that the autograder shows me that I got everything right.

Is it just the test that contains a check for the wrong number of words? I got:
927631

This really took me a lot of time, am I the only one with that bug? Something I liked about PERL was that it comes with an interactive debugger similar to gdb built in -- breakpoints, stepping through code, etc.

The Python course I just finished didn't cover interactive Python debuggers, and I was hoping that Spark, or a standard Spark development environment environment, would include an interactive Python debugger.

Spark's use of Py4j seems to force just the opposite of what I was hoping for, though, since it gives Java rather than Python error messages.

Is it possible to obtain an interactive Python debugger for the Python code used with Spark? If so, how? Sorry, but I simply cannot create the directory for downloading and installing the virtual machine.  I followed the videos to set up VMWare and Vagrant just fine, though the machines look pretty different - everything ended up safely in the Applications folder.  But, trying to follow along in the Terminal is impossible.  Bash will not accept anything or create any directory.  I only get error messages saying nothing I am trying to enter exists. 

I have never had this happen before so I'm really quite stuck.
 Hi 

I am trying lab0 notebook and when I do shift+enter for the very first cell I get the following error: 



---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-3-f7aa330f6984> in <module>()
      1 # Check that Spark is working
----> 2 largeRange = sc.parallelize(xrange(100000))
      3 reduceTest = largeRange.reduce(lambda a, b: a + b)
      4 filterReduceTest = largeRange.filter(lambda x: x % 7 == 0).sum()
      5 

NameError: name 'sc' is not defined

It looks it makes sense as in the python code sc is not defined. I already have a ipython notebook installed through anaconda. I did install vagrant and Virtualbox and in the custom directory i started ipython notebook from the terminal like this: 

ipython notebook. 

Then i opened the lab0 notebook in it.  First week was awesome. I was able to successfully complete it. I downloaded and installed the required VM's and submitted test notebook. Kudos to the team. Hi guys thanks for your time
Buit I unable to get the first assignament
I download more than once time the vagrant file because I see and error in my cmd screen

Any tip or a web page to tunning my enviroment?

Kind regards, Marco I finished this weeks lab, but I was confused about one thing. 
Let's say we have:
rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])

why do these lines print the same things?print rdd.reduceByKey(lambda k,v:v+v).collect()
print rdd.reduceByKey(lambda k,v:k+k).collect()
print rdd.reduceByKey(lambda k,v:k+v).collect()

Most of the examples I could find while googling were just basic lists like [1,2,3,4], and those seem pretty simple to understand.  I understand multidimensional arrays in C as well, but I don't understand the logic behind the results in the above example.  Anyone know of a good resource that explains this?   I have Fedora 22 and just installing the .rpm package from the VirtualBox official website didn't work out with the following error message. Then I found a good tutorial on installing VirtualBox properly. It worked for me. For those who have the same problem, you can go to http://www.if-not-true-then-false.com/2010/install-virtualbox-with-yum-on-fedora-centos-red-hat-rhel/ for a step-by-step instruction.

WARNING: The vboxdrv kernel module is not loaded. Either there is no module
         available for the current kernel (3.14-kali1-amd64) or it failed to
         load. Please recompile the kernel module and install it by

           sudo /etc/init.d/vboxdrv setup

         You will not be able to start VMs until this problem is fixed.
 Hi,

I installed Vagrant and the virtual box and copied the Vagrant file to the parent directory:

drwxr-xr-x@ 7 davidlaxer staff 238 Jun 7 00:12 mooc-setup-master-rw-r-----@ 1 davidlaxer staff 23618 Jun 7 17:38 mooc-setup-master.zipDavid-Laxers-MacBook-Pro:Downloads davidlaxer$ mv mooc-setup-master ~David-Laxers-MacBook-Pro:Downloads davidlaxer$ cdDavid-Laxers-MacBook-Pro:~ davidlaxer$ cd mooc-setup-master/David-Laxers-MacBook-Pro:mooc-setup-master davidlaxer$ lsREADME.md lab1_word_count_student.ipynbVagrantfile spark_tutorial_student.ipynblab0_student.ipynbDavid-Laxers-MacBook-Pro:mooc-setup-master davidlaxer$ mv Vagrantfile ..David-Laxers-MacBook-Pro:mooc-setup-master davidlaxer$ cd ..David-Laxers-MacBook-Pro:~ davidlaxer$ vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Importing base box 'sparkmooc/base'...==> sparkvm: Matching MAC address for NAT networking...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Setting the name of the VM: sparkvm==> sparkvm: Fixed port collision for 4040 => 4040. Now on port 4042.==> sparkvm: Clearing any previously set network interfaces...==> sparkvm: Preparing network interfaces based on configuration... sparkvm: Adapter 1: nat==> sparkvm: Forwarding ports... sparkvm: 8001 => 8001 (adapter 1) sparkvm: 4040 => 4042 (adapter 1) sparkvm: 22 => 2222 (adapter 1)==> sparkvm: Booting VM...==> sparkvm: Waiting for machine to boot. This may take a few minutes... sparkvm: SSH address: 127.0.0.1:2222 sparkvm: SSH username: vagrant sparkvm: SSH auth method: private key sparkvm: Warning: Connection timeout. Retrying... sparkvm: Warning: Connection timeout. Retrying... sparkvm: Warning: Remote connection disconnect. Retrying... sparkvm: Warning: Remote connection disconnect. Retrying... sparkvm: Warning: Remote connection disconnect. Retrying...==> sparkvm: Machine booted and ready!==> sparkvm: Checking for guest additions in VM...==> sparkvm: Setting hostname...==> sparkvm: Mounting shared folders... sparkvm: /vagrant => ...

1. ipython notebook in http://localhost:8001/ 
doesn't show the right filesScreen_Shot_20150607_at_6.03.49_PM.png

2. $ vagrant ssh
   
David-Laxers-MacBook-Pro:~ davidlaxer$ vagrant sshWelcome to Ubuntu 14.04.2 LTS (GNU/Linux 3.13.0-53-generic i686)
* Documentation: https://help.ubuntu.com/
System information disabled due to load higher than 1.0
Get cloud support with Ubuntu Advantage Cloud Guest: http://www.ubuntu.com/business/services/cloud
Last login: Sat May 30 08:07:03 2015 from 10.0.2.2vagrant@sparkvm:~$ ipythonPython 2.7.6 (default, Mar 22 2014, 22:59:38) Type "copyright", "credits" or "license" for more information.
IPython 3.1.0 -- An enhanced Interactive Python.? -> Introduction and overview of IPython's features.%quickref -> Quick reference.help -> Python's own help system.object? -> Details about 'object', use 'object??' for extra details.
In [1]: sc.parallelize(range(1000)).count()---------------------------------------------------------------------------NameError Traceback (most recent call last)<ipython-input-1-75d1b9fe6bd8> in <module>()----> 1 sc.parallelize(range(1000)).count()
NameError: name 'sc' is not defined
In [2]: 

3. the virtual machine is running:

Screen_Shot_20150607_at_6.06.56_PM.png

I'm guessing this is a conflict with the Anaconda stack on my Mac.

Thanks in advance! I complained last week about lab0 being too easy 20% : https://piazza.com/class/i9esrcg0gpf8k?cid=257
I think lab1 makes up for it - I should have kept my big mouth shut.
I ended up spending 8 hrs to get it right. I have little background in python ( finished the recommended quiz /tut before the course). I had to review all post related to Q4 to get it working.

I would like to hear from others about their views.
enjoying this course now! Q1. I am listing my understanding of data transfers with respect to function calls. I would appreciate confirmation/corrections .
1. {map, filter, reduce} category of functions are done entirely on workers
2. collect triggers the transfer to workers based on parallelize calls and results back

Q2. how are the custom functions known to workers? There was some mention of closure, but I didn't get it. The file carrying my python function is on the driver. Is it sent across to worker and python on worker box runs it?

thanks I managed to get uniqueWords equal to [1, 1, 1] but I cannot sum it up using the function sum. Is there something I am missing? While, it was bold and clear, I missed the above deadline as I thought it's my time zone (California).

I submitted the HW the next day [my June-06, which is 8 hr behind UCT ] any way and attempted ALL quizzes of Week1 [Lecture1 & Lecture 2].
Has anyone faced with the same time zone issue, just curious !

Best
-Deb

 I got the following warning when trying to run the *.ipynb file.

Failed to retrieve MathJax from 'https://cdn.mathjax.org/mathjax/latest/MathJax.js'

Math/LaTeX rendering will be disabled.

If you have administrative access to the notebook server and a working internet connection, you can install a local copy of MathJax for offline use with the following command on the server at a Python or IPython prompt:

>>> from IPython.external import mathjax; mathjax.install_mathjax()

This will try to install MathJax into the IPython source directory.

If IPython is installed to a location that requires administrative privileges to write, you will need to make this call as an administrator, via 'sudo'.

When you start the notebook server, you can instruct it to disable MathJax support altogether:

$ ipython notebook --no-mathjax

which will prevent this dialog from appearing.

This error didn't show up when I was connected to the internet. Can someone help/guide me with installing this library offline?
 Problem solved.. It's because of a typo in the previous part :)
# TODO: Replace <FILL IN> with appropriate code
top15WordsAndCounts = wordCount(shakeWordsRDD).takeOrdered(15, key=lambda (k, v):-v)
print '\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15WordsAndCounts))

results:
rat: 2
cat: 2
elephant: 1
emm... Shouldn't shakeWordsRDD be words from input text since I passed all the previous test. In one of the slide author mentioned Oracle Argus and Razorflow are two Dashboard technologies. Razorflow is fine but never heard of Oracle Argus? Is this correct or author meant something else? This is my codes:
Dont post code.

Why my shakespeareWordCount's result is 927631 instead of 928908?  After downloading and running VirtualBox and Vagrant, my Win 8/8 GB RAM/i4 quad-core machine keeps crashing with some Windows error or another (MEMORY_MANAGEMENT happened once). Any ideas what might have gone wrong? I have quite a few services (like MySQL, MongoDB servers) running on my laptop automatically, but there is more than 4 GB of free memory. If I were to attempt to replicate your VM on VMware Workstation + Ubuntu 14.4, would you have a set of directions to follow to set things up?
- Install Hadoop
- Install Spark
- Install python notebook

Ed Hello -

So I'm stuck on question 2c of Lab 1, regarding the "reduce by key".

The example (tutorial, 6b) shows " pairRDD.reduceByKey(add) "

However, when I respond on the Lab 1 notebook with " wordCounts = wordPairs.reduceByKey(add) " I get this error:

NameError: name 'add' is not defined     
How can " add " not be a valid argument for reduceByKey ? 
pyspark      running     2


Try printing out sc to see its type.

In [6]:
















# Display the type of the Spark Context sc
type(sc)

















---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-6-8acce61ffdda> in <module>()
      1 # Display the type of the Spark Context sc
----> 2 type(sc)

NameError: name 'sc' is not defined








 I have everything correct until I try to run the wordCount function, the output is this:

1: 85
10: 3
100: 1
101: 1
102: 1
103: 1
104: 1
105: 1
106: 1
107: 1
108: 1
109: 1
11: 1
110: 1
111: 1
And I do not know whats wrong, Ive checked with the autograder and everything is ok even the wordCount function what are user login name and password for the sparkvm on
Ubuntu 14.04.2 LTS Problem resolved. I did a "vagrant destroy", followed by "vagrant up" to get jupyter to work again. Still not sure what happened or what corrupted my vagrant halt last week.

-Casey

--------------
Set up went great last week and I submitted week1's test notebook successfully, and "vagrant halt" the sparkvm after I was done.

Today, I tried to do a "vagrant up" at the same myvagrant directory, and got the following logs that looked ok but accessing "http://localhost:8001" with the chrome browser no longer opens up the page to sparkvm. Same behavior when I swapped the localhost string to 127.0.0.1. Chrome gives the message "No data received (ERR_EMPTY_RESPONSE)". But "ssh -p 2222 vagrant@localhost" logins ok. Didn't change anything on the sparkvm since the clean setup and shutdown last week.

Would appreciate any help!

-Casey

----------------

Kc-MacBook-Air:myvagrant kcc$ vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Clearing any previously set forwarded ports...==> sparkvm: Clearing any previously set network interfaces...==> sparkvm: Preparing network interfaces based on configuration...    sparkvm: Adapter 1: nat==> sparkvm: Forwarding ports...    sparkvm: 8001 => 8001 (adapter 1)    sparkvm: 4040 => 4040 (adapter 1)    sparkvm: 22 => 2222 (adapter 1)==> sparkvm: Booting VM...==> sparkvm: Waiting for machine to boot. This may take a few minutes...    sparkvm: SSH address: 127.0.0.1:2222    sparkvm: SSH username: vagrant    sparkvm: SSH auth method: private key    sparkvm: Warning: Connection timeout. Retrying...==> sparkvm: Machine booted and ready!==> sparkvm: Checking for guest additions in VM...==> sparkvm: Setting hostname...==> sparkvm: Mounting shared folders...    sparkvm: /vagrant => /Users/kcc/myvagrant==> sparkvm: Machine already provisioned. Run `vagrant provision` or use the `--provision`==> sparkvm: to force provisioning. Provisioners marked to run always will still run.
-----------------
Kc-MacBook-Air:~ kcc$ ssh -p 2222 vagrant@localhostvagrant@localhost's password: Welcome to Ubuntu 14.04.2 LTS (GNU/Linux 3.13.0-53-generic i686) * Documentation:  https://help.ubuntu.com/  System information as of Mon Jun  8 04:52:28 UTC 2015  System load:  0.5               Processes:           76  Usage of /:   3.9% of 39.34GB   Users logged in:     0  Memory usage: 3%                IP address for eth0: 10.0.2.15  Swap usage:   0%  Graph this data and manage this system at:    https://landscape.canonical.com/  Get cloud support with Ubuntu Advantage Cloud Guest:    http://www.ubuntu.com/business/services/cloud2 packages can be updated.2 updates are security updates.Last login: Mon Jun  8 04:52:28 2015 from 10.0.2.2vagrant@sparkvm:~$ This is my result for 4c,
[u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds']

the u seem meaningless. Why do they appear in any print results? I am trying to run all the cells in the notebook. All the cells above 4f compiles and runs properly and displays result (all passed). But when it comes to 4f, it does not do anything (no error etc.). The input looks like: In[*]: code..
Even if I try "Run All", "Run" by clicking on that cell, nothing happens.

I have tried restarting the kernel many times and my vagrant is also up and running.

Please suggest a workaround.

Edit: Please refrain from posting solutions. I am interested more in the reason for the error. A dumb question..where is wordCount defined or is it a library function? Are there plans to cover Spark Streaming in future classes?
I mean, is there some extension to the course CS 100.1X?

Thanks! Lab1 is super awesome and able to complete with 100%. Thanks for the detailed explanation.  While running some tests, the Oracle VirtualBox crashed twice showing "Guru meditation" in the Oracle VB console. Vagrant successfully re-initiated and brought it up but all the tests had to be run again. This happened twice before final success.

The Log viewer from the VB does not reveal much. Wondering if I need to tweak or check something.. Question :Which of the following problems does a MapReduce implementation handle?
MyAnswer is :

Recovering from machine failuresShuffling data between the Map and Reduce functionsRunning the Map and Reduce functions on many machinesAutomatically parallelizing an algorithmRecovering from slow machines

It is not accepting any answer. Title of the thread covers most of it. I picked up that LISP was noteworthy because it removed FORTRAN's unspecified restriction on recursion, I/O was (arguably) more problematic in the 1950s through 70s than it is now in iterative operations within clusters, computation itself was too time-consuming, and expressing things as lists isn't going anywhere. What did you notice?

I'm not especially confident about my theoretical background, so I would like to start a conversation about what else people walked away from that article with.

Furthermore, the word "parallel," as in the sentence introducing the link's "Stanford, MIT, CMU, and other universities develop set/list operations in LISP, Prolog, and other languages for parallel processing" does not appear in the article once. What did that article have to do with parallelism? Hi,

I am trying to submit my assignment (lab2.py). Every time I run it on my notebook in Jupyter it passes all the tests successfully, but when I try to submit it I get the error posted bellow. Could you help me with that?
The whole stack trace:
Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 156
    'incorrect value for top15WordsAndCounts')    return locals()
                                                       ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --
 Would there be a deadline for the quiz? I didn't see any information about that. Is there any function available in spark to count no of characters?
What should go in place if Fill IN here?
pluralLengths = (pluralRDD <FILL IN> .collect())print pluralLengths

Any help would be much appreciated. how do i count the total number of words using map()...Exercise 3b Hi,

I tried to answer this question "Which of the following problems does a MapReduce implementation handle?" in lecture 3 but I could not find anything , even on the net. Can anyone help me out on figuring the right answers?

I understood the following with map reduce :

Map reduce is supposed to help me map all the counting and reduce help me collect the counting.

My best guess are : 
Recovering from machine failures
Recovering from slow machinesBut that does not seem to be good. Anyone can explain why and give me some hint, please?
TIA

 What should go in place of FILL IN for map and reduce? Below is what I have written:

What I understood is map will convert 
[('rat', 2), ('elephant', 1), ('cat', 2)] to [2,1,2] and then reduce will sum all these values. But how map will map convert pair rdd to values  . Moreoever, reduce takes 2 parameters. Here after map, we will get only 1 parameter. Can someone please help ?. Debugging spark python api in the ipython is hit and miss ... you just have to seem to use trial and error to find the syntax errors.

Any hints and tips .. it may have been mentioned in the lecture but cant find a page on how to debug. In the Spark Tutorial and word count lab it tells me to "Download the IPython notebooks.  Make sure that the file extension is .ipynb..." however I cannot find any notebooks with this extension in http://localhost:8001/
Can anyone help me or instruct me how to get these notebooks, and tell me what they should look like / contain? Thanks 
 The following error msg occurs and I could not load vagrant vm on giving the command 'vagrant up'. 
 On giving the command "Vagrant up" gives a message "sparkvm: Box 'sparkmooc/base could not be found. Attempting to find and install.' ": the system tries to download sparkvm but ends up giving an error msg.head error 000000:lib<0>:func<0>:reason<0>, error 100054An error occured while downloading a remote file.
2) Removed Vagrant completely and installed all software  again  including 'virtual box' but still fails to load.
3) Virtual box runs but does not display Vagrant as shown in the video.FESTUS  Hi,

Here is a bit of an off-the-wall question. Is there a way to use Microsoft Visual Studio 2013, with the Python Tools installed, as the development environment for this course instead of the Notebook? 

We would still be running the Notebook on our computers, and the execution environment within the Notebook would be active and would still retain its state. But, I am imagining something where we could highlight a section of code in Visual Studio, and somehow use a REST service to send the code to the Notebook for execution.

Thanks I went through the tutorial before Lab 1, and I opened the Spark UI at http://localhost:4040/jobs/
I ran all the cells in the Notebook but the UI is empty, no jobs, no RRDs.

When I run pyspark from the CLI in the VM, I can see my jobs and RRDs in the UI.

I played around in Jupyter with restarting the Clusters and reconnecting the notebook, and I got it to work.
What are the steps to start Jupyter with Spark UI? And why can I run a notebook without having a cluster started?
 I could successfully create plurals using lambda :

pluralLambdaRDD = wordsRDD.map(lambda word: makePlural(word)) print pluralLambdaRDD.collect()

after referring http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map

But could not get the same with functions.

Options tried :

pluralRDD = wordsRDD.map(makePlural(word)) --- Says word not defines

pluralRDD = wordsRDD.map(word='',makePlural(word))

SyntaxError: non-keyword arg after keyword arg Just in case anyone else is using Gentoo, I am posting what worked for me. I am running on the default/linux/amd64/13.0/desktop/kde profile.

Add the following to /etc/portage/package.keywords

app-emulation/vagrant ~amd64
app-emulation/vagrant-bin ~amd64
app-emulation/virtualbox ~amd64
app-emulation/virtualbox-additions ~amd64
app-emulation/virtualbox-extpack-oracle ~amd64
app-emulation/virtualbox-modules ~amd64

Make sure portage is up to date.
emerge --sync

This one is in the main portage tree.
emerge virtualbox

Vagrant is too old in the main portage tree even with the experimental keyword set. I used an overlay. If you don't have layman, then that needs to be setup first.
emerge layman

Setup the tatsh-overlay. It's not in the main list of overlays, so I had to manually install it.
layman -o https://raw.githubusercontent.com/Tatsh/tatsh-overlay/master/layman.xml -fa tatsh-overlay

Use your favorite editor to create the file /etc/layman/overlays/tatsh-overlay.xml and paste the following code in there.
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE repositories SYSTEM "http://www.gentoo.org/dtd/repositories.dtd">
<repositories xmlns="" version="1.0">
  <repo priority="-3000" quality="experimental" status="unofficial">
    <name>tatsh-overlay</name>
    <description lang="en">Overlay for updated vagrant-bin</description>
    <homepage>https://github.com/Tatsh/tatsh-overlay</homepage>
    <owner type="person">
      <email>ddrtist@gmail.com</email>
    </owner>
    <source type="git">git://github.com/Tatsh/tatsh-overlay.git</source>
    <feed>https://github.com/feeds/Tatsh/commits/tatsh-overlay/master</feed>
  </repo>
</repositories>

The priority level ensures we only use the overlay for vagrant-bin which doesn't exist in the main portage tree. Since this is an unofficial overlay, I didn't want to take chances. You can then get a recent enough version of Vagrant for this class.
emerge vagrant-bin

My virtual box didn't start up the first time I tried. I had to issue this command to load my kernel modules. Luckily, I already had virtual box for doing Android development. If you are not so lucky, you might be missing some kernel modules for it.
for m in vbox{drv,netadp,netflt}; do modprobe $m; done

Here is a list of my relevant modules.
Module                  Size  Used by
vboxnetflt             15442  0 
vboxnetadp             17574  0 
vboxdrv               320815  3 vboxnetadp,vboxnetflt
 Unable to proceed with the course because the estimated download time is never less than 10 hours, and gets interrupted or failed every single time.

Any others with the same problem?Any solutions? Can I not download this file from somewhere else and install it manually?This makes me realise, I really don't understand how virtual machines work. Why can't I download, say a .iso for Vagrant and install it? IDK somebody help please. When I get ahead on my work for this course, as I am now, I'm going to start doing some of the things I've been asking about:
Downloading Spark and making a native installation of it on my machineDownloading and installing a version of Python that Spark can useTranslating my PERL campaign-contributions analysis code into Python, using pdb for debuggingRun Spark on the collection of Federal Election Commission reports that gave me out-of-memory failures in PERL
Will it be okay with the instructors if I ask questions about any problems that come up to the class?

My questions won't be class material, but they'll be highly related to class material, and everyone can ignore them if they want. I hope people will find answering my questions fun, though, and if I'm lucky, I'll be able to tell you all interesting things about the 2014 Congressional elections. Hi everyone,

I have successfully installed vagrant and ran lab 0 perfectly. Now when I try to run the spark tutorial notebook. The notebook is not being run (in the sense I'am not seeing the results after a long time of waiting also). I have tried reinstalling the vm by using vagrant halt, vagrant destroy, vagrant remove (to remove the VM stuff) and then again download it using vagrant up. I'am posting some screenshots of the problem (I have highlighted the areas which might help my case).

The First screenshot was taken by running the type(sc) cell in the notebook. Initially in the top corner it showed kernel busy then it went to kernel idle and then I have waited for 15 min while it showed kernel starting, please wait



Then I shut down the notebook and vm using vagrant halt, and increased the memory of the vm from virtualbox UI to 3.5GB and again started vm and this time I ran the simple python print cells in the notebook, after waiting for 10 min and not getting any result, this time on the top corner the message was No connection to kernel. I took the second screenshot.



Here are the some of the details of my environment, don't how much help they will be:
1. I'am not behind any proxy server.
2. OS: Windows 7 64 bit; Ram: 6 GB
3. Lastly after running vagrant up. When I open virtualbox UI, it still shows sparkvm powered off but i can access the notebook url. is it normal?

As you can see I have tried everything, reinstalling VM, increasing VM memory and waiting. I don't know what to do more, Can any student/course coordinator help me before it's too late for the lab 1 submission.

Thanks in advance I don't know if I wrote a right code.But I think there shouldn't be a NameError on this section.
The original code is like below:wordsGrouped = wordPairs.<FILL IN>for key, value in wordsGrouped.collect(): print '{0}: {1}'.format(key, list(value))
After I replace the <FILL IN> with "groupByKey()", it says "NameError: name 'wordPairs' is not defined"I did nothing with wordPairs in this section, and should wordPairs be defined in a previous section, 1f? I click Upload, select the file then nothing happened? Anybody facing the same? Thanks Hello!

By enrolling to the Apache Spark courses, I had in mind to attend data mining, machine learning processes, and I'm happy about how things go.

But, as an IT engineer student (last year), I want to start a company, using Artificial Intelligence (And data mining obviously thus the course).

I'd like to share with some of my classmates interested in working with me on this project.

To know more about that, please email me! nyfa2009[at]gmail.com

I hope this post fits with piazza policy.

Raphael http://localhost: 8001 is not opening for me, I am getting an error message for the browser. Can someone help me figure this out. Hello everyone!

I have a question, not directly related to course, but to Pyspark.
Is it possible to import external python libraries? When I try to load GDAL, it is denied.

Any suggestions on this?

Best,
Ricardo Are there any free online books (epub, pdf) that are reccomended by the instructors for learning spark?

Thanks! Need a bit more explanation to understand the divide and conquer approach of hashing. Please help :| In the video on pySpark closures, the instructor says that pySpark creates "closures" for functions that run on RDD's at workers.

What exactly is a closure?

I know that Closures were defined on another thread by the instructor as 
"Closures encapsulate the state needed to run a piece of code - that includes all of the code and data. Spark tracks and creates this automatically for you."

But I am a bit confused.... what  are the "state(s) needed" and what are the benefits of using closures? How do they work?

This forum is a bit shaky at explaining this: http://programmers.stackexchange.com/questions/40454/what-is-a-closure.

Is a closure like a function object? A pointer to a function that is passed as a parameter?

Many thanks in advance to those that reply. Hi everybody,

I am a bit confused by the question following video 5 (lecture 3). 
It is asking which of the following problem the map-reduce implementation would handle. I thought the failure recovery and slow-machine are the two problems Map-REDUCE handle. But it seems it is wrong. In fact, all the combination of answers are giving wrong answers. Is this a problem with the grader or am I missing the point here. 

Any help would be appreciated.

Thanks. The problem says to use map() function to get the count: So I coded as below, but it results in error so I added lambda word: sum(word) but it says 'word' not defined. Can someone please help what we need to do?

wordCountsGrouped = wordsGrouped.map()print wordCountsGrouped.collect()

TypeError                                 Traceback (most recent call last)
<ipython-input-49-d46bf01a2eb2> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
----> 2 wordCountsGrouped = wordsGrouped.map()
      3 print wordCountsGrouped.collect()

TypeError: map() takes at least 2 arguments (1 given)
Thanks. (4c) Load a text file
all previous test cases have been passed
 only the code that has to be run gives the following error..
tried all methods past 24 hrs..pls help



error:
_____________________________________________________________________________________


Py4JJavaError                             Traceback (most recent call last)
<ipython-input-31-cbd912244396> in <module>()
      8                   .textFile(fileName, 8)
      9                   .map(removePunctuation))
---> 10 print '\n'.join(shakespeareRDD
     11                 .zipWithIndex()  # to (line, lineNum)
     12                 .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in zipWithIndex(self)
   1986         starts = [0]
   1987         if self.getNumPartitions() > 1:
-> 1988             nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
   1989             for i in range(len(nums) - 1):
   1990                 starts.append(starts[-1] + nums[i])

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 92, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <lambda>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <genexpr>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "<ipython-input-27-52dcee918cb9>", line 19, in removePunctuation
IndexError: string index out of range

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGSchedul Hello,
I'm haing trouble submitting my file lab0_student.py
I can upload it fine, then check it, I get no pb :


But then when I want to save it, it tells me "Your answers have been saved but not graded. Click 'Check' to grade them."...
So I try "Check" again, but it then tells me "You did not select any files to submit"... I go on an on uploading the file then check and trying to save, wiithout success.
Thanks for your help,
MB In the "Spark Broadcast Variables" section, when we call 
contactCounts.map(processSignCount)
it was discussed processSignCount is passed as a closure. If we defined signPrefixes outside the processSignCount function, is it safe to assume we can change the function's signature to
def processSignCount(sign_count):
and use the signPrefixes variable within the function or do we have to pass it in? It appears that signPrefixes is already defined in the closure so the function has access to it within its definition. With my limited understanding I have coded it as below. Maybe I misunderstood the question. Please put me in the right direction.

# TODO: Replace <FILL IN> with appropriate codefrom operator import addwordCountsCollected = (wordsRDD .map(lambda k, v: (k, sum(v))) .reduceByKey(add) .collect())print wordCountsCollected

which gives error: 
TypeError: <lambda>() takes exactly 2 arguments (1 given)
appreciate your help.
 Hi,
can i submit the lab0 assignment now. This is the first time I'm accessing a course and couldn't update it on 6th.
 In lecture 3's video titled "Technology Trends & Opportunity" Prof. Anthony mentions that instead of disk, memory is used in order to reduce the reading and writing time. My question is: Does that mean that at the hardware level, the disk storage system which MapReduce uses to read and write its data cannot be used by the Spark to do read and write(Since it needs memory and not disk) which in turn might mean that Spark might not be compatible with the existing storage systems(namely disk) on which MapReduce works and that it requires totally different data storage systems(read memory) and hence would require effort to set them up against the existing storage systems i.e. disk storage systmes? I naturally wonder?

SAP Hana is not part of Apache - it costs money - other than that, in terms performance how these compare? Even SAS has great products including in-memory computing - not sure what it is called?

Personal computer (PC, MAC) architecture often allocate some space in the harddrive for additional memory (at least they used to) - so instead of in-memory computing, could we implement in-harddisk computing - or we have a hybrid disk that is capable of performing the functions of both memory and hard disk?

What do you think? after mapping function how to add values using reduce(lambda ...)
help Hello,

I have a question regarding Distinct Transformation example::

Distinct :::
rdd = sc.parallelize([1,4,2,2,3])>>> rdd.distinct() --> then we apply transformation distinct to rdd which will pick unique elements from list- [1,4,2,3]
---> So the code in black runs at the driver (rdd = sc.parallelize([1,4,2,2,3])

Here, there is no green code. So the entire code runs here on driver or the distinct part runs on the worker?

so basically what I am trying to ask is that which code gets executed at the driver and which code gets executed at the worker nodes?

Thanks,
vibhor


 This is not really a big deal just more of an FYI.
The H4 tags (#### Text ####) are not translating appropriately in Spark Tutorial Notebook.  

You can just search in the places in the text with '####' if you feel like correcting. I see a possible update to VirtualBox 5.0.0 RC1 (Beta). Any reason to install it? Or should I keep my current version (4.3.28.0.0)? Hi, If anyone can list all the possible punctuation to be removed in 4b. Thanks  I would like to suggest that the due date for assignments be moved from Saturday to Sunday evening or Monday morning if it's possible, so that we have a full weekend to work on assignments. hi it says "
Now pass each item in the base RDD into a map() transformation that applies the makePlural() function to each element. And then call the collect() action to see the transformed RDD."

but it does not matter what i do it is always false.
my solution
# TODO: Replace <FILL IN> with appropriate codepluralRDD = wordsRDD.map(makePlural( 'cat'))print pluralRDD.collect()

this works but this should not  be the solution for this question
 wordsRDD.map(lambda x: x+'s') I think there is a small omission in the word count lab in 3a.

The division operator does not work: 5 / 3 results in 1.0
I think I'm not giving much away when I say that you have to add the line
from __future__ import division
to the top of the cell

at least thats what I did to get it working...

 Olá amigos, brasileiros ou falantes do português.
Vamos montar um lista de discussão ou um grupo no Facebook.
O que acham?
Algum sinal de vida? Hi all,

I'm having trouble with Lab 1 question 1f running the test.

The output i get from 1f is:

[['cat', 1], ['elephant', 1], ['rat', 1], ['rat', 1], ['cat', 1]]
But when i try to run the test i get a test failed. 

Some help would be appreciated. 

/E Hi there. I have a need to run my programs on multiple machines (one at home, one at work). Is there any guidance on how to best achieve this? 

At present, I re-installed VirtualBox, Vagrant and the class' VM on multiple machines but am looking for a way to replicate the notebooks easily (it doesn't for example register the uploaded notebook from the first week).

Any advice would be appreciated.

Best,
Nosh As per lecture, I fill in the code for 1C as follows:
pluralRDD = wordsRDD.map(makePlural(wordsRDD))

but i get a Type Error and this error
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.Any help will be appreciated.  thxRam Hi, I have created the removePunctuation function. When I run 4d.. it gives shakespeareWordCount = 928908 as expected which means total number of words including '' is exactly matching my removePunctuation and split function. But to my surprise when I run 4e, it gives shakeWordCount = 882999 which is 3 more than the expected result. (882996 ) after removing ''.

I tried every possible case as per my understanding. Now I am exhausted. If someone can guide how to debug for those 3 words among ~ 8.8k words.

Thanks Hi I am not able to figure out how to solve 4d In[101] I am not able to find out how do we return values after we do map(lambda line : line.split(" ")) Please guide. It would make things 1000x easier if you gave the Videos and Quiz questions unique numbers so that students and faculty can refer to them in our questions and comments.

Currently, when a problem is found there is no way to refer to the specific location in the Courseware.  For example:
Week 2: Lecture 3: Video 5: Question 1  or something similar. Everytime I try vagrant up, it shows a 7+ hour download time.

I've reached 35% twice, 60% once, after waiting hours and hours. And then it fails.

On entering vagrant box remove sparkmooc/base, I am told that sparkmooc does not exist/was not found, and on retyping vagrant up, the download begins from 0% once again.

What should I do? Is any one tried the last quiz of lecture 3 . why Spark is faster that Traditional Map Reduce
looks like all are correct but it's not true .. not getting the exactly which ones are true


Thanks for reply , But my ask is

Which options are Corrects from below list

1 It sends less data over the network 2 Results do not need to be written to disk 3 It detects machine failures more quickly 4 It replicates the output of each task to recover from failures quickly 5 Results do not need to be serialized
 this is reg. Lab1/3b (Mean using reduce) ex. Any help?
# TODO: Replace <FILL IN> with appropriate code
from operator import add
totalCount = (wordCounts
              .<FILL IN> PLEASE DO NOT POST SOLUTIONS
average = totalCount / float(uniqueWords)
print totalCount
print round(average, 2)


TypeError                                 Traceback (most recent call last)
<ipython-input-100-f178bb422a4d> in <module>()
      4               .map(lambda k: (k, 1))
      5               .reduce(add))
----> 6 average = totalCount / float(uniqueWords)
      7 print totalCount
      8 print round(average, 2)

TypeError: unsupported operand type(s) for /: 'tuple' and 'float' I tried to do the all together as <FILL IN> PLEASE DO NOT POST SOLUTIONS
This provides an error.
Not sure how to approach the
 The Courseware text listed under "
THE SPARK COMPUTING FRAMEWORK
Reads....
"The time to read or write a value to memory is only a few nanoseconds, while the time to read or write is several milliseconds - that means memory is a million times faster than disks."

You left out the word "disks".  "...while the time to read or write from disks is several milliseconds"

FYI. I am totally confused by the hardware & software setup for this course.
I will start with some leading questions:

1. Where do we actually running our programs, and where do we set it up?
 Where do we  run the programs? (on our local machine, on remote server or both?)
2. Why do we need an extra  Virtual Machine if we already have one installed for java applications?
3. Why is this mix-up of software and how is it interacts with the base computer, for example ubuntu operating system on a windows machine with a mix of java & python and probably some more under the hood?

4. On a different note: The software shape that we encounter on lecture 4 resembled (to me) functional programing. Is it my imagination or there is something to do with it?

5. Is there something similar to a standard debugger for the Spark software?

Thanks in advance for any help. It seems to me that a slow-running task could be caused by either a hardware or software failure, or a partitioning that just results in one node having to do more work. Let's say you have a list of random integers, and then you do a rdd.map(lambda x:fak(x)), where fak(x) takes much longer for larger integers. The node that (basically by chance) ends up with the largest integer will take much longer than all other nodes, even though nothing is wrong. How do you distinguish such a scenario from a "straggler", a node that is running slowly because it has a problem? I've finished the first lab assignment and it passes all the tests in the notebook. However, every time I try to have my lab1 assignment marked it returns a tonne of errors. Something goes wrong at the beginning and then it doesn't recognise the 'Test' function. Another user had a similar problem but fixed it with a file name change. That didn't work for me. Full output:

(Excellent lab, by the way. Nice and tough. Really felt like I was learning something.)

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 354
SyntaxError: Non-ASCII character '\xc2' in file /ok/submission.py on line 354, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) -- When we run a notebook on Jupyter for this course, where do the computations actually occur?

My guess is that a computer in Berkeley, California holds the data files we use -- in memory for efficiency -- and translates our notebooks into Java byte code. This Java byte code is then executed by Java Virtual Machines in our browsers on our own machines.

Is this right, or have I misunderstood the process? How can I write the regular expression for re.sub() such that one expression can remove punctuation, white spaces and neglect capitalized letters?

Thanks  With the exception of a few words it appears that all my counts come in with a value 1-4 less than they should be. I'm almost positive this is something wrong with my regex, but I can't quite pin it down, I substitute everything that isn't a alphanumeric character or a space with an empty string. Also of note is that my total word count is correct, it is just the individual word counts that seem to be acting up. 
the: 27359
and: 26028
i: 20678
to: 19150
of: 17461
a: 14590
you: 13614
my: 12477
in: 10954
that: 10890
is: 9132
not: 8495
with: 7769
me: 7766
it: 7676 Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 577, in main
    brokenRDD.collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 713, in collect
    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 538, in __call__
    self.target_id, self.name)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/protocol.py", line 300, in get_return_value
    format(target_id, '.', name), value) C:\Users\mfirozx\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0The box 'sparkmooc/base' could not be found orcould not be accessed in the remote catalog. If this is a privatebox on HashiCorp's Atlas, please verify you're logged in via`vagrant login`. Also, please double-check the name. The expandedURL and error message are shown below:
URL: ["https://atlas.hashicorp.com/sparkmooc/base"]Error: Failed connect to atlas.hashicorp.com:443; No error Hi,
Could you please explain how could it be possible to have 3 for the following computation:

Calculate the number of unique words in wordsRDD. You can use other RDDs that you have already created to make this easier.

RDD [('rat', 2), ('elephant', 1), ('cat', 2)]

Test.assertEquals(uniqueWords, 3, 'incorrect count of uniqueWords')
TVM Hello,

Is it possible to download the PY notebooks as pdf? The option is available, using LaTeX, however I receive a 500 error when attempting.
Has anyone else encountered this?

Thanks.

nbconvert failed: Pandoc wasn't found.
Please check that pandoc is installed:
http://johnmacfarlane.net/pandoc/installing.html Regarding the four lecture: SPARK KEY-VALUE RDDS
RDD:[(1,2), (3,4),(3,6) ] => [(1,2),(3,10)]
What are the aritmetic operitation for this?? There is some distributed systems and cloud computing terminology 
such as Platform as a service, Software as a service, overlay networks and may be some other terms.

I'm interesting to know how do the Spark and Hadoop fit in this terminology
and whether or not this terms somehow applicable to this parallel
frameworks (Spark, Hadoop) at all?

As for me, PaaS and overlays are good candidates for Spark framework.
Am I right?

Thanks in advance for any additional comments! Somehow when I test my code it shows 

NameError: name 'Test' is not defined

 
but if I add 

print removePunctuation(" The Elephant's 4 cats. ")

into the box above the test, it works perfectly fine

the elephants 4 cats

Anyone know what is going on?

P.S.: I did not remove any code from the notebook. Vagrant - is it normal for it to take over an hour ?

Image 1 shows 10% complete and an hour and 15 minutes to go
then about 3o minutes later
Image 2 shows 47% complete and 39 minutes left to go




vagrant_hung2.png Maybe put a hint into the Description, that one shouldn't use line.split(), because this kills empty lines (which is ultimately desired in a later stage ...) Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 577, in main
    brokenRDD.collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 713, in collect
    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 538, in __call__
    self.target_id, self.name)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/protocol.py", line 300, in get_return_value
    format(target_id, '.', name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 1 times, most recent failure: Lost task 0.0 in stage 48.0 (TID 221, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/ok/submission.py", line 566, in brokenTen
    if (val < 10):
NameError: global name 'val' is not defined

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) -- This is one of the tasks asked in lab1. It's not very clear to me to which IPython notebooks it's referred. There is no link and I'm not sure does it mean to this IPython notebooks or not...

http://ipython.org/notebook.html


Thanks, Hi

Being curious, Does the instructor and team perform any Analytics on the course students.
Such as
1) Which quiz questions is mostly answered wrong by the students
2) How long were the videos viewed, which course videos is most viewed, etc.
3) Geographical location of students and course attendance, etc.

If it can be shared, will be interesting to know ?
 In Exercise 4c, I am running the given code for loading the shakespeare.txt, and I'm getting the following error:

Py4JJavaError: An error occurred while calling o18446.partitions.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/vagrant/data/cs100/lab1/data/cs100/lab1/shakespeare.txt

Looks like the path got injected twice.  This particular command says "Just run this code", and I haven't made any settings to file locations or bathing so far.  Any thoughts?

 Hi I'm new in vagrant and find it's really interesting because the ipython notebook(jupyter) is localhost:8001 in vm, but we also can visit it in host in the same way.

PySpark is good but I want to try scala notebook such as zeppelin, so I use ssh log in the vm and install zeppelin in /home/zeppelin. zeppelin use the port 8080 and I use links to test the address - localhost:8080 and it success in vm.

But I cannot visit it in my host by localhost:8080, and I want to do that just like jupyter, so how can I do?
 When adding values to the accumulator, is it guaranteed to be atomic, i.e. only one worker can add a value at at time? Here is a RDD resource to help beginners:

Link: https://www.usenix.org/conference/nsdi12/technical-sessions/presentation/zaharia Hello 

I have problem in submitting lab.when i download the python file for upload (.py) it does not have any kind of output results. i uploaded the file and saved it after that i submitted for grading and shows there is no file there.I do not know what is problem please help me There are many technologies, which are much easier to learn and implement than MapReduce like PIG, Hive and Apache Spark. Considering the fact that it is challenging to write programs in MapReduce compare to other technologies, do we still need to learn MapReduce programming? If yes than why?

Can we say that Apache Spark is better substitute for MapReduce technology? If not than why?

I understand it is not required to learn MapReduce programming for this course. My question is generic in nature.

I'll appreciate any clarification, which will help making decision on these technologies. My mac has small ssd, and I don't want use VM. 
I use
mvn -DskipTests clean package 
to install spark, and I have ipython and java installed before. I use pyspark command, then the following:
 
[I 13:30:59.232 NotebookApp] Using MathJax from CDN: https://cdn.mathjax.org/mathjax/latest/MathJax.js
[I 13:30:59.291 NotebookApp] Serving notebooks from local directory: /Users/guorui/Downloads/spark-1.3.1/bin
[I 13:30:59.291 NotebookApp] 0 active kernels 
[I 13:30:59.291 NotebookApp] The IPython Notebook is running at: http://localhost:8888/
 
I see the jupyter is running, and I upload lab0_student.ipynb. I want to know where is shakespeare.txt and all the other files if I don't use virtual machine.

I also try to use vagrant on a linux server, but due to network issue, I downloaded
https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box using some network drive. I want to know what to do next if I have the .box file now. If possible, I only want to copy the necessary files out of the VM and use spark on my mac with those files, not using VM. Apache spark has driver node and worker nodes.  I understand these nodes are physical commodity hardware and setup as cluster; lets call it as Apache Cluster. Is this cluster is a separate infrastructure than Hadoop cluster or we configure Apache Spark on existing Hadoop cluster environment?

Since Apache Spark process data in memory hence i believe the data in Apache cluster is the copy of data stored in Hadoop cluster. Since we are duplicating data hence we need to invest more in hardware to get benefit of Spark capabilities.

Is my understanding correct? Would appreciate your feedback/input. for [in 37]
I don't understand why my solution below is incorrect

wordCountsGrouped = <FILL IN> PLEASE DO NOT POST SOLUTION CODEprint wordCountsGrouped.collect()

it's giving me the following error
TypeError: unsupported operand type(s) for +: 'int' and 'ResultIterable' I had some questions on running Spark in parallel. 
As for my knowledge on parallel, I think I understand it fairly well. I have used PThreads, OpenMP, and MPI to run programs in parallel with C. I understand that you have to separate the tasks passed to processes so each returns a desired result. I have also used the NVIDIA graphics card to run .cu files(cuda) to run on roughly 100 processors. So i understand it decently well.

But our high school course doesn't really delve deep into python 2 scripts in parallel. But I've heard of modules like processing and subprocess (a little bit about the threading module). But I do not totally understand how to translate the same idea that I used in C/NVIDIA to python scripts. Also, there is apparently something that runs python scripts across multiple interpreters?

Along with my question to how to run python in parallel, how would you translate all the efforts of parallel computing to Apache Spark? Or is it impossible/inefficient since the real world data is "dirty?" If you were able to, how much of a speed up can we get for like 1000TB of data? I think the lecture said that a test run gave 234 minutes for 1000TB. Was that running in parallel (sorry if it was in the lecture)? 
Sorry for the loaded question just wondering! I've arrived at 4c. So far, so good. :)

When I run the "# Just run this code" freebe I get:





0: 1609
1: 
2: the sonnets
3: 
4: by william shakespeare
5: 
6: 
7: 
8: 1
9: from fairest creatures we desire increase
10: that thereby beautys rose might never die
11: but as the riper should by time decease
12: his tender heir might bear his memory
13: but thou contracted to thine own bright eyes
14: feedst thy lights flame with selfsubstantial fuel
__________________________________________________________In 4d,  I get  (my <FILL_IN> xxx'd out):# TODO: Replace <FILL IN> with appropriate code
shakespeareWordsRDD = shakespeareRDD.xxxxxxxxxxxx
shakespeareWordCount = shakespeareWordsRDD.count()
print shakespeareWordsRDD.top(5)
print shakespeareWordCount
___________________________________________________________
[u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds']  <==== This passes the next test...BUT
882996                                                     <==== This FAILS the next test...___________________________________________________________...'cause it should be 927631 or 928908WASSUPWIDAT? Has anyone else come up with 882996 for shakespeareWordCount?I'd be happy to share my 4d <FILL IN> with a teaching assistant if you care to contact me.Thanks. Randy HawkinsBTW: This is a v.cool course!


 Hi,

I made it through all the week 1 installations with no problems, but I'm not very IT-literate, so I don't really understand what they are/do. Virtual Box can run virtual machines? And Vagrant gives it the configurations to use? But then we use the Juypter Web UI to run iPython notebooks? How does Juypter interface with Virtual Box and/or Vagrant?
Would love to understand the technology a bit better (and I suspect I'll have more questions of this type as the course progresses)

Thanks in advance,
Felicity I am sure I am just a fool but I believe the test funciton requires top15 to be an RDD.
I created an RDD that is sorted and formatted the way I would like,let's call it xRDD. xRDD.collect()[:15] will print the correct list in terms of values.
If using top or takeOrdered on xRDD this creates a list as far as I know
and cannot be passed to the test function of the last cell since it requires an RDD.
I thought to create an RDD from a list I would use yRDD = sc.parallelized(listData)
But this fails. I have spent an embarrassing amount of time attempting various solutions and I am sure everyone
has done it in some minutes. In short:
1. Have RDD desired but cannot truncate to 15 values as RDD.
2. Cannot convert list to RDD and vice versa.

Ending my circle of incompetence will be much appreciated.


 I noticed that the tutorial notebook refers to key as a custom comparator. Is that really an accurate description? I've always understood comparator functions to be those that take in two arguments and return a positive, negative or zero value based on some inherent ordering logic for the two items.

The key parameter on the other hand seems to refer more to a pre-processing transformation on the data before it is ordered using the underlying comparator logic of the data types being compared.

Just curious if I am misunderstanding or if the Notebook mistakenly refers to the key as a comparator. It seems that the functionality of accumulators can be duplicated by using reduce transformations. Is this always true? If so, why have accumulators at all? Hi, everyone. I want to ask a non-technical general question. I do not come from background on computer science, and I do not have some rigorous training on some computer science courses like database, network, computer architecture, and so on. (I am major in automation (undergraduate) in China, where some courses like control theory, operation research, optimization, etc are the core courses, and I also took courses like C++, data structure, algorithms, operating system, and so on)

I want to ask if I really want to master spark, or some other big data tools, do I need to have a good computer science training. 

Up until now, I do not have big difficulties in understanding the materials of this course; moreover, I do not find it very hard for me to understand some other hackers' techniques, and so on. But if I want to choose data science as a career, or pursue a higher degree on this topic, do I need to understand the mechanisms under the hook, like how to implement spark, and so on.

Sorry that I recently thinked a lot of my knowledge background, and always cannot have a taste of mastering something really solidly.  Hello,

This is the brief summary of collect action:

a. The collect action returns all of the elements of the RDD as an array. b. Ensure that all the data will fit in driver program.

Now, there is precautionary or warning note  mentioned as :

" Ensure that all the data will fit in driver program"

Please explain me why this warning is there bcoz we are talking here about processing terabytes or petabytes of data in-memory?

Does the driver program has any data limit? What is the way we can ensure that our data falls within the limits of driver program? Is there any kind of pre-check or something?

Thanks,
vibhor I am unable to execute 2 (a) and 2 (b) of spark_tutorial_student.ipynb.  I get the below error:

NameError                                 Traceback (most recent call last)
<ipython-input-9-8acce61ffdda> in <module>()
      1 # Display the type of the Spark Context sc
----> 2 type(sc)

NameError: name 'sc' is not defined

I have provisioned 2GB RAM to Spark VM.

My IPython parallel computing clusters (default and pyspark) are not running.

I started these by making # of engines = 1 or 2, but still the same.

Any help?

Thanks,
Arvind.

 This is a great course and really helping us to learn a lot. I guess it will be good if course name can be updated by removing 'Introduction to' and just leave the course name as 'Big Data and Apache Spark'. It will make this course more powerful and better for student to highlight the achievements by completing this course.

Is it possible? Being a JAVA developer i am interested to know how can we integrate Apache Spark and JAVA..  HI,

Can you please help someone how to access quizzes for the lecturers because I couldn't able to find to find quizz option anywhere.


Thanks,
Srikanth  
Hi,
When I tried running the cell i get the output1 but again when i run the cell i got the output2. Please give suggestions.

In [12]:










# TODO: Replace <FILL IN> with appropriate code
wordPairs = wordsRDD.map(lambda (k,v): (k,len(k.split())))
print wordPairs.collect()






Output 1:
 [('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]







Output 2:




---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-12-f9a9d993197e> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
      2 wordPairs = wordsRDD.map(lambda (k,v): (k,len(k.split())))
----> 3 print wordPairs.collect()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 20, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-12-f9a9d993197e>", line 2, in <lambda>
ValueError: too many values to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)






 Tutorials and labs are the meat . We only really internalize the material by running the tutorial and doing the lab.
Unfortunately, this only happen last. 
For those who like me can't wait to get their hands dirty, here the prepared meat for the upcoming week:
https://databricks.com/blog/2015/04/21/analyzing-apache-access-logs-with-databricks-cloud.html
enjoy  ....

 Hi Instructor,

From where Spark read data and bring it into memory for execution.

Like in case of Hadoop the client first contact NameNode. Then, nameNode give details about the size of the data block and addresses of the dataNodes depending on replication factor, on which dataNode client needs to copy the input data blocks.
After that, the MapReduce job code will be sent to the dataNodes on which the data blocks are available for execution.
Basically, each dataNode will have Java(JVM) and HDFS installed on it. Therefore, the data will be loaded into memory from local system and then executed.

In a similar manner, I need to understand how spark does the same thing. I mean when I execute

lines = sc.textFile("README.md", 10) // with 10 partitionsORlines = sc.parallelize(["pandas", "i like pandas"], 10) // with 10 partitions

what exactly happens underneath in Spark standalone or cluster system.

Does spark divide the input file into 10 files and then shuffle them to free node's memory directly (does not save them on node's disk) and start execution. If this case there will be too much traffic on the network.ORDoes it divide and store data files on the intended node's disk, then wait for the machine to complete its current execution. (Provided there are only four cores available(i.e four partitions) and we have partitioned the data with 10)

If it stores data on to the disk of the worker node, then which file system does it use to store. How do we know it or can we change that file system.
Basically I am looking for internal architecture of the Spark. Basically I need to know how data flows internally, so that I can design solutions in proper manner taking all these scenarios into consideration.
What software are installed on Spark's worker node. Like in case of hadoop we have Java and HDFS.

Waiting for your reply.
Thank you.   I'm very interested in comparing these two choices for big data.
  Scalding  is almost pure domain logic with very little boilerplate compared to hadoop, when writing mapreduce job, with less infrastructure types, less configuring, more focused on the algorithm, maximize expressiveness and extensibility. It wraps on cascading and mapreduce, offering substantial api just like spark.
  Tez have a lot in common with spark: both possess in-memory capabilities.
  So what about Spark  VS  Scalding + Tez  ???
  seems they offer similar power for big data. Spark fit well for iterative algorithms and interactive data mining, beside this，
  when is good to choosing Scalding + Tez ? 
  when is Spark ?
 Thanks for your attention. "(3a) Unique words.. i think the answer shouldbe which words appear once e.g.
('elephant', 1)
The answer expected is a the number of words = 3 If after uploading and clicking "Check", it seems like an endless time before anything happens: don't panic.

When I say endless, I really mean endless.
For me it happened like that:
- I uploaded the python (.py) file (a second time, the first time I forgot to remove any additional debug code... even though I was warned... -___-' )
- I clicked check.
- The work in progress appears.
- The uploader part, resets to it's initial state (ready to upload again)
- It looks like it's computing without end.
- Nothing else happens....

If this happens to you don't panic, don't use your precious submissions by trying to re-upload many times.
I just reloaded the courseware, went back to the lab, and found out my submission was actually graded.

It is not the first EdX class I had this problem, and it is probably a connection issue with the client side Java-script not being able to get server information.

I share this experience hoping it can help someone. (this problem costed me a few point in another class :p ) I  tried to install  spark in standalone cluster mode using the link -  http://spark.apache.org/docs/1.2.1/quick-start.html   but every time i run submit command it says 

Exception in thread "main" java.net.ConnectException: 
Call From Anshus-MacBook-Pro.local/127.0.0.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
while i am not using hdfs:// relative path. i was  just trying  to run  -
SimpleApp.scala  code given on the quick start page. Why it needs  haddop hdfs  running ???
 HI, 
trying to do setup for vagrant, but given ports are not showing up anything, but just a blank window.

dont know how to check the output of vagrant , can any one help Well, everything is in the summary! 

I'm just not very used to this term, and would like to understand it well, especially when the instructor talks about it in the pySpark closure video.

Thanks! The result i got is 882996;
any thing wrong?
shakespeareRDD.flatMap(lambda x: x.split(' ')).filter(lambda x: x!=' ') A few people I work with are using a python library that interfaces to a C++ library called root, developed by CERN. Would it be possible to use this library in a spark job?

I think it is unlikely, since spark runs on the JVM. But it would be extremely useful. Hi,

when I execute
lines = sc.textFile("README.md", 10) // with 10 partitions

we say "text is parallelized"

Does, it mean divide a large file into predefined number of files(i.e number of partitions) and then multiple threads will read these files and load them in parallel to worker nodes.

From where the driver node get the details of worker nodes. Like in case of Hadoop, client gets this information from "JobTracker".

Thank you. As I expect my virtual machine will grow bigger as time goes on, can I put it in an external hard disk to save the space of my local computer? (2b) Use groupByKey() to obtain the counts
The following line : 

wordCountsGrouped = wordsGrouped.map(lambda word,value : (word,sum(list(value)))

is giving 

File "<ipython-input-31-1389236edafe>", line 5
    #print wordCountsGrouped.collect()
                                      ^
SyntaxError: invalid syntax Is it on the driver node? If yes, then driver node will become single point of failure.

Is there any situation like Single point of failure in Spark?

Thank you.

 I manually added the VM box since I am in China and behind the great fire wall.  The first start up was successfully.
But while I want to restart it, I encounter following error:
C:\WINDOWS\system32>vagrant upBringing machine 'default' up with 'virtualbox' provider...==> default: Box 'base' could not be found. Attempting to find and install... default: Box Provider: virtualbox default: Box Version: >= 0==> default: Adding box 'base' (v0) for provider: virtualbox default: Downloading: base default:An error occurred while downloading the remote file. The errormessage, if any, is reproduced below. Please fix this error and tryagain.
Couldn't open file /WINDOWS/system32/base

And this is weird because I've changed the name to sparkmooc and if I ran vagrant box  list, only the sparkmooc box will be shown. 
Seems I need to pass in a parameter to indicate I need to start the sparkmooc box. But I don't know how to do that. 

Any suggestion will be appreciated. wordCounts = wordPairs.reduceByKey(add)

The above mentioned line gives the following error :

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-36-a2c4eeb4c6d9> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
      2 # Note that reduceByKey takes in a function that accepts two values and returns a single value
----> 3 wordCounts = wordPairs.reduceByKey(add)
      4 print wordCounts.collect()

NameError: name 'add' is not defined
  where should i run my code in the vm or in host machine if so how to install the test_helper module
 Okay so when I execute 'vagrant up' the command prompt displays that the sparkvm is running but in the virtualbox the sparkvm is powered off. I am able to access the localhost, but I sense that in the future it will create some problems for me. What is the solution to this problem.
Thank you.   I understand we are all for learning out here. But they say learning multiplies when it is shared and understood. If any student(s) who is interested in shared learning please let us get together and help each other out. Everything about this class and nothing else. Post your gmail account name without @domain part and I'll send invites. Thanks.
join here: https://edxlearning.slack.com/messages/apache-spark/
 Hi all,

I tried to install VirtualBox and Vagrant on my MacBook Air under OS X 10.8.5, and I was having the same issues that many of you have had with the VM trying to boot:

The guest machine entered an invalid state while waiting for it to boot. Valid states are 'starting, running'. The machine is in the 'poweroff' state. Please verify everything is configured properly and try again.

It turned out to be a permission issue. To solve it, I went into the Terminal, and ran the following two commands:

sudo chmod 755 /Applications

sudo chmod 755 /Applications/Virtualbox.app

After I made those changes, I powered the VM on through the VirtualBox GUI, and everything worked.
 print 'type of xrangeRDD: {0}'.format(type(xrangeRDD) Hi,
I know I'm too late!! I had some serious problems while downloading the virtualbox.box machine by using the code 'vagrant up', so I copied the link and downloaded it manually. But the problem is that I can't add it to my Oracle Virtual Box. 
Any suggestions? 
P.S. I'm using Ubuntu 14.04.
Thanks Hello I am trying to submit my lab1_wordcount for the recent lab. Although I have tested all the results and they are passing all the tests locally, when I submit the python file I get errors. I could modify the python file to include those libraries and see if it works but that would keep reducing my attempts.

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 335, in main
    print removePunctuation('Hi, you!')
  File "/ok/submission.py", line 332, in removePunctuation
    pat = ur'['+string.punctuation+']+'
NameError: global name 'string' is not defined

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) -- I do not understand how to combine groupByKey() and map(). Can someone help me with the code? Because inadequate description of the application of this method. Hello,

Please explain How exactly RDD can shared memory what happen if memory full?!
Does cluster need to be same CPU, memory,physical or can be any kind of cpu and memory?
If we have many workstations can we make it as distributed shared memory?

Please help me to understand more about DSM.

Thanks. I couldn't locate an answer to this in the existing posts.
I would like to log into the VM and wget files from various locations.
This is because I would like to invent my own exercises.

What is the username/password? Hi Guys,
I'm looking at problem 3B and I'm able to add values of an array.
However I'm stuck in translating it to lambda format.
The structure in basic  python would be:
def function(....):	count = 0	for i in range(0,len(...)		count = ....[i][1]	return count
How is this done using a lambda?
lambda i :... I get this error on 4c point, it seem can't load the file, but with the simple python open file command i get the file on that path .... so i'm looking for a bit help ! 











# Just run this code
import os.path
baseDir = os.path.join('data')
inputPath = os.path.join('cs100', 'lab1', 'shakespeare.txt')
fileName = os.path.join(baseDir, inputPath)
​
shakespeareRDD = (sc
                  .textFile(fileName, 8)
                  .map(removePunctuation))
print '\n'.join(shakespeareRDD
                .zipWithIndex()  # to (line, lineNum)
                .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'
                .take(15))

















---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-106-cbd912244396> in <module>()
      8                   .textFile(fileName, 8)
      9                   .map(removePunctuation))
---> 10 print '\n'.join(shakespeareRDD
     11                 .zipWithIndex()  # to (line, lineNum)
     12                 .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in zipWithIndex(self)
   1985         """
   1986         starts = [0]
-> 1987         if self.getNumPartitions() > 1:
   1988             nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
   1989             for i in range(len(nums) - 1):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in getNumPartitions(self)
    319         2
    320         """
--> 321         return self._jrdd.partitions().size()
    322 
    323     def filter(self, f):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o527.partitions.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/vagrant/Lab/data/cs100/lab1/shakespeare.txt
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:203)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:57)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:64)
	at org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:46)
	at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)



 I receive the following error while uploading the .py file for evaluation
It seems like it interprets add as a local variable. . . 

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 204, in main
    wordCounts = wordPairs.reduceByKey(add)
UnboundLocalError: local variable 'add' referenced before assignment Hi,
      I am using VirtualBox 4.3.20 version in my windows 7 34-bit. So, I started learning Spark by following all the instructions in the given videos except with out installing new VirtualBox version i.e. 4.3.28. I am able download the spark/mook project but could not able to start the Project as it was complaining below problem.     " Failed to open a session for the virtual machine sparkvm.  The VMMRC.rc module version does not match VBoxVMM.dll/so/dylib. Re-install if you are a user. Developers should make sure the build is complete or try with a clean build. (VERR_VMM_RC_VERSION_MISMATCH) "To resolve the above issue, i was started with latest Virtualbox 4.3.28. While installing this version i noticed that it by defaults updates the network related drivers ans serial drivers on the host machine which created lot of problems like continuously getting blue screen even in safe mode. So, I took another laptop with VirtualBox 4.3.28 installed correctly but after installing i am not able to connect to WIFI. So, I made some changes by un checking the bridge network drivers related to VirtualBox installation which did not work.

I am in dilemma now that how can i continue this course?   When I am running Jupyter web UI for running IPython notebooks by navigating your web browser to "http://localhost:8001/" (or "http://127.0.0.1:8001/").
I can see only
Data folder
lab0_student.ipynb
spark_mooc_version
spark_notebook.py

Please help me from where to see Lab1 word count

Once i click on the link from lab 1 course , it is opening in a browser . So do I need it to copy and save it in a notepad?

 The sparkVm can boot and login but can't access the localhost:8001,
there's a little difference between the copied one and the installation using 'vagrant up'.
what's the problem?

 Hi,I found accidentally that we need a internet connection for the python notebook to display them properly. For example it require MathJax JS library that is downloaded from a cdn.If it is possible to have a self contained VM with no online connection required could improve some user experience, I mean right now I am on holiday in Italy and there is no such IT infrastructure available (free wifi and so on). I mean for the labs / practice exercise, could be useful to have a self contained VM in case there is no connection available. There is no error or whatever, is just a very minor request just for very rare circumstances. Anyway I know at a certain point a internet connection it is required. :)But for labs / practice and solve the exercises it could be not required at all.Thanks I'm interesting to understand the difference between Apache Spark and Google Big Query, of course with Apache i can manage the distribuited infrastructure but on the other side i can start query just in time i log-in with all the power of google infrastructure ....   Hi all,
In section 5(a) of the tutorial, the code applies the reduce action to a repartioned RDD:
  filteredRDD.repartition(4).reduce(lambda a, b: a - b)
This returns the value 21

Using collect() to inspect filteredRDD.reparation(4) returns:
[1, 2, 3, 4, 5, 6, 7, 8, 9, 0]
I cannot see how applying lambda subtraction in pairs to the above list can possibly return the value 21

If you try to do this in Python you get:
>>> repartition = [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 0]>>> print reduce(lambda x,y: x-y, repartition)
 -43

However you rearrange the elements of repartition, you will not get 21 returned.

All the best
Jim Hi all!

I am trying to get lab 1 done... I opened the machine, opened the tree host.

I tried to download the iPython files, but the result was just a black and white page of code appearing in a new tab of my chrome and safari browsers (tried both).... below is an example of what the beginning of the page for the spark tutorial looked like:

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "# **Spark Tutorial: Learning Apache Spark**\n",
    "#### This tutorial will teach you how to use [Apache Spark](http://spark.apache.org/), a framework for large-scale data processing, within a notebook. Many traditional frameworks were designed to be run on a single computer.  However, many datasets today are too large to be stored on a single computer, and even when a dataset can be stored on one computer (such as the datasets in this tutorial), the dataset can often be processed much more quickly using multiple computers.  Spark has efficient implementations of a number of transformations and actions that can be composed together to perform data processing and analysis.  Spark excels at distributing these operations across a cluster while abstracting away many of the underlying implementation details.  Spark has been designed with a focus on scalability and efficiency.  With Spark you can begin developing your solution on your laptop, using a small dataset, and then use that same code to process terabytes or even petabytes across a distributed cluster.\n",
    "#### **During this tutorial we will cover:**\n",
    "#### *Part 1:* Basic notebook usage and [Python](https://docs.python.org/2/) integration\n",
    "#### *Part 2:* An introduction to using [Apache Spark](https://spark.apache.org/) with the Python [pySpark API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) running in the browser\n",
    "#### *Part 3:* Using RDDs and chaining together transformations and actions\n",
    "#### *Part 4:* Lambda functions\n",
    "#### *Part 5:* Additional RDD actions\n",
    "#### *Part 6:* Additional RDD transformations\n",
    "#### *Part 7:* Caching RDDs and storage options\n",
    "#### *Part 8:* Debugging Spark applications and lazy evaluation\n",
    "#### The following transformations will be covered:\n",
    "* #### `map()`, `mapPartitions()`, `mapPartitionsWithIndex()`, `filter()`, `flatMap()`, `reduceByKey()`, `groupByKey()`\n",
    "#### The following actions will be covered:\n",
    "* #### `first()`, `take()`, `takeSample()`, `takeOrdered()`, `collect()`, `count()`, `countByValue()`, `reduce()`, `top()`\n",
    "#### Also covered:\n",
    "* #### `cache()`, `unpersist()`, `id()`, `setName()`\n",
    "#### Note that, for reference, you can look up the details of these methods in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1: Basic notebook usage and [Python](https://docs.python.org/2/) integration **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1a) Notebook usage**\n",
    "#### A notebook is comprised of a linear sequence of cells.  These cells can contain either markdown or code, but we won't mix both in one cell.  When a markdown cell is executed it renders formatted text, images, and links just like HTML in a normal webpage.  The text you are reading right now is part of a markdown cell.  Python code cells allow you to execute arbitrary Python commands just like in any Python shell. Place your cursor inside the cell below, and press \"Shift\" + \"Enter\" to execute the code and advance to the next cell.  You can also press \"Ctrl\" + \"Enter\" to execute the code and remain in the cell.  These commands work the same in both markdown and code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a Python cell. You can run normal Python code here...\n",
    "print 'The sum of 1 and 1 is {0}'.format(1+1)"
   ]
  },


So maybe you know what I should do to be able to download the iPython files as I see only text appearing in my browser tab with no obvious way to download.

Please help.
Thanks in advance! Hi please help me solve the 3b it is the only question I am not able to solve

totalCount = (wordCounts .flatMap(lambda (x,y): [y]) .reduce(lambda a,b : a + b))average = totalCount / uniqueWords

gives output as the following which is wrong

5
1.0 Hi,

Please help. I tried the following

uniqueWords = wordsRDD.map(lambda x: set(x)).collect()

But the test is failing as it  forms the set of chars instead of words 
Example sortByKey from the lecture video Spark Key-Value RDD:
 
rdd = sc.parallelize( [ (1,’a’),(2,’c’),(1,’b’) ] )
rdd.sortByKey()
[ (1,’a’), (1,’b’), (2,’c’) ]
 
Explanation:
There are 3 tuples in the result.  

Q1: Is the ordering of ‘a’ followed by ‘b’ for key=1 a happenstance result -- not guaranteed -- or is this a deterministic result?  Do the values have any impact on these results?

Q2: Here’s a slightly different question: Is the input list’s ordering going to determine the ordering of the output, or is this result just happenstance as well?  


I could do some experimentation but I am wondering if anyone already knew what to expect from Spark, so we could save time. If you know with certainty what to expect and can share that with everyone, thank you so much!

My hypothesis:

Ans for Q1: nondeterministic (it's happenstance). However, it may be predictable on a single-core "cluster".
Ans for Q2: nondeterministic (it's happenstance). However, it may be predictable on a single-core "cluster".


I don't know what lecture number, so sorry.  The tag (folder) might be wrong. The video name is Spark Key-Value RDD.

   top15WordsAndCounts =wordCount(shakeWordsRDD).takeOrdered(15,key=lambda(w,c):-c),I write this code to finish to the 4f problem.But in fht function wordCount(),it used wordPairs.reduceByKey(add),so the output of the code"print '\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15WordsAndCounts))" is
rat: 2 cat: 2 elephant: 1 How can I correct the mistakes?thank u. uniqueWords = wordsRDD.reduceByKey(lambda x:x).collect()

or groupByKey is giving following error

ValueError: too many values to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745) I have had a lot of problems getting Vagrant to start up and connect to the VM reliably, and I have already had to "destroy" the VM and start again with Vagrant on several occasions.  I have worked with VirtualBox images previously and never had these problems, and I would be happy to work directly with a VirtualBox image here without Vagrant.  I realise the course team believe we need Vagrant to simplify some interactions with the VM, but this is not going to work if Vagrant itself won't talk to the VM.

If there are going to be tasks where we are supposed to use Vagrant during the course, could the course team please provide a set of instructions for executing these tasks manually as well, so that we can perform any relevant updates etc on the VM without Vagrant if necessary?  

If Vagrant is working for you, then you don't need to follow the manual process. But at least the rest of us would have a chance to complete the course just using the VirtualBox image on its own.

Right now, Vagrant is simply a source of tiresome and time-consuming problems and is adding no value whatsoever to my course experience. Been stuck on 4d for a while.  When I use flatmap and split like I've seen on the forum I always get a count of 927633, but if I do
split(' ',54)
I can then get the correct answer and it passes the test.  I know this isn't correct, it just seems to be a trick to get this one answer but isn't actually solving the problem correctly.  I'm really at a loss where to go next.  Seems like anything else I try just throws an error. When I try to use this answer (either the 927633 or the 927631) for problem 4e I then always get 883016 (looking for 882996).  Appears to be one minor problem I'm carrying through, but I can't find it.

Funny enough, I get 882996 in 4d (using same tansformation).  split(' ',54) means that you are looking for total of 54 lines and adding blank after each comma right?  No idea why it works...Just a hunch is that the number of lines and blanks changes the count somehow?  Anyway it shouldn't be this way, should it?
 when we run the code,
rdd  = sc.parallelize(data,4)data = sc.textfile("readme.md",4)
we are creating one RDD object with 4 clusters or workers or what? I don't understand. I'm also confused about the differences between workers, clusters, rdd? Is 'workers' used interchangeably with Clusters here? The RDD object created above is it distributed among 4 something or what? Assuming its 4 workers or clusters created, is it 4 RDDs or one RDD on 4 the 4 workers or clusters?. Please what am I missing here? Given 2 lists of integers of equal length, I want to use Spark to find all elements which are common to both lists.
On a single machine this would be an O(n log(n)) operation.
What's a good strategy to use Spark for parallelizing it? While trying to solve Lab1-3b problem, encountered the error - 
when looked under, found that we are expected to write the wordCount solution in problem 4a. Are we supposed to solve problem 4a before attempting to solve problem 3b? Sorry, Its a little confusing.
NameError: name 'wordCounts' is not defined Python Closure Late Binding is an Official Language Gotcha

http://docs.python-guide.org/en/latest/writing/gotchas/

This note pertains to the pySpark Closures lecture video. (Sorry, I don't know if it's lecture 1..10 in the folders list for piazza.)

The python closure is listed as a gotcha in the official documentation, in particular surrounding its late binding behaviors.  Closures might be fun for teachers and their quiz material, but a gotcha is really something to be completely avoided in production systems to the greatest practical extent.  Even if your software is just research software and not a production system per-se, what will happen if someone actually reproduces your research?  

Speaking as a software engineer, I ask you to please consider the impact of your work.  Do what you can to stay far away from official gotchas if you intend to be an efficient member of software developmen in general.  We don't like wasting time on other peoples avoidable messes quite honestly, as wasted time is wasted money.  In fact wasted human time is probably even worse than wasted money.

Gotchas are bad. Stay away from gotchas. Do not embrace gotchas even if you figure them out, or think you did.  Thanks from the rest of us who might have to repair, improve, or just look at your work.  Thanks! Hi everyone,

I'd like to hear your opinion on how you do organize your Spark code (yes, I'm assuming you are already using Spark for production purposes)

I come from the Java world with quite some background on Java EE applications. Here you can easily find very big/fat applications bringing in lots of dependencies and other frameworks making a simple Hello-World easily an xxGB application in size (just kidding, not that much)

Now, when working with Spark there couple of concerns that comes to my mind:
The Spark jobs you are generally very vertical and focused on achieving a specific goal. (is reusability a concern?)Those jobs should be distributed across multiple nodes (easily transferable over the local network)I understand that the tendency is to generate fat-JARs (or uber-JARs) which might be very difficult as the complexity increases (again speaking of Java)

What do you think should be the correct approach in developing Spark Jobs?
Still aiming for the maximum reusability and abstraction introducing frameworks like Spring or similar?Aiming for smallest fingerprint possible for you application? Still following the good practices but avoiding/limiting dependencies with other frameworks? 

Disclaimer: This post is intentionally very high level and wants to be just a starting point for a discussion around Spark code best practices.
Any comment is welcome Hi guys,

Here you can find a podcast from Matei Zaharia, CTO of Databricks, is good to take a look, it offers insight information about spark, how start and what drives the project.
http://www.rce-cast.com/ Does anyone know what this error means and what would be the solution?
AttributeError                            Traceback (most recent call last)
<ipython-input-36-50672052bacb> in <module>()
     10     """
     11     wordListRDD.map(lambda k: (k, 1)).reduceByKey(add)
---> 12 print wordCount(wordsRDD).collect()

AttributeError: 'NoneType' object has no attribute 'collect'  Hi,

I have installed the PySpark and Ipython as per the below site. I had a problem with VM and used the below approach as provided in another thread.

http://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/

When I try to run the first program lab0_student, i am getting the following error message.

C:\Users\arvin>ipython notebook --profile=pyspark[I 21:49:46.002 NotebookApp] Using MathJax from CDN: https://cdn.mathjax.org/mathjax/latest/MathJax.js[I 21:49:46.065 NotebookApp] Serving notebooks from local directory: C:\Users\arvin[I 21:49:46.065 NotebookApp] 0 active kernels[I 21:49:46.065 NotebookApp] The IPython Notebook is running at: http://localhost:42424/[I 21:49:46.079 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[I 21:49:59.528 NotebookApp] Using existing profile dir: u'C:\\Users\\arvin\\.ipython\\profile_pyspark'[I 21:50:18.246 NotebookApp] Kernel started: 7ff1a196-014a-43ef-8169-70d028f2ead1Failed to find Spark assembly JAR.You need to build Spark before running this program.

In notebook I am getting below error..
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-1-f7aa330f6984> in <module>()
      1 # Check that Spark is working
----> 2 largeRange = sc.parallelize(xrange(100000))
      3 reduceTest = largeRange.reduce(lambda a, b: a + b)
      4 filterReduceTest = largeRange.filter(lambda x: x % 7 == 0).sum()
      5 

NameError: name 'sc' is not defined
I read in some of the threads that it could be due to the RAM size. Note that I have 8 GB of RAM.

Request your help to resolve this issue.

Thanks in advance,
Arvind Re: video Spark Broadcast Variables:

(Sorry I do not know if this is lecture1...10)

https://github.com/databricks/learning-spark/blob/master/src/python/ChapterSixExample.py

The above link contains the github link to the python files.  They contain the definition of the lookupCountry function.  

By the way, this function is actually not in the book Learning Spark, despite the purchase page for this book being referenced by the instructor in his presentation at http://shop.oreilly.com/product/0636920028512.do   I have this book on my desk.   

Fortunately, the code for all the examples used in this book, appears to be freely published in github, in the ChapterSixExample.py file in the case of this particular function.  You don't need to buy the book to see the rest of the source code.





 Hi,

I looked at lab 1 tutorial and was able to execute all the cells. I was wondering how spark context is setup. It is not initialized in the notebook, who creates it?

 PLEASE DO NOT POST SOLUTIONS

The following seems to be incorrect:









Test.assertEquals(uniqueWords, 3, 'incorrect count of uniqueWords')










Should be: Test.assertEquals(len(uniqueWords), 3, 'incorrect count of uniqueWords')


My submission failed because of this:



Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect count of uniqueWords


 I am trying to do this code and there is no python part of it is green so when I add code is also all gray which makes me think that is part of the problem, I restarted the python code and the vagrant but no change. There rest of the codes on the lab1 are normal and contain green python syntax. Like print here is not green, here is in blue, but gray while the rest of the lab is green. So when I do ctrl+enter nothing happens. As if it does not recognize it as python code
# TODO: Replace <FILL IN> with appropriate code
# Note that groupByKey requires no parameters
wordsGrouped = wordPairs.<FILL IN>
for key, value in wordsGrouped.collect():
    print '{0}: {1}'.format(key, list(value)) I am using split() function directly on shakespeareRDD and getting this error. I think it could be a type format error so I cannot use it directly on RDD. How do I convert RDD into a string? maybe then I can use split() on that string, I hope. I submit my lab1_word_count_student.py. The first time it shows test fail because some of some line has indent. I remove the line that has indent and submit again and it shows all test passed. However when I check my "Course Progress", it shows that for Lab1, my problem scores is 0/100.
 
I submitted two more time each time it pass all test but my problem scores is still 0/100. What is wrong? SparkContext is created in the lab1 by the course team.

I am wondering how to create SparkContext object from scratch (e.g. using iPython).
Could you please provide details of the SparkContext object that is used in the lab1?
How did you configure Master parameter? 
# TODO: Replace <FILL IN> with appropriate code
wordCountsCollected = (wordsRDD
                       .<FILL IN> DO NOT POST SOLUTIONS
                       .collect())
print wordCountsCollected

Giving this Error What is wrong with my approach it is Correct as per lecture 4 Example

[('rat', <pyspark.resultiterable.ResultIterable object at 0xb0e4e42c>), ('elephant', <pyspark.resultiterable.ResultIterable object at 0xb0e4e16c>), ('cat', <pyspark.resultiterable.ResultIterable object at 0xb0e4edcc>)] VM seems to load in CMD window but finishes with the above message, Virtual box won't load VM

Failed to open a session for the virtual machine sparkvm.


Failed to assign the machine to the session (E_FAIL).
Result Code:VBOX_E_VM_ERROR (0x80BB0003)Component:MachineInterface:IMachine {480cf695-2d8d-4256-9c7c-cce4184fa048}Callee RC:E_FAIL (0x80004005)

Any clues? Had the VM up and running and was working through the work book when the machine crashed.

 I could pass all my tests locally but when I would go to upload my generated .py file I get this fail. ¿Could  anybody help me? Tanks!
Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 384, in main
    shakespeareWordCount = shakespeareWordsRDD.count()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 923, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 713, in collect
    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 538, in __call__
    self.target_id, self.name)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/protocol.py", line 300, in get_return_value
    format(target_id, '.', name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 105, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in 
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in 
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/ok/submission.py", line 340, in removePunctuation
    return re.sub(pattern, '', text.lower().strip()).split(' ')
AttributeError: 'list' object has no attribute 'lower'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --
 # TODO: Replace <FILL IN> with appropriate code
I followed the same pattern as before for length of word
pluralLengths = (wordsRDD.map(lambda w: len(w)).collect())print pluralLengths

I get wordsRDD not defined.

Basically I am mapping the words to get wordsRDD, use lambda to determine the len and collect.

What is worng.

thanks for your help
 How to take a general list of RDD in a function What empty elements exactly we are talking about??? I just finished the first lab. Pretty cool stuff. Of course it's just running on the VM on my laptop. At some point in the course will we be able to submit jobs to a bigger cluster with a lot of cores? Hi All,

When I trying to submit my work I am getting the below error message.

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 261
    .map()
         ^
SyntaxError: invalid syntax
 
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined
 
-- 0 cases passed (0.0%) --
 
Can Somebody help what is the issue here
  Many things like map, filter, flatMap etc. come from corresponding Scala functions, but collect() does not (it seems). I'm wondering if someone could comment on this? Did it happen by chance that the same name has been chosen in two different contexts? 

Also, has there been any crossover (meaning, have the developers in the two communities talked to each other) between Scala parallelizable Collections and Spark RDDs? 

Note that collect in Scala is in the Traversable trait and usage such as

someCollection.collect(pf)   //pf is a PartialFunction
is roughly equivalent to: 

someCollection.filter(pf.isDefinedAt _).map(pf) 

meaning, you drop all elements of someCollection where your pf is not defined and map the rest via pf. See http://ochafik.com/blog/?p=393

Any insight would be much appreciated! Lab1/4d help!

This is turning out to be a python class. I need a hint as to what am I supposed to do for this issue?

1 # TODO: Replace <FILL IN> with appropriate code
      2 shakespeareWordsRDD = <FILL IN>
----> 3 shakespeareWordCount = shakespeareWordsRDD.count()
      4 print shakespeareWordsRDD.top(5)
      5 print shakespeareWordCount

TypeError: count() takes exactly one argument (0 given)

  Can someone explain in detail (or forward me to a link) what the virtual box and vagrant is doing? I get the basics but wanted to know what exactly is happening in the background?

 Hi all!

When ever I was doing part 4d of the lab, I used (in a part of my code)

lambda x : x.split() #x is a line in the shakespeare
and that gave me the wrong count.

However, I used 
lambda x : x.split(' ')
and that gave me the right count!

My question: what's the real difference between x.split() and x.split(' ')? Aren't both the same? 
wordCountsGrouped = wordsGrouped.<FILL IN>
print wordCountsGrouped.collect()
I get error :-

File "<ipython-input-60-c07da6eb6232>", line 3, in <lambda>
TypeError: unsupported operand type(s) for +: 'int' and 'ResultIterable'
Not sure whats wrong here ? Greetings to those that completed this arduous lab and those that are working on it.

I was working on the lab, and I finally reached section 4e.

I got 928908 as a result for section 4d. It's right! So, I got started on 4e.

My code for 4e is below:

(since the instructions said to "filter" out the empty elements, I implemented a filter with a lambda function that returned if an item is empty.
The code is below:

# TODO: Replace <FILL IN> with appropriate code
shakeWordsRDD = shakespeareWordsRDD.filter(lambda x: x == '')
shakeWordCount = shakeWordsRDD.count()
print shakeWordCount
I got  45912 as a result which is incorrect. Could somebody please hint at what might have happened and what I need to do in order to fix this?
 I scrolled up and down the page, could not find the "Storage" tab. Count the words (4f) -------------------- Traceback (most recent call last): File "", line 5, in File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals cls.assertTrue(var == val, msg) File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue raise TestFailure(msg) TestFailure: incorrect value for top15WordsAndCounts  Good Afternoon, I have joined the course a bit late ,this afternoon, and did see that I have not so much time to finish first lab... just a couple of hours.

I am in the proccess of downloading the sparkmoc base but looks as it is not going to be possible, the time given is more than 12 hours. Is there an alternative to?:

https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box

thanks

 The removePunctuation function has a direct effect on the number of words in 4d. It clearly states to omit everything except space, letter and numbers so [^a-zA-Z0-9] and then lowering and stripping would be the solution.

In 4d, a simple split(' ') is the way to go but it result in the 927631.

While the suggested result 928908 of words seems incorrect, if I progress and run 4e, I can get correct 882996 number of words based on my incorrect 4d solution.

Two different numbers (927631, 928908)
One Transform: x != ''
One result (882996)

How that's possible?
 The following is the error I got. Based on the error message, I suspect it is the following part of code breaks down the codeimport re
def removePunctuation(text):
    from string import ascii_letters, digits
    pattern = re.compile('[%s]' % string.punctuation)
    return re.sub(pattern, '', text.lower().strip())
print removePunctuation('Hi, you!')
print removePunctuation(' No under_score!')------------------------------------------------Error Message from auto-grader is below------------------------------------------------Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 330, in main
    print removePunctuation('Hi, you!')
  File "/ok/submission.py", line 328, in removePunctuation
    pattern = re.compile('[%s]' % string.punctuation)
NameError: global name 'string' is not defined

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

  Hello everybody,

A RDD consists en elements key/value. To do exercise 2b, I would like to obtain the values of  RDD pair key/value. But I don't find in the  Spark tutorial that function I have to use. Is it a Python function?

Could somebody help me?

Thanks in advance

Carlota Vina When I run my code with these cases:    
  
print removePunctuation('Hi, you!')print removePunctuation(" The Elephant's 4 cats. ")print removePunctuation(' No under_score! ')

I've got these results:

  
hi you
the elephants 4 cats
no underscore

 but the grader gives me this:

  # TEST Capitalization and punctuation (4b)Test.assertEquals(removePunctuation(" The Elephant's 4 cats. "), 'the elephants 4 cats', 'incorrect definition for removePunctuation function')

1 test failed. incorrect definition for removePunctuation function
Any clue? please help!


 I've got these results in 4d, any hint would be appreciated, thanks!

[u'zwagger\x00d', u'zounds\x00', u'zounds\x00', u'zounds\x00', u'zounds\x00']
992904 hi, in the example of counting blank lines the accumulator 'blankLine' is incremented inside the transformation function flatMap. 

In the example there is a print statement that reads the value of blankLine on the driver. There is no action that is performed on the RDD. Therefore I am not sure how does the flatMap transformation kicks in.

Or is reading the value of the accumulator on the driver is an action on the RDD ? Also I have understood flatMap() to be a transformation. 

can you please explain.  thanks After executing exercise 7a. Caching RDD, I checked the storage tab in Spark Web UI, yet nothing is recorded there.  Shouldn't the cached RDD show up there? 
Now sum the iterator using a map() transformation

wordCountsGrouped = wordsGrouped.map(lambda w: w,count)

Not sure what to use here

help.
thx Just wondering is there is a straightforward way to modify the code in 4(c) to extend this lab so that text files with non-English characters could be processed correctly?
I have adapted the removePunctuation function so that it retains all of the following characters: ĄĆĘŁŃÓŚŻŹąćęłńóśżź. It runs as expected in the local test in 4(b).
However, in 4(c) when it is mapped to a text file such as http://www.gutenberg.org/files/31536/31536-0.txt all of these very much needed characters get removed...
 I am trying to run the virtual machine on a public server I own.
nginx is redirecting like so:

server { listen 80; server_name <my host>; return 301 https://$host$request_uri; } server { listen 443; server_name <my host>; ssl on; ssl_certificate /etc/nginx/ssl/server.crt; ssl_certificate_key /etc/nginx/ssl/server.key; access_log /var/log/nginx/<my host>.access.log main; location / { auth_basic_user_file /etc/nginx/htpasswd; proxy_pass http://127.0.0.1:8001; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_redirect http:// https://; proxy_connect_timeout 150; proxy_send_timeout 100; proxy_read_timeout 100; proxy_buffers 4 32k; client_max_body_size 8m; client_body_buffer_size 128k; } }
Notebook complains like so:

A connection to the notebook server could not be established. The notebook will continue trying to reconnect, but until it does, you will NOT be able to run code. Check your network connection or notebook server configuration.

Any ideas?

E.g. RStudio Server works with the analogous nginx config. I've input the code below:

# TODO: Replace <FILL IN> with appropriate codeshakespeareWordsRDD = <FILL IN>shakespeareWordCount = shakespeareWordsRDD.count()print shakespeareWordsRDD.top(5)print shakespeareWordCount

This gives the result:
[[u'zounds', u'i', u'will', u'speak', u'of', u'him', u'and', u'let', u'my', u'soul'], [u'zounds', u'i', u'was', u'never', u'so', u'bethumpd', u'with', u'words'], [u'zounds', u'i', u'lie', u'for', u'they', u'pray', u'continually', u'to', u'their', u'saint', u'the'], [u'zeal', u'and', u'obedience', u'he', u'still', u'bore', u'your', u'grace'], [u'youths', u'a', u'stuff', u'will', u'not', u'endure']]
122395
Why is my word count wrong? I think the hyphens and underscores should really be converted to spaces rather than removed. If you do this all the word count checks will not agree with the "answers" but, it is probably more accurate.  If you do it the way the lab is designed you will end up with some long bogus "words".

As an experiment I went looking for the longest and average word lengths.  When I did this I discovered words like these... (top 15 biggest words (word, length, count))
[(u'tragicalcomicalhistoricalpastoral', 33, 1), (u'honorificabilitudinitatibus', 27, 1), (u'sixorseventimeshonourd', 22, 1), (u'waterfliesdiminutives', 21, 1), (u'kinghenryviiiepilogue', 21, 1), (u'toandfroconflicting', 19, 1), (u'wholesomeprofitable', 19, 1), (u'deathcounterfeiting', 19, 1), (u'castalionkingurinal', 19, 1), (u'senselessobstinate', 18, 1), (u'shamelessdesperate', 18, 1), (u'obligationarmigero', 18, 1), (u'historicalpastoral', 18, 1), (u'honorabledangerous', 18, 1), (u'candlewastersbring', 18, 1)]

Then I looked at the data with grep and found the culprits.  Phrases like these...
tragical-historical,
tragical-comical-historical-pastoral
shameless-desperate
six-or-seven-times-honour'd
strong-bonded 
all-hurting
heart-wished 

When I convert hyphens and underscores to spaces I get something like these...(  top 15 biggest words (word, length, count))
[(u'honorificabilitudinitatibus', 27, 1), (u'undistinguishable', 17, 2), (u'anthropophaginian', 17, 1), (u'indistinguishable', 17, 1), (u'northamptonshire', 16, 1), (u'incomprehensible', 16, 1), (u'superserviceable', 16, 1), (u'particularities', 15, 2), (u'notwithstanding', 15, 22), (u'interrogatories', 15, 2), (u'misconstruction', 15, 1), (u'superstitiously', 15, 1), (u'enfranchisement', 15, 6), (u'distemperatures', 15, 1), (u'praeclarissimus', 15, 1)]



 Hi,
Could you please explain why the following code doesn't work:

RDD.filter(words => words.size > 0)

I tried to filter empty element for the lab1 : 4e but i get a syntax error Lab 1 was great and had a great time solving it. How do I use my new learnt ability to count words on a txt file residing on my local drive? hi 
do the partitions on the dependent rdd that is after  a flatmap transformation always correspond to the partitions of the parent ?  Meaning, do each instance of the iterator, iterate over the elements of a single partition on the parent. 

thanks  My VM passed all test successfully including lab1 4a.

The result of 4a shows:[('rat', 2), ('elephant', 1), ('cat', 2)]

The test of 4a:
1 test passed.but the autograder complains about:wordCount function (4a)-----------------------Traceback (most recent call last): File "", line 1, in AttributeError: 'tuple' object has no attribute ‚collect'

I would insist, that the return value is a list of tuples.
Any idea? I'm working on lab1 exercise and I stuck at this point:

"Use groupByKey() to generate a pair RDD of type ('word', iterator)"

I have used the groupByKey() command as it's given in the video and tutorials, but all the time it returns an error. What I am doing wrong?

My code looks like:

wordsGrouped = wordPairs.groupByKey()for key, value in wordsGrouped.collect():    print '{0}: {1}'.format(key, list(value))

And this is the error message:

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-49-58a02a279356> in <module>()
      2 # Note that groupByKey requires no parameters
      3 wordsGrouped = wordPairs.groupByKey()
----> 4 for key, value in wordsGrouped.collect():
      5     print '{0}: {1}'.format(key, list(value))

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.io.IOException: Failed to create local dir in /tmp/spark-70f50f5a-984c-4ee7-9b87-6befc71dce1b/blockmgr-b317cbc1-733f-42e4-85c1-9d2562437101/22.
org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:73)
org.apache.spark.storage.DiskStore.contains(DiskStore.scala:167)
org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:404)
org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:805)
org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:637)
org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:991)
org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:98)
org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:84)
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)
org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
org.apache.spark.SparkContext.broadcast(SparkContext.scala:1051)
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:839)
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:778)
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:781)
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:780)
scala.collection.immutable.List.foreach(List.scala:318)
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:780)
org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:762)
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1362)
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:847)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:778)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:781)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:780)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:780)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:762)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1362)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
<tt></tt>	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)



Thanks,




 Hi all,

Some time ago I read somewhere that Spark extends Hadoop MapReduce. I have my doubts about that, I would like to share them to see what the advisors and my fellow students think about it. The point is that a Hadoop Reducer http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Reducer.html has access to the key, as the signature for reduce is:

void reduce(KEYIN key, Iterable<VALUEIN> values, org.apache.hadoop.mapreduce.Reducer.Context context)

On the other hand, in Spark in the corresponding methods reduceByKey and its variants aggregateByKey, combineByKey or foldByKey, we don't have access to the key when combining the values. Imagine for example that the key corresponds to the data for some user, and that each value contains data for other users, and that we want to get the top 3 most similar users to the user in the key. We cannot do that directly in Spark because we don't have access to the user-key in the aggregation function. A workaround is copying the key in the value, transforming RDD[(K, V)] into RDD[(K, (K, V))], but that is ugly and implies unwanted data redundancy. But in Hadoop MapReduce we have access to the key during the reduce phase, in the "key" argument of the reduce method.

I'd be glad to hear your thoughts on this, maybe there is some technical reason for this?

Greetings,

Juan Rodriguez Hi all!

I know you probably have seen the likes of this message before, but when uploading the file, I get tons of error messages, even though there were none in the notebook, and all syntax was followed correctly.

My errors are below:
Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 299
    print wordCount(wordsRDD).collect()
        ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

The first error refers to section 4A, I believe, which is this:
# TODO: Replace <FILL IN> with appropriate code
def wordCount(wordListRDD):
    """Creates a pair RDD with word counts from an RDD of words.

    Args:
        wordListRDD (RDD of str): An RDD consisting of words.

    Returns:
        RDD of (str, int): An RDD consisting of (word, count) tuples.
    """
    return (wordListRDD.map(lambda x: (x,1)).groupByKey().map(lambda (s,lst):(s,sum(lst))))
print wordCount(wordsRDD).collect()

This is very confusing. If anybody can help me understand what is going on.... it would be greatly appreciated... this autograder is a pain in the but. they even use the exercises from AMP lab at U C Berkeley to do the hands-on activities
http://stanford.edu/~rezab/dao/
maybe cover both machine learning, next 
 Hi,

I was thinking that groupByKey returns RDD[(K, Iterable[V])], and as seen in the lectures that is a problem because that Iterable can be too big if too many values are paired with the same key. On the other hand, mapPartitions works with Iterators, not Iterables, I guess because we want to avoid having the whole partition in memory if possible. But, if we had a new version of groupByKey, let's call it groupByKeyIter that returns RDD[(K, Iterator[V])], then we could do efficient aggregations like those performed with reduceByKey, aggregateByKey and similar, by doing a single pass traversal of the Iterator, isn't it? In fact, we could use groupByKeyIter and then map to perform an aggregation of the values by having access to the corresponding key at the same time, which is something that we cannot do with reduceByKey and similar. This is a kind of follow up of my previous question "Does Spark extend or generalize MapReduce?".

So, what do you think? You think there is any technical limitation by which a kind of groupByKeyIter method could not be implemented?

Greetings,

Juan Rodriguez I've written 2 parallel programs in CUDA and find common similarities between them and the distributed program we just wrote here in Spark.
parallel and distributed are always mentioned together. Any excuse for that coupling In the section "Failures and Slow tasks" under lecture 3 it is stated:

This functionality is enabled by the requirement that individual tasks in a Map Reduce job are idempotent and have no side effects.

I take it that idempotent means that it always produces the same result, no matter how many times it is executed. However I'm not sure why idempotence is required. Sometimes there are several acceptable outputs to the same input and it seems unnecessary to require that only one of these is ever produced. 

Example: We wish to sort the words in a document by order of how often they appear (as in the example given in the lecture). For words that occur equally often we don't care what order they are in. Then reducer 0 may produce
[(Sam, 5), (I, 7), (am, 7)]
but it may equally well produce
[(Sam, 5), (am, 7), (I, 7)]
However if both of these are possible then the process is not strictly speaking idempotent. These changes might for instance happen if the reduction algorithm is multithreaded.

More generally if when run twice it produces two data structures that represent the same data, but organized differently in an internal representation. This might for instance be the case if the output is a binary tree which represents an ordered list.

Another class of examples I'm thinking of is distributed probabilistic algorithms. The individual tasks would almost surely produce different output on every execution, but the statistical properties of the algorithm as a whole are respected as long as we have an assumption that the probability of termination is independent in some sense of which output was about to be produced by that node. For instance maybe we don't care about small errors when sorting and our reduction instead of sorting in the usual sense guarantees a permutation of the original list, but where permutations get exponentially more unlikely the more deviations they have from the sorted list.

The only issue I can see with removed the idempotence assumption is that the MapReduce implementation might see several nodes giving different results on the same task, and therefore assume that at least one of them must have failed in some way.

I understand that the probabilistic issues could probably be worked around by passing a random number generator seed together with the data to ensure determinism (this might be preferable for other reasons as well), but there are other reasons why output might differ in irrelevant ways like a multithreaded reduction implementation, and it just appears an unnecessarily strict requirement to me. How could we use mapreduce for sorting by sending words to different reducers by their frequency?
The partitioner sort based on keys first then partiton by count?
This sounds like local sorting but the question seems to be total order sorting?

Anyone can help clear my doubt? I have closed the notebook for lab1 and when tried to open again got the following error:
Error loading notebook

An unknown error occurred while loading this notebook. This version can load notebook formats v4 or earlier. See the server log for details.

What should I do?  Please, help!

Thanks,
 Although I generally don't agree with the assumptions of the Myspace and Facebook article, it has yet to be disproved based on the time frame. So, realistically, it may very well be proved true, and the "model" (which is what they use) could be a correct interpretation in this instance. I don't think the authors of the article were doing anything but interpreting a potential model, which is exactly what our financial system does based on similar considerations.

That being said "Correlation does not imply causation" has no relevance in this case since it is the application of a model and not a research study to identify whether or not there is a significant effect from a given cause.   I submit the code but I can't get the feedback.And the http packages are as follow:
 

Can anyont help me?thanks. 
My lab0 was marked as incorrect, but I don't understand what the issue is. Is it because I submitted it late? I'm sorry if someone's already asked this question. I believe there is a minor typo on page 19 of the Week2Lec4.pdf slide deck.  There the map transformation is being compared to flatMap, but the example code for the former is written as "rdd.Map(...)" instead of "rdd.map(...)".  It's just a case mismatch, but might be worth fixing for next time. We now have a Wiki that students can edit and post their own things like FAQs.

Please use responsibly.

https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/wiki/BerkeleyX.CS100.1x.1T2015/student-wiki/
#pin Sir I am a part of an Non-Government Organization namely PENCIL and will be absent for two weeks. In PENCIL we aim to teach students of rural areas where there are no schools or other educational institutions. I am off to a 10 day camp and with travel it would take about 14 days for me to return. The place as I mentioned before is a rural area where there is no network coverage and so I will not be able to take part in the course during this time. Please excuse me and lend me permission to continue the course after that time. I am sure that I can catch up to the others within a week.

Thank you
Karthik K I am also interested in GraphX since my research is related to graph processing. I am wondering that whether pySpark includes API for GraphX. I don't see it in the document so ask here. Thank you. I have loaded the lab0_student IPython Notebook to the root (home) of jupyter, not data folder. I have gone through all the steps (shift-enter) [and tried just play button the second time, after restarting] with all the steps passing. I save as a .py file (not IPython Notebook). I upload it to grader. Everything passed except it shows a red X followed by the information below. I did not see anyone post anything on this but maybe someone solved it - if so, can you help please.


Compare with hash (2a)
----------------------
All tests passed
Compare lists (2b)
------------------
All tests passed
-- 2 cases passed (100.0%) --



 Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 202, in main
    Test.assertEquals(sorted(wordCounts.collect()), [('cat', 2), ('elephant', 1), ('rat', 2)],
UnboundLocalError: local variable 'wordCounts' referenced before assignment

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --  Hello everyone, I ran the code the the 4f part of the lab and got this as an output : 
 I have printed the value of top15WordsAndCounts as well. My 4a function passed the test. 
[(u'1', 85), (u'10', 3), (u'100', 1), (u'101', 1), (u'102', 1), (u'103', 1), (u'104', 1), (u'105', 1), (u'106', 1), (u'107', 1), (u'108', 1), (u'109', 1), (u'11', 1), (u'110', 1), (u'111', 1)]
1: 85
10: 3
100: 1
101: 1
102: 1
103: 1
104: 1
105: 1
106: 1
107: 1
108: 1
109: 1
11: 1
110: 1
111: 1


I used the wordCount function that we created in 4a and then the 
takeOrdered() function to generate the 15 words. And what I got as output really confused me. 

 I have started the lab , but confused as per lecture we can use lamda in map()
I wrote
wordsRDD.<FILL IN> return ( wordsRDD)
but getting the error
wordsRDD.<FILL IN>
                          ^
SyntaxError: invalid syntax
please help me to understand and also where i can get help Hello,

Any advice why uploading the py file would fail every case even I know I have some of them right. Here is the message its spits out:

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 405
    shakeWordsRDD = shakespeareWordsRDD.<fill_in>
                                        ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) -- This is my function:

def wordCount(wordListRDD):       wordRDD=<FILL IN> Please do not post solutions    wordCountsCollected = <FILL IN>    return wordCountsCollected                                 print wordCount(wordsRDD)

and it is the error

TypeError                                 Traceback (most recent call last)
<ipython-input-37-6715860d55ba> in <module>()
      7 
      8 
----> 9 print wordCount(wordsRDD)
     10 

<ipython-input-37-6715860d55ba> in wordCount(wordListRDD)
      2 def wordCount(wordListRDD):
      3 
----> 4     wordRDD=<FILL IN>
      5     wordCountsCollected = wordRDD.<FILL IN>
      6     return wordCountsCollected

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in parallelize(self, c, numSlices)
    336         # Make sure we distribute data evenly if it's smaller than self.batchSize
    337         if "__len__" not in dir(c):
--> 338             c = list(c)    # Make it a list so we can compute its length
    339         batchSize = max(1, min(len(c) // numSlices, self._batchSize or 1024))
    340         serializer = BatchedSerializer(self._unbatched_serializer, batchSize)

TypeError: 'RDD' object is not iterable

I can't figure out what the problem is,can anybody help me ,thx I got stucked in lab 1 exercise 2b.
(2b) Use groupByKey() to obtain the counts 
Using the groupByKey() transformation creates an RDD containing 3 elements, each of which is a pair of a word and a Python iterator.
Now sum the iterator using a map() transformation. The result should be a pair RDD consisting of (word, count) pairs.
my code : wordCountsGrouped = wordsGrouped.<FILL-IN>print wordCountsGrouped.collect()

Error : 
File "<ipython-input-138-b56e6de47bc7>", line 2
    wordCountsGrouped = wordsGrouped.map(lambda (wordsList,[x,y]):(wordsList,x+y))
                                                           ^
SyntaxError: invalid syntax I am using a Mac and when I try to download the files for the mood setup, they are not recognized as a zip file and so I cannot complete the unzip using the Terminal function as noted in the instructions. I also cannot open them with any other program. Thoughts? Here's the link I pasted:https://github.com/spark-mooc/mooc-setup/archive/master.zip

When I go to my downloads, the files are there, but they are not zip files - they are fils that according to the properties are Adobe Read Documents, however, I cannot open them. I get the following error: Adobe Acrobat Reader DC could not open 'lab0_student.ipynb' because it is either not a supported file type or because the file has been damaged (for example, it was sent as an email attachment and wasn't correctly decoded).

I can power up Virtual box, but cannot access any of the files. Anyone else have issues with the Mooc setup files? I cannot proceed without them and I cannot seem to get them downloaded. 

 When I click on the links: https://raw.githubusercontent.com/spark-mooc/mooc-setup/master/spark_tutorial_student.ipynb and https://raw.githubusercontent.com/spark-mooc/mooc-setup/master/lab1_word_count_student.ipynb I get the error: 
Certificate-based authentication failed

ERR_BAD_SSL_CLIENT_AUTH_CERT

I tried changing the browser, but the same thing happens. I am not able to download the notebook files for either of them. Any solutions? All,

I think I am not getting something obvious. So the goal of the punctuation is just to normalize the text to get the word counts by removing punctuation and what not. I did that but I am getting only 122K words, not 896K for some reason my list is contains every word per line instead of just a list of words that can be reduced. I am missing something, but not sure what. Any help is appreciated.

-Andrew got below errror please help. i have got all the correct test results

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 335, in main
    print removePunctuation('Hi, you!')
  File "/ok/submission.py", line 333, in removePunctuation
    pattern = re.compile('[%s]' % string.punctuation)
NameError: global name 'string' is not defined

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

  How can word count be 927631 ?

I removed everything other than characters and numbers in 4b

I am getting only 882996

What could be the possible mistake here ?

NB : Solved Thanks all :) I already have linux installed.

Is it possible to set up an environment on the system without installing a VM?

My system has only 2 gb ram and I would like to run it natively. Submitted ipython file to autograder .. all the tests passed in ipython

However autograder does not agree ...

End of the week so maybe better luck next lab

Pluralize and test (1b) ----------------------- All tests passed Apply makePlural to the base RDD(1c) ------------------------------------ All tests passed Pass a lambda function to map (1d) ---------------------------------- All tests passed Length of each word (1e) ------------------------ All tests passed Pair RDDs (1f) -------------- All tests passed groupByKey() approach (2a) -------------------------- All tests passed Use groupByKey() to obtain the counts (2b) ------------------------------------------ All tests passed Counting using reduceByKey (2c) ------------------------------- All tests passed All together (2d) ----------------- All tests passed Unique words (3a) ----------------- All tests passed Mean using reduce (3b) ---------------------- All tests passed wordCount function (4a) ----------------------- All tests passed Capitalization and punctuation (4b) ----------------------------------- Traceback (most recent call last): File "", line 3, in  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals cls.assertTrue(var == val, msg) File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue raise TestFailure(msg) TestFailure: incorrect definition for removePunctuation function Words from lines (4d) --------------------- Traceback (most recent call last): File "", line 3, in  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals cls.assertTrue(var == val, msg) File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue raise TestFailure(msg) TestFailure: incorrect value for shakespeareWordsRDD Remove empty elements (4e) -------------------------- Traceback (most recent call last): File "", line 1, in  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals cls.assertTrue(var == val, msg) File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue raise TestFailure(msg) TestFailure: incorrect value for shakeWordCount Count the words (4f) -------------------- Traceback (most recent call last): File "", line 5, in  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals cls.assertTrue(var == val, msg) File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue raise TestFailure(msg) TestFailure: incorrect value for top15WordsAndCounts -- 12 cases passed (75.0%) for public cases -- I have installed vagrant and oracle Virtual box. Virtual machine is running but browser is not showing anything .  This is the error message:Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 424, in main
    top15WordsAndCounts = wordCount(shakeWordsRDD).takeOrdered(15, lambda x: -x)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1174, in takeOrdered
    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 713, in collect
    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 538, in __call__
    self.target_id, self.name)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/protocol.py", line 300, in get_return_value
    format(target_id, '.', name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 137, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1174, in 
    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
  File "/usr/lib/python2.7/heapq.py", line 432, in nsmallest
    result = _nsmallest(n, it)
  File "/ok/submission.py", line 424, in 
    top15WordsAndCounts = wordCount(shakeWordsRDD).takeOrdered(15, lambda x: -x)
TypeError: bad operand type for unary -: 'tuple'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --
 How shall I change the name of a folder in the Jupyter ?? When I try to import numpy, I get an error: "module" object has no attribute "ranf" (or any other function I'm trying to use).

I know that the python version and libraries we are using on this course are not the same that I am using on my local machine (where I have Anaconda and therefore numpy installed).
So, how can I install new libraries? Or should I use the version of python installed on my local machine.

I really don't know. Thanks.

 Hey, I am stuck on exercise 4a lab1.
I have written this code:

# TODO: Replace <FILL IN> with appropriate codedef wordCount(wordListRDD): """Creates a pair RDD with word counts from an RDD of words.
Args: wordListRDD (RDD of str): An RDD consisting of words.
Returns: RDD of (str, int): An RDD consisting of (word, count) tuples. """ return wordListRDD.map(lambda w: (w, 1)).reduceByKey(add)#(wordCounts)print wordCount(wordsRDD).collect()

but it wont work properly when I use the wordCount function in (4f). When I run:

# TODO: Replace <FILL IN> with appropriate codetop15WordsAndCounts = wordCount(shakeWordsRDD).takeOrdered(15)#<FILL IN>, I THINK THIS CODE IS CORRECT BUT NEED TO CHANGE DEFINITION OF WORDCOUNTprint '\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15WordsAndCounts))

All I get is:

1: 85
10: 3
100: 1
101: 1
102: 1
103: 1
104: 1
105: 1
106: 1
107: 1
108: 1
109: 1
11: 1
110: 1
111: 1

Could some one please help me? What should my return line have in it? Thanks I submitted a .py file to the grader and got zero points:


I haven't finished all subsections but there are more than half test cases passed in my python notebook. Must I finish all subsections to score any point?? Is Lab1 either 0 points or 100 points?
 When working on the 4d question, I've replaced the <FILL IN> section with a map, passing a string split (' ') function, it seems to work, but it throws a pY4JJavaError on the following line (the count() line).

Any thoughts?

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-121-0af6cd92b823> in <module>()
      2 import string
      3 shakespeareWordsRDD = shakespeareRDD.map(string.split(' '))
----> 4 shakespeareWordCount = shakespeareWordsRDD.count()
      5 print shakespeareWordsRDD.top(5)
      6 print shakespeareWordCount

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 96.0 failed 1 times, most recent failure: Lost task 0.0 in stage 96.0 (TID 316, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <genexpr>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
TypeError: 'list' object is not callable

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
 Are there any instructions how to use Jupyter? The Help is not particularly helpful, imo. In particular I misunderstood the functions of the cut and paste buttons.  In my ignorance I assumed would delete and paste the text that I had hilighted. Instead they delete and paste entire cells.  There doesn't seem to be any way to undo a delete, or to paste the deletes back where they came from, and I wound up totally mangling the notebook. As I didn't have a checkpoint as a  backup I had reload the notebook and start again from scratch, wasting hours of work.  I have started vagrant using "vagrant up" command. It finished successfully. 
 
After that I am not able to open http://localhost:8001 or http://127.0.0.1:8001 in IE or chrome. I was able to do the same yesterday but today it is not opening. I am getting error message: "The Web Page is not available"
 
Is anyone facing the same problemjQuery1710801274592988193_1433931872951?
 
Kindly let me know the solution as my struck to finish my assignment...
 
Regards,
Mayank 
It seems to me all tests passed, but it gave me this red 'X'

Here's my python file:

lab0_student3.py

Any idea? As weird as it sounds, my count is 122395.
It seems that I made a list of every line, instead of every word...

However, in my split function, I passed an (' ') argument.

In the 4b exercise, I first used the lower function, then a re.sub just for the ' on the middle of words (it's -> its), then a re.sub [W_]+, and finally a strip function.

Any ideas?



[[u'zounds', u'i', u'will', u'speak', u'of', u'him', u'and', u'let', u'my', u'soul'], [u'zounds', u'i', u'was', u'never', u'so', u'bethumpd', u'with', u'words'], [u'zounds', u'i', u'lie', u'for', u'they', u'pray', u'continually', u'to', u'their', u'saint', u'the'], [u'zeal', u'and', u'obedience', u'he', u'still', u'bore', u'your', u'grace'], [u'youths', u'a', u'stuff', u'will', u'not', u'endure']]
122395
 I have started vagrant using "vagrant up" command. It finished successfully. 
 
After that I am not able to open http://localhost:8001 or http://127.0.0.1:8001 in IE or chrome. I was able to do the same yesterday but today it is not opening. I am getting error message: "The Web Page is not available"
 
Is anyone facing the same problem???
 
Kindly let me know the solution as my struck to finish my assignment...
 
Regards,
Mayank Hi all,

I am stuck on 2c of lab 1. Actually, I've gotten all my code to work and all the tests have been passed for lab1, but I feel I have cheated in 2c. I had to import add from the operator and use it as the argument in reduceByKey. That is the only way I could find to sum up the values in the RDD.

my code looks like this:

# TODO: Replace <FILL IN> with appropriate code# Note that reduceByKey takes in a function that accepts two values and returns a single valuefrom operator import add #I DON'T KNOW IF I'M CHEATING BY IMPORTING 'ADD' THIS EARLY ON... 'ADD' IS IMPORTED LATER ONwordCounts = wordPairs.reduceByKey(add)print wordCounts.collect()


This passes the test, but I had to edit more than just the <FILL IN> section and so I am sure there is a simpler way to do it since 'add' is automatically imported later anyway. Can anyone help me out? Thanks! Can I get some hint for map() in 3b?
I understood I should extract only values from pairs and make a list.
But I can't find how to do.
Is there any documentation I can refer to? Python docs suggest to use translate() instead of re - https://docs.python.org/2/howto/regex.html#use-string-methods, so I used the following code in removePunctuation:

t = text.strip().lower()return t.translate(None, string.punctuation)

It passes the test but does not work with Spark code (4c) and throws TypeError:
...File "<ipython-input-45-e925849f70fb>", line 19, in removePunctuation
TypeError: translate() takes exactly one argument (2 given)...
What's happening here?
 Exercixe 3B.

wordcounts not defined is the error that I am getting. Please help I'm assuming that 'take' executes in one of the worker and 'takeOrdered' executes in driver program. Is this correct? Hello,
I submit the lab, just want to check .
I didn't complete section 4(c-e), but grader gives %0.

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 403
    shakeWordsRDD = shakespeareWordsRDD.<fill_in>
                                        ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

Thanks Hello Everyone,

I seem to be stuck on the first line on the lab. I try to run the first line and it returns "sc" not defined. How do I solve this problem? I exported the finished test notebook to a python file and tried to upload the file to the autograder. After selecting the file in the file-chooser and pressing the "check" button the text next to the file-chooser button switches from the selected file-name to "no file selected" and starts the checks with an empty-file. Obviously the check fails with the output:

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 267
    .map()
         ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --
I tried it three times with always the same result.  Any idea what's going wrong and how to submitt the test notebook correctly? Note, that I could submitt  the first notebook (workplace setup) without any problems.

regards,
 Olaf This is my code :

# TODO: Replace <FILL IN> with appropriate codefrom operator import add
totalCount = (wordCounts .<FILL IN> Please do not post solutionsprint totalCountprint round(average, 2)

------------------------------
This is my error






---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-2-4adf45aa297c> in <module>()
      2 from operator import add
      3 
----> 4 totalCount = (wordCounts
      5               .<FILL IN>)

NameError: name 'wordCounts' is not defined




 I cannot understand why the optional key function has to be:

takeOrdered(15, lambda x: -x[1])

instead of 

takeOrdered(15, lambda x: -x)

Any precisions? The relation between Data Science and BA was made clear in the presentation but not the diference. Can anybody contrast these two? It is something I find confusing. Thanks! This is a recurring issue.

I am repeatedly getting the error that some variable/ rdd is not defined, specially after starting a new session after stopping using vagrant halt.

Please help I just submitted my first assignment . Attaching the screenshot of the autograder, when I did check it says all tests passed.

But it says select a file. Attaching the screenshot, can u please let me know if I am good , or do I need to do anything else.

Thanks. I have already wget'd the two required ipynb files to here as you can see:

ubuntu@ip-172-31-60-32:~/mooc-setup$ lslab0_student.ipynb lab1_word_count_student.ipynb README.md spark_tutorial_student.ipynb Vagrantfile
However my browser that requests he url here is not showing the two files, but just the lab0_student file only:

http://ec2-52-0-207-87.compute-1.amazonaws.com:8001/


What else needs to be done to show the two files in the browser?  Physical directory seems to be same one where lab0 file was, but that's not actually doing the trick.  

Please advise.  Thanks so much. 

 I think I pass the testings in the Virtualbox machine, and when I  upload the python code to autograder, it keeps saying like this:

I checked the Line 334, even re-type the whole line to ensure there is no such '/Xe2'  in the line, but  the problem is still there.

 I am using windows ,and I am wondering if this is just my case or there is something wrong with the autograder...


Alan

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 334
SyntaxError: Non-ASCII character '\xe2' in file /ok/submission.py on line 334, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --


 Hi everyone!

I was thinking about some problems of key-value in Spark. Based in our knowledge until the present stage in this course, how could we solve the question below?

I have a list [(1,1),(1,2),(2,3),(2,4)]. The goal is to return the list [(1,4),(1,5),(2,10),(2,11)]. The first values are the keys and the second values are calculated by the sum of the current second value and the sum of the second values of the same key. For example, (1,4) is the result of (1,1+(1+2)).  

Assuming that I just have the list, what would be the best approach to solve it with spark?

The first approach that came to me was to map the the list by key with the sum function (as discussed earlier). If I just wanted the sum of the values by key it would be easily solved by reduceByKey function. But this is a bit more complicated.

What would you do? There is a note in the instructions not to post the exercises to public repositories like github.

Does that apply even after the deadline and grace period are over (e.g. if the instructors want to reuse the exercises for future courses)? In spark tutorial when I execute following command[ "print filteredRDD.takeSample(withReplacement=False, num=6)" ] its generating two Jobs. All earlier actions tested are generating only 1 job (sometimes with multiple stages). I am curious to know why takeSample action creates two jobs and what operations are happening in each job. How can we know that? 

Regards Good day,
 I can't seem to figure out how to edit in the lab1_word_count file, that is to do the <FILL IN> because the file does not allow editing. How someone give me some help please. Do you have the same problem? Hi,

I've uploaded my exported lab1.py up to 5 times without success.

The first times, the autograder did not show any answer. Just the "thinking circle" moving.

The two latests times I received two different but similar errors:
Unable to deliver your submission to grader (Reason: unexpected HTTP status code [504]). Please try again later.
Unable to deliver your submission to grader (Reason: unexpected HTTP status code [503]). Please try again later.

I'm running out of submissions.
I will try tomorrow again, but any help is appreciated.

Kind regards.
Santi. Do I have the choice to upload the python file with what  solved so far ( I've reached the 3_b) or I have to finish it all no matter what.
Thanks Any R lovers around? I'm interesting in R on Spark but I was wondering if there are any 'drawbacks' in using R with Spark because R is not a 'general purpose' programming language after all... What do you think?

In addition, do you know any good resources about R and Spark?


Best regards,

Iván Venzor C. After coding for the punctuation exercise, I got the following value :

hi you
no underscore
But the test fails. Is this the intended output ? I successfully ran the VM , uploaded lab0 , uploaded tutorial and ran everything with no issues . i started lab1 and got the following issue 

if i copy the same code ot the tutorial file and run it , it works fine .... 











wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']
wordsRDD = sc.parallelize(wordsList, 4)
# Print out the type of wordsRDD
print type(wordsRDD)

















---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-11-6d1090bed016> in <module>()
      1 wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']
----> 2 wordsRDD = sc.parallelize(wordsList, 4)
      3 # Print out the type of wordsRDD
      4 print type(wordsRDD)

NameError: name 'sc' is not defined



 shakespeareWordsRDD = shakespeareRDD.<FILL IN> PLEASE DO NOT POST SOLUTIONS

The mentioned line gives syntax error. If I use only lambda line : line.split(), I get error in test but not in exercise.

Please help me understanding why this issue Hi folks!

Here in Chicago, the Spark Users Group will be running several meetings to discuss some of the MOOC's aspects, and trying to expand the course. Anyone is welcome!

http://www.meetup.com/pt/Chicago-Spark-Users/

Regards,
Ricardo Hello,

I'm stuck on this question,i'm trying to use split function but couldn't get the correct answer.
Need help.

Thanks, I am definitely missing something!! I need to convert(map) the RDD to a string first prior to using the split function. I went back and looked at the previous exercises and the mapping function converts/modifies lists. Can anyone point me in the right direction? Thx Hello,

I am a new student this my first course on edx. I submitted the python code for lab0 at the end of the "Setting up the Course Software Environment". But for more than 10 minutes. All I can see is the following message "Your files have been submitted. As soon as your submission is graded, this message will be replaced with the grader's feedback." 

Is that NORMAL?

Thanks
EbinSaad  Starting with section 1D the test scripts start give me error name 'Test' is not defined.  I have no experience with Python yet the test scripts look exactle like Node.ja Mocha/Chai/Supertest so I understand how it all works.  The cell previous to the test cell runs fine and gives me an expected result that matches the test result.  The test prior to 1D runs fine.  I have no idea what is up with it.  Would appreciate an advice. 




















# TODO: Replace <FILL IN> with appropriate code
top15WordsAndCounts = wordCount(shakeWordsRDD).<FILL IN> DO NOT POST SOLUTIONS
print '\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15WordsAndCounts))

















cat: 2
rat: 2
elephant: 1





 The circle just keeps on moving . I even tried signing in again,but the circle keeps moving in lecture 4 we are given the list of tuples [(1,2),(3,4),(3,6)] and the action sc.reduceByKey(lamba a,b:a+b) which yields

[(1,2),(3,10)] . Could someone explain how the answer was generated. From the lecture:

The second element is the tuple 3, 10.The 10 came from taking the two tuples-- 3, 4 and 3,6-- and combining the values usingthe lambda function, which summed themup giving us the value 10. 

Does not explain for me how that lambda states to do that (and for instance ignore the first tuple (1,2)

Thanks in advance Hi, So I don't remember how to do this step. I'm in Jupyter but I don't remember how to get a .ipynb file accessible to me? Do I use the upload button to do the "download" Thanks. The files shown here are urls, not sure how to download or upload them?

Download the IPython notebooks.  Make sure that the file extension is .ipynb.  If the download adds an extension (e.g. ".txt"), rename the file so that the extension is just .ipynb.
Spark Tutorial: https://raw.githubusercontent.com/spark-mooc/mooc-setup/master/spark_tutorial_student.ipynb. You can view this tutorial online here.Lab 1 Word Count : https://raw.githubusercontent.com/spark-mooc/mooc-setup/master/lab1_word_count_student.ipynb. You can view this lab exercise online here.
 Hi,

Whenever I boot my computer and start the vagrant with vagrant up, I always need to download the box. Is it the right process to booting the vagrant up?

Is there any way using which I can store the files (in this case, box) and use it as many times I want.

Thanks,
Prateek What is "Master" - new terminology introduced in Lab 1 tutorial?

We were told about Driver and Executioner mot master ...  I have completed the first lab and now I am wondering how does the checker work:
I tried opening the .py file and it seems to have no information about my code or my result for that matter.
Maybe I'm missing something? Is there some information inside that is not seen in the text editor? 
rdd = sc.parallelize([(1,2), (3,4), (3,6)])
rdd.reduceByKey(func)(lambda a, b: a+b)
RDD: [..] -> [(1,2),(3,10)]

The video instructor explains that the 10 came from adding 4 and 6 in the lambda func. But He does not explain where 1,2 and 3 came from. Please explain. It would also help to understand what exactly gets passed into the lambda function.  sometimes i have the right solution but i get a error. when i restart the notebook it works. does anyone experienced the same ? We've been advised not to run run multiple notebooks simultaneously.What if I do run multiple notebooks simultaneously? Hi,
I submitted .py file for Lab1 with correct (test passed) as well incorrect solution but I got the following error for both correct and incorrect solution. Please let me know if you experienced the same:

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

  
Still not sure what to do here. Should a create a function and call it in map function for counting each key/value length or is there a way to do it with lambda?
Thanks

wordCountsGrouped = wordsGrouped.map(count) My test cases passes in jupyter notebook but after submitting the same, I am getting lot of errors which are below:
Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 264, in main
    average = totalCount / float(uniqueWords)
TypeError: unsupported operand type(s) for /: 'tuple' and 'float'

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

Did someone else also come across this error? What's the issue here? Hello all, I'm doing the lab 1 5b advanced actions part (see below) and I got an error message "defaultdict..." is there a way to overcome this error?

# Create new base RDD to show countByValuerepetitiveRDD = sc.parallelize([1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 3, 3, 4, 5, 4, 6])print repetitiveRDD.countByValue()

Error!

defaultdict(<type 'int'>, {1: 4, 2: 4, 3: 5, 4: 2, 5: 1, 6: 1}) I use Ubuntu 15.04 . Will i require to download virtual box and vagrant ? If so why ?
 In the "# Just run this code ... " section I get the error below, related to my removePunctuation code (which passed 4b test):

The offending line is: 
sampleString = str.lower(text)

any suggestions ??

line 18, in removePunctuation
TypeError: descriptor 'lower' requires a 'str' object but received a 'unicode' I keep clicking on the link http://localhost:8001/ and http://127.0.0.1:8001/  and my browser doe not connect. How do I fix this please?

I'm getting a little tired of running into issues with this course. I was experimenting with parallelizing a large-ish csv file (200MB).  I got a memory error.  My first big data problem ;-)  Some of the fields are text but the cardinality is low enough where I can easily map to integers.  My question: is this a common task to have to consider in your pipeline, and if so, are there any best practices around this?  I wrote a little class to map values to numbers with the ability to remap back to the original text but I worry that I'm re-inventing the wheel.  Thanks! My issue




I tried out all possible solutions I could find, on net, on piazza, on cs1001x FAQs:
1. Tried starting sparkvm from virtualbox UI
2.The CPU Virtualization  has been enabled
3. Everything operated is run as Admin.
4. Windows Update KB3004394 is not there
5. Tried with VirtualBox 4.3.29
My machine details:
HDD free space : 54GB on C:\ drive
RAM: 4 GB
OS: Windows 8 (64-bit)
Processor: Intel Core i5-2450M @ 2.5*2

Now i am stuck for last 5 days........ please help I am learning Python in parallel with learning Spark.  When I came to item 2b I realized that I have no idea how to sum up values in the list that is a part of the dictionary (2a output is a dictionary right?) and do it all in a lambda function.  Intuitively it is clear that I should  pick an element by its key, somehow access the array of values and sum those up yet I have not idea how to right in a way Python would understand....If someone could point me in a right direction it would be awesome, thanks! Week 2 Lecture 4 says "About RDD's" 
They are immutable once you create an RDD.You cannot change IT.You can transform it.

If you cannot "change" an RDD how can you "transform" it. ^=^  any ideas. 

Thank Yoo
 I have submitted my code with every single test of exercise 1 (1b-1f) passed on my VM. I don't know if I will manage to submit the rest on time, but I was expecting to get credit for what I've done so far. I have checked the py file before uploading to make sure it actually contains my solutions.  However, the autograder does not recognize anything of what I did and its output seems to indicate that nothing is actually done in my file regarding these tests.
It does not seem to be anything related to non-ASCII chars.
Can someone provide me with any hints?

Thanks!

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 164
    wordsGrouped = wordPairs.
                             ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --


 Does Spark lazy evaluation perform modifications on the computation graph specified by the RDD transformations?

I have a concrete question: let's say I run rdd.map(f).map(g). where f and g are unary functions. How far can the lazy evaluation reason about the computation graph? Could it, for example, reorder the execution and run rdd.map(lambda x: g(f(x))) using one less map?
 Is there an autocomplete function in the Notebook? No spoilers, but in section 3 of this weeks assignment I found a 'builtin' in the pyspark
docs I could use. 

Is that ok, or are we supposed to be only using methods we've been told about? Hello there,

I have been able to worked out until (4d). I am, however, having issues with (4e) as I get a count of 927631 as opposed to 882996 that the TEST is expecting. Any ideas or hints on this?

Many thanks!

Rogelio (Roger) I submitted lab 1,but submitted it to the wrong autograder(I went to the auto-grader for set up)...so it deducted 20% as a late submission even though I had already turned it in...can you fix this? I think I can avoid this in the future...thx Can anyone help with this issue? The subsequent test requires the count to be 927631 or 928908 for passing. I am returning the split of the line passed in. I do pass the first 5 item test though!

Any idea what may be going wrong? I just sign up for the course and am setting up my vm . Please can I submit my Exercise without the deduction?  Hi, I just registered for this class today and see some of the labs are have late due dates. Are there adjustments for those who signed up late? (Otherwise why offer the ability to sign up and provide the option of  certificate). 

Just a question since I just set up my environment and it seems dumb to penalize someone for something so simple.

Thanks,

Ray Can anyone please suggest what would be best to make my hands dirty with spark programming. Thank you in advance. Hello , I have the following list :

list1 = ["one","two"]
list1RDD = sc.parallelize(list1, 4)

daRDD = list1RDD.map(lambda x:list1)
print daRDD.collect()

And I have the following result :
[["one","two"],["one","two"],["one","two"],["one","two"]]

What should I do for having this result :
["one","two"] ?

THanks
Gorka Some tips? I'm thinking to use filter function, but apparently I'm not doing it in the right way, since I'm getting the error:

PythonRDD[52] at RDD at PythonRDD.scala:43


Thanks, Hi,
  My counts up to 4d are correct. I got 928908 for the word count,  but I seem to be doing something wrong in 4e.  I filtered out the empty elements using the "filter" command, but then I get a count of 883237

Not sure what I am doing wrong --any help would be appreciated!

Thanks!
Srini

 I was going through the exercise okay, then suddenly I kept getting these kinds of errors:

NameError: name 'wordsRDD' is not defined

So I started again with a new notebook. For awhile everything was going fine, now it's happening again, even for the parts that were previously running and passing the tests. These are now throwing 'not defined' errors. print filteredRDD.collect()

[0,1,2,3,4,5,6,7,8,9]


print filteredRDD.repartition(3).glom().collect()

[[3, 4, 5, 6], [0, 7, 8, 9], [1, 2]]


print filteredRDD.repartition(3).reduce(lambda a, b: a - b)

13


Could you please explain why the result is 13? Hello,

I just joined the class today but the deadline for lab0 already passed. I got error in submitting it. Anyone can help me?

Thanks 

What 's wrong with my job? How to solve that ?
PLEASE DO NOT POST SOLUTION CODE



<img src="https://d1b10bmlvqabco.cloudfront.net/attach/i9esrcg0gpf8k/iafy8ynzhn94lw/iarjsxfhxblp/屏幕快照_20150611_上午10.09.10.png" /> Hi,

I am trying to do lab1 and had to FILL IN the following:

pluralRDD = wordsRDD.map(FILL IN)print pluralRDD.collect()

Since in the notebook we have already created RDD I assume that its definition is already known and we should not get any error for wordsRDD as undefined. 

Now I add makePlural() in side the map function:

pluralRDD = wordsRDD.map(makePlural('cat'))

Again I am getting error that makePlural() is not defined, so I added the definition of makePlural() so that the compiler sees the definition of it.

def makePlural(word): return word + 's'pluralRDD = wordsRDD.map(makePlural('cat'))print pluralRDD.collect()

After running this I am getting the following error:

**********************************************************
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-7-a9f59a35758c> in <module>()
      3     return word + 's'
      4 pluralRDD = wordsRDD.map(makePlural('cat'))
----> 5 print pluralRDD.collect()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
TypeError: 'str' object is not callable

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

 
***********************************************************

Now I want to understand how to solve these errors in the lab1 exercise. 

My understanding is that as we proceed in the assignments all the variables and functions declared earlier should be visible but that is not happening.

Any help will be appreciated... Hello,
I spend 10 Hrs to finish lab 1.
I'm curious to know how many hours do you spend to finish lab 1?!

Thanks,
 When I attempt to run the code provided in part 4c of lab 1, I receive the following error message:
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-85-cbd912244396> in <module>()
      8                   .textFile(fileName, 8)
      9                   .map(removePunctuation))
---> 10 print '\n'.join(shakespeareRDD
     11                 .zipWithIndex()  # to (line, lineNum)
     12                 .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in zipWithIndex(self)
   1986         starts = [0]
   1987         if self.getNumPartitions() > 1:
-> 1988             nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
   1989             for i in range(len(nums) - 1):
   1990                 starts.append(starts[-1] + nums[i])

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 102, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <lambda>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <genexpr>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "<ipython-input-82-de640ca49a03>", line 17, in removePunctuation
IndexError: string index out of range

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) My 4d and 4e passed the tests, but I don't understand why. What is the 'u' for in 
[u'zwaggerd', u'zounds',
etc. Also, I thought I needed to get rid of the line numbers/index that were zipped in by the previous code block that was provided for us, but my code didn't work when I tried to get rid of the line numbers, and to my surprise, when I stopped trying to do so and just split what's there, it passed. Why? Aren't the line numbers mixed in with the words now?
 C:\Users\prof.BOLA\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Clearing any previously set forwarded ports...==> sparkvm: Clearing any previously set network interfaces...==> sparkvm: Preparing network interfaces based on configuration... sparkvm: Adapter 1: nat==> sparkvm: Forwarding ports... sparkvm: 8001 => 8001 (adapter 1) sparkvm: 4040 => 4040 (adapter 1) sparkvm: 22 => 2222 (adapter 1)==> sparkvm: Booting VM...==> sparkvm: Waiting for machine to boot. This may take a few minutes... sparkvm: SSH address: 127.0.0.1:2222 sparkvm: SSH username: vagrant sparkvm: SSH auth method: private key sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying...













this is the message i got when i tried to run my vm. 
















C:\Users\prof.BOLA\myvagrant>vagrant up Bringing machine 'sparkvm' up with 'virtualbox' provider... ==> sparkvm: Checking if box 'sparkmooc/base' is up to date... ==> sparkvm: Clearing any previously set forwarded ports... ==> sparkvm: Clearing any previously set network interfaces... ==> sparkvm: Preparing network interfaces based on configuration... sparkvm: Adapter 1: nat ==> sparkvm: Forwarding ports... sparkvm: 8001 => 8001 (adapter 1) sparkvm: 4040 => 4040 (adapter 1) sparkvm: 22 => 2222 (adapter 1) ==> sparkvm: Booting VM... ==> sparkvm: Waiting for machine to boot. This may take a few minutes... sparkvm: SSH address: 127.0.0.1:2222 sparkvm: SSH username: vagrant sparkvm: SSH auth method: private key sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying...






 Check out these local groups:

Silicon Valley Hands On Programming Events
http://www.meetup.com/HandsOnProgrammingEvents/events/223044246/

Chicago Spark Users
http://www.meetup.com/Chicago-Spark-Users/events/222875700/

Seattle Spark Meetup
http://www.meetup.com/Seattle-Spark-Meetup/events/222816210/

Vancouver Spark Meetup
http://www.meetup.com/Vancouver-Spark/events/223042642/

Brussels, Belgium
http://www.meetup.com/Open-Data-Innovation-Training-Hub/events/222837534/

Perth, Australia
http://www.meetup.com/Perth-Data-Science/events/222727946/ 

Melbourne, Australia
http://www.meetup.com/Melbourne-Apache-Spark-Meetup/ 

Sydney NSW, Australia
http://www.meetup.com/Sydney-Apache-Spark-User-Group/ 

Dublin, Ireland
http://www.meetup.com/Dublin-Spark-Meetup/ 

London, England
http://www.meetup.com/Spark-London/

Nashville, Tennesseehttp://www.meetup.com/Nashville-Machine-Learning-Meetup/

Philly (Philadelphia, Pennsylvania) Scalable Machine Learning
http://www.meetup.com/Philly-Scalable-Machine-Learning/events/223577222/


#pin I have some questions for those who are currently working in Data Science at various different positions (Data Analyst, Data Engineer, Data Scientist, Business Analyst):

1. What is your current role in the company? Is it the role of a Data Engineer, Scientist or an Analyst?

2. What do you do in your day to day work? 

3. What kind of tools do you use? R? Hadoop? Any Other Tool?

4. What kind of data do you work on? Is it medical science related data, customer survey or any other data?5. How did you get into this field? Do you have a degree in Data Science? Or you had an interest in it and ended up choosing this as a profession?

I would love to read your responses because one of my motives for signing up for this course was to interact with professionals who are already working on data to and to see what kind of work do they do in the industry.

I hope this helps other interested folks as well!  1e
# TODO: Replace <FILL IN> with appropriate codepluralLengths = (pluralRDD <FILL IN> .collect())print pluralLengths

What length function i can use ????

2a
# TODO: Replace <FILL IN> with appropriate code# Note that groupByKey requires no parameterswordsGrouped = wordPairs.<FILL IN>for key, value in wordsGrouped.collect(): print '{0}: {1}'.format(key, list(value))

I am not getting clearly what to do by group by . and how to use group by Hi,
The lec videos are not loading.



Thanks,
Arun Gunalan.B I am learning python in parallel with spark and, in my subjective opinion, the quality of the documentation for pyspark that one can easily get via googling is just dreadful.  As an example looking at the reduceByKey() description (I got there by link in the notebook section 2c) one can infer that there is some operator "add" yet the reference writeup never explains where it comes from nor how it works in case of key value pairs.  Going back to 2c, it is not difficult to figure out  using lectures that lambda x,y: x+y is going to do the trick yet I am still wondering why it works the way it does since there is no explanation to be found...So if anyone ever found a good pyspark written by someone with descent command of logic and the English language, I would really appreciate it  Consider we have a RDD varRDD and a function funcFoo. 

varRDD = sc.textFile("someFile")
def funcFoo(arg):
	return funcFoo.lower()


Now if I'm doing,
varRDD.map(funcFoo)
every line in varRDD is passed to funcFoo and result will be a new RDD with return values from funcFoo. 

But what happens when,
funcFoo(varRDD)

here we are passing the RDD as parameter to funcFoo, is funcFoo reading line by line from RDD and returning the same result as map method (as shown above) ?. If that's the case how is spark handling this  (just being curious) ?  Which of python libraries (data science related) are compatible with spark ?to name a few pandas pandas, numpy, ..... The answer to 4D seemed obvious:
shakespeareWordsRDD = do not post solutions print shakespeareWordCount

Meanwile, I am getting the answer 882996 as expected only after empty lines are filtered out on step 4E.
Clearly, I am filtering out empty strings too early (on 4D instead of 4E), but how to avoid doing this while using split function?
Any advice?

@575 does not help.
I am not using \w.
I am doing exactly what method comment says - first removing everything but letters, numbers and spaces; then stripping.
How can I get help if I can't post a solution? 
This cell is the only one not working for me.
It took me around 30 mins to get the rest of notebook working.
 When I submit the code it complains on line 11:
def wordCount(wordListRDD):... return wordListRDD.map(lambda k:(k,1)).reduceByKey(lambda k,v :k+v)... print wordCount(wordsRDD).collect() File "<stdin>", line 3 print wordCount(wordsRDD).collect() ^SyntaxError: invalid syntax

Can anybody let me know, what I am doing wrong? Hello, people,
I've done lab1 for 100% and it was more challenging then I expected, but it was really fun, really brakes your mind after RDBMS and ORM!
I came around 2 issues (which took me a lot of time), but maybe it's not issues, maybe I just a different way to do things, as I'm far from being Spark pro :D
1) question 3b: didn't find a way to use "map" here, used "values" instead
2) question 4f: "takeOrdered" returns ascending values, why are we asked to use it? Used "top" instead.

Did anyone managed to do things the way these exercise notes requested (using "map" in 3b and "takeOrdered" in 4f)? (please do not post the direct answer)

PS. The only thing that I don't quite get is forced usage of python 2.x in 2015 for cutting edge tech like Spark... Here is my code :
# TODO: Replace <FILL IN> with appropriate codeuniqueWords = sorted(sc.parallelize(['cat','elephant','rat','rat','cat']).distinct().collect())print uniqueWords

['cat', 'elephant', 'rat']

When I test, it still fails ... 

# TEST Unique words (3a)Test.assertEquals(uniqueWords, 3, 'incorrect count of uniqueWords')


1 test failed. incorrect count of uniqueWords

Can anyone advice please ?2nd Method I try as below : 

# TODO: Replace <FILL IN> with appropriate codeuniqueWords = sorted(sc.parallelize(['cat','elephant','rat','rat','cat']).distinct().collect())print sc.parallelize(uniqueWords).count()

But when I test, it still fails ...
# TEST Unique words (3a)Test.assertEquals(uniqueWords, 3, 'incorrect count of uniqueWords')

1 test failed. incorrect count of uniqueWords
 I am taking the honor code.... My question is whether i would receive an honor code certificate on successful completion of course... If so what is the minimum marks i must obtain..... thank you.... I get weird output when running cells that use rdd.toDebugString(). E.g. cell 24 of the first lab gives

(8) PythonRDD[3] at RDD at PythonRDD.scala:43 []
 |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:392 []
which looks like a stack trace. The rest executes normally. Any ideas? Hello, I am getting the error below when checking code for lab
Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 577, in main
    brokenRDD.collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 713, in collect
    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 538, in __call__
    self.target_id, self.name)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/protocol.py", line 300, in get_return_value
    format(target_id, '.', name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 1 times, most recent failure: Lost task 0.0 in stage 48.0 (TID 221, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/ok/submission.py", line 566, in brokenTen
    if (val < 10):
NameError: global name 'val' is not defined

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

  I have removed cells which are not passing and saved my notebook as python code. The autograder never finishes (I have never seen a mark). I get no errors, just that spinning 'I'm working on it buddy' graphic. Thanks. Hi I'm late in joining the course. Today I'm trrying to setup my environment. I have downloaded orcale Vm Virtual Box and vagrant. I navigated to the custom directory created by me in the command prompt and issued the following command in it "vagrant up --provider=virtualbox".....I have tried many times but downloading doesn't progress at all. Once it went till 11% and an error occured and downloading stopped. The download speed is also too slow and it is showing estimated time of around 4 hours. I'm attaching the screen shot of the command prompt.




Please help me and thanks in advance...! In the lecture, it is told that we must cache the RDD if it is being re-used repeatedly. But is it not the case that the RDD gets stored in the memory when it is called by an action? Can you give me an example where caching an RDD actually makes a difference ? 

Also, I came across the terms .cache() and .persist() in the spark documentation. What is the difference between them? I am not able to see RDDs listed in "storage" tab in web UI.

 # TODO: Replace  with appropriate code
import string
shakespeareWordsRDD = shakespeareRDD.flatMap(string.split)
shakespeareWordCount = shakespeareWordsRDD.count()
print shakespeareWordsRDD.top(5)
print shakespeareWordCount

the output:
[u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds']
882996

where have i gone wrong?? Is it possible to download all lectures in one shot ?
Right now i am able to download lectures in pieces of 2 min videos . I have tried downloading Ubuntu multiple times ....... It get disconnects and unable to resume ...... please help
 i am getting the output as this,is it correct or do i have to modify my code...?
rat: [1, 1]
elephant: [1]
cat: [1, 1] The final result looks like this: besides, the word counting is not correct either.
the: 27353
and: 26016
i: 20638
to: 19128
of: 17430
a: 14580
you: 13294
my: 12475
in: 10926
that: 10765
is: 9104
not: 8365
with: 7765
for: 7531
it: 7489 Hey guys, 
Although  this is not relevant to the lab exercise , I  have some sensor datasets as csv files with timestamps . i want to tweak the sensor data as source for spark streaming ( means data should be emitted using timestamp ) . I found that Queue is somewhat relevant to look through .Any suggestions or some more explanation about Queue of RDDs as a Stream.


Queue of RDDs as a Stream: For testing a Spark Streaming application with test data, one can also create a DStream based on a queue of RDDs, using streamingContext.queueStream(queueOfRDDs). Each RDD pushed into the queue will be treated as a batch of data in the DStream, and processed like a stream.


Thanks & Regards, 
Anshu Shukla Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 99, in main
    Test.assertEquals(pluralLambdaRDD.collect(), ['cats', 'elephants', 'rats', 'rats', 'cats'],
UnboundLocalError: local variable 'pluralLambdaRDD' referenced before assignment

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) -- I had done the test ("passed") from 1a to 4a. 
From 4b to 4f, I remove <FILL IN> and did auto grading.
I understand that I will get the partial grading result probably like 60% or 70%.But I got 0%. any idea ?
Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 373
    shakespeareWordsRDD = shakespeareRDD.
                                        ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) -- Hi all,

As a follow-up to my previous question about using Spark on your local machine with Visual Studio .... I got it to work.

Here is what I did (I assume that you have already installed Python 2.7 or 3.0 on your computer):

1) Install the Python Tools for Visual Studio package

2) Install Spark with Hadoop 2.6. I installed Spark into my c:\dev directory

3) Start Visual Studio and create a new Python Application

4) Make sure that you change the Python Environment to 32-bit Python 2.7

5) Start your application with the following lines:

# Path for spark source folder
os.environ['SPARK_HOME']="C:\\dev\\spark-1.3.1-bin-hadoop2.6"
 
# Append pyspark  to Python Path
sys.path.append("C:\\dev\\spark-1.3.1-bin-hadoop2.6\\python")
 
 
try:
    from pyspark import SparkContext
    from pyspark import SparkConf
 Is it worth the money to purchase the certificate option for $100 dollars if you are planning to complete the course anyway?

How beneficial will the certificate be in getting college credit as an elective?  top15WordsAndCounts = wordCount(shakespeareWordsRDD).takeOrdered(15, lambda s: -s[0])

bad operand for type unary - is the error I am getting

Next I tried 
top15WordsAndCounts = wordCount(shakespeareWordsRDD).sortByOrder(false).takeOrdered(15, lambda s: -s[0])

and I am getting 
'PipelinedRDD' object has no attribute 'sortByOrder' what to give in sparkvm login and password? please help. Thank you I couldn't run code cells given in the links provided in lab1. vagrant is up. But I couldn't run. please help. Thank you Hello.

So Vagrant use to work for me, but last night stopped. I went through the sticky and tried destorying the copy and following through the instructions, but no change. Hoping someone can help me get back up and running.



 







# TODO: Replace <FILL IN> with appropriate code

uniqueWords = len(wordsRDD.distinct())
print uniqueWords

















---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-9-a5d202668832> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
----> 2 uniqueWords = len(wordsRDD.distinct())
      3 print uniqueWords

NameError: name 'wordsRDD' is not defined
Why it is showing NameError??




 
what ever I type, it results in error, what it si saying, I couldn't understand. please help. thank you For shakespeareWordCount I got 1279289 with split(" "), and 882996 with split().
.
I removed punctuations with the following parameters in re.sub:
re.sub(<FILL IN>, "", text)  PLEASE DO NOT POST SOLUTIONS
I have no idea what could be the problem. Hi, is available an online spark service to run my spark code instead on install the vm on my pc?
thanks Hello, 
  I think there is an error in the voice description of the transformation filter(isComment) in the video at 3:25.

  comments = lines.filter(isComment)

  My understanding is the result RDD will retain only comments, and filter out the  rest.(the code also is clear)
  Instead, the video says, it will filter out the comment, and the result RDD will contain no comments.

 Please clarify

Thanks Hello All,I'm also doing labs and exercises using Sparkling (Clojure library for Spark) along with PySpark. If anyone interested? may be we can help each other. Here is the lab spark_turorial_student lab exercise - https://github.com/tilakthapa/berkeley-spark/blob/master/src/berkeley_spark/spark_tut_stu.clj  Hi Instructor,

Request you to explain what exactly task, job and stage are in the context of Spark.

As per my understanding
job :- It is an action like count(), collect() etctask :- ?stage :- ?

Executor will execute jobs in the serial manner. But, I am not able to understand what exactly task and stage are.

How do Executor execute task and stage?

Please explain it with some simple example.

Thank you.

 Hi All,
 I successfully installed this course software environment on my home pc before last week's deadline.

I need to install the same thing on my work pc.

If I do that, will I be flagged as being late for this assignment?

Thanks
Vidya

 Hi,
I've been trying for hours to find out where the problem is in this code:

from operator import addtotalCount = (wordCounts.map(lambda x,y : y).reduce(add)average = totalCount / float(uniqueWords)print totalCountprint round(average, 2)

and I get this error:

 average = totalCount / float(uniqueWords)
          ^
SyntaxError: invalid syntax

Any help appreciated,
Thank you Though when using ipython notebook no errors occur upto 4b.

Following is the error message


Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 413
    shakeWordsRDD = shakespeareWordsRDD.<fill_in>
                                        ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

 







In the last task, I receive the following results. 
The assertment fails cause totals "and" is supposed to be 26028 where my result is 26027.
Same goes for "with" where the total is supposed to be 7771 and mine is 7770.
Is it smg that I should look again or could this be relevant to the the fact that the total words may be 
shakespeareWordCount == 927631 or shakespeareWordCount == 928908

Thank you in advance 












the: 27361
and: 26027
i: 20681
to: 19150
of: 17463
a: 14593
you: 13615
my: 12481
in: 10956
that: 10890
is: 9134
not: 8497
with: 7770
me: 7769
it: 7678



 So the software were using is all open, but what about using a service like AWS or Digital Ocean or Google App Engine? How much should one expect to pay for servers with sufficient number of cores, memory, etc?

Moreover, how much is enough? I know that depends on the application and it's scale, but how does one determine this?

 Hi all,

What can i response to the Lecture1 Quizzes?

Best

 Next week is this year's Spark Summit in SF. Are any of you going? I will be in the area attending a different conference but perhaps it's worth considering having a co-located meetup for participants of this course? i get the error "The name test is not defined  " when ever i submit my py file to auto grader
 I am getting the 4e result for 4d exercise, ie, the word count before filter. So my test is failing. Is anyone encounter this issue? Do you guys have any idea that will this class provide statement of accomplishment (SoA) for those who did not pay for the signature track?  Other class, for example MIT python class, they provide a SoA for anyone who passed with a passing grade.  Here is my current answer and I keep getting a syntax error

# TODO: Replace <FILL IN> with appropriate codetop15WordsAndCounts = <FILL IN> PLEASE DO NOT POST SOLUTIONSprint '\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15WordsAndCounts))

NameError                                 Traceback (most recent call last)
<ipython-input-5-76f78ba4e68a> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
----> 2 top15WordsAndCounts = <FILL IN>
      3 print '\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15WordsAndCounts))

NameError: name 'wordsRDD' is not defined

  In the Spark Action video, I didnt understand how we got this output for the following code

rdd = sc.parallelize([5,3,1,2])
rdd.takeOrdered(3,lambda s: -1*s)

Does Key function of takeOrdered refer to lambda? In lambda we are negating all the values, but how did we end up getting positive values i.e. [5,3,2] 
Pluralize and test (1b)
-----------------------
All tests passed
Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect values for pluralRDD

Pass a lambda function to map (1d)
----------------------------------
All tests passed
Length of each word (1e)
------------------------
All tests passed
Pair RDDs (1f)
--------------
All tests passed
groupByKey() approach (2a)
--------------------------
All tests passed
Use groupByKey() to obtain the counts (2b)
------------------------------------------
All tests passed
Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect value for wordCounts
what does the errors in 1c and 2c mean I was wondering where does the code run which we run on cells on notebook using the Virtual Machine. Does it run on our local machine or on cloud?

Thanks I downloaded the two files for lab 1, but getting the following error when trying to Upload into Juoyter. Sorry if its a repeat, but couldn't find any related post.


 For this question, I know that I need to use the makePlural function I created as an argument to the map function.  My problem is, I don't know what argument to give the makePlural function.  Any suggestions? just get text code of lab files - renamed to ipynb. and uploaded to Jupiter but are not interactive cells - just code- they remain as text files in description on windows

I've got python on windows 7 machine and changed permissions but no joy .
 I am so confused.
Have reinstalled both vagrant and VirtualBox several times double checking the access permissions as admin. 
Have closed(halted) and rebooted several times.
Use CMD as admin on all occassions.
Have checked the permissions to all directories and that the paths are as described in the tutorial.
I can call vagrant up and access the website and finish the workbook but Virtualbox shows fail to assign error and remains powered off.
I can power up the VM in Vbox and also access the website and the workbook, but then the vagrant up shows assignment error.
Does it matter that I am unable to use vagrant and Vbox at the same time? Python seems to work with either but not both.
Is it a OS issue I'm using 8.1?
What is provisioning and does this matter?
 I am using a windows 8.1 computer. At the command prompt, I type "vagrant up" but the command fails to execute. The messages say: Box 'base' could not be found An error occurred while downloading: "Couldn't open file /users/Martin/base
I reinstalled Vagrant and still nothing
I also have a problem with Virtual Box it says: "VT-x/AMD-v hardware acceleration is not available on this system. Certain guests (e.g. OS2 and QNX) require this feature"  I was confused when I came across repeated references to 'pair RDD'. While I would appreciate a more thorough response from other students or instructors, I believe it refers to (key, value) pair; ie, an RDD that has (k,v) tuple as its elements. If that's the case, why just call it a pair RDD and not a tuple RDD? Tuple is a more general term that applies to 2 or more elements bound in a cell (datastructure).  As someone with both a technical and (increasingly) Biz Dev background, I am thoroughly excited by the prospect of Apache Spark disrupting big data analytics. However, I have a question for those with a broader background than technical wizardry. What do you expect to get out of class? Is this something you look to apply in your career development?  Hi Staff,

As it said Spark will have to store the data when we run any action command. But inspark tutorial (spark_tutorial_student) I could not see any RDD in the storage page(http://localhost:4040/storage/).

Currently I am in code cell [40].

Please correct me if I am wrong.  for top15wordsAndCounts what key function should be provided as takeOrdered arguments Anyone had success on get Spark running into Docker containers? Can you share your experience (setup, adding nodes)?  Hi,

I submitted my Lab 1 submission two hours ago and the Auto Grader still gave no feedback at all. Currently there are running two Chrome-threads waiting for a) two and b) one hour in parallel for the very same notebook. Is it worth waiting or is there any other solution for this.

Thank you in advance.

Kind regards, Daniel. I can do a standard function with len()

But I need help to incorporate it into lambda?
The lambda functions I have seen is numerical in nature - eg
lambda x: x*2

I am wondering how to insert the len() into it? When I do the following I am getting syntax error:
lambda  w : len(w)

Thanks Hi,

  I have passed all the tests in the notebook locally, but the auto grader is not passing any of the tests.
  This is what I am getting in all tests in autograder:
[ I do have this in the file:
from test_helper import Test]
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

I do have the import 

Any ideas? 

I did down load it as .py and chose the file , saved it and then graded it.

Do I have to remove all import statements (including the ones that were there in the notebook)?


 After first weeks work my virtual machine is not starting . So i destroyed the mooc vm and downloaded but vagrant up command doesn't work and the session aborts giving the following error .Result Code: E_FAIL (0x80004005)Component: SessionMachineInterface: ISession {12f4dcdb-12b2-4ec1-b7cd-ddd9f6c5bf4d}

please need help... Hey guys,

I'm wondering how many people in this course are nearby so we can probably form a meetup sometime.

 All I am seeing in jupyter is lab0_student.ipynb. Shouldn't i be seeing lab1_student.ipynb.
Since the cells are not working, i figure this is my issue.
I tried both addresses listed.

thanks much for any help. I submitted my lab1 code to autograder, file name is lab1_word_count_student _1.py, but no feedback for over 2 hours; then resubmitted it again by using the "choose file" button, and no feedback either for over 45 minutes.  Hi I have two questions.
1. On week 1's syllabus it said that you can watch the videos and take the quizes as many times as you want. Does that mean that if I get the answer wrong many times and after 10 rounds get them right I will still get a  grade?
2. to answer the quiz questions, are we supposed to do extra reading or is watching the lecture enough? I am asking this because I often find that the videos do not cover enough and I get the answers wrong. Lets take for example.
Lecture 4 : PySpark.The question is asking : Which of the following is not a property of RDDs?And no one of the options its giving are in the video? Not sure what to do. While running the Question 4c, I am getting the following error 

File "<ipython-input-63-52dcee918cb9>", line 19, in removePunctuationIndexError: string index out of range

But my 4b executed successfully and my TestCase Passed. Please help Hi, Notebook passes all tests but while upload i get below error and 0 cases passed (0.0%).Please helpThanksPluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 317, in main
    help(re)
UnboundLocalError: local variable 're' referenced before assignment

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

  I've done 1 and 2, will not have the time to do 3 and 4. Passed the tests locally. Downloaded as .py, Tthe grader fails 

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 240
    uniqueWords = 
                  ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined I've worked on the lab1 for many time.... but now when I open the file i obtaion the following error:
Error loading notebook
Unreadable Notebook: /home/vagrant/lab1_word_count_student.ipynb UnicodeDecodeError('utf8', 'ation.1.space.0.capacity\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xab\x00\x00\x00\x00\x00@\x00\x00\x00\x14\x00\x00\x00\x00

Anyone can help me? many thanks! The lectures says the following
1.The time to read or write a value to memory is only a few nanoseconds, while the time to read or write is several milliseconds - that means memory is a million times faster than disks.
2. Serializing and deserializing objects is a very expensive and time consuming process. Keeping intermediate results in memory avoids this significant overhead.

Based on the above statements I would conclude that Spark should be more than a million times faster that Map-reduce.  So, why is it just 100 times faster I've seen other posts about 2b, but none of them have helped me. I've been stuck with no progress for hours. i understand we need to map some lambda function. the errors my computer keeps giving me are very long and not beneficial. do we need to split the tuple before map? i'm having trouble summing an iterable. do we need to split the value?  Nothing in the tutorials or lectures explains how to use the functions.

I'm trying to use reduceByKey, but there are no concrete examples anywhere in our notes on how to actually use it. Am I supposed to scrape Google for 15 hours to figure this out? I followed all the instructions but it generated error, so I gave up. It would have been better if the submission can be done per question, not per assignment. By then  It would be easy to trace error.
Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 20, in 
  File "/ok/submission.py", line 84, in main
    Test.assertEquals(pluralRDD.collect(), ['cats', 'elephants', 'rats', 'rats', 'cats'],
NameError: global name 'Test' is not defined

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

 I am having serious doubts that the quiz questions are being correctly graded.  Since most of the time I am failing the multiple answer questions, but am not seeing what is being graded as incorrect, I have no feedback to educate my suspicion.  However , for a true false question in the fourth lecture asks

In iterative or repeated computations, broadcast variables avoid the problem of sending the same data to workers:

  False False - incorrect    True



but the lecture notes clearly states:

The first is broadcast variables.These enable us to efficiently send large read-only valuesto all of the workers.

Clearly the same data is sent to all the workers.  Perhaps you meant to insert the word repeatedly in the question.  I do not know.  But as it stands it's been misgraded. I am getting the following out put for 4c, I am not sure if its right, could somebody please clarify :

0: 16091: 2: the sonnets3: 4: by william shakespeare5: 6: 7: 8: 19: from fairest creatures we desire increase10: that thereby beautys rose might never die11: but as the riper should by time decease12: his tender heir might bear his memory13: but thou contracted to thine own bright eyes14: feedst thy lights flame with selfsubstantial fuel This is maybe a slightly weird question, but exactly what kind of software am I using?

What is virtual box, what is vagrant, what is jupyter? How are all of these connected to Apache Spark? Are they mandatory for using Apache Spark?
Why aren't we using a regular IDE? Is there an Apache Spark program to download and write code in?

It's not the kind of programming I'm used to, so there's alot to process. Hopefully you guys have some newbie friendly explanation on what is going on.
Thanks in advance
wbr is there any italian taking this course ? Can't understand what to do in Lab 1 or rather from where I am supposed to Start?
  Hi,

I'm unable to upload the lab1.py assignment successfully. tried it for two times, but 
You submitted a iPython notebook. Please export it as a Python file(.py) and submit the python file again
is being displayed though i've uploaded a .py file. Really, sad! I submitted 4 times a .py file that passed all tests except the GroupBy test, and scored zero. The 5th time I submitted a file with all .collect() removed, but no joy. still zero. What did I do wrong?  this might be completely moronic but i cant seem to figure out the solution for this error please help.




---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-41-ccbc8e3ef48b> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
----> 2 import bs4
      3 from bs4 import BeautifulSoup
      4 
      5 def removePunctuation(text):

ImportError: No module named bs4




 I have the function converting a key into keyvalue pair
def bc(w):    w = '(' + w + ',' + '1' + ')'

How do I convert this into a lambda? Dear organizers of this course,

could you be so kind as to schedule the course so that the due date/time for labs is SATURDAY EVENING, but NOT in the middle of the week ???

I am working during my working day, and actually have no time doing this all in a working day.
But on Saturday I would have a calm time, when noone bothers me, and I would gladly do all  labs and sleep well.

Currently is 1:30am, I need to go to work tomorrow morning, today came late from work.   Actually, not too much desire to sit at night and come at work tomorrow as a "drunk man".

Thank you. After finishing lab1, I have got several questions. Hope anyone could give me some insights.Thanks!

First, in the tutorial, it says

Note that for the first() and take() actions, the elements that are returned depend on how the RDD is partitioned.

How to understand this? May I assume that it depends on the order of the element and takeOrdered() and top() can return deterministic results because it's ordered.

#2
When I run
xrangeRDD.toDebugString()
rangeRDD.toDebugString()
it gives
(8) My first RDD PythonRDD[9] at RDD at PythonRDD.scala:43 []
 |  ParallelCollectionRDD[7] at parallelize at PythonRDD.scala:392 []
(8) My second RDD ParallelCollectionRDD[8] at parallelize at PythonRDD.scala:392 []
What does these information mean? Why the debug information of rangeRDD different from xrangeRDD?

#3
To me, it seems that reduceByKey() and foldByKey() are very similar except that foldByKey() has a neutral "zero value". What's the difference between them? Is there any case one can't one replaced by another?

 Hi ,

I did submit my lab 1 to the setup folder. Now the autograder give me a 20 point reduction. Can this be corrected.

Kind Regard,

Ruud In the video titled "Transformations" at about 0:15 there is a discussion of the transformation map(). Later at about 1:49 a similar example is given with the transformation Map(). Different functions? Typo?

edit: Never mind, I don't see Map() in the documentation so I'm guessing typo. 

But, just finished that video and I have another question. The documentation says the filter() transformation returned an RDD with elements for which the function passed in returns true. The function passed in is named isComment which I would expect to return true if a line is a comment. But the lecture says we'll get an RDD with comments removed. ?? Hi,
Is there a shortcut to reset code within a cell in Python notebook?

I accidentally changed code in one of the cells and do not want to load a fresh copy of the notebook again.

Thanks. can't dowload either of these when I click on links get all the following trash spark_tutorial_student.ipynb and lab1_word_count_student.iptnb
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "# **Spark Tutorial: Learning Apache Spark**\n",
    "#### This tutorial will teach you how to use [Apache Spark](http://spark.apache.org/), a framework for large-scale data processing, within a notebook. Many traditional frameworks were designed to be run on a single computer.  However, many datasets today are too large to be stored on a single computer, and even when a dataset can be stored on one computer (such as the datasets in this tutorial), the dataset can often be processed much more quickly using multiple computers.  Spark has efficient implementations of a number of transformations and actions that can be composed together to perform data processing and analysis.  Spark excels at distributing these operations across a cluster while abstracting away many of the underlying implementation details.  Spark has been designed with a focus on scalability and efficiency.  With Spark you can begin developing your solution on your laptop, using a small dataset, and then use that same code to process terabytes or even petabytes across a distributed cluster.\n",
    "#### **During this tutorial we will cover:**\n",
    "#### *Part 1:* Basic notebook usage and [Python](https://docs.python.org/2/) integration\n",
    "#### *Part 2:* An introduction to using [Apache Spark](https://spark.apache.org/) with the Python [pySpark API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) running in the browser\n",
    "#### *Part 3:* Using RDDs and chaining together transformations and actions\n",
    "#### *Part 4:* Lambda functions\n",
    "#### *Part 5:* Additional RDD actions\n",
    "#### *Part 6:* Additional RDD transformations\n",
    "#### *Part 7:* Caching RDDs and storage options\n",
    "#### *Part 8:* Debugging Spark applications and lazy evaluation\n",
    "#### The following transformations will be covered:\n",
    "* #### `map()`, `mapPartitions()`, `mapPartitionsWithIndex()`, `filter()`, `flatMap()`, `reduceByKey()`, `groupByKey()`\n",
    "#### The following actions will be covered:\n",
    "* #### `first()`, `take()`, `takeSample()`, `takeOrdered()`, `collect()`, `count()`, `countByValue()`, `reduce()`, `top()`\n",
    "#### Also covered:\n",
    "* #### `cache()`, `unpersist()`, `id()`, `setName()`\n",
    "#### Note that, for reference, you can look up the details of these methods in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1: Basic notebook usage and [Python](https://docs.python.org/2/) integration **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1a) Notebook usage**\n",
    "#### A notebook is comprised of a linear sequence of cells.  These cells can contain either markdown or code, but we won't mix both in one cell.  When a markdown cell is executed it renders formatted text, images, and links just like HTML in a normal webpage.  The text you are reading right now is part of a markdown cell.  Python code cells allow you to execute arbitrary Python commands just like in any Python shell. Place your cursor inside the cell below, and press \"Shift\" + \"Enter\" to execute the code and advance to the next cell.  You can also press \"Ctrl\" + \"Enter\" to execute the code and remain in the cell.  These commands work the same in both markdown and code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a Python cell. You can run normal Python code here...\n",
    "print 'The sum of 1 and 1 is {0}'.format(1+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here is another Python cell, this time with a variable (x) declaration and an if statement:\n",
    "x = 42\n",
    "if x > 40:\n",
    "    print 'The sum of 1 and 2 is {0}'.format(1+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1b) Notebook state**\n",
    "#### As you work through a notebook it is important that you run all of the code cells.  The notebook is stateful, which means that variables and their values are retained until the notebook is detached (in Databricks Cloud) or the kernel is restarted (in IPython notebooks).  If you do not run all of the code cells as you proceed through the notebook, your variables will not be properly initialized and later code might fail.  You will also need to rerun any cells that you have modified in order for the changes to be available to other cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell relies on x being defined already.\n",
    "# If we didn't run the cells from part (1a) this code would fail.\n",
    "print x * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1c) Library imports**\n",
    "#### We can import standard Python libraries ([modules](https://docs.python.org/2/tutorial/modules.html)) the usual way.  An `import` statement will import the specified module.  In this tutorial and future labs, we will provide any imports that are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the regular expression library\n",
    "import re\n",
    "m = re.search('(?<=abc)def', 'abcdef')\n",
    "m.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the datetime library\n",
    "import datetime\n",
    "print 'This was last run on: {0}'.format(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Part 2: An introduction to using [Apache Spark](https://spark.apache.org/) with the Python [pySpark API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) running in the browser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Spark Context**\n",
    "#### In Spark, communication occurs between a driver and executors.  The driver has Spark jobs that it needs to run and these jobs are split into tasks that are submitted to the executors for completion.  The results from these tasks are delivered back to the driver.\n",
    "#### In part 1, we saw that normal python code can be executed via cells. When using Databricks Cloud this code gets executed in the Spark driver's Java Virtual Machine (JVM) and not in an executor's JVM, and when using an IPython notebook it is executed within the kernel associated with the notebook. Since no Spark functionality is actually being used, no tasks are launched on the executors.\n",
    "#### In order to use Spark and its API we will need to use a `SparkContext`.  When running Spark, you start a new Spark application by creating a [SparkContext](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext).  When the `SparkContext` is created, it asks the master for some cores to use to do work.  The master sets these cores aside just for you; they won't be used for other applications. When using Databricks Cloud or the virtual machine provisioned for this class, the `SparkContext` is created for you automatically as `sc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2a) Example Cluster**\n",
    "#### The diagram below shows an example cluster, where the cores allocated for an application are outlined in purple.\n",
    "![executors](http://spark-mooc.github.io/web-assets/images/executors.png)\n",
    "#### You can view the details of your Spark application in the Spark web UI.  The web UI is accessible in Databricks cloud by going to \"Clusters\" and then clicking on the \"View Spark UI\" link for your cluster.  When running locally you'll find it at [localhost:4040](http://localhost:4040).  In the web UI, under the \"Jobs\" tab, you can see a list of jobs that have been scheduled or run.  It's likely there isn't any thing interesting here yet because we haven't run any jobs, but we'll return to this page later.\n",
    "#### At a high level, every Spark application consists of a driver program that launches various parallel operations on executor Java Virtual Machines (JVMs) running either in a cluster or locally on the same machine. In Databricks Cloud, \"Databricks Shell\" is the driver program.  When running locally, \"PySparkShell\" is the driver program. In all cases, this driver program contains the main loop for the program and creates distributed datasets on the cluster, then applies operations (transformations & actions) to those datasets.\n",
    "#### Driver programs access Spark through a SparkContext object, which represents a connection to a computing cluster. A Spark context object (`sc`) is the main entry point for Spark functionality. A Spark context can be used to create Resilient Distributed Datasets (RDDs) on a cluster.\n",
    "#### Try printing out `sc` to see its type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the type of the Spark Context sc\n",
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2b) `SparkContext` attributes**\n",
    "#### You can use Python's [dir()](https://docs.python.org/2/library/functions.html?highlight=dir#dir) function to get a list of all the attributes (including methods) accessible through the `sc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List sc's attributes\n",
    "dir(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2c) Getting help**\n",
    "#### Alternatively, you can use Python's [help()](https://docs.python.org/2/library/functions.html?highlight=help#help) function to get an easier to read list of all the attributes, including examples, that the `sc` object has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use help to obtain more detailed information\n",
    "help(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After reading the help we've decided we want to use sc.version to see what version of Spark we are running\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Help can be used on any Python object\n",
    "help(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3: Using RDDs and chaining together transformations and actions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Working with your first RDD**\n",
    "#### In Spark, we first create a base [Resilient Distributed Dataset](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) (RDD). We can then apply one or more transformations to that base RDD. *An RDD is immutable, so once it is created, it cannot be changed.* As a result, each transformation creates a new RDD. Finally, we can apply one or more actions to the RDDs.  Note that Spark uses lazy evaluation, so transformations are not actually executed until an action occurs.\n",
    "#### We will perform several exercises to obtain a better understanding of RDDs:\n",
    "* ##### Create a Python collection of 10,000 integers\n",
    "* ##### Create a Spark base RDD from that collection\n",
    "* ##### Subtract one from each value using `map`\n",
    "* ##### Perform action `collect` to view results\n",
    "* ##### Perform action `count` to view counts\n",
    "* ##### Apply transformation `filter` and view results with `collect`\n",
    "* ##### Learn about lambda functions\n",
    "* ##### Explore how lazy evaluation works and the debugging challenges that it introduces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3a) Create a Python collection of integers in the range of 1 .. 10000**\n",
    "#### We will use the [xrange()](https://docs.python.org/2/library/functions.html?highlight=xrange#xrange) function to create a [list()](https://docs.python.org/2/library/functions.html?highlight=list#list) of integers.  `xrange()` only generates values as they are needed.  This is different from the behavior of [range()](https://docs.python.org/2/library/functions.html?highlight=range#range) which generates the complete list upon execution.  Because of this `xrange()` is more memory efficient than `range()`, especially for large ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = xrange(1, 10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data is just a normal Python list\n",
    "# Obtain data's first element\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can check the size of the list using the len() function\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3b) Distributed data and using a collection to create an RDD**\n",
    "#### In Spark, datasets are represented as a list of entries, where the list is broken up into many different partitions that are each stored on a different machine.  Each partition holds a unique subset of the entries in the list.  Spark calls datasets that it stores \"Resilient Distributed Datasets\" (RDDs).\n",
    "#### One of the defining features of Spark, compared to other data analytics frameworks (e.g., Hadoop), is that it stores data in memory rather than on disk.  This allows Spark applications to run much more quickly, because they are not slowed down by needing to read data from disk.\n",
    "#### The figure below illustrates how Spark breaks a list of data entries into partitions that are each stored in memory on a worker.\n",
    "![partitions](http://spark-mooc.github.io/web-assets/images/partitions.png)\n",
    "#### To create the RDD, we use `sc.parallelize()`, which tells Spark to create a new set of input data based on data that is passed in.  In this example, we will provide an `xrange`.  The second argument to the [sc.parallelize()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.parallelize) method tells Spark how many partitions to break the data into when it stores the data in memory (we'll talk more about this later in this tutorial). Note that for better performance when using `parallelize`, `xrange()` is recommended if the input represents a range. This is the reason why we used `xrange()` in 3a.\n",
    "#### There are many different types of RDDs.  The base class for RDDs is [pyspark.RDD](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and other RDDs subclass `pyspark.RDD`.  Since the other RDD types inherit from `pyspark.RDD` they have the same APIs and are functionally identical.  We'll see that `sc.parallelize()` generates a `pyspark.rdd.PipelinedRDD` when its input is an `xrange`, and a `pyspark.RDD` when its input is a `range`.\n",
    "#### After we generate RDDs, we can view them in the \"Storage\" tab of the web UI.  You'll notice that new datasets are not listed until Spark needs to return a result due to an action being executed.  This feature of Spark is called \"lazy evaluation\".  This allows Spark to avoid performing unnecessary calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parallelize data using 8 partitions\n",
    "# This operation is a transformation of data into an RDD\n",
    "# Spark uses lazy evaluation, so no Spark jobs are run at this point\n",
    "xrangeRDD = sc.parallelize(data, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's view help on parallelize\n",
    "help(sc.parallelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see what type sc.parallelize() returned\n",
    "print 'type of xrangeRDD: {0}'.format(type(xrangeRDD))\n",
    "\n",
    "# How about if we use a range\n",
    "dataRange = range(1, 10001)\n",
    "rangeRDD = sc.parallelize(dataRange, 8)\n",
    "print 'type of dataRangeRDD: {0}'.format(type(rangeRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Each RDD gets a unique ID\n",
    "print 'xrangeRDD id: {0}'.format(xrangeRDD.id())\n",
    "print 'rangeRDD id: {0}'.format(rangeRDD.id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can name each newly created RDD using the setName() method\n",
    "xrangeRDD.setName('My first RDD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's view the lineage (the set of transformations) of the RDD using toDebugString()\n",
    "print xrangeRDD.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's use help to see what methods we can call on this RDD\n",
    "help(xrangeRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see how many partitions the RDD will be split into by using the getNumPartitions()\n",
    "xrangeRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3c): Subtract one from each value using `map`**\n",
    "#### So far, we've created a distributed dataset that is split into many partitions, where each partition is stored on a single machine in our cluster.  Let's look at what happens when we do a basic operation on the dataset.  Many useful data analysis operations can be specified as \"do something to each item in the dataset\".  These data-parallel operations are convenient because each item in the dataset can be processed individually: the operation on one entry doesn't effect the operations on any of the other entries.  Therefore, Spark can parallelize the operation.\n",
    "#### `map(f)`, the most common Spark transformation, is one such example: it applies a function `f` to each item in the dataset, and outputs the resulting dataset.  When you run `map()` on a dataset, a single *stage* of tasks is launched.  A *stage* is a group of tasks that all perform the same computation, but on different input data.  One task is launched for each partitition, as shown in the example below.  A task is a unit of execution that runs on a single machine. When we run `map(f)` within a partition, a new *task* applies `f` to all of the entries in a particular partition, and outputs a new partition. In this example figure, the dataset is broken into four partitions, so four `map()` tasks are launched.\n",
    "![tasks](http://spark-mooc.github.io/web-assets/images/tasks.png)\n",
    "#### The figure below shows how this would work on the smaller data set from the earlier figures.  Note that one task is launched for each partition.\n",
    "![foo](http://spark-mooc.github.io/web-assets/images/map.png)\n",
    "#### When applying the `map()` transformation, each item in the parent RDD will map to one element in the new RDD. So, if the parent RDD has twenty elements, the new RDD will also have twenty items.\n",
    "#### Now we will use `map()` to subtract one from each value in the base RDD we just created. First, we define a Python function called `sub()` that will subtract one from the input integer. Second, we will pass each item in the base RDD into a `map()` transformation that applies the `sub()` function to each element. And finally, we print out the RDD transformation hierarchy using `toDebugString()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create sub function to subtract 1\n",
    "def sub(value):\n",
    "    \"\"\"\"Subtracts one from `value`.\n",
    "\n",
    "    Args:\n",
    "       value (int): A number.\n",
    "\n",
    "    Returns:\n",
    "        int: `value` minus one.\n",
    "    \"\"\"\n",
    "    return (value - 1)\n",
    "\n",
    "# Transform xrangeRDD through map transformation using sub function\n",
    "# Because map is a transformation and Spark uses lazy evaluation, no jobs, stages,\n",
    "# or tasks will be launched when we run this code.\n",
    "subRDD = xrangeRDD.map(sub)\n",
    "\n",
    "# Let's see the RDD transformation hierarchy\n",
    "print subRDD.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3d) Perform action `collect` to view results **\n",
    "#### To see a list of elements decremented by one, we need to create a new list on the driver from the the data distributed in the executor nodes.  To do this we call the `collect()` method on our RDD.  `collect()` is often used after a filter or other operation to ensure that we are only returning a *small* amount of data to the driver.  This is done because the data returned to the driver must fit into the driver's available memory.  If not, the driver will crash.\n",
    "#### The `collect()` method is the first action operation that we have encountered.  Action operations cause Spark to perform the (lazy) transformation operations that are required to compute the RDD returned by the action.  In our example, this means that tasks will now be launched to perform the `parallelize`, `map`, and `collect` operations.\n",
    "#### In this example, the dataset is broken into four partitions, so four `collect()` tasks are launched. Each task collects the entries in its partition and sends the result to the SparkContext, which creates a list of the values, as shown in the figure below.\n",
    "![collect](http://spark-mooc.github.io/web-assets/images/collect.png)\n",
    "#### The above figures showed what would happen if we ran `collect()` on a small example dataset with just four partitions.\n",
    "#### Now let's run `collect()` on `subRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's collect the data\n",
    "print subRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3d) Perform action `count` to view counts **\n",
    "#### One of the most basic jobs that we can run is the `count()` job which will count the number of elements in an RDD using the `count()` action. Since `map()` creates a new RDD with the same number of elements as the starting RDD, we expect that applying `count()` to each RDD will return the same result.\n",
    "#### Note that because `count()` is an action operation, if we had not already performed an action with `collect()`, then Spark would now perform the transformation operations when we executed `count()`.\n",
    "#### Each task counts the entries in its partition and sends the result to your SparkContext, which adds up all of the counts. The figure below shows what would happen if we ran `count()` on a small example dataset with just four partitions.\n",
    "![count](http://spark-mooc.github.io/web-assets/images/count.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print xrangeRDD.count()\n",
    "print subRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3e) Apply transformation `filter` and view results with `collect` **\n",
    "#### Next, we'll create a new RDD that only contains the values less than ten by using the `filter(f)` data-parallel operation. The `filter(f)` method is a transformation operation that creates a new RDD from the input RDD by applying filter function `f` to each item in the parent RDD and only passing those elements where the filter function returns `True`. Elements that do not return `True` will be dropped. Like `map()`, filter can be applied individually to each entry in the dataset, so is easily parallelized using Spark.\n",
    "#### The figure below shows how this would work on the small four-partition dataset.\n",
    "![filter](http://spark-mooc.github.io/web-assets/images/filter.png)\n",
    "#### To filter this dataset, we'll define a function called `ten()`, which returns `True` if the input is less than 10 and `False` otherwise.  This function will be passed to the `filter()` transformation as the filter function `f`.\n",
    "#### To view the filtered list of elements less than ten, we need to create a new list on the driver from the distributed data on the executor nodes.  We use the `collect()` method to return a list that contains all of the elements in this filtered RDD to the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function to filter a single value\n",
    "def ten(value):\n",
    "    \"\"\"Return whether value is below ten.\n",
    "\n",
    "    Args:\n",
    "        value (int): A number.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether `value` is less than ten.\n",
    "    \"\"\"\n",
    "    if (value < 10):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "# The ten function could also be written concisely as: def ten(value): return value < 10\n",
    "\n",
    "# Pass the function ten to the filter transformation\n",
    "# Filter is a transformation so no tasks are run\n",
    "filteredRDD = subRDD.filter(ten)\n",
    "\n",
    "# View the results using collect()\n",
    "# Collect is an action and triggers the filter transformation to run\n",
    "print filteredRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 4: Lambda Functions **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4a) Using Python `lambda()` functions **\n",
    "#### Python supports the use of small one-line anonymous functions that are not bound to a name at runtime. Borrowed from LISP, these `lambda` functions can be used wherever function objects are required. They are syntactically restricted to a single expression. Remember that `lambda` functions are a matter of style and using them is never required - semantically, they are just syntactic sugar for a normal function definition. You can always define a separate normal function instead, but using a `lambda()` function is an equivalent and more compact form of coding. Ideally you should consider using `lambda` functions where you want to encapsulate non-reusable code without littering your code with one-line functions.\n",
    "#### Here, instead of defining a separate function for the `filter()` transformation, we will use an inline `lambda()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambdaRDD = subRDD.filter(lambda x: x < 10)\n",
    "lambdaRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's collect the even values less than 10\n",
    "evenRDD = lambdaRDD.filter(lambda x: x % 2 == 0)\n",
    "evenRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 5: Additional RDD actions **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (5a) Other common actions **\n",
    "#### Let's investigate the additional actions: [first()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.first), [take()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take), [top()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.top), [takeOrdered()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.takeOrdered), and [reduce()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduce)\n",
    "#### One useful thing to do when we have a new dataset is to look at the first few entries to obtain a rough idea of what information is available.  In Spark, we can do that using the `first()`, `take()`, `top()`, and `takeOrdered()` actions. Note that for the `first()` and `take()` actions, the elements that are returned depend on how the RDD is *partitioned*.\n",
    "####  Instead of using the `collect()` action, we can use the `take(n)` action to return the first n elements of the RDD. The `first()` action returns the first element of an RDD, and is equivalent to `take(1)`.\n",
    "#### The `takeOrdered()` action returns the first n elements of the RDD, using either their natural order or a custom comparator. The key advantage of using `takeOrdered()` instead of `first()` or `take()` is that `takeOrdered()` returns a deterministic result, while the other two actions may return differing results, depending on the number of partions or execution environment. `takeOrdered()` returns the list sorted in *ascending order*.  The `top()` action is similar to `takeOrdered()` except that it returns the list in *descending order.*\n",
    "#### The `reduce()` action reduces the elements of a RDD to a single value by applying a function that takes two parameters and returns a single value.  The function should be commutative and associative, as `reduce()` is applied at the partition level and then again to aggregate results from partitions.  If these rules don't hold, the results from `reduce()` will be inconsistent.  Reducing locally at partitions makes `reduce()` very efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's get the first element\n",
    "print filteredRDD.first()\n",
    "# The first 4\n",
    "print filteredRDD.take(4)\n",
    "# Note that it is ok to take more elements than the RDD has\n",
    "print filteredRDD.take(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve the three smallest elements\n",
    "print filteredRDD.takeOrdered(3)\n",
    "# Retrieve the five largest elements\n",
    "print filteredRDD.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pass a lambda function to takeOrdered to reverse the order\n",
    "filteredRDD.takeOrdered(4, lambda s: -s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtain Python's add function\n",
    "from operator import add\n",
    "# Efficiently sum the RDD using reduce\n",
    "print filteredRDD.reduce(add)\n",
    "# Sum using reduce with a lambda function\n",
    "print filteredRDD.reduce(lambda a, b: a + b)\n",
    "# Note that subtraction is not both associative and commutative\n",
    "print filteredRDD.reduce(lambda a, b: a - b)\n",
    "print filteredRDD.repartition(4).reduce(lambda a, b: a - b)\n",
    "# While addition is\n",
    "print filteredRDD.repartition(4).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (5b) Advanced actions **\n",
    "#### Here are two additional actions that are useful for retrieving information from an RDD: [takeSample()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.takeSample) and [countByValue()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.countByValue)\n",
    "#### The `takeSample()` action returns an array with a random sample of elements from the dataset.  It takes in a `withReplacement` argument, which specifies whether it is okay to randomly pick the same item multiple times from the parent RDD (so when `withReplacement=True`, you can get the same item back multiple times). It also takes an optional `seed` parameter that allows you to specify a seed value for the random number generator, so that reproducible results can be obtained.\n",
    "#### The `countByValue()` action returns the count of each unique value in the RDD as a dictionary that maps values to counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# takeSample reusing elements\n",
    "print filteredRDD.takeSample(withReplacement=True, num=6)\n",
    "# takeSample without reuse\n",
    "print filteredRDD.takeSample(withReplacement=False, num=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set seed for predictability\n",
    "print filteredRDD.takeSample(withReplacement=False, num=6, seed=500)\n",
    "# Try reruning this cell and the cell above -- the results from this cell will remain constant\n",
    "# Use ctrl-enter to run without moving to the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create new base RDD to show countByValue\n",
    "repetitiveRDD = sc.parallelize([1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 3, 3, 4, 5, 4, 6])\n",
    "print repetitiveRDD.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 6: Additional RDD transformations **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (6a) `flatMap` **\n",
    "#### When performing a `map()` transformation using a function, sometimes the function will return more (or less) than one element. We would like the newly created RDD to consist of the elements outputted by the function. Simply applying a `map()` transformation would yield a new RDD made up of iterators.  Each iterator could have zero or more elements.  Instead, we often want an RDD consisting of the values contained in those iterators.  The solution is to use a [flatMap()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap) transformation, `flatMap()` is similar to `map()`, except that with `flatMap()` each input item can be mapped to zero or more output elements.\n",
    "#### To demonstrate `flatMap()`, we will first emit a word along with its plural, and then a range that grows in length with each subsequent operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's create a new base RDD to work from\n",
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "\n",
    "# Use map\n",
    "singularAndPluralWordsRDDMap = wordsRDD.map(lambda x: (x, x + 's'))\n",
    "# Use flatMap\n",
    "singularAndPluralWordsRDD = wordsRDD.flatMap(lambda x: (x, x + 's'))\n",
    "\n",
    "# View the results\n",
    "print singularAndPluralWordsRDDMap.collect()\n",
    "print singularAndPluralWordsRDD.collect()\n",
    "# View the number of elements in the RDD\n",
    "print singularAndPluralWordsRDDMap.count()\n",
    "print singularAndPluralWordsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simpleRDD = sc.parallelize([2, 3, 4])\n",
    "print simpleRDD.map(lambda x: range(1, x)).collect()\n",
    "print simpleRDD.flatMap(lambda x: range(1, x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (6b) `groupByKey` and `reduceByKey` **\n",
    "#### Let's investigate the additional transformations: [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) and [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey).\n",
    "#### Both of these transformations operate on pair RDDs.  A pair RDD is an RDD where each element is a pair tuple (key, value).  For example, `sc.parallelize([('a', 1), ('a', 2), ('b', 1)])` would create a pair RDD where the keys are 'a', 'a', 'b' and the values are 1, 2, 1.\n",
    "#### The `reduceByKey()` transformation gathers together pairs that have the same key and applies a function to two associated values at a time. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions.\n",
    "#### While both the `groupByKey()` and `reduceByKey()` transformations can often be used to solve the same problem and will produce the same answer, the `reduceByKey()` transformation works much better for large distributed datasets. This is because Spark knows it can combine output with a common key on each partition *before* shuffling (redistributing) the data across nodes.  Only use `groupByKey()` if the operation would not benefit from reducing the data before the shuffle occurs.\n",
    "#### Look at the diagram below to understand how `reduceByKey` works.  Notice how pairs on the same machine with the same key are combined (by using the lamdba function passed into reduceByKey) before the data is shuffled. Then the lamdba function is called again to reduce all the values from each partition to produce one final result.\n",
    "![reduceByKey() figure](http://spark-mooc.github.io/web-assets/images/reduce_by.png)\n",
    "#### On the other hand, when using the `groupByKey()` transformation - all the key-value pairs are shuffled around, causing a lot of unnecessary data to being transferred over the network.\n",
    "#### To determine which machine to shuffle a pair to, Spark calls a partitioning function on the key of the pair. Spark spills data to disk when there is more data shuffled onto a single executor machine than can fit in memory. However, it flushes out the data to disk one key at a time, so if a single key has more key-value pairs than can fit in memory an out of memory exception occurs. This will be more gracefully handled in a later release of Spark so that the job can still proceed, but should still be avoided.  When Spark needs to spill to disk, performance is severely impacted.\n",
    "![groupByKey() figure](http://spark-mooc.github.io/web-assets/images/group_by.png)\n",
    "#### As your dataset grows, the difference in the amount of data that needs to be shuffled, between the `reduceByKey()` and `groupByKey()` transformations, becomes increasingly exaggerated.\n",
    "#### Here are more transformations to prefer over `groupByKey()`:\n",
    "  + #### [combineByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.combineByKey) can be used when you are combining elements but your return type differs from your input value type.\n",
    "  + #### [foldByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.foldByKey) merges the values for each key using an associative function and a neutral \"zero value\".\n",
    "#### Now let's go through a simple `groupByKey()` and `reduceByKey()` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairRDD = sc.parallelize([('a', 1), ('a', 2), ('b', 1)])\n",
    "# mapValues only used to improve format for printing\n",
    "print pairRDD.groupByKey().mapValues(lambda x: list(x)).collect()\n",
    "\n",
    "# Different ways to sum by key\n",
    "print pairRDD.groupByKey().map(lambda (k, v): (k, sum(v))).collect()\n",
    "# Using mapValues, which is recommended when they key doesn't change\n",
    "print pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect()\n",
    "# reduceByKey is more efficient / scalable\n",
    "print pairRDD.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (6c) Advanced transformations ** [Optional]\n",
    "#### Let's investigate the advanced transformations: [mapPartitions()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mapPartitions) and [mapPartitionsWithIndex()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mapPartitionsWithIndex)\n",
    "#### The `mapPartitions()` transformation uses a function that takes in an iterator (to the items in that specific partition) and returns an iterator.  The function is applied on a partition by partition basis.\n",
    "#### The `mapPartitionsWithIndex()` transformation uses a function that takes in a partition index (think of this like the partition number) and an iterator (to the items in that specific partition). For every partition (index, iterator) pair, the function returns a tuple of the same partition index number and an iterator of the transformed items in that partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mapPartitions takes a function that takes an iterator and returns an iterator\n",
    "print wordsRDD.collect()\n",
    "itemsRDD = wordsRDD.mapPartitions(lambda iterator: [','.join(iterator)])\n",
    "print itemsRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itemsByPartRDD = wordsRDD.mapPartitionsWithIndex(lambda index, iterator: [(index, list(iterator))])\n",
    "# We can see that three of the (partitions) workers have one element and the fourth worker has two\n",
    "# elements, although things may not bode well for the rat...\n",
    "print itemsByPartRDD.collect()\n",
    "# Rerun without returning a list (acts more like flatMap)\n",
    "itemsByPartRDD = wordsRDD.mapPartitionsWithIndex(lambda index, iterator: (index, list(iterator)))\n",
    "print itemsByPartRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 7: Caching RDDs and storage options **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (7a) Caching RDDs **\n",
    "#### For efficiency Spark keeps your RDDs in memory. By keeping the contents in memory, Spark can quickly access the data. However, memory is limited, so if you try to keep too many RDDs in memory, Spark will automatically delete RDDs from memory to make space for new RDDs. If you later refer to one of the RDDs, Spark will automatically recreate the RDD for you, but that takes time.\n",
    "#### So, if you plan to use an RDD more than once, then you should tell Spark to cache that RDD. You can use the `cache()` operation to keep the RDD in memory. However, if you cache too many RDDs and Spark runs out of memory, it will delete the least recently used (LRU) RDD first. Again, the RDD will be automatically recreated when accessed.\n",
    "#### You can check if an RDD is cached by using the `is_cached` attribute, and you can see your cached RDD in the \"Storage\" section of the Spark web UI. If you click on the RDD's name, you can see more information about where the RDD is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Name the RDD\n",
    "filteredRDD.setName('My Filtered RDD')\n",
    "# Cache the RDD\n",
    "filteredRDD.cache()\n",
    "# Is it cached\n",
    "print filteredRDD.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (7b) Unpersist and storage options **\n",
    "#### Spark automatically manages the RDDs cached in memory and will save them to disk if it runs out of memory. For efficiency, once you are finished using an RDD, you can optionally tell Spark to stop caching it in memory by using the RDD's `unpersist()` method to inform Spark that you no longer need the RDD in memory.\n",
    "#### You can see the set of transformations that were applied to create an RDD by using the `toDebugString()` method, which will provide storage information, and you can directly query the current storage information for an RDD using the `getStorageLevel()` operation.\n",
    "#### ** Advanced: ** Spark provides many more options for managing how RDDs are stored in memory or even saved to disk. You can explore the API for RDD's [persist()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.persist) operation using Python's [help()](https://docs.python.org/2/library/functions.html?highlight=help#help) command.  The `persist()` operation, optionally, takes a pySpark [StorageLevel](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.StorageLevel) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note that toDebugString also provides storage information\n",
    "print filteredRDD.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If we are done with the RDD we can unpersist it so that its memory can be reclaimed\n",
    "filteredRDD.unpersist()\n",
    "# Storage level for a non cached RDD\n",
    "print filteredRDD.getStorageLevel()\n",
    "filteredRDD.cache()\n",
    "# Storage level for a cached RDD\n",
    "print filteredRDD.getStorageLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 8: Debugging Spark applications and lazy evaluation **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** How Python is Executed in Spark **\n",
    "#### Internally, Spark executes using a Java Virtual Machine (JVM). pySpark runs Python code in a JVM using [Py4J](http://py4j.sourceforge.net). Py4J enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. Methods are called as if the Java objects resided in the Python interpreter and Java collections can be accessed through standard Python collection methods. Py4J also enables Java programs to call back Python objects.\n",
    "#### Because pySpark uses Py4J, coding errors often result in a complicated, confusing stack trace that can be difficult to understand. In the following section, we'll explore how to understand stack traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (8a) Challenges with lazy evaluation using transformations and actions **\n",
    "#### Spark's use of lazy evaluation can make debugging more difficult because code is not always executed immediately. To see an example of how this can happen, let's first define a broken filter function.\n",
    "#### Next we perform a `filter()` operation using the broken filtering function.  No error will occur at this point due to Spark's use of lazy evaluation.\n",
    "#### The `filter()` method will not be executed *until* an action operation is invoked on the RDD.  We will perform an action by using the `collect()` method to return a list that contains all of the elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def brokenTen(value):\n",
    "    \"\"\"Incorrect implementation of the ten function.\n",
    "\n",
    "    Note:\n",
    "        The `if` statement checks an undefined variable `val` instead of `value`.\n",
    "\n",
    "    Args:\n",
    "        value (int): A number.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether `value` is less than ten.\n",
    "\n",
    "    Raises:\n",
    "        NameError: The function references `val`, which is not available in the local or global\n",
    "            namespace, so a `NameError` is raised.\n",
    "    \"\"\"\n",
    "    if (val < 10):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "brokenRDD = subRDD.filter(brokenTen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we'll see the error\n",
    "brokenRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (8b) Finding the bug **\n",
    "#### When the `filter()` method is executed, Spark evaluates the RDD by executing the `parallelize()` and `filter()` methods. Since our `filter()` method has an error in the filtering function `brokenTen()`, an error occurs.\n",
    "#### Scroll through the output \"Py4JJavaError     Traceback (most recent call last)\" part of the cell and first you will see that the line that generated the error is the `collect()` method line. There is *nothing wrong with this line*. However, it is an action and that caused other methods to be executed. Continue scrolling through the Traceback and you will see the following error line:\n",
    "    NameError: global name 'val' is not defined\n",
    "#### Looking at this error line, we can see that we used the wrong variable name in our filtering function `brokenTen()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (8c) Moving toward expert style **\n",
    "#### As you are learning Spark, I recommend that you write your code in the form:\n",
    "    RDD.transformation1()\n",
    "    RDD.action1()\n",
    "    RDD.transformation2()\n",
    "    RDD.action2()\n",
    "#### Using this style will make debugging your code much easier as it makes errors easier to localize - errors in your transformations will occur when the next action is executed.\n",
    "#### Once you become more experienced with Spark, you can write your code with the form:\n",
    "    RDD.transformation1().transformation2().action()\n",
    "#### We can also use `lambda()` functions instead of separately defined functions when their use improves readability and conciseness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleaner code through lambda use\n",
    "subRDD.filter(lambda x: x < 10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Even better by moving our chain of operators into a single line.\n",
    "sc.parallelize(data).map(lambda y: y - 1).filter(lambda x: x < 10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (8d) Readability and code style **\n",
    "#### To make the expert coding style more readable, enclose the statement in parentheses and put each method, transformation, or action on a separate line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Final version\n",
    "(sc\n",
    " .parallelize(data)\n",
    " .map(lambda y: y - 1)\n",
    " .filter(lambda x: x < 10)\n",
    " .collect())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
} If truth be told then I couldn't understand the VM integration with Spark..Nothing really made sense to me. Maybe my fault.
So I figured the solution was to install Spark on my laptop that is using Windows 7 but how?
I scrapped Google relentlessly for two days and was able to figure out a solution after several trials and failures. This solution I have posted here on my blog. If anyone is interested, take a look.

I also downloaded and installed Anaconda Python on my windows 7 environment and its interface is exactly the same that you are using in this MOOC. Well, at least now I have the software setup installed both on VM as you suggested and on windows OS.
Cheers,
Ashish I ran 4f and received the correct words in correct sorting order, but not with u'prefix.
the: 27355
and: 26028
i: 20681
to: 19150
of: 17451
a: 14593
you: 13615
my: 12481
in: 10956
that: 10890
is: 9134
not: 8497
with: 7771
me: 7769
it: 7678All prev tests I passsed. How do I fix this final one, plz give me a clue or smth Will there be a solution for lab1 after grading?
Thx i am stuck with 2b , might be i am not clear about questions .
Can someone explain what need to be done with some hint samle From now (06/11/2015 17:30PM (PST), we will update autograder servers. There might be some submissions missed if you submited your code in maintenance period. In that case, please resubmit it again after the maintenance of autograder server finished. 

Once the maintenance is finished, I will update this announcement. It will take around 10-30 minutes.
 I think there is some kind of a bug in Lab 1 notebook.  In my case, cells that passed before stop working at random (start giving errors).  The same happens with the tests (in the notebook). The names of the rdds are getting undefined unpredictably.  I already uploaded it twice, redoing the the work, not doing it again.  Seems pointelss  for 4d, i passed the second test, but im off by two words in the word count... any idea what i could have mistakenly stripped that would only account for 2 words? I don't know where to download Lab1, anyone can help? I have a question about the result for 4d. In 4d Words from Line, I got the output as below.
[u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds']
882996
The wordcount,882996, is wrong for this test but turns out to be right answer for next test. This is not a big mistake and I can still get all the tests passed except this one. Why did I get this wordcount one step earlier? I was thinking maybe I remove the empty elements earlier in the removePunctuation function? Did anyone have the same problem? How can I fix it and get the right result for 4d.

Here is my removePunctuation function. Thanks!

import re
def removePunctuation(text):
 """Removes punctuation, changes to lower case, and strips leading and trai  ling spaces."""
 #convert all text to lower case
 lowerText = text.lower()
 nopunctuationText = re.sub(r'[^\w\s]','',lowerText)
 noleadingSpace = nopunctuationText.strip()
 return noleadingSpace
print removePunctuation('Hi, you!')
print removePunctuation(" It's under_score!")



 I've passed all tests, but when I run the code to load the text file, I'm getting a huge error. Why?

Thanks, Hello,

wordsRDD = sc.parallelize(wordsList, 4)
wordPairs = wordsRDD.map(lambda x: (x,1))
wordsGrouped = sorted(wordPairs.groupbykey()) - Gives the error AttributeError: 'PipelinedRDD' object has no attribute 'groupbykey'

Any help on how this error can be resolved is appreciated !

Thanks !

  Hi!

My tests are passing, but autograder doesn't accept submission when i hit "Save" (i clicked "Check" before that, see screenshot):

"Your answers have been saved but not graded. Click 'Check' to grade them."

Screenshot: https://www.dropbox.com/s/lcsm4ox8v8x4swz/Screenshot%202015-06-11%2019.41.12.png?dl=0


I cllicked "Check", but it didn't help. Good evening.

I'm new to Spark, and I'm looking for suggestions for a relatively simple vector operation. I need to sum, subtract and sometimes divide each term of two vectors of a same length. But they are big, and I want to distribute the processing. I thought on creating key value pairs, with their indexes as key, and after doing the calculations with reduce by key.
Is there any other approach? I've tried to use dataframes on SparkR, but I wasn't able to make it there.

Regards,Ricardo Totally two questions.

First:

top15WordsAndCounts = sorted(wordCount(shakeWordsRDD), key=lambda wordpair:wordpair[1]).takeOrdered(15, lambda x: -x)

TypeError: 'PipelinedRDD' object is not iterable

Could you please give me any hint about which RDD I should take or how to sort?


PS:

def wordCount(wordListRDD):    """Creates a pair RDD with word counts from an RDD of words.    Args:        wordListRDD (RDD of str): An RDD consisting of words.    Returns:        RDD of (str, int): An RDD consisting of (word, count) tuples.    """    return wordListRDD.map(lambda word: (word, 1)).reduceByKey(add)



Second:

tempRDD=wordCount(shakeWordsRDD) is OK,
but why cannot I do the following:

tempRDD=shakeWordsRDD.map(wordCount)

 what can i do.. I cant able to run vagrant and Lab1 assignments . For 4b , my code  prints out the correct answer when I run print removePunctuation(" The Elephant's 4 cats. "), but the test fails .
NameError                                 Traceback (most recent call last)
<ipython-input-50-b782bfb79fad> in <module>()
      1 # TEST Capitalization and punctuation (4b)
----> 2 Test.assertEquals(removePunctuation(" The Elephant's 4 cats. "),
      3                   'the elephants 4 cats',
      4                   'incorrect definition for removePunctuation function')

NameError: name 'Test' is not defined Saving work off of Phython Notebook online and resume later?

What happens if the laptop shuts down with or without vagrant destroued - do you need to start all over again? As far as we have seen, Spark programming model is purely functional built around map-reduce. This is great for processing large amount of data in parallel as we did in lab 1.

To program a matrix-vector or matrix-matrix multiplication using for instance MPI or CUDA you have to  go in quite a lot of details on how to partition the matrices and how to collect and assemble the workers results into the output matrix.

I know that the functional programming model should be able to solve any problem, but I have no idea of where to start to design an algorithm for this one.

Are there any references to get started? Dear All

When I am installing the vagrant, i am getting error as
C:\Users\dhanunjaya\myvagrant>vagrant up --provider=virtualboxBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box sparkvm: Progress: 100% (Rate: 1027k/s, Estimated time remaining: --:--:--)The executable 'bsdtar' Vagrant is trying to run was notfound in the %PATH% variable. This is an error. Please verifythis software is installed and on the path.

Please help me... Congrats to all the spark contributors. Finally we see 1.4.0 before the summit next week.  Disclaimer: 1.4.0 should *NOT* be used for this course but thought people here may interest to try.  M.  I completed all the tests in notebooks but failed in autogravder, and got the error below, any one know why?

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 332
SyntaxError: Non-ASCII character '\xe2' in file /ok/submission.py on line 332, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) -- It is giving me the following error now
It was working till yesterday

Bringing machine 'sparkvm' up with 'virtualbox' provider...C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.6.5/lib/vagrant/environment.rb:450:in `initialize': Invalid argument - C:/Users/Viresh Dhawan/.vagrant.d/data/lock.machine-action-6361c5434bf287b70b60366efef0008e.lock (Errno::EINVAL) from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.6.5/lib/vagrant/environment.rb:450:in `open' from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.6.5/lib/vagrant/environment.rb:450:in `block in lock' from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.6.5/lib/vagrant/environment.rb:474:in `lock' from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.6.5/lib/vagrant/environment.rb:449:in `lock' from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.6.5/lib/vagrant/machine.rb:161:in `call' from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.6.5/lib/vagrant/machine.rb:161:in `action' from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.6.5/lib/vagrant/batch_action.rb:82:in `block (2 levels) in run' Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 28, in 
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 24.0 failed 1 times, most recent failure: Lost task 2.0 in stage 24.0 (TID 74, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 734, in func
    initial = next(iterator)
TypeError: () takes exactly 2 arguments (1 given)
And my code up to line 29 is too trivial:


wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']
wordsRDD = sc.parallelize(wordsList, 4)
# Print out the type of wordsRDD
print type(wordsRDD)

# TODO: Replace <FILL IN> with appropriate code
def makePlural(word):
    """Adds an 's' to `word`.

    Note:
        This is a simple function that only adds an 's'.  No attempt is made to follow proper
        pluralization rules.

    Args:
        word (str): A string.

    Returns:
        str: A string with 's' added to it.
    """
    return word + 's'

print makePlural('cat')

# One way of completing the function
def makePlural(word):
    return word + 's'

print makePlural('cat')
Is this the grader, or this is just me? Hi,

today after starting up by 'vagrant up' on Ubuntu 14.04 i got following error:

Bringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...A VirtualBox machine with the name 'sparkvm' already exists.Please use another name or delete the machine with the existingname, and try again.

any way to fix it quickly ? I have almost finished lab01 and I hope i won't lost data....thanks in advance By mistake i deleted the lab1 file . How can i recover it I did submission of my "lab0_student" again by mistake(after due date) and my scored decreased from 100 to 80. What shall I do to get back my lost points. While working on lab1 I realized that the implementation of removePunctuation function would determine the total number of words in shakespeareRDD variable of 4c. It isn't correct in my case since the shakespeareRDD.count() and shakespeareWordCount.count() from 4d have a big difference
shakespeareWordCount should be 927631 or 928908 (from the test) while shakespeareRDD.count() from 4c is itself 122395. 

What could be going wrong? 

My implementation is

def removePunctuation(text):
 PLEASE DO NOT POST SOLUTIONS

The below code gives me 122395
shakespeareRDD = (sc
                  .textFile(fileName, 8)
                  .map(removePunctuation))
print shakespeareRDD.count()
I am blocked at 4d.
 Hi,
I've passed all the previous test but I'm getting 'PipelinedRDD' object has no attribute 'takeOrderer' when I tried to apply takeOrderer function to my wordCounted shakeWordsRDD.

Any help with this please? Hi,

In the lab1, till 4a everything is working as expected.

Below is 4a output:
hi youno underscore

However, when running 4b, I am getting below error. Any idea on what's going wrong here.
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-11-b782bfb79fad> in <module>()
      1 # TEST Capitalization and punctuation (4b)
----> 2 Test.assertEquals(removePunctuation(" The Elephant's 4 cats. "),
      3                   'the elephants 4 cats',
      4                   'incorrect definition for removePunctuation function')

NameError: name 'Test' is not defined Just for fun I was wondering how often numeric words appear in the Shakespeare RDD so I put together this filter (no idea if this is good practice, I'm new to both python and spark, but it works):

numbersRDD = shakeWordsRDD.filter(lambda x: 
                                 x=="one" or 
                                 x=="two" or 
                                 x=="three" or 
                                 x=="four" or 
                                 x=="five" or 
                                 x=="six" or 
                                 x=="seven" or 
                                 x=="eight" or 
                                 x=="nine")
And here are the results, in order of frequency:

one: 1779
two: 702
three: 384
four: 148
five: 126
seven: 70
nine: 60
six: 57
eight: 36
 This may not be very appropriate to ask this given that the course syllabus is available. Nevertheless, would this course touch upon some aspects of spark streaming as well? If not this course, a followup course on the other capabilities of spark would be very helpful.

Thanks How to use split() on shakespeareRDD.
I am getting error message. 'PipelinedRDD' object has no attribute 'split'

I had
shakespeareWordsRDD = shakespeareRDD.split() Hi guys. I need help with python lambda key value associative container. .. than using lambda x: x[1] to get the value from the (key,value) pair? Ok, we know that key is x[0] and value is x[1], but it looks inelegant to rely on the position of variables in the tuple, IMHO.

 My tests are running.
But how to export it as a  .py file.
Help I submitted lab 1 word count assignment and get back incorrect result. The tests are passed for questions in the notebook but the autograder posted the result as incorrect/fail for some questions.

Anyone seeing this issue?

Thanks,
Kid Q1) Where does spark physically stores the lineage information for creating RDD?
Q2) Is it possible to customize partitioning in Spark. So that we can force connected datasets to the same worker.? I am getting syntax error on
top15WordsAndCounts = wordCount(shakeWordsRDD).takeOrdered(15,key=lambda(w,c):−c) No idea...why i am getting this error....basically i did as per in the spark tutorial code i.e.pairRDD.groupByKey().map(lambda (k, v): (k, sum(v))).collect()....still getting the error.....anyone plz give some clue.... Py4JJavaError                             Traceback (most recent call last)
<ipython-input-25-7e950b00eb25> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
      2 wordCountsGrouped = wordsGrouped.groupByKey().map(lambda (x,y):(x,sum(y)))
----> 3 print wordCountsGrouped.collect()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 42.0 failed 1 times, most recent failure: Lost task 2.0 in stage 42.0 (TID 126, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-25-7e950b00eb25>", line 2, in <lambda>
TypeError: unsupported operand type(s) for +: 'int' and 'ResultIterable'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

  Just for fun, here is how to count words using one line of UNIX commands

If you:

cd myvagrant
vagrant ssh
cd data/cs100/lab1/
tr -s '[[:punct:][:space:]]' '\n' < shakespeare.txt | sort | uniq -c | sort -r | head -n 15

23243 the
 22225 I
 18618 and
 16339 to
 15687 of
 12780 a
 12163 you
 10840 my
 10005 in
 8954 d
 8394 is
 8333 that
 8035 not
 7753 me
 7486 s

It does not ignore cases and some other details, but may be a simple start to get a sense of the data...

Enjoying a lot this course :)

 its saying when i execute name 'sc' is not defined . what am i suppose to do its said in video that spark context will be auto generated. Hi 
I have uploaded output for lab1 in to set up section by mistake. This reduced my score :(. 
Can someone please help me and let me know how to fix it ?

Regards
Arijit Here is what i am trying
 # TODO: Replace <FILL IN> with appropriate codeshakespeareWordsRDD = shakespeareRDD.collect().split()print shakespeareWordsRDD.top(5)print shakespeareWordCount

Error :
AttributeError                            Traceback (most recent call last)
<ipython-input-75-c04469bc0fe1> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
----> 2 shakespeareWordsRDD = shakespeareRDD.collect().split()
      3 print shakespeareWordsRDD.top(5)
      4 print shakespeareWordCount

AttributeError: 'list' object has no attribute 'split' Virtual box is not showing my vm, however the vagrant up command works. I am able to access the files using localhost but the 'kernel busy' is always there and I can't execute my code. 
Tell me what to do.  Dear Professor / TAs ,Can we expect "Answer with Explanations for Lab1 word count program" after week2 session ? 
Example : for each question from 1a to 4f- Why we need to answer this way - Suggested Reading / Reference for Particular Question / AnswerThanks. when I run the command "vagrant up --provider=virtualbox" as mentioned in the course instructions, I run into the following error. Kindly Hep as I registered and started the course very late. Thanks!!

error:    Failed connect to s3-us-west-2.amazonaws.com:443; No error

Full details of the error are given below.

Bringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box sparkvm: Progress: 0% (Rate: 0curl:/s, Estimated time remaining: --:--:--)An error occurred while downloading the remote file. The errormessage, if any, is reproduced below. Please fix this error and tryagain.
Failed connect to s3-us-west-2.amazonaws.com:443; No error
 Tests are running in  http:// localhost:8001/ . But when  submit the file in autograder , the autograder shows :

NameError: name 'Test' is not defined.

Kindly Help. please tell me how to install pyspark in ubuntu

i tried but finally  i always got the error


"Failed to find Spark assembly in /home/abhishek/HADOOP&& SPARK/spark-1.3.1/assembly/target/scala-2.10You need to build Spark before running this program."  Hey, is there someone taking this course from Paris?

Would you be interested to have a meetup to share our experience in this course? I would be interested :)

Enjoy! It took almost 4 hours, but I'm done!  Thank you, all who posted about your difficulties with Lab 1.  Your posts were a great help.  This is really a great group! There is something about broadcast variables I don't understand. Why can't Spark itself decide if a variable should be broadcast or not? Or even better, why not broadcast all variables (or for that matter, Lambdas) if it is so efficient? I am guessing there is some overhead that means always broadcasting is not a good idea, but even so, what do as a programmer know that Spark doesn't which prevents Spark from making the decision whether or not to broadcast a variable automatically? Am I correct in assuming that whether or not a partition is bumped out of cache will affect the value of an accumulator? If so, won't the accumulator values be non-deterministic? I am a bit unclear as how Lab1 works. Where is the Shakespeare text file stored? I presume it is somewhere local (I even disconnected from Internet to see if it still works to make sure). If so, where is it? And when did I download it? Was it part of the set up we went through? I've upload the .py file exported from the notebook and check it on autograder. Everything worked fine running in the local environment, but the autograder throws this output (these are the first lines):

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 231
SyntaxError: Non-ASCII character '\xc3' in file /ok/submission.py on line 231, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details
It looks like an encoding error in an internal autograder file. Can someone help with this issue? Thanks in advance. Hi, 

I am new to this course and edX and would like to know if it's possible to change from auditing the course to a certificate.
If so, what steps do I need to take to do so.

Thanks I am unable to get the right solution ! Please help ...
This is what I've used :wordCounts = wordPairs.reduceByKey(map(lambda (x,y):(x,len(y))))I am unable to understand the error. It says map() requires at least 2 args My application is correct , I got 100 in Lab1. I want to use this application on a different file. How can it be done? Hi everyone,

We can start a study group on "whatsapp". We can discuss questions, lecture and notes.

Please, put a reply with your name and phone number(with country code).


Thanks
 I am am stuck.   I think it is because don't think I understand the question (or my laptop is too small).  My code is simple and I would like to post it but I know its not cool to post code for labs so let me try to explain it in words (not code).   I hope this is OK...let me know if it isn't.

Here is my understanding of the problem.... We have shakespeareRDD that has 122395 items where each item is a pair in the form (lineNum, textline). We want to create a new RDD that has one item for each word in each textline.   As part of the transformation, we want to use python split() on each textline and we want the output 'flat' so each new element is a single word (and not a list of words).

When I run a simple flatMap transformation with the python code operating on the textline I get a long stack dump and picking through it I see the line "ValueError: too many values to unpack".

I am trying to understand this error message.  Is it because my transformation is being done at the driver level instead of the worker level?   Is it possible my laptop does not have enough resources to handle this problem? 

To help debug, I created a new ipython notebook, with a subset of the data and with this smaller dataset I can get a new RDD where each word is its own element.   It looks good to me but perhaps I am not understanding the question properly?

Any advice on what I should do to learn more about this problem?  
 Hi,
I guess that i need to upload the iPython notebooks (both the spark tutorial and lab1 notebooks) once I open jupyter (localhost:8001) in my browser (chrome in my case), as I did for the first lab0 to take advantage of the spark environment. 

When I am uploading (using the upload button on the right upper corner of the opened browser window) the lab1 iPython notebook, it is uploaded. No issues. But when I am trying to upload the spark tutorial notebook (in both cases, I had remove the .txt and .html tagged after the .ipynb when I downloaded them) I am getting the following error:
Cannot upload invalid Notebook
The error was: SyntaxError: JSON Parse error: Unrecognized token '<'
Can anybody help ? I am running a bit behind and I guess today (Jun 12th) is the last day to submit.
Thanks in mynotebook code run ok 
but autograder send message about using non-ascii charcater

how can i find it? Is 'add' a function or action, and how do I import it? Hi I typed this my lambda (word,value):(word,sum(value))).take(3) doesn't work I am confused I am following all lab tutorial but I can't continue. Please any advise for this. Thanks. Only advise.  I'm a little unhappy with the autograding system being based off test cases. For some of the shakespear examples my numbers were off, and as with the shakespear example RDDs from one example are used by other example and the way the test cases are built, not only is the output of the structure tested, but the exact number result is tested.. so all the questions where I got the spark concept correct answer were still marked wrong because my number was off from a previous question. Dear all,

I have completed almost all lab exercises, and now I am at 4(d), when I am running it I am getting:
122395 which is very small number

<SNIP> Please do not post solutions, you are breaking the honor code.



Thanks in advance. After running my code, I get the following
1 test failed. incorrect definition for removePunctuation function
I can't turn "it's"  into "its", I don't know what else to do, my code is
def removePunctuation(txt):
    import re
    return re.sub(r "[^A-Za-z0-9]"," ",txt.lower()).strip()

I really miss R's gsub() function
 Successfully finished Lab 1, but it felt mechanical. The forums helped me a lot.

However in a real world situation, how would you know that some of the issues in @575 were even issues (i.e. the counts were incorrect due to a faulty removePunctuation function. For instance, in @575, Steeve Brechmann writes, " I obtain a word count of 882996 instead of 927631 ").

We can see from the tests what the correct count would be, but if that wasn't given to us, how would we know there was an error in removePunctuation, causing an incorrect count ??

This is an important issue because errors compound due to pipelining.
Thanks Hi everybody,

I've registered this course with verified certificate recently. But I found that I might be so late. Today is the deadline of week 1's lab (lab 1). I cannot finish it without taking time to learn. In this case, can I submit the lab assignment in next week ? Is there any problem with it ? Or Can I reverse verified certificate for next course ?

Anyone can help me answer my question ? Thanks a lot  
Input path does not exist: file:/home/vagrant/Lab 1/data/cs100/lab1/shakespeare.txt
I am getting this on step 4c - Load a Text file.  Is this because I created a subdirectory in Jupyter for Lab 1 and so the data file is not a subdir of my working dir?  Thoughts?

EDIT: to answer my own question, Yes that was the problem.  I created the directory stucture in my working dir and moved a copy of the text file there.  Then the code ran fine and printed out first 15 lines. VM is running but http://127.0.0.1:8001/ is not available.  What should I do? Hello After trying halting,destroying,reinstalling the VM,i still cannot upload the .ipynb file.Totally new to all this stuff.Thanks  Hi,

I have customer folder in my local machine and able to run with localhost at port 8001.

For lab1 please help me on how to download lab1 and other files necessary for assignment submission. http://www.ibm.com/analytics/us/en/technology/spark/?S_TACT=M16100GW&iio=panalytics&cmp=m1610&ct=m16100gw&cr=newsroom&cm=h&csot=-&ccy=-&cpb=-&cd=-&csr=socialtech Hi all, can someone explain to me what is needed to do in 4d, I am not understanding it at all. Sorry but english is not my first language.
 Hi,

My top15WordsAndCounts variable looks Ok: 

print top15WordsAndCounts[(u'the', 27361), (u'and', 26028), (u'i', 20681), (u'to', 19150), (u'of', 17463), (u'a', 14593), (u'you', 13615), (u'my', 12481), (u'in', 10956), (u'that', 10890), (u'is', 9134), (u'not', 8497), (u'with', 7771), (u'me', 7769), (u'it', 7678)]
But when i run the test i got this:
IndentationError: unexpected indent

I'll preciate any help! thanks Now sum the iterator using a `map()` transformation.  The result should be a pair RDD consisting of (word, count) pairs.

any clue? When I run vagrant up, I get:
Bringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Clearing any previously set forwarded ports...==> sparkvm: Clearing any previously set network interfaces...==> sparkvm: Preparing network interfaces based on configuration... sparkvm: Adapter 1: nat==> sparkvm: Forwarding ports... sparkvm: 8001 => 8001 (adapter 1) sparkvm: 4040 => 4040 (adapter 1) sparkvm: 22 => 2222 (adapter 1)==> sparkvm: Booting VM...==> sparkvm: Waiting for machine to boot. This may take a few minutes...The guest machine entered an invalid state while waiting for itto boot. Valid states are 'starting, running'. The machine is in the'poweroff' state. Please verify everything is configuredproperly and try again.
If the provider you're using has a GUI that comes with it,it is often helpful to open that and watch the machine, since theGUI often has more helpful error messages than Vagrant can retrieve.For example, if you're using VirtualBox, run `vagrant up` while theVirtualBox GUI is open.

When I start the machine from VirtualBox I get this error:


I  have already tried running command prompt and virtualbox as admin, I have tried reinstalling virtualbox, restarting the system, disconnecting all devices. The VM had worked yesterday.
Please help. Thank You.  wordCountsGrouped = wordsGrouped.groupByKey().map(lambda k,v:k,sum(v)) 

giving following error ..not sure how to sum the iterable
TypeError: unsupported operand type(s) for +: 'int' and 'ResultIterable'

 Hello everybody,

Before to pass test, I would like to check in my function the following:

print removePunctuation(" The Elephant's 4 cats. "), 'the elephants 4 cats', 'incorrect definition for removePunctuation function')

But I see the message:

File "<ipython-input-100-e82282b63092>", line 27
    'the elephants 4 cats',
    ^
IndentationError: unexpected indent

 
I would like to ask another questions:

Have I use an expression regular to remove leading and trailing spaces?

I use a Python function.




 I received my grade after checking (75%). Now what? The box 'sparkmooc/base' could not be found or could not be accessed in the remote catalog.
If this is a private box on HashiCorp's Atlas, please verify you're logged in via `vagrant login`.
Also, please double-check the name.
The expanded URL and error message are shown below:
URL: ["https://atlas.hashicorp.com/sparkmooc/base"]

Error: The requested URL returned error: 403 Forbidden
 
I am getting the above error , Running on Ubuntu 15.04 ,
Thanks


 Hi When I open the browser 

 "http://localhost:8001/

I get a screen with the below :-



data






lab0_student.ipynb






spark_mooc_version






spark_not

I am not sure what the instructions mean when they say :-
Download the IPython notebooks.  Make sure that the file extension is .ipynbIn terms of the above screen which folder do I click? is it lab0_student.ipynb?

Thanks for any help given


 Hi. I've sent my notebook 3 times already, but it doesn't show me results and I see the same "Your files have been submitted. As soon as your submission is graded, this message will be replaced with the grader's feedback." for more than 5 hours. Could you please tell me how to deal with this problem? shakeWordsRDD.wordCount()

ttributeError                            Traceback (most recent call last)
<ipython-input-144-071bc80a8083> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
      2 # TODO: Replace <FILL IN> with appropriate code
----> 3 shakeWordsRDD.wordCount()
      4 #top15WordsAndCounts = <FILL IN>
      5 #print '\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15WordsAndCounts))

AttributeEirror: 'PipelinedRDD' object has no attribute 'wordCount'

 
How do I get wordCount().. Hi, I am in California. Is the deadline for submission over? I could submit but not sure if it will be counted.
Thanks! Totally stumped here...Been browsing through the forums but can't even seem to find someone with similar bad results to mine.
# TODO: Replace <FILL IN> with appropriate code
top15WordsAndCounts = wordCount(shakeWordsRDD).takeOrdered(15, key=lambda (word, count): -count)
print top15WordsAndCounts
Results look like this:



[(u'fawn', 4), (u'yellow', 4), (u'four', 4), (u'cyprus', 4), (u'looking', 4), (u'worser', 4), (u'granting', 4), (u'scold', 4), (u'pierce', 4), (u'foul', 4), (u'politician', 4), (u'bringing', 4), (u'grecian', 4), (u'disturb', 4), (u'wooden', 4)]

edit: Resolved. Thanks everyone for the help, this was driving me crazy.



 Hi Instructors,

Is there any possibility to having the new weeks' courseware released on Fridays, instead of Saturdays ?

I'm in the UK so by the time it gets to 16:00 UTC on Saturday nearly 1/2 my weekend has gone - the weekend being when I have the most spare time to do personal projects/courses such as this and not "work work"

It'd be great if you could accomodate this.

Thanks
Andrew Hi i'm stuck, in local the grader say is all ok, but the autograder online:


Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 28, in 
UnboundLocalError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
local variable 'add' referenced before assignment

All tests passed
Apply makePlural to the base RDD(1c)
------------------------------------
All tests passed
Pass a lambda function to map (1d)
----------------------------------
All tests passed
Length of each word (1e)
------------------------
All tests passed
Pair RDDs (1f)
--------------
All tests passed
groupByKey() approach (2a)
--------------------------
All tests passed
Use groupByKey() to obtain the counts (2b)
------------------------------------------
All tests passed
Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'wordCounts' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'wordCountsCollected' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueWords' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'average' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'wordCount' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'removePunctuation' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'shakespeareWordCount' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'shakeWordCount' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'top15WordsAndCounts' is not defined

-- 7 cases passed (43.0%) --

 
But there is no way that i have a error in 2C:

wordCounts = wordPairs.reduceByKey(lambda (x): (x+1))

Any Hint??? For those who have trouble to learn Python as myself. This web site is great place to learn any language with many real worlds problems.

Hope this can help someone!

http://www.codewars.com/ I am royally stuck in 4d. I figured I have to use flatmap with ShakespeareRDD. But, when I try to, I get this 'pipelinedRDD' object has no attribute 'flatmap' error.
 
A map, however, works fine but doesn't produce expected result. It splits words for each line in a separate iterator which needs to be combined. How do I do this?  

What am I doing wrong?  All parts of my code is working except 4d which works partially.
1) I have seen this issue in other threads - I am getting the count in 4d which is expected in 4e. Can I submit it - will I get partial grade - what grade will I get?

Also should I fix this - perhaps one of the step is redundant?

2) How many times can you submit your notebook - meaning resubmit is allowed?  When I select 'File' and then 'Download as' in the Lab1 notebook, I do not get an option for 'Python file.'

Thinking that perhaps the instructions for Lab0 were faulty, I selected 'iPython Notebook' and submitted it to the autograder. The autograder didn't like that and said that I need to submit a Python file AND it dinged me by reducing the number of submissions I have remaining.

What should I do? I must have had the choice to download Lab0 as a Python file but I don't remember if I did.

To be clear, the only options I see are:

iPython Notebook
Script
HTML
Markdown
reST
PDF via LaTeX

Thank you.
 I have all other tests passed  in autograder except 2d  . Autograder gives 93% due to 2d error . But the test passes  locally.   Can some one please help ? Thanks much

Following is the autograde error
All together (2d) ----------------- Traceback (most recent call last): File "", line 1, in TypeError: 'PipelinedRDD' object is not iterable

wordCountsCollected = (wordsRDD.map(lambda x : (x,1)).reduceByKey(lambda a, b : a+b).collect() ) Hi Sir,

I just finished lab1. I can see the progress as with the following details.Please let me know how I can access Lab2,3....

Lecture 3: Big Data, Hardware Trends, and Apache Spark 5 of 5 possible points(5/5) 100%

Quizzes

Problem Scores:
 1/1
 
1/1
 
1/1
 
1/1
 
1/1


Lecture 4: Spark Essentials 6 of 6 possible points(6/6) 100%
Quizzes

Problem Scores:
 1/1
 
1/1
 
1/1
 
1/1
 
2/2


Lab 1 - Learning Apache Spark (Due June 12, 2015 at 00:00 UTC) 87 of 100 possible points(87/100) 87%
Lab

Problem Scores:
 87/100
 In the tradition of "Keep Calm and Carry On", I will accept zero for this Lab 1 assignment if I run out of time. However, I am wondering if there is some extension to the deadline. Otherwise, is it possible to keep submitting our work after the deadline, just so we can learn from our coding?

The fact is life is too hectic, and when it comes to learning, I want to dig deep beyond just finishing the assignment on time. Apparently, this strategy is not a very good one sometimes, like right now.

having Monday as Deadline make sense as we can work in weekend.
Thanks for extending 

Thank you for your consideration. unable to run the test shown in the lecture and the Download as Python(.py) is also not there.
It's working in firefox. Is there problem with chrome to support  the application.

Regards
Kamlesh Hi Sir,

I just finished lab1. I can see the progress as with the following details.Please let me know how I can access Lab2,3....

Lecture 3: Big Data, Hardware Trends, and Apache Spark 5 of 5 possible points(5/5) 100%

Quizzes

Problem Scores:
 1/1
 
1/1
 
1/1
 
1/1
 
1/1


Lecture 4: Spark Essentials 6 of 6 possible points(6/6) 100%
Quizzes

Problem Scores:
 1/1
 
1/1
 
1/1
 
1/1
 
2/2


Lab 1 - Learning Apache Spark (Due June 12, 2015 at 00:00 UTC) 87 of 100 possible points(87/100) 87%
Lab

Problem Scores:
 87/100
 Can I submit spark job to vagrant in standalone mode. If so on what port can i connect to the master. Hi,

I am not able to connect to this link  to get the file.

http://localhost:8001/" or "http://127.0.0.1:8001/" .

Any one can assist what is the problem. My internet is working fine. Hi,

For 2b I have written map(lambda word, iterator : (word, count(iterator)))
It throws error saying lambda takes 2 arguments i given.

and for 2c, reduceByKey(count)--> here where do we define count function. Hello,

I have a f*****g problem with inentation when submitting. All tests passed in the the notebook, but impossible to get the result of the submission. I looked every lines and verything looks ok, so I don't understand ... Can someone help me please ?

Here is the stack trace :

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 139
    + #### The operation requires a lot of data movement to move all the values into the appropriate partitions.
    ^
IndentationError: unexpected indent

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

Thanks I am writing the following code : 
from operator import add
wordCountsCollected = (wordsRDD.map(lambda (x,y): (x, [y])).reduceByKey(add).collect())
                       
                       
print wordCountsCollected
Its not working, Please help me out!  I tried to start vagrant today from a command line and got an error.  Although I could start it from the VMBox.  When VMBox was running the Spark VM, I could not access the inotebook at local:80?? (I am not at my home computer), can't provide details.First post, want to see if get a response and eventually be able to finish the lab. Imagine I have  [('rat', 2), ('elephant', 1), ('cat', 2)], now how do I make a list like [2,1,2] from it ... ?? Help is greatly appreciated. any ideas?

4d:

[u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds']
928976
4e:
883064 Hi, I'm submitting my lab, but it shows up this error, I'm wondering why ?Can someone give me a hand, Im stuckThank in advance Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 467
    shakespeareWordsRDD = shakespeareRDD.<fill_in>
                                         ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) -- I completed some tests. When I uploaded file to auto grader, it is evaluating everything as test failed. but i passed some tests. why is it like that?  can't we submit without passing all tests?  please help. Thank you Hello everyone,

I printed the 4 lines of RDD used in exercise 4c. The lines are:

[u'1609', u'', u'the sonnets', u'']
I would like to ask a question:

How many words are then in these  lines?

Is the word u' ' a word?

Have I eliminate the words u' '?

Thanks in advance

Carlota Vina 


 Good evening,

I have finished the 'Lab1' and I have upload it as a python file (*.py) within the main server.
There is no feedback in the "Progression" page ..

Could you help me please ?

Thanks a lot

Benoit Hello!

First of all, thank you very much for this amazing course.
First 2 weeks was very interesting, especially links to white papers.

I and my friend decide to take both from XSerial. But summer is a vocation time. It is very hard to study 2 months one-by-one.

As I see in this course we have only 1 week for submit week project.
Could you please increase first week "Scalable machine learning" deadline from 1 to 2 weeks if it possible?    I passed tests up to 2a. so, It has to grade up to that but why it is showing error test not defined. I didn't modify any other code except fill in section.please solve this problem. thank you.
 Would be very helpful if rectified :
Bringing machine 'sparkvm' up with 'virtualbox' provider...
==> sparkvm: Checking if box 'sparkmooc/base' is up to date...
==> sparkvm: Clearing any previously set forwarded ports...
==> sparkvm: Clearing any previously set network interfaces...
==> sparkvm: Preparing network interfaces based on configuration...
    sparkvm: Adapter 1: nat
==> sparkvm: Forwarding ports...
    sparkvm: 8001 => 8001 (adapter 1)
    sparkvm: 4040 => 4040 (adapter 1)
    sparkvm: 22 => 2222 (adapter 1)
==> sparkvm: Booting VM...
==> sparkvm: Waiting for machine to boot. This may take a few minutes...
The guest machine entered an invalid state while waiting for it
to boot. Valid states are 'starting, running'. The machine is in the
'poweroff' state. Please verify everything is configured
properly and try again.

If the provider you're using has a GUI that comes with it,
it is often helpful to open that and watch the machine, since the
GUI often has more helpful error messages than Vagrant can retrieve.
For example, if you're using VirtualBox, run `vagrant up` while the
VirtualBox GUI is open.

 

I am pretty much sure my answers are correct. Even though I haven't completed all the parts. Pls help. hi ,
 I have all correct answers , but autograder stops after some test cases 
NameError: name 'wordCounts' is not defined
NameError: name 'wordCountsCollected' is not defined
NameError: name 'top15WordsAndCounts' is not defined
Help needed badly I am using : top15WordsAndCounts = wordCount(shakeWordsRDD.takeOrdered(15,key = lambda x: x[1]))and i get errors. I am unable to figure out the problem and now i have no options left i used shakespeareWordsRDD = shakespeareRDD.flatMap(lambda x:x.split())

the second assert is correct but not the first one, my wordCount is 882996

any ideas?

Best,
  Below is what I have and it indicates- NameError: name 'wordPairs' is not defined?


# TODO: Replace <FILL IN> with appropriate code
# Note that groupByKey requires no parameters
wordsGrouped = wordPairs.groupByKey().mapValues(lambda word: list(word))
for key, value in wordsGrouped.collect():
    print '{0}: {1}'.format(key, list(value)) I check the script more than 3 hours ago and still it's checking...
I've tried from other computer and I have the same problem¿?
What happen with this? Anybody know something about that? Does anyone encountered this:

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-50-cbd912244396> in <module>()
      8                   .textFile(fileName, 8)
      9                   .map(removePunctuation))
---> 10 print '\n'.join(shakespeareRDD
     11                 .zipWithIndex()  # to (line, lineNum)
     12                 .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in zipWithIndex(self)
   1986         starts = [0]
   1987         if self.getNumPartitions() > 1:
-> 1988             nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
   1989             for i in range(len(nums) - 1):
   1990                 starts.append(starts[-1] + nums[i])

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 1 times, most recent failure: Lost task 0.0 in stage 40.0 (TID 112, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <lambda>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <genexpr>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "<ipython-input-33-e4fe26daea03>", line 21, in removePunctuation
TypeError: translate() takes exactly one argument (2 given)

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
I don't know how to rule this out. Hi,
I am getting blow error when uploading the lab1_word_count_student.ipynb notebook jupyter.

The error was: SyntaxError: JSON Parse error: Unterminated string

I have made below changes on the notebook

"outputs": [], "source": [ "# TODO: Replace <FILL IN> with appropriate code\n", "pluralLambdaRDD = wordsRDD.map(lambda x:x+'s')\n”, "print pluralLambdaRDD.collect()" ]

Am I missing something, please help
Thanks Can the *.py file uploaded on jupyter and we can continue where we left - or is there an online converter for *.py to *.ipynb format? I have a question here.
As I understand split will separate the words in the RDD using space as separator, by which as I understand the blank lines will be eliminated. Then I believe we do not need the filter.

Please validate my understanding.

Regarding the solution:
I used flatmap and split but getting word count of around 88k. Any guidance!!

Thanks I must be doing something silly. I added a single transform how ever my count is 882,996

the assert expects either 927,631 or 928,908

any idea what I might be doing wrong our how to debug this

thanks

Andy


 After hours of work, i am finally on the last question. however the test seems to have disappeared. i think i double clicked it or something. but refreshing the page doesn't bring it back. how do i get the test back?

Screen_Shot_20150612_at_23.35.18.png Hi all,

Shouldn't we see week #3 material available on edX?

Organisers, how about publishing all material for future lectures (the same way as Coursera JHU doing for Data Science)?

Thanks the for consideration!
Pranas wordCountsGrouped = wordsGrouped.groupByKey().map(lambda (key, value): (key, sum(value)))
print wordCountsGrouped.collect()

Why this is not working and giving following error.

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-88-b85e739a0a66> in <module>()
      7 #            map (lambda (key,value):(key,sum(value)))
      8 
----> 9 print wordCountsGrouped.collect()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 73.0 failed 1 times, most recent failure: Lost task 2.0 in stage 73.0 (TID 184, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-88-b85e739a0a66>", line 3, in <lambda>
TypeError: unsupported operand type(s) for +: 'int' and 'ResultIterable'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

 Keep getting this error when I want to run the test at different stages. I have to download lab1 again and enter everything again before I can run test for it to recognize. What is this all about? I know couple of people asked this question about downloading, but I didn't quite understand how it was resolved for them. I am on Windows 8.1 and using Firefox.

When I click on the links for downloading the tutorial and lab1, the following pages opens in the web browser, and I do see a "Download" icon on the Top Right , but when I click on that Download icon, it shows something like this in the browser, but I don't see any python files in my Windows Download folder.. What am I missing or doing wrong??

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
http://nbviewer.ipython.org/github/spark-mooc/mooc-setup/blob/master/lab1_word_count_student.ipynb
http://nbviewer.ipython.org/github/spark-mooc/mooc-setup/blob/master/spark_tutorial_student.ipynb
Spark_Lab1_Question.png

Thanks,
Heman
 My word count seems way off, the following is what I get from the print out and used split(' ')  to write the words into a list. Any thoughts thanks,


[[u'zounds', u'i', u'will', u'speak', u'of', u'him', u'and', u'let', u'my', u'soul'], [u'zounds', u'i', u'was', u'never', u'so', u'bethumpd', u'with', u'words'], [u'zounds', u'i', u'lie', u'for', u'they', u'pray', u'continually', u'to', u'their', u'saint', u'the'], [u'zeal', u'and', u'obedience', u'he', u'still', u'bore', u'your', u'grace'], [u'youths', u'a', u'stuff', u'will', u'not', u'endure']]
122395 The materials suggest that "experts" chain their pyspark calls. Does this actually confer any advantage aside from a more compact syntax? Does chaining improve auto-optimization or something? Hi - just in case I didn't complete the lab in time, I submitted a partially completed lab.

However after I submitted it, the autograder returned:

-- 0 cases passed (0.0%) --



and many of the following for sections that I've already passed locally:

NameError: name 'Test' is not defined



I have passed all tests locally on my machine up to: (1f) Pair RDDs

Thanks

TJ Most big data labs focus on word counts - what is the importance of counting words in a document when it comes to Big Data (BD) (other than learning the platform of BD)?

 Hello Fellows,

What is wrong here?

# TODO: Replace <FILL IN> with appropriate codepluralRDD = wordsRDD.map(makePlural(wordsRDD.collect()))print pluralRDD.collect()

I'm getting this error

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-30-850323f86d0f> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
----> 2 pluralRDD = wordsRDD.map(makePlural(wordsRDD.collect()))
      3 print pluralRDD.collect()

<ipython-input-28-58e2bc4f0f4b> in makePlural(word)
      1 # TODO: Replace <FILL IN> with appropriate code
      2 def makePlural(word):
----> 3     return word + 's'
      4 print makePlural('cat')

TypeError: can only concatenate list (not "str") to list
Thanks Is it possible to submit the lab tomorrow? i answer the most question of the lab not all of them but when i try to check i get this message :
Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 187
    print wordCountsGrouped.collect()
        ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

help please  Hi, there:
  I can not upload the lab1 file (.ipynb) to http://127.0.0.1:8001.
  It showed as follows

Cannot upload invalid Notebook

The error was: SyntaxError: JSON.parse: unexpected character at line 10 column 1 of the JSON data

please help. thanks.
  Jane
 Hi,

I missed the last date of submission week 2 Lab 1. Is it possible to get an extension.

Thanks in advance. I am not sure what regex i use this is frustating , Can some one help ...

#patten = re.compile('[%s]' % string.punctuation)    #pattern = re.compile("[a-zA-Z0-9\s]+")     #return re.sub(pattern, '', text.lower().strip())    #pattern = re.compile('[^A-Za-z0-9]' % string.punctuation)    #return re.sub(pattern, '', text.lower().strip())    #return re.sub(r'[^A-Za-z0-9]','',text).lower().strip()        #regex = re.compile('[%s]' % re.escape(string.punctuation))    #return regex.sub('',text.lower().lstrip().rstrip())    text=text.lower().strip()    text = re.sub(r'[^A-Za-z0-9\s]','',text) Good evening.

I'm dealing with two dictionaries that I want to extract a feature. Each one contains Key-value pairs, and with common keys among the dictionaries (but into each one, the keys are unique).

Each key it's assciated with a vector, like this:

7963: array([0, 0, 0, ..., 0, 0, 0], dtype=uint16)

My initial idea was loading these two dictionaries and apply some form of MapReduce. Reading the chapter of "Learning Spark" on key value pairs processing didn't help so much, because, despite of bein able to load them into a RDD using .parallelize , I'm with in serious difficulties on write my Map Function (and sequentially, after doing a Inner Join, Process each key with the ReduceByKey function.

My idea is calculating each array, by key giving a normalization like:

norm = (A+B)/(A-B)


Any suggestion on how to implement this in Python?

Best,
Ricardo Hello, I could not find the correct regular expression to remove the special chars without also removing the unwanted spaces.

I could get here:

pattern = re.compile("'")newText = pattern.sub("", text)            pattern = re.compile("[^\w]|_")newText2 = pattern.sub(" ", newText)    return newText2.lower().strip()

but this is resulting in a string with 2 spaces between "hi" and "you", which doesnt seems correct

hi  you
no under score
the elephants 4 cats
Thanks in advance hi. Any tamil people taking this course?
Study group for Tamil? uniqueWords = wordsRDD.distinct().collect ()
print uniqueWords


Above is working but Adding count is throwing an error .  Are we expected to write a long function as stated in other post ? Can anyone advice this ? 
my code : 

# TODO: Replace <FILL IN> with appropriate codetop15WordsAndCounts = wordCount(shakeWordsRDD).takeOrdered(15, key=lambda x: -x[1])print '\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15WordsAndCounts)) I was surprised to read this in the Spark tutorial that is part of the 2nd lab: "Spark automatically manages the RDDs cached in memory and will save them to disk if it runs out of memory" 

Is that only for the Python API? is it also for the Scala API? I thought the default for Scala is MEMORY_ONLY, not MEMORY_AND_DISK. 

So is Python's default cache MEMORY_AND_DISK? Or is Scala's MEMORY_ONLY is not really memory only?

Thanks! Hi all! The code for 4f worked fine on the notebook but showed up an error when submitted for the Autograding.
Please let me know where I've gone wrong!

The code is:
top15WordsAndCounts = wordCount(shakeWordsRDD).takeOrdered(15, key=lambda x : -x[1])print '\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15WordsAndCounts))

And this is the Autograder's feedback:

Count the words (4f)--------------------Traceback (most recent call last):  File "", line 5, in   File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals    cls.assertTrue(var == val, msg)  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue    raise TestFailure(msg)TestFailure: incorrect value for top15WordsAndCounts
 Can anyone please tell me how to run spark programs (as is lab exercise) locally without using VM. For the course, of course, I am using VM. But, I would also like to run them locally.  There are two shakespeare word count.

shakespeareWordCount == 927631
and other is 
shakeWordCount ==  882996

How is first one different from second one.
When i do a regex operation I directly got the second ans.Because of which only on test case in 4d is failing.Please help me understand. i am stuck here i really dont know how to proceed i tried lot of code i am getting error in the code ,can anyone suggest what to do with this question Hi,

For lab1 4(b) need help on re,sub pattern. I have finished lab1 up to 4a) two days ago and everything was working fine.
Today I wanted to finish it but I get 

NameError: name 'Test' is not defined
even on excersises previously were passed localy:

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 375
    shakespeareWordsRDD = shakespeareRDD.<fill_in>
                                         ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined


<fill_in>Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined



Generally even on my VM I get thge same error even though everything was working fine tho days ago!

Please help... In question 4(d), I found a list of list where each element is a phrase, thus my count is wrong I suppose. help!!
# TODO: Replace <FILL IN> with appropriate codeshakespeareWordsRDD = shakespeareRDD.map(lambda x: x.split(' '))shakespeareWordCount = shakespeareWordsRDD.count()print shakespeareWordsRDD.top(5)print shakespeareWordCount

[[u'zounds', u'i', u'will', u'speak', u'of', u'him', u'and', u'let', u'my', u'soul'], [u'zounds', u'i', u'was', u'never', u'so', u'bethumpd', u'with', u'words'], [u'zounds', u'i', u'lie', u'for', u'they', u'pray', u'continually', u'to', u'their', u'saint', u'the'], [u'zeal', u'and', u'obedience', u'he', u'still', u'bore', u'your', u'grace'], [u'youths', u'a', u'stuff', u'will', u'not', u'endure']]
122395 I am stuck in lab 1 2a... i cannot understand how to sum up the elements using map function..... I can't use virtualbox on my local machine so I'm using ec2 with Spark and notebook installed. Both are working fine, except that I get error on these lines
from test_helper import Test
It can't find the required module test_helper, where do I get it and where should I put it? shakespeareWordsRDD = shakespeareRDD.map(lambda line: line.split())

Please help me to fix the error in the code. I have tried to submit my file 3 times...no idea why i am getting the below error!!!....anyone please help me out...
Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 376
    shakespeareWordsRDD = shakespeareRDD.<fill_in>
                                         ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

<fill_in> Its Urgent .. 
Hello, I got a question when I trying the spark tutorial, part 3B, which is try to create a RDD. the code as following shows.
# Parallelize data using 8 partitions
# This operation is a transformation of data into an RDD
# Spark uses lazy evaluation, so no Spark jobs are run at this point
xrangeRDD = sc.parallelize(data,2)
while, I got a error when I run this code, as:



---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-61-5393d1ad9996> in <module>()
      2 # This operation is a transformation of data into an RDD
      3 # Spark uses lazy evaluation, so no Spark jobs are run at this point
----> 4 xrangeRDD = sc.parallelize(data,2)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in parallelize(self, c, numSlices)
    329                 return xrange(getStart(split), getStart(split + 1), step)
    330 
--> 331             return self.parallelize([], numSlices).mapPartitionsWithIndex(f)
    332         # Calling the Java parallelize() method with an ArrayList is too slow,
    333         # because it sends O(n) Py4J commands.  As an alternative, serialized

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in parallelize(self, c, numSlices)
    342         tempFile.close()
    343         readRDDFromFile = self._jvm.PythonRDD.readRDDFromFile
--> 344         jrdd = readRDDFromFile(self._jsc, tempFile.name, numSlices)
    345         return RDD(jrdd, self, serializer)
    346 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.
: java.lang.NullPointerException
	at org.apache.spark.api.python.PythonRDD$.readRDDFromFile(PythonRDD.scala:392)
	at org.apache.spark.api.python.PythonRDD.readRDDFromFile(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)

Does anyone can help me with this problem?
Much thanks in advance
Kexin I have downloaded the spark_tutorial_student in the .ipynb format. But when  i upload it to the localhost, its getting uploaded in the format ".ipynb.webarchiver" . Please help. QQ group number: 210085644
edX: cs100.1x Spark

Welcome to join from where i get IPython notebooks I completed the assignment with no problems and I have found the concepts covered in this course to be straightforward

However, I have two questions:

1. Is the Virtual Machine that we run in our Window systems (typing "vagrant up") an actual instance of Apache Spark? Or is it merely a simulation of the real thing?
2. What services does Databricks provide companies that wish to use Apache Spark? Surely these organisations can run their own Apache Spark instances in AWS without paying money to another organisation... I think I am missing something here. What has to be passed to reduceByKey function to get it working Hello,

I have followed the instructions on how to import .dbc files from here:

https://github.com/spark-mooc/mooc-setup-dbc/

into databricks. I click on the little arrow next to my home folder, click import, choose the file and do upload.

Then I click ok, but the file does not appear in my folder.

Does anybody know what I could be doing wrong? Where do I download the slides from? This is what I get after check

Compare with hash (2a)
----------------------
All tests passed
Compare lists (2b)  It seems to have actually  penalised my lab0 for %20 even though all was well and on time. Could someone please fix this or explain what is wrong. I exported lab1, and then tried to import it and use check/ I am using steps for lab0 for submission
------------------
All tests passed
-- 2 cases passed (100.0%) --


As for late submissions after 3 day grace period of the due date, you lose 20 points.
 Your final score is 80

Am I submitted, because I do not see the lab1 as submitted. Do we get 20 submissions per lab? Thanks I finished the Spark tutorial and I am trying to download the file. I only get a new blank tab after requesting the download as .py
Where could the problem be? Any clues?
Thanks! I am pretty new to python and I am having a difficult time understanding how to apply split() to the shakespearRDD in problem (4d). Here is my code:

shakespeareWordsRDD = shakespeareRDD.flatMap(lambda x: x.split(" "))

The answer I get is:
[u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds'] 1279363

If I add removePunctuation, then the word count is reduced a bit:
shakespeareWordsRDD = shakespeareRDD.map(removePunctuation).flatMap(lambda x: x.split(' '))

Answer:
[u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds'] 1169705

Can someone help me? What am I doing wrong?


 
In [1]: rdd = sc.parallelize([(1,2), (2,3), (3, 6)])

In [2]: rdd.reduceByKey(lambda a,b: a + b)

Out[2]: PythonRDD[5] at RDD at PythonRDD.scala:43
 Quizz is asking Spark is often faster than a traditional MapReduce implementation because:
  It sends less data over the network . Why not  this also a  correct  option?

Since the intermediate results are  performed in the memory , does  this not mean we are sending less data over the wire ?

 I submitted lab1 on Jun13 11am EST ,but i got a message :
Compare with hash (2a)
----------------------
All tests passed
Compare lists (2b)
------------------
All tests passed
-- 2 cases passed (100.0%) --


As for late submissions after 3 day grace period of the due date, you lose 20 points.
 Your final score is 80

We have a grace period of 3 days(ie June 15) ,correct?Why did i lose 20 points? lab 1 is challenging enjoyed programming!!! I have passed all the cells in the notebook, yet got the following in my final submission.We have TWO problems here:-1-What is wrong with this line of code	"shakespeareWordsRDD = shakespeareRDD.flatMap(lambda stri : stri.split(' '))"??????????!!!!!!!!!!!2-The autograder is not "auto" at all, one thing malfunctions,and every thing stops immediately ,so in effect you either get 100% or a "BIG" zero?????!!!!	After all I will send my code by E mail if staff accepts .Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 127
    shakespeareWordsRDD = shakespeareRDD.flatMap(lambda stri : stri.split(' '))
                      ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

 This week I was not able to get through the entire assignment. Is there anyway to submit the file that is only partial completed to get some credit? I imagine I just will not always get through every exercise. Or is it all or nothing? I watched the video that Zaharia introduces the RDD

https://www.usenix.org/conference/nsdi12/technical-sessions/presentation/zaharia

At the end one question was asked about memory leakage for this kind of distributed system; and Zaharia answered that Spark does garbage collection.

I'm wondering why it's likely to have memory leakage for distributed systems? under which scenarios?  I am getting a 404 on this link.

http://nbviewer.ipython.org/github/spark-mooc/mooc-setup/blob/master/lab2_apache_log.ipynb

Thanks.
Preetham NOT FOUND: Lab 2 IPython notebook

https://raw.githubusercontent.com/spark-mooc/mooc-setup/master/lab2_apache_log.ipynb 
 
Hi all,I am working on Lab 1 and got stuck with 2d question. I'm getting an error here. I do not understand what to do with the map here. I'm posting my code below. Please let me know what 'm doing wrong. Thank you for your help.
(2d) All together The expert version of the code performs the map() to pair RDD, reduceByKey() transformation, and collect in one statement.
wordCountsCollected = (wordsRDD                       .map(lambda x: (x + 's'))                       .reduceByKey(lambda x, y: x + y)                       .collect())print wordCountsCollected




 Lab 2 is not present , when i clicked on .ipnyb file it shows no file found.Please upload lab 2

Thank You The 5x performance difference referenced in lecture 3 suggests that data frames are always better.  Is there some kind of tradeoff? i tried to start my sparkvm from the vm and am getting (it wouldnt start from cmd)  " spark vm login .... sombody should please help me real fast. i avnt done anything. this is my 2nd week of having problem with vagrant install. am yet to even post my first lab Is this course offered this fall as a regular course? If not, when will it be offered next?

Thank you. Mike  The "pandas and semi-structured data in pySpark" video has a graph that shows that for a particular integer aggregation the RDD Scala significantly outperforms RDD Python. What is the reason for this? In general does the Scala binding to Spark deliver better performance than the Python binding?   As per Week 3 lectures, Scala is much efficient then python. So please recommend which language we should used.
 In lecture 3, in the 2nd short video, the professor says that "the size of storage is doubling every 18 months". What is intended to be meant by this statement?   Does lab 2 have explicit tutorial like lab 1?  am using proxy with my connection and my vagrant goes into this infinite loop everytime i tried to open it






C:\Users\prof.BOLA\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Clearing any previously set forwarded ports...==> sparkvm: Clearing any previously set network interfaces...==> sparkvm: Preparing network interfaces based on configuration... sparkvm: Adapter 1: nat==> sparkvm: Forwarding ports... sparkvm: 8001 => 8001 (adapter 1) sparkvm: 4040 => 4040 (adapter 1) sparkvm: 22 => 2222 (adapter 1)==> sparkvm: Booting VM...==> sparkvm: Waiting for machine to boot. This may take a few minutes... sparkvm: SSH address: 127.0.0.1:2222 sparkvm: SSH username: vagrant sparkvm: SSH auth method: private key sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying... sparkvm: Warning: Connection refused. Retrying...





any special config pls Using the groupByKey() transformation creates an RDD containing 3 elements, each of which is a pair of a word and a Python iterator.
Now sum the iterator using a map() transformation. The result should be a pair RDD consisting of (word, count) pairs.

In [29]:










# TODO: Replace <FILL IN> with appropriate code
wordCountsGrouped = wordsGrouped.groupByKey().map(lambda (k, v): k,sum(v))
print wordCountsGrouped.collect()













---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-29-f15faf1a68b6> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
----> 2 wordCountsGrouped = wordsGrouped.groupByKey().map(lambda (k, v): k,sum(v))
      3 print wordCountsGrouped.collect()

NameError: name 'v' is not defined




 Great course. I would like to suggest not having to spend time on regex in the next version of this course in order to complete lab 2,
since having to master regex before being able to complete the exercise can be time consuming.
 Hi,

I am using split function with lambda inside the map function. But it is giving less words. Is any way to split the lines without map function. Please help me  to resolve this problem.

Thanks
Veerendra

 

"Bad" log line
ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:43:39 -0400] "GET / HTTP/1.0 " 200 7131

The good log line contains an ip address, and a path after GET
An example of a bad log line contains a domain name, no path after GET, and a space after HTTP/1.0
Note that the bad log lines differ.

Deconstructing the regex

^(\S+) (\S+) (\S+)
Matches 3 first groups/splits in good and bad log

\[([\w:/]+\s[+\-]\d{4})\]
Matches date and time in good and bad log

"(\S+) (\S+)\s*(\S*)" 
There seems to be a problem in this regex for the bad log

(\d{3})
finds 3 digits in good and bad log

(\d{3}) (\S+)
finds digit groups in good and bad log
1 with 3 digits
1 with 4 digits

#Hint:-
Try to understand what the s* does.  

Note: According to Lab2, "This is the IP address (or host name, if available) of the client (remote host) which made the request to the server...". Thus, a host name such "ix-sac6-20.ix.netcom.com" should be OK; does not have to be a numeric address.
#pin For users having no problem loading up vagrant but still is receiving the annoying warning on Mac OSx:

"/Users/Username/.rvm/rubies/ruby-1.9.1-p378/bin/gem:4: warning: Insecure world writable dir /Users/Username in PATH, mode 040777"

Just type in the following into your terminal (substituting your computer user name for UserName:

"sudo chmod go-w /Users/UserName/"

Voila! Error no longer shows up.

Can someone place this in the Wiki? I wanted to suppress INFO messages in the pyspark shell according to Spark configuration guidelines. The vagrant user is not a sudoer though and we don't have the ubuntu user credentials in order to create files in SPARK_HOME/conf so instead we need to override SPARK_CONF_DIR to a directory in the vagrant home:

vagrant@sparkvm:~$ mkdir conf
vagrant@sparkvm:~$ cd conf/
vagrant@sparkvm:~/conf$ cp $SPARK_HOME/conf/* .
vagrant@sparkvm:~/conf$ cp log4j.properties.template log4j.properties
vagrant@sparkvm:~/conf$ vi log4j.properties

Edit using to the highest ranked response to this StackOverflow question then:

vagrant@sparkvm:~/conf$ export SPARK_CONF_DIR=$PWD

Enjoy a more peaceful shell experience.



 Hi,
I have a problem understanding the right outer joint (Lecture 6, "Joins in Spark")
the algorithm is described as: for each element IN Y .......
But Y has only one element ("a", 2). So the element ("b", 4) should be filtered out. The output is consistent with having the element ("b", 4) in the set Y instead of X.
Any idea?Thanks As Spark 1.4 released 2days back I would like to update my virtual machine (sparkvm )
Could some please let me know how to update the current spark version to spark 1.4

Thanks in advance.
 Working my way through this weeks lecture (5) and got to the section on
'File Performance'. It states binary data will be consistently faster to read/write 
than text data.

This doesn't seem to make sense to me ; the filesystem doesn't care what data is in the file, right?

I'd have assumed binary was a more compact format, but the slide explicitly says the binary
file is larger than the text one. I'm not able to execute the first full code snippet in section 4c. 

This section - 
print '\n'.join(shakespeareRDD .zipWithIndex() # to (line, lineNum) .map(lambda (l, num): '{0}: {1}'.format(num, l)) # to 'lineNum: line' .take(15))

throws an error : 

TypeError: translate() takes exactly one argument 
. I do have the shakespeare.txt in the right directory. 

Can anyone please advice ?

Thanks!
 
hosts = (access_logs
        .map(<EDIT>)

uniqueHosts = hosts.<FILL IN>

I *think* that should read log.host, not log.content_size.  Kind of an obvious thing, but I figured others might get tripped up if not reading carefully. spark.png

>>>rdd=sc.parallelize([1,2,3,4,])
>>> rdd.map(lambda a:a+2)

PythonRDD[1] at RDD at PythonRDD.scala:43

i do this operation and get the output as   "PythonRDD[1] at RDD at PythonRDD.scala:43" 

how to see the output as professor shows in videos (Late post, but might help others)

Another option to open the VirtualBox in order to check if your vm is running could be running it from command line by just typing:  VirtualBox.

After searching for VirtualBox in Ubuntu, it didn't show Oracle VM VirtualBox as shown in the setup videos.
 can i use vagrant with vmware instead??????????? should it be ('a', (1,2)) instead of the slide's ('a', (2,1))
that is if the bullet about (k, (v,w)) is in result where v in X and w in Y
and where ('a', 1) in X and ('a', 2) in Y

or maybe i'm just dyslexic :)

cheers
 after installing the vm and the vagrant, it demands a login credential and i could find no reference to that in the materials.  It wouldn't take my computer user/pw.

any insight? Hi,
What is the required grade percentage to get a certificate for this course?
I have searched the site but cannot find this answer.
Thanks
Joshua Hi,
Anybody else find the following notebook cells very slow ?

ln [6] - Change to REGEX and call parse_logs ()
ln [7] - Tests for above ?

It took over 5 minutes for the tests to return results for me I get the following error messages in Lab 2 (2f)
What should I do?
The assignment doesn't call for  inserting or changing code.
However, the top 10 endpoints are incorrect.
Are the errors on purpose?

AssertionError                            Traceback (most recent call last)
<ipython-input-13-606dbe4c203b> in <module>()
      7 
      8 print 'Top Ten Endpoints: %s' % topEndpoints
----> 9 assert topEndpoints == [(u'/images/NASA-logosmall.gif', 59737), (u'/images/KSC-logosmall.gif', 50452), (u'/images/MOSAIC-logosmall.gif', 43890), (u'/images/USA-logosmall.gif', 43664), (u'/images/WORLD-logosmall.gif', 43277), (u'/images/ksclogo-medium.gif', 41336), (u'/ksc.html', 28582), (u'/history/apollo/images/apollo-logo1.gif', 26778), (u'/images/launch-logo.gif', 24755), (u'/', 20292)], 'incorrect Top Ten Endpoints'

AssertionError: incorrect Top Ten Endpoints As much as I am enjoying this course, I am wondering what career opportunities are there as a result of completing this course? Why is it required to cache RDDs?

According Spark Program life-cycle, initially a RDD is created.
The if we need to access it as many times in the code it should be still in the memory, how will there be any re-computations (as said in Caching RDDs lecture), as the RDD was initially already created ?

Can someone please explain this.
Thnx
 Is it possible to download the Apache Web Server log file that is being used for Lab 2? I will be on an airplane on Monday, and it would be great to try to make some headway with the Lab while I am inflight and disconnected from the Internet.

Much appreciated,

marc Dear Instructors,

in my humble opinion, function parseApacheLogLine regexp should use named groups (e.g. (?P<id>(\d{20})) )
It's better way for creating own regexps, because in case when someone want create own regexp based on previously non named groups (based only on count of groups) is slightly hard to match specified number (must count brackets...)




 Hey whats the difference between 1c and 1d of Lab 1. Is there any other way to do it. I am not able to find it. Sorry new to python. 
Thanks for answering the silly question An errata was added to lecture 6 on the slide about right outer joins. But the result is still wrong. I should be:

[('a', (1, 2))]

Instead of:
 Here is the update of my program ..Its so wired that I am able to get correct answers for 4f but my 4a give me the wrong answer

# TODO: Replace <FILL IN> with appropriate codedef wordCount(wordListRDD): """Creates a pair RDD with word counts from an RDD of words.
Args: wordListRDD (RDD of str): An RDD consisting of words.
Returns: RDD of (str, int): An RDD consisting of (word, count) tuples. """ return wordListRDD.map(lambda word: (word,1)).reduceByKey(lambda x,y: x+ y)
print wordCount(wordsRDD).collect()

output:
[((('cat', 2), 1), 1), ((('elephant', 1), 1), 1), ((('rat', 2), 1), 1)]
 any help to fix this would be appreciated as I have already spent 4 hours on this single question Hi, Could you help explain how Spark partitions a compressed file(s)?

For example:

Semi-structured files were compressed into a single large file.  How does Spark partition this compressed file and distribute them into worker machines?

Semi-structured files: SS1 + SS2 + SS3 + SS4 + SS5 + SS6 ---> CF (compressed file)

Partition of CF into 3 will give: CF(SS1 and SS3), CF(SS2 and SS4), and CF(SS5 and SS6) (or in other random way)? Or, it could also be that the partitioned files will randomly contain part of SS1, part of SS2, SS3 in one partition; part of SS1, part of SS2, SS4 in one partition; and SS5 SS6 in one partition? As some people pointed out, executing some of the cells for Lab 2 are pretty slow. Your host machine is almost certainly more powerful than the default performance characteristics allotted to the default sparkvm in Virtualbox. Go into the sparkvm settings and pump up your RAM and number of CPUs (I usually pick about half of what my host system has). Cut down processing times on those slow cells significantly for me.

Of course RAM almost always helps but wondering what impact CPU count has on our single unclustered lab VM setup? Presumably it will split jobs across CPUs but I guess we'd need to see how the SparkContext is created for full understanding?  Hi All,

after executing 3c, I have the following output
Unique hosts per day: [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4404), (9, 4317), (10, 4522), (11, 4346), (12, 2863), (13, 2650), (14, 4454), (15, 4213), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2559), (21, 4133), (22, 4456)]
which is different from the answer, 8 is off by 2, 10 is off by 1, 12 is off by 1, etc...

The solution logic: apply `set` the the list of hosts and get the length of the list...
I don't know where this could go wrong. Please let me know if I should post my solution. Thanks for helping Now I've spent almost 3 hours on 4b - reading all kind of comments here in the piazza without success. Porbably I'm on the wrong track...

That's my code:

text.strip()return re.sub('[^a-zA-Z0-9]',' ', text.lower()).split()
which returns:

['hi', 'you']
['no', 'under', 'score']
I can exchange .strip() with .split() which gives me:
hi  you
no under score

Both versions are not passing the test:
Test.assertEquals(removePunctuation(" The Elephant's 4 cats. "),
                  'the elephants 4 cats',
                  'incorrect definition for removePunctuation function')1 test failed. incorrect definition for removePunctuation function

I am a bit frustrated by not beeing able to find the correct solution. Maybe some of you can give me hint - and yes, I read post 575 already. Still clueless. I'd really appreciate your suggestions!

Thanks in advance! It takes a long time, to master regex. I am concerned that it maybe a requirement to pass the Apache Spark course Hello
I finished 2d. But I stuck in 3a. I'm using the list I found in 2d. In 2d I generate the list which has words and their occurrence number.
I think I should use this list and some count object. But I use filter or count in this list I've got the exception 

'list' object has no attribute 'filter'

So ony one help me what methods I can use and which parameters I can use with this method

Thanks Would there be suggested answer posted after... There are deadlines for the Lab assignments, somehow, right? Sometimes, people just get stuck at certain questions... :)  I am totally lost on how to proceed with lab 2 question 3e. As in, I am lost on how to start tackling it. Totally out of ideas on it. Please where hints or pseudo-code would be grealty appreciated. Please just a pseudo-code not the actual code. I would love to figure out the code on my own. Thanks

By the way, i am really enjoying the course. Much thanks to Prof Anthony and his team for putting together such a wonderful and challenging course.    Hello I have been having trouble submitting my Lab 1. I managed to complete up to 4b before I couldn't finish the rest (due mostly, to errors on 4c). Any suggestions/alternate routes towards receiving (partial) credit?
Thanks

Here is the dialog the autograder feed me:

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 420
    top15WordsAndCounts = 
                          ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

  "dayGroupedHosts = <FILL IN>" seems to imply using "groupByKey()". Is that the correct assumption?

Isn't that a performance hit? (ref: https://spark.apache.org/docs/latest/programming-guide.html#rdd-operations and http://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html). Part (1b) runs and produces the expected output, but when I try to run part (1c) (after modifing the regex) I get a stack trace. As best as I can figure out this is the error:
EOFError
ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server
I stopped the notebook, reloaded it and commented the line:
#parsed_logs, access_logs, failed_logs = parseLogs()
in part (1b). with this modification part (1c) ran fine and produced the output:
Read 1043177 lines, successfully parsed 1043177 lines, failed to parse 0 lines
Any Ideas? should I leave the call to parseLogs comented in (1b)

I think that parseLogs failed to close the file and on the second execution it was already past the end. How do you close a file in Spark? We don't have a file handle.

Also, I think that a better style would be to put the print statement outside of the function. whats wrong, please help


In [2]:
















# TODO: Replace <FILL IN> with appropriate code
# Note that reduceByKey takes in a function that accepts two values and returns a single value
wordCounts = wordPairs.reduceByKey(lambda x,y:x+y)
print wordCounts.collect()

















---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-2-b9b99d47e68e> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
      2 # Note that reduceByKey takes in a function that accepts two values and returns a single value
----> 3 wordCounts = wordPairs.reduceByKey(lambda (x,y):x+y)
      4 print wordCounts.collect()

NameError: name 'wordPairs' is not defined





 Hello all!

Could someone please explain in simple words as to the difference between schema and data model as well as giving some examples?

The lecture left me a bit confused as to this.

Thanks! Getting a counts much higher than in the test value..Conceptually I have the following  [ date, [host1,host2,host3] ]  actual snapshot ...[('1995-08-08', set([u'markvogt.xnet.com', u'pm3_27.digital.net', u'hobbes.ksc.nasa.gov', u'calnews.demon.co.uk', u'rbailey_pc.orem.novell.com', u'128.203.144.4', u'nssdca.gsfc.nasa.gov', u'y1a.kootenay.net', u'pm1d02.iaehv.nl', u'193.216.125.31', u'drjo022a237.embratel.net.br', u'193.216.125.32', u'norden.com', u'146.229.22.28',Anyone else facing similar issue ? thanks  Out of the 3V's in Big data, does variety refers to the different structures of data like unstructured, semi-structured & structured OR did it refer to different data sources like social media, heath data, insurance data etc? For e.g. when an enterprise considers itself having a big data problem in terms of variety, does it mean it has variety of data sources it has to now integrate with or does it mean it has to now support ETL for different structures of data? While the latter seems logical I wanted to have expert thoughts on it.

On a related note my second question is , just having to integrate with different varieties of data shouldn't be considered as a big data problem and hence leading to the use of big data technologies like spark, hadoop etc, right? Variety could be even dealt with standard enterprise technologies. A need of having to process huge amounts of data (volume) faster (velocity) would be valid needs of using big data technologies. Thoughts? I've been tweaking the code that came before this too in case it affected this part too, but I keep getting an answer of 883019 regardless of what I do after the filtering stage.

I have a lambda function lambda x: len(x)>0, but it seems like there are 23 words that are not getting filtered out properly?

Everything tested fine up to this point, so I would really appreciate it if someone could help point me in the right direction.  Thanks! Databricks provides certification via their "Certified Spark Developer" program. We have already installed a virtual machine for use in this course. Is this VM good enough for preparing for this certification/exam? Ipython notebook worked fine in the lab1, but for lab2, notebook is not embedding output. I already did a vagrant reload, but it did not work. So any ideas.

Thanks. I have NOT yet submit the homework of lab1, but the deadline have past!!! How can I remedy it？？？ help needed lab1:part(2b) hi how does spark determine unique values in this method  if the RDD elements are not primitive type but  user defined type like for example  objects of a class. 

The same question also applies to distinct() method. 

thanks.  I ran the first lab assignment in the notebook and it appeared to run fine. When I uploaded the file and then ran the autograder I got the following errors... Not sure how to proceed




Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 23, in 
NameError: name 'Test' is not defined

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) -- Hi Guys For this problem 
# TODO: Replace <FILL IN> with appropriate code
pluralLengths = (pluralRDD
                 <Fill IN>
                 .collect())
print pluralLengths
Not sure where to go from here.   I know that the purpose of lambda is to create a function. Not sure what to fill out here . I read through https://spark.apache.org/docs/latest/programming-guide.html
and see a count() function, but not sure how to apply it here . I assume I have to use a 
map(lambda....).collect     syntax but not sure how to calculate the number of characters Thanks  So, turns out with all that initial parsing, we created an element in each log entry called "date_time" composed of 5 elements that looks like this:

datetime.datetime(1995, 8, 1, 0, 0, 1)
Where the third element (in this case, 1) is the day. And for this question, that's the only element we care about.

There's no where obvious in the coding template where this gets explicitly separated or reduced, so I can only imagine we are supposed to either suck it out or point to it somehow inline within the coupling of the dayToHostPairTuple.

Any ideas? It's approaching 1AM this end, and my brain is done for the night...I'm not aware that we've been taught to do that. Hi there instructors,  
I found the lab very straight forward, however, 3e, I was wondering if there was a better way then the way I did it.  I stored a couple of temporary variables to do it another way, but end up falling back to an easier way than without using reduceByKey.  I kept the temp variables to see where I was going.
I was wondering if you could review my answer to this question and send me if my way is the preferred way or is there a better way.  I would like to learn from feedback. 

thanks in advance,
Andrew
 
Hi all,

  I just submitted lab1, but I see no update in my course progress.  I know this was due yesterday, but I remember reading there was a 3 day grace period on all submitted work.  Was I too late?  Any hints?

TIA,

-fdo Hello, 
I cannot solve 3b in lab 1 .
When I write totalCount = (wordCounts                                           .map(lambda word : 1)                                            .reduce(add))
3 is returned. But It should be 5. I think the argument inside map is wrong. but I don't know what it should be.
Can anyone help me? Are there any cheaper alternatives to Databricks out there? According to Databricks FAQ, "Pricing is tiered based on usage capacity, support model, and feature-set, and will start at a few hundred dollars per month."

I would like to conduct large scale data-science experiments in Apache Spark, and from their looks of their website, it comes with very nice and easy to use GUIs. 

If I were to conduct large-scale data-science experiments on a non-commercial basis, Databricks seems rather expensive. I prefer not to spend too much time learning the intricate details on how to setup and manage AWS clusters. I understand that there is a month-long trial on that site, but I prefer to use such a site over a much longer period. Hi please help me solve quiz 6 about File compression. its killing me I have understood video correctly tried to answer it many times but not able to get right answer

Which of the following statements are true when using compression for reading and writing binary or text files:
-Reading an uncompressed file is slower than writing a compressed file.
-Some compression methods are faster than others

I see above two answer points please guide am I missing any other points Just completed Lab 2 and had some difficultly with 3c and 3e. Part 4 was quite easy in comparison.

Even though I got the right answer I'm not sure that my solutions are optional. Without trying to give too much away does anyone have insight into the following:

For 3c:
- The variable name dayToHostPairTuple seem a little confusing is it expected to hold one value or more than one value per key?
- is it a good idea to use the non performant group by call? Can this be done in another way?
- Is it a good idea to use len to calculate the size of a tupple inside a anonymous function?

For 3e:
- Is there another way of doing this without join or cojoin? It one better than the other?

Is there a way to avoid the ugly array style syntax or worse multi-dimentional array like syntax (i.e. [0][1]) inside anonymous functions?

Thanks
 What are the intended sort orders for hourRecordsSorted and errHourList?  Does the sort order change between the two?  Thx.  
.filter(lambda log : log.response_code > 200)
The counts match for all except: 
(u'/', 2208) instead of (u'/', 2199) I get shakespeareWordCount to be 882996, which fails the test, though all other tests in the whole workbook pass!

Any hints what's going on here: 

Test.assertTrue(shakespeareWordCount == 927631 or shakespeareWordCount == 928908, 'incorrect value for shakespeareWordCount') 
Would you be interested in exchanging our personal contacts and in organizing a meetup? I am getting different host count for day 8. It appears that the tests expect the count to be 4406 whereas I am getting 4404. All other day counts are as expected. Has anybody else observed this? Greetings,

I made a new REGEX and it is working fine in REGEX validator site however when I run 1c code then I'm getting below error.

I also want to mention that when I run 1b part then it runs but doesn't print any output. I was getting expected output in the morning but none now.

Error:

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-5-28279154e3f7> in <module>()
      4 APACHE_ACCESS_LOG_PATTERN = '<< I removed my REGEX from here >>'
      5 
----> 6 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-3-4ea7d1a44622> in parseLogs()
     22                    .filter(lambda s: s[1] == 0)
     23                    .map(lambda s: s[0]))
---> 24     failed_logs_count = failed_logs.count()
     25 
     26     if failed_logs_count > 0:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 23, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-1-0940d9fe8819>", line 48, in parseApacheLogLine
ValueError: invalid literal for int() with base 10: ''

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
<p><p><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansiyellow"><span class="ansicyan"><span class="ansigreen"><span class="ansiyellow"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"> </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p></p> Is this normal?

Number of invalid logline: 108
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:43:39 -0400] "GET / HTTP/1.0 " 200 7131
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:43:57 -0400] "GET /images/ksclogo-medium.gif HTTP/1.0 " 200 5866
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:07 -0400] "GET /images/NASA-logosmall.gif HTTP/1.0 " 200 786
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:11 -0400] "GET /images/MOSAIC-logosmall.gif HTTP/1.0 " 200 363
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:13 -0400] "GET /images/USA-logosmall.gif HTTP/1.0 " 200 234
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:15 -0400] "GET /images/WORLD-logosmall.gif HTTP/1.0 " 200 669
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:31 -0400] "GET /shuttle/countdown/ HTTP/1.0 " 200 4673
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:41 -0400] "GET /shuttle/missions/sts-69/count69.gif HTTP/1.0 " 200 46053
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:45:34 -0400] "GET /images/KSC-logosmall.gif HTTP/1.0 " 200 1204
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:45:46 -0400] "GET /cgi-bin/imagemap/countdown69?293,287 HTTP/1.0 " 302 85
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:45:48 -0400] "GET /htbin/cdt_main.pl HTTP/1.0 " 200 3714
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:45:52 -0400] "GET /shuttle/countdown/images/countclock.gif HTTP/1.0 " 200 13994
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:22 -0400] "GET / HTTP/1.0 " 200 7131
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:29 -0400] "GET /images/ksclogo-medium.gif HTTP/1.0 " 200 5866
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:35 -0400] "GET /images/NASA-logosmall.gif HTTP/1.0 " 200 786
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:37 -0400] "GET /images/MOSAIC-logosmall.gif HTTP/1.0 " 200 363
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:38 -0400] "GET /images/USA-logosmall.gif HTTP/1.0 " 200 234
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:40 -0400] "GET /images/WORLD-logosmall.gif HTTP/1.0 " 200 669
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:47:41 -0400] "GET /shuttle/missions/sts-70/mission-sts-70.html HTTP/1.0 " 200 20304
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:47:48 -0400] "GET /shuttle/countdown/count.html HTTP/1.0 " 200 73231
Read 1043177 lines, successfully parsed 1043069 lines, failed to parse 108 lines Hi,

I spent at least an hour on question 4d and solved it finally.

The issue was I used split() with no arguments, but doing split(' ') i.e with space as an argument solved the problem. I am wondering why is this, since python by default splits on space, right? I'm getting below error while I executed 1B part of the notebook on DBC. First to command got executed well.

ImportError Traceback (most recent call last) <ipython-input-3-5ac33294358e> in <module>() 1 import sys 2 import os ----> 3 from test_helper import Test 4 5 baseDir = os.path.join('mnt', 'spark-mooc') ImportError: No module named test_helper

How code can find test_helper module?


(EDIT: DBC == DataBricks Cloud) I am doing this in 4b: 
return re.<FILL IN>

and this in 4d:
flatMap(lambda x: x.split(" "))

Still I get the word count: 928904

Please help! Why are dataframes faster as compared to RDDs? Would we talking about the Spark Internals as well? My progress score has been deducted from the last time I checked, why is that please?  Also, the when I completed lab1, I passed all the test when working on the problems in IPython but when I submitted it, it returned these errors  

 
Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 28, in 
UnboundLocalError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
local variable 'removePunctuation' referenced before assignment

All tests passed
Apply makePlural to the base RDD(1c)
------------------------------------
All tests passed
Pass a lambda function to map (1d)
----------------------------------
All tests passed
Length of each word (1e)
------------------------
All tests passed
Pair RDDs (1f)
--------------
All tests passed
groupByKey() approach (2a)
--------------------------
All tests passed
Use groupByKey() to obtain the counts (2b)
------------------------------------------
All tests passed
Counting using reduceByKey (2c)
-------------------------------
All tests passed
All together (2d)
-----------------
All tests passed
Unique words (3a)
-----------------
All tests passed
Mean using reduce (3b)
----------------------
All tests passed
wordCount function (4a)
-----------------------
All tests passed
Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'removePunctuation' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'shakespeareWordCount' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'shakeWordCount' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'top15WordsAndCounts' is not defined
-- 12 cases passed (75.0%) --
Why is this please?

 I have answered all the quiz questions with every lecture so far. However, in the Progress tab they still appear in gray. Do I need to do something else to submit them?  EDIT: Was able to solve this. The exercise is quite correct. :)



In this part of the lab, the test expects a sorted list by day.

The only way I know how to do that is using a call to sorted() or do a takeOrdered().
However, the final line of the this lab exercise is hard coded as:
dailyHostsList = dailyHosts.take(30)
I do not see how this could return a sorted list and make the lab pass?

I can make it work by changing this line using sorted() or takeOrdered(), but since we are not supposed to change anything but the <FILL IN> parts, there must be something I am missing....

Or should the test use sorted()? Hello, I need more hints for Lab 2 3c. Spent a lot of time here, totally stuck. :(

dayToHostPairTuple : can PairTuple be something like (log.date_time.day, log.host)?
and when should I start to use groupByKey()?

Thank you! Hi everyone,

I got stucked in Lab 2 - 3c.  

In the third step it says, dayHostCount it means (day , (host, count) , (host, count) , (host,count) .

Should i call another rdd? Because in the second step i have (day , (host1, host2, host3)) . I need to convert that list to map. How can i do that?

Any ideas?

Bilal Does any getting error(traceback) in lab2 2f part for top endpoints , i am not able to figure it out as why this traceback is coming as my all previous test are passed??


Top Ten Endpoints: [(u'/images/NASA-logosmall.gif HTTP/1.0', 59657), (u'/images/KSC-logosmall.gif HTTP/1.0', 50412), (u'/images/MOSAIC-logosmall.gif HTTP/1.0', 43822), (u'/images/USA-logosmall.gif HTTP/1.0', 43595), (u'/images/WORLD-logosmall.gif HTTP/1.0', 43208), (u'/images/ksclogo-medium.gif HTTP/1.0', 41258), (u'/ksc.html HTTP/1.0', 28526), (u'/history/apollo/images/apollo-logo1.gif HTTP/1.0', 26761), (u'/images/launch-logo.gif HTTP/1.0', 24736), (u'/ HTTP/1.0', 20156)]
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
 in ()
      7 
      8 print 'Top Ten Endpoints: %s' % topEndpoints
----> 9 assert topEndpoints == [(u'/images/NASA-logosmall.gif', 59737), (u'/images/KSC-logosmall.gif', 50452), (u'/images/MOSAIC-logosmall.gif', 43890), (u'/images/USA-logosmall.gif', 43664), (u'/images/WORLD-logosmall.gif', 43277), (u'/images/ksclogo-medium.gif', 41336), (u'/ksc.html', 28582), (u'/history/apollo/images/apollo-logo1.gif', 26778), (u'/images/launch-logo.gif', 24755), (u'/', 20292)], 'incorrect Top Ten Endpoints'

AssertionError: incorrect Top Ten Endpoints Hi everybody,

I don't understand something. And I hope, your help will make it clearer for me.
A database is structured. I understand it perfectly.
But I don't see why an XML or a Log file is semi-structured : Yet, Prof Anthony Joseph has shown that there is a structure in Log file (each column has a fixed place). for XML, tags tell us the differents columns.Why an Excel file, I also need to grasp why it is semi-structured : it has columns and rows like a Database

Regards! Hi All,
 I have following code in my 3a cell:
# TODO: Replace <FILL IN> with appropriate codemywordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']mywordsRDD = sc.parallelize(mywordsList, 4)uniqueWords = mywordsRDD.distinct().count()
print uniqueWords

When I press cntr-ENTER it prints value 3.

I have following code in my TEST cell for 3a ( I did not change anything there):
# TEST Unique words (3a)Test.assertEquals(uniqueWords, 3, 'incorrect count of uniqueWords')

When I press cntr-ENTER in this I get following error:

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-22-7b1f5f2c5b8a> in <module>()
      1 # TEST Unique words (3a)
----> 2 Test.assertEquals(uniqueWords, 3, 'incorrect count of uniqueWords')

NameError: name 'Test' is not defined

Please help.

Thanks in advance

 Help !! I am getting below error in section 4cwhile running the code. My data file is at correct location aka data/cs100/lab1. No change has been made by me.
Error log:
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-39-29ca80bfa498> in <module>()
      7                   .textFile(fileName, 8)
      8                   .map(removePunctuation))
----> 9 print '\n'.join(shakespeareRDD.zipWithIndex()  
     10                 .map(lambda (l, num): '{0}: {1}'.format(num, l))
     11                 .take(15))

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in zipWithIndex(self)
   1986         starts = [0]
   1987         if self.getNumPartitions() > 1:
-> 1988             nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
   1989             for i in range(len(nums) - 1):
   1990                 starts.append(starts[-1] + nums[i])

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 118, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <lambda>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <genexpr>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "<ipython-input-34-578666e06e25>", line 20, in removePunctuation
TypeError: translate() takes exactly one argument (2 given)

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

  Are we allowed to rearrange the expressions or we must respect the lines to fill in and only one transform or action per line?

Juan Manuel 

[u'your debt but a good conscience will make any possible', u'young lucius and another with a bundle of weapons and verses writ upon them', u'you good night', u'you but indeed to pray for the queen', u'yorkshire within the forest of gaultree']What does the u mean ? Is it some problem with my instance
 anyone there plz help me
I am unable to run codes which were discussed in lecture 4.
I am unable to find where I need to write these codes.
Also where I need to write bin/pyspark as given in documentation..
I want to link python 2.7 to spark.

I am really confused please help.

Thank you I have no idea how to solve Lab 2  Part 3C and what is the best way to crack it.I tried the following ways but failed to achieve anything. 1. [ day , (host1 , value), (host2 , value)  ]2. [day , HostCountValue]Please help me out.... As I have understood
lambda k,v: (....) 
is a function that takes two arguments while
lambda (k,v): (...)
only takes one argument. But which of the two arguments in case #2 are given to the function? I have left 3e running for over 5 minutes and I haven't seen any crashes or any results.

Is it supposed to be slow?

I am trying to do two reduceByKeys and a join. In the video of "FILE PERFORMANCE" we see that reading and writing compressed data is much faster than uncompressed one.
In the video of "File Performance - Compression" we see that reading and writing raw data is the fastest and that there are compression algorithms which can get close to it.
Which argument is correct?
Maybe I missed something. Hi,
I am not able to submit as it fails with this error.
unsupported operand type(s) for +: 'int' and 'str'
all the tests were passing my local, not sure what could be the problem, any hits to fix it.

Thanks
Viswadeep shakeWordsRDD = shakespeareWordsRDD.flatMap(<split>).filter( len > 0)
shakeWordCount = shakeWordsRDD.count()
print shakeWordCount
I'm not able to figure out why its not giving me correct value.
No matter what I do I get
883253
Any help would be appreciated

 Hello,I'm struggled with how information that I need return to the variable, only guessing by the name of the variable.I think will help more if put a comment, with a simple example that is suppose to expect, like the simple example below:dayToHostPairTuple = access_logs.<FILL IN># dayToHostPairTuple = ((1, "host1"), (2, "host1"), ...)I'm still stuck in 3c (dayHostCount) and 3e (avgDailyReqPerHost) because I don't know what the result expect for the operation. Hi,

I am getting error from 1B downwards, however this was pass in iPython  browser.  Bit tring

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 338
    
    ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) -- How to interpret the output of toDebugString(), for the code form lab1 :
  def sub(value):
     return (value - 1)
  subRDD = xrangeRDD.map(sub)
  print subRDD.toDebugString()

OUTPUT:
(8) PythonRDD[22] at RDD at PythonRDD.scala:43 []
 |  My first RDD PythonRDD[4] at RDD at PythonRDD.scala:43 []
 |      CachedPartitions: 8; MemorySize: 30.7 KB; TachyonSize: 0.0 B; DiskSize: 0.0 B
 |  ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:392 []
is it topdown or bottomup trace ?What does PythonRDD.scala:# [ ] reprsent ?what is PythonRDD[#] or ParallelCollectionRDD[#].  I have prepared this exercise and the output is exactly the same that the one used in the test, but the test reports test failed.
These are the outputs and the test fails:

Anyone else has the same problem.

[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
[2582, 3222, 4190, 2502, 2537, 4106, 4406, 4317, 4523, 4346, 2864, 2650, 4454, 4214, 4340, 4385, 4168, 2550, 2560, 4134, 4456] Need a hint for this problem don't have that much time left. Tried but somehow cannot combine split and filter.  Thanks to Databricks and the MOOC, I got access to Databricks Cluster today for the course.

I ran dbc-mooc-setup before running lab0 on DBC as per the instructions.
But lab0 throws error test_helper seems not be installed.

Please could anyone let me know if I am missing something here.

Code:
from test_helper import Test
twelve = 12
Test.assertEquals(twelve, 12, 'twelve should equal 12')
Test.assertEqualsHashed(twelve, '7b52009b64fd0a2a49e6d8a939753077792b0554',
 'twelve, once hashed, should equal the hashed value of 12')

Errors:
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-4-0e7250ed2c35> in <module>()
      2 # Check our testing library/package
      3 # This should print '1 test passed.' on two lines
----> 4 from test_helper import Test
      5 
      6 twelve = 12

ImportError: No module named test_helper
 I had initially received a grade of 100% on Setup after submitting it on time. 

After the deadline, I received my free Databricks account and decided to try out the Setup on Databricks and resubmit the .py script again. While I receive the output below, I noticed that my Setup score was reduced to 80% on the Progress page.

Compare with hash (2a)
----------------------
All tests passed
Compare lists (2b)
------------------
All tests passed
-- 2 cases passed (100.0%) --

I had the impression that the highest score for each lab assignment would be considered, and scores could not be reduced.

Perhaps the course TAs could look into this issue, that may be affecting other students as well? Thank you!
 My solution to Lab1 (4b) is:

return text.strip().lower().translate(None, string.punctuation)

This uses the string package instead of the re package, but feels a lot more natural to me than using regex for such simple matters. Unfortunately, (4c) fails with this solution:

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-64-2e6e4ca8764d> in <module>()
      9                   .textFile(fileName, 8)
     10                   .map(removePunctuation))
---> 11 print '\n'.join(shakespeareRDD
     12                 .zipWithIndex()  # to (line, lineNum)
     13                 .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in zipWithIndex(self)
   1986         starts = [0]
   1987         if self.getNumPartitions() > 1:
-> 1988             nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
   1989             for i in range(len(nums) - 1):
   1990                 starts.append(starts[-1] + nums[i])

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 59.0 failed 1 times, most recent failure: Lost task 0.0 in stage 59.0 (TID 169, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <lambda>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <genexpr>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "<ipython-input-63-a8c2460359c5>", line 19, in removePunctuation
TypeError: translate() takes exactly one argument (2 given)

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

How can this be? I was already bothered that we had to set up this entire stack of pre-configured solutions, which is buggy for a lot of people, but now it won't even process my code if I don't use it exactly the way you dictate?

Why can't I even use default Python packages? 
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'wordCounts' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'wordCountsCollected' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueWords' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'average' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'wordCount' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'removePunctuation' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'shakespeareWordCount' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'shakeWordCount' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'top15WordsAndCounts' is not defined

 
i am a little confused that i have passed test all of them
but when i check my file, i got these problems....
are these solutions like 'Test' defined?

 Hello

I recently joined in an analytic company and I see spark helping the company in a multifold way. we do have a small server room hosting servers for our data.(which will grow in future) I want to create a local spark cloud so that I can compute it locally. Can anybody help me setting up a spark executors and whole setup. Or any documentation on how to setup the spark local cloud. It looks very interesting . I'm totally confused about the format of each RDD. can some one describe the format of each RDD to get the result.
For Eg:
1. dayToHostPairTuple should be is like [(day, host).....] or [(day, (host, value)), (day, (host, value)), ......].
2. dayGroupedHosts. ??

Thanks in advance Hi,
I solved the problem 3C and the others but in my notebook it passes all the tests. But when i upload the grader it gives the error :

Number of unique daily hosts (3c)---------------------------------Traceback (most recent call last):  File "", line 1, in   File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals    cls.assertTrue(var == val, msg)  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue    raise TestFailure(msg)TestFailure: incorrect dailyHostsList

My output is :

Unique hosts per day: [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)]
What is the problem? Do you face the same problem? Hello, 

I finished LAB 2 (everything is correct). I used only map/reduce/filter/groupby... function instead of "SQL" (functions {right, left, ""}x{join}).  And I am not sure If I did it correctly, because I didn't use SQL functions...


Thank you.




 NameError                                 Traceback (most recent call last)
<ipython-input-2-b782bfb79fad> in <module>()
      1 # TEST Capitalization and punctuation (4b)
----> 2 Test.assertEquals(removePunctuation(" The Elephant's 4 cats. "),
      3                   'the elephants 4 cats',
      4                   'incorrect definition for removePunctuation function')

NameError: name 'Test' is not defined is it possible to have triples rather than the (x, y) doubles we see all the time?

How would distinct() count() and reduceByKey work with them?
if I have (x1, x2, x3) is Key be default x1? can I recast it to be combos of x1 and x2? need hint of 2b lab 1 struced since 2 days.
just need logic Lab 2 (1b & 1c) seem to take forever and I was getting an out of memory error in the home dir of notebook.
I finally realized that a [*] meant that the script was running and I should wait till I get results before moving on. 
If I increase the memory assignment in Virtualbox for the sparkvm Instance will it stay or does Vagrant change it back and will this help?
gonna try it now and see. I have 32gb on this windows station no since not using some of it. Has anyone tried to print or PDF output the lab ipython notebooks? I get different fonts and the text boxes don't show up well (using Chrome), if I try to print the doc. 

Save as "PDF" fails because "Latex" is missing.

I installed all the missing plugins for latex only only to get this error: 

$ ipython nbconvert lab2_apache_log_student.ipynb --to latex --post PDFNBFormatError: Unsupported nbformat version 4
 Sorry for the stupid question, but I am not able to find anywhere the link to submit the code to the autograder in lab2. I am using chrome.

Thanks Hi All,

While I am working on the problem, it's showing syntax error. Can anyone help?

<EDIT>

 File "<ipython-input-58-91b0be9dcca4>", line 6
    average = totalCount / float(uniqueWords)
          ^
SyntaxError: invalid syntax

  As titled - the test passes locally, but not when I upload the .py file.
All other tests are passed.

The solution is quite straightforward, I'm positive I'm not doing anything wrong.
Also, the subsequent test (2d, where I use the same approach) is passed.

The only thing in the "test output" when run in the notebook is that the keys are sorted - cat, elephant, rat - and in my output they are not.
This shouldn't be an error since there is no mention on key sorting in the assignment (and pairs are sorted any way in the assertion).

Any idea on how to get my 100%?

Thanks, 
Damir Anyone else get this comment from the Autocoder:

Counting 404 (4a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect badRecords.is_cached

 

Passed the tests in the assignment itself, so my guess is I am leaving too much in the RDD...but I wonder what it wants.  No worries, I can live with 93% for this assignment. :-)  It has taken me a good half a day as I got stuck on one question.  Naturally banged my head on it too long, and when I went away to relax the answer popped into mind.  Always the way, isn't it? ;-) I made several attempts but in vain. I am getting either syntax error or a message saying "reduceByKey takes 2 argumets and returns 1. i is given". Not able to proceed. Any hint/help is appreciated.

I was running the following statement --

wordCounts = wordPairs.reduceByKey(map(lambda word: count(word))))

Regards,
Venkat Hi,
Is there any detailed doc on Spark Architecture ? When I use pyspark on the VM it starts another Spark instance. What I need to do is interact with current running Spark session using  a shell . How can I do that ?

Basically if we have cluster running how do we connect ? . Current Tutorials directly connect to the shell . I have to run the python scripts from a different client how to specify the cluster details ? Also since Spark uses Master/Slaves how does it address single point of failure ?

I need to start setting up a cluster and try all these.. But if any has some handy materials it will be helpful.

thanks
  Sorry for late work.

when I am trying to use "flatmap" by
"shakeWordsRDD = shakespeareWordsRDD.flatmap(lambda str:str.split())" i

but the error comes as 
AttributeError: 'PipelinedRDD' object has no attribute 'flatmap'

I feel really confusing. Can anyone help?  DBs are well organized into rows and columns - they have indices supporting high performance - why would it be slow? I want to work on a different machine when I am home.  When I go to revisit the setup instructions in the class, all I get is the autograder telling me I already set it up successfully.  How can I see the original instructions? Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 186
    wordCountsGrouped = wordsGrouped.
                                     ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) -- Hi Guys, I've used the former and god a "failure" flag,
I think the test script should be able to handle both options without lossing points. Have to admit I'm a bit puzzled by Lab 1: 4D as I was under the impression that strip function's default separator, being white space, will automatically take out blank lines so I got 882996 straight.    As such I'll be curious to see the answer that produce 927631 or 928908 word count. I have reinstalled everything . But i keep getting this error
Bringing machine 'sparkvm' up with 'virtualbox' provider...
==> sparkvm: Checking if box 'sparkmooc/base2' is up to date...
==> sparkvm: Clearing any previously set forwarded ports...
==> sparkvm: Clearing any previously set network interfaces...
==> sparkvm: Preparing network interfaces based on configuration...
    sparkvm: Adapter 1: nat
==> sparkvm: Forwarding ports...
    sparkvm: 8001 => 8001 (adapter 1)
    sparkvm: 4040 => 4040 (adapter 1)
    sparkvm: 22 => 2222 (adapter 1)
==> sparkvm: Booting VM...
==> sparkvm: Waiting for machine to boot. This may take a few minutes...
The guest machine entered an invalid state while waiting for it
to boot. Valid states are 'starting, running'. The machine is in the
'poweroff' state. Please verify everything is configured
properly and try again.

If the provider you're using has a GUI that comes with it,
it is often helpful to open that and watch the machine, since the
GUI often has more helpful error messages than Vagrant can retrieve.
For example, if you're using VirtualBox, run `vagrant up` while the
VirtualBox GUI is open.
Please help , I am eager to get started with the course ,
Thanks and regards
ps: i have tried redownloading , virtualization enable , HELP ASAP Hi how do we debug and print each program line content/output? Please guide thanks in advance. Following the advice to try and avoid groupByKey() when possible because of the potential problems, I managed to get Q3 to work without it, but only by using quite a complicated aggregateByKey() expression.

I don't want any specific code, but I was just wondering if anyone has a simple solution to this?  I'm wondering whether it's worth spending a lot of time trying to work one out :-).  Essentially, what I need is something like a distinctByKey() method, but I couldn't find a built-in solution for that.

Edit: specifically 3(c) and 3(e).  distinct() isn't any good (in a straightforward way, anyway) because it's distinct per day.

Thanks. As far as I understand, the variable "badUniqueEndpoints" is suppoused to be a List, but however it is requested to use the function take(40) later that is only supported by  a RDD.

what is the expected type of "badUniqueEndpoints"  and "badUniqueEndpointsPick40" I need some help in solving problems .. 

now i am able to access " http://localhost:8001/tree/" , Where i need to make code changes.

P I think the correct slide posted for RightOuterJoin still needs one more modification in the step for assigning x and y. Providing the updated version below:

#RightOuterJoin
x = sc.parallelize([("a", 1)])y = sc.parallelize([("a", 2), ("b", 4)])sorted(x.rightOuterJoin(y).collect())

Output: 
[('a', (1, 2)), ('b', (None, 4))] i am stuck in lab 1 4(d)... pls give some hint...... It would be awesome if there was a youtube playlist released for each lecture instead of having to click through each video. I'm at a bit of a loss. Is there any editing that we need to do on the first three groupings of code? I run 1b and this is the error that I get. Any help is appreciated.

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-17-8d8d6629d991> in <module>()
     32 
     33 
---> 34 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-17-8d8d6629d991> in parseLogs()
     22                    .filter(lambda s: s[1] == 0)
     23                    .map(lambda s: s[0]))
---> 24     failed_logs_count = failed_logs.count()
     25     if failed_logs_count > 0:
     26         print 'Number of invalid logline: %d' % failed_logs.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/vagrant/data/cs100/lab2/data/cs100/lab2/apache.access.log.PROJECT
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:203)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:57)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:57)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1512)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:813)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:374)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745) Hi All,
My codes pass all previous tests, but when it comes to the final one (4f), my answer is as follows, slightly different from the correct one. Do you guys encounter such situation? Thanks all in advance.

the: 27359
and: 26028
i: 20678
to: 19150
of: 17461
a: 14590
you: 13614
my: 12477
in: 10954
that: 10890
is: 9132
not: 8495
with: 7769
me: 7766
it: 7676 Hello  I hace t He followin testrdd = [ u' 2345, u ' ', u'how los are yo u, u' I am a bug Kong, u' '] and I World Lake yo know how to split t He spaces and Bank lunes using python.

I have tried t He f ollowing : 
Allwdrdd = nwrdd.map ( lambda x:x.split (" "))
Butt I did not have an y result.
An y idea ????
Hanks
Gorka
 Hi,

Can I able to submit the notes for grading even if I can't able to complete all the questions.

I mean,  I am not able to find the answer for the question 4d and 4f . but before that I have completed all and which is passed .

Shall I able to get any points in this scenario?

Thanks I am getting:
Pluralize and test (1b)-----------------------Traceback (most recent call last):  File "", line 15, in   File "/ok/submission.py", line 296        ^SyntaxError: invalid syntax
Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not defined
Apply makePlural to the base RDD(1c)------------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not defined

This is after using Shift&Enter through each block of the workbook I completed.  All internal tests were passed for the blocks I completed.  I saved the workbook as a .py file and loaded it into the autograder.  No I did not import any libraries and yes I uploaded a new copy and cut&pasted the answers into it.  I burned through more than half my submissions.  At this point I am just trying to get partial credit for what I completed.
 
  I am running 3a step by step and wrote the following, but getting syntax error

not200 = access_logs.map (lambda logs : (logs.response_code,logs.endpoint) if logs.response_code != '200')print not200.first(). Please help

  File "<ipython-input-37-13ea57438627>", line 6
    not200 = access_logs.map (lambda logs : (logs.response_code,logs.endpoint) if logs.response_code != '200')
                                                                                                             ^
SyntaxError: invalid syntax
 This is my code :
# TODO: Replace <FILL IN> with appropriate code
from operator import add
totalCount = (wordCounts
              .map(lambda s: (s,1))
              .reduce(add))
average = totalCount / float(uniqueWords)
print totalCount
print round(average, 2)
I am getting a Type Error :
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-59-480fb6770379> in <module>()
      4               .map(lambda s: (s,1))
      5               .reduce(add))
----> 6 average = totalCount / float(uniqueWords)
      7 print totalCount
      8 print round(average, 2)

TypeError: unsupported operand type(s) for /: 'tuple' and 'float'

Please help & thanks in advance. I'm trying to execute the cell, but when I do so, it jumps to next cells and does weird things with the following cells, not sure what I am doing wrong? I don't want to delete the file and re-upload it, is there any other solution to this or I have to delete the file - please ignore put this question under wrong section dayHostCount = dayGroupedHosts.map(lambda (a,b):(a,b.count()))
It gives a completely jibberish error(to me).
dayGroupedHosts is in the form (day, list of hosts), so why doesn't the above code just give me a touple with the list of days and the number of hosts for each day?  

full error...  I use .count() to execute the transform, and comment out the incomplete lines in the block

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-59-24622bf255e6> in <module>()
      6 
      7 dayHostCount = dayGroupedHosts.map(lambda (a,b):(a,b.count()))
----> 8 print dayHostCount.count()
      9 """
     10 dailyHosts = dayGroupedHosts.map(lambda (a,b):(a,b.count()))

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 342, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <genexpr>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "<ipython-input-59-24622bf255e6>", line 7, in <lambda>
AttributeError: 'ResultIterable' object has no attribute 'count'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) Hallo:

I´m trying to review the lab2_apache_log_student file but the autograder always reports errors and 0 cases pases while I have passed all the cases except one, it should be 95% passed.

Is there any problem with the autograder.? I have spent 5 attempts to upload the file.

Loreto In lecture 6, the relational database video, we learn that: 
1) a schema gives columns and types 
2) the cardinality is the number of rows and the degree is the number of fields.
3) the fields are defined in the columns by a schema

But we don't have a definition of a field.  What's the difference between a field and a column?  Is there one?

To make this more concrete: at around 45 seconds in the video, we get an example of a student database that has a column "sid" which is a string; another column "name," also a string, etc.  So those are the columns.  And presumably there are N students, where each row represents a single students, so the cardinality would equal N.  But how would we determine the degree?  Would we just count up the columns?  (So in the example, columns are sid, name, email, age, gpa; there are five columns, therefore five fields, therefore degree of five?  Or can columns and fields mean something different?)

thanks!

[edit: I see two videos on expands this example some, and we learn that degree is = the number of columns...] Sorry, I posted this on a different section.
I'm trying to execute the cell, but when I do so, it jumps to next cells and does weird things with the following cells, not sure what I am doing wrong? I have tried to deleting the file and re-uploading it, destroy the vagrant and re-run it but to no-avail, I am kind of stuck. Now its happening for all the cells. Have already tried ctrl + enter, or the play button.

Currently,
1) Fresh VM
2) I have new lab file uploaded


Screenshot when I pressed execute for the first shell:
 Only just managed to get my edx course verified.  I mistakenly thought I had to wait for verification until I could begin the course.  So I'm down 20% on my score for a late submission of 'download the vagrant image; vagrant up'.  Any chance of a correction to scoring?

Though, arguably you could say I deserve a 20% reduction for failure to read instructions, lol. I am using this code

shakeWordsRDD = shakespeareWordsRDD.flatMap(split ('')).filter( len > 0)

error : name split not defined Assert error message:
 
Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals cls.assertTrue(var == val, msg) File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue raise TestFailure(msg) TestFailure: incorrect dailyHostsList 
But in ipython notebook everything is good:
# TEST Number of unique daily hosts (3c)
Test.assertEquals(dailyHosts.count(), 21, 'incorrect dailyHosts.count()')
Test.assertEquals(dailyHostsList, [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)], 'incorrect dailyHostsList')
Test.assertTrue(dailyHosts.is_cached, 'incorrect dailyHosts.is_cached')
 
<strong>1 test passed.</strong>
<strong>1 test passed.</strong>
<strong>1 test passed.</strong>
My dailyHostsList is just taking 30 elements from dailyHosts.
I do not know where I have a problem, anybody faced with similar issue? Hi,

I do not understand item 2e in lab 2.

How are the x-axis values populated? I do not see ends used except to format the x-axis in

plt.axis([0, len(ends), 0, max(counts)])
 In 2b, the following statement is provided:
responseCodeToCountList = responseCodeToCount.take(100)

I think that there is an assumption here that the number of distinct codes will be <100 in the result. However, we may not be able to make this assumption in an unknown dataset.

So, I feel that the below may be a better way to get the number of distinct codes without hard-coding it.

responseCodeCount = responseCodeToCount.count()responseCodeToCountList = responseCodeToCount.take(responseCodeCount)

Is this the right way to think and implement this? 
Thanks. print removePunctuation(' No under_score!  skakjs askajs    akskjas j')
"no underscore skakjs askajs akskjas j"

is this output show the right behaviour?

I still have problems to get the right number for "the"

the: 27355

instead of 

 u'the', 27361

something wrong with the removePunctuation I guess

do you have an idea? I don't know how to get a number using reduce for totalcounts.
Can anyone help me conceptualize this. Obviously my understanding is flawed here but I have not had luck finding information or examples.
from operator import addtotalCount = (wordCounts .map( lambda a: a,1) .reduce(add) .count())average = totalCount / float( uniqueWords)print totalCountprint round(average, 2) I have submitted lab0 ahead of time, getting 100% (lab setup).
Everything was ok till today evening when I performed 1st submission of lab1.
Now system reports 40% for lab0 and significant reduction of total progress %.
I am using eDx for >1y and 1st time I have experienced such issue.
Pls, I don't know, repair?
andrzej
 Hell all,

I have my .py file ready and it passed tests in VM, however, when I submitted it and check it always shows all tests have failed !

any help ?


thanks How to apply sum function on the iteratable. 

the groupByKey() returns K, Iteratable and I want to sum the iteratable values.
 
I have passed all the previous tests, but I have no idea whats going on with my 4f results. Anyone has any idea? Thank you! 
Hi Instructors,

I completed lab2, and all the tests pass.  I saved the file and uploaded to autograder (twice), but when I run "check" I get many errors I don't get when running notebook.  

For example, the first error claims "daysWithHosts is not defined" (see below).  Yet I can see in my notebook that it is defined and the test that uses it passed:

#...
daysWithHosts =  <remove code>
#...
Test.assertEquals(daysWithHosts, test_days, 'incorrect days')
#...
1 test passed.
#...

any hints?  Here is the  log from autograder check, up to the first error.

Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 28, in 
NameError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
global name 'dailyHost' is not defined

All tests passed
Top ten error endpoints (3a)
----------------------------
All tests passed
Number of unique hosts (3b)
---------------------------
All tests passed
Number of unique daily hosts (3c)
---------------------------------
All tests passed
Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithHosts' is not defined
...

TIA for any help Hello,

What i'm understand is need to clean data from parseLogs() and the out put should be match:

"127.0.0.1 - - [01/Aug/1995:00:00:01 -0400] "GET /images/launch-logo.gif HTTP/1.0" 200 1839"

Here is my output:


Number of invalid logline: 1043177
Invalid logline: in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] "GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0" 200 1839
Invalid logline: uplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] "GET / HTTP/1.0" 304 0
Invalid logline: uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] "GET /images/ksclogo-medium.gif HTTP/1.0" 304 0
Invalid logline: uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] "GET /images/MOSAIC-logosmall.gif HTTP/1.0" 304 0
Invalid logline: uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] "GET /images/USA-logosmall.gif HTTP/1.0" 304 0
Invalid logline: ix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:09 -0400] "GET /images/launch-logo.gif HTTP/1.0" 200 1713
Invalid logline: uplherc.upl.com - - [01/Aug/1995:00:00:10 -0400] "GET /images/WORLD-logosmall.gif HTTP/1.0" 304 0
Invalid logline: slppp6.intermind.net - - [01/Aug/1995:00:00:10 -0400] "GET /history/skylab/skylab.html HTTP/1.0" 200 1687
Invalid logline: piweba4y.prodigy.com - - [01/Aug/1995:00:00:10 -0400] "GET /images/launchmedium.gif HTTP/1.0" 200 11853
Invalid logline: slppp6.intermind.net - - [01/Aug/1995:00:00:11 -0400] "GET /history/skylab/skylab-small.gif HTTP/1.0" 200 9202
Invalid logline: slppp6.intermind.net - - [01/Aug/1995:00:00:12 -0400] "GET /images/ksclogosmall.gif HTTP/1.0" 200 3635
Invalid logline: ix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:12 -0400] "GET /history/apollo/images/apollo-logo1.gif HTTP/1.0" 200 1173
Invalid logline: slppp6.intermind.net - - [01/Aug/1995:00:00:13 -0400] "GET /history/apollo/images/apollo-logo.gif HTTP/1.0" 200 3047
Invalid logline: uplherc.upl.com - - [01/Aug/1995:00:00:14 -0400] "GET /images/NASA-logosmall.gif HTTP/1.0" 304 0
Invalid logline: 133.43.96.45 - - [01/Aug/1995:00:00:16 -0400] "GET /shuttle/missions/sts-69/mission-sts-69.html HTTP/1.0" 200 10566
Invalid logline: kgtyk4.kj.yamagata-u.ac.jp - - [01/Aug/1995:00:00:17 -0400] "GET / HTTP/1.0" 200 7280
Invalid logline: kgtyk4.kj.yamagata-u.ac.jp - - [01/Aug/1995:00:00:18 -0400] "GET /images/ksclogo-medium.gif HTTP/1.0" 200 5866
Invalid logline: d0ucr6.fnal.gov - - [01/Aug/1995:00:00:19 -0400] "GET /history/apollo/apollo-16/apollo-16.html HTTP/1.0" 200 2743
Invalid logline: ix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:19 -0400] "GET /shuttle/resources/orbiters/discovery.html HTTP/1.0" 200 6849
Invalid logline: d0ucr6.fnal.gov - - [01/Aug/1995:00:00:20 -0400] "GET /history/apollo/apollo-16/apollo-16-patch-small.gif HTTP/1.0" 200 14897
Read 1043177 lines, successfully parsed 0 lines, failed to parse 1043177 lines 
1 test failed. incorrect failed_logs.count()
1 test passed.
1 test failed. incorrect access_logs.count()


Any help will be great. Friends, I thoroughly enjoyed doing Lab 1, and just finished watching all the lectures related to Lab2. For those of you are attempting or finishing up Lab2, In terms of "degree of difficulty", how does Lab2 compare with Lab 1? Just asking so I know how much time to set aside and don't miss the deadline.

Cheers
Ram I thought all the files are stored as 0 and 1 in the computer and it doesn't make a difference. Without breaking any rules I am curious how people approached 3e.  My code passes the grader but feels inelegant, requiring a double sort.  Also it feels like sortedByDay is not needed which makes me question if there is a better way.

Thanks i couldn't download my file as py ! i just downloaded as text then converted to py and uploaded to auto-grader , but all test fails and i gain 0 :( however i made many examples successfully , but didn't complete the whole lab Hi,

I did not manage to successfully complete all the Lab 1 assignments (had issues with regular expressions).
All of the test up to problem 4(b) have completed successfully and passed all the built0in tests. 
however, when I uploaded the .py file, nothing passed, not even the built-in tests and the first questions.
This is very frustrating. I have followed the instructions in the submission page. I did not import new libraries (except re and operator, which were imported by the original code in the notebook).
I removed print statements and empty cells.
Please help me with this problem since I do not want to loose all the credit for this assignment (even though I know I will not get full credit since the reg-expression part (removing punctuation) is still not working.
I am making every effort to learn how to do the missing steps, even after the deadline and grace period.

Thank you
Rona 


---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-32-178b87988252> in <module>()
     10               .cache())
     11 
---> 12 dailyHostsList = dailyHosts.take(30)
     13 print 'Unique hosts per day: %s' % dailyHostsList

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 92.0 failed 1 times, most recent failure: Lost task 0.0 in stage 92.0 (TID 299, localhost): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:172)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:108)
	... 14 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span>
</div> Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 297
    
    ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

Can somebody help me with this? Hi there,

I've been trying to get Vagrant to work. I'm pasting my error message below.

It is obvious that I won't make it to the new deadline (due in 20 minutes)

Any chances to extend further this deadline? I'll try to install in another machine...

Thanks
----

C02MX0VUFH04:myvagrant larry$ vagrant up
Bringing machine 'sparkvm' up with 'virtualbox' provider...
==> sparkvm: Checking if box 'sparkmooc/base' is up to date...
/opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/environment.rb:492:in `initialize': Permission denied - /Users/larry/.vagrant.d/data/lock.fpcollision.lock (Errno::EACCES)
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/environment.rb:492:in `open'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/environment.rb:492:in `block in lock'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/environment.rb:516:in `lock'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/environment.rb:491:in `lock'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/handle_forwarded_port_collisions.rb:41:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/prepare_forwarded_port_collision_params.rb:30:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/env_set.rb:19:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/provision.rb:80:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/clear_forwarded_ports.rb:15:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/set_name.rb:50:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/clean_machine_folder.rb:17:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/check_accessible.rb:18:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:95:in `block in finalize_action'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builder.rb:116:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `block in run'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/busy.rb:19:in `busy'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `run'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/call.rb:53:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:95:in `block in finalize_action'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builder.rb:116:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `block in run'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/busy.rb:19:in `busy'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `run'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/call.rb:53:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:95:in `block in finalize_action'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builder.rb:116:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `block in run'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/busy.rb:19:in `busy'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `run'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/call.rb:53:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/box_check_outdated.rb:68:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/config_validate.rb:25:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/check_virtualbox.rb:17:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:95:in `block in finalize_action'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:95:in `block in finalize_action'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builder.rb:116:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `block in run'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/busy.rb:19:in `busy'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `run'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/call.rb:53:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/config_validate.rb:25:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:95:in `block in finalize_action'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:95:in `block in finalize_action'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builder.rb:116:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `block in run'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/busy.rb:19:in `busy'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `run'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/call.rb:53:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/check_virtualbox.rb:17:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builder.rb:116:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `block in run'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/busy.rb:19:in `busy'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `run'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/machine.rb:214:in `action_raw'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/machine.rb:191:in `block in action'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/environment.rb:516:in `lock'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/machine.rb:178:in `call'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/machine.rb:178:in `action'
 from /opt/vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/batch_action.rb:82:in `block (2 levels) in run'


 I completed my lab 2 except problem 3 and then I thought I would give it a try to see how much points I can get. 

But three times I uploaded. All I get is 0 points. My tests for to other three problems were passed in my ipython. Does it mean that if I don't finish problem 3. I can never get any point?

The message from autograder:
Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 401
    dayToHostPairTuple = access_logs.
                                     ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Top ten error endpoints (3a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Number of unique hosts (3b)
---------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting 404 (4a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Listing 404 records (4b)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badUniqueEndpointsPick40' is not defined

Top twenty 404 URLs (4c)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Top twenty-five 404 response code hosts (4d)
--------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

404 response codes per day (4e)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Five dates for 404 requests (4g)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Hourly 404 response codes (4h)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

  I did one submission of the week zero lab on time, but today I made the mistake to resubmit it to test the autograder because it was not passing any of the tests for lab 1 that had passed in the notebook, and the progress chart was showing my grade at 20% less. Could this please be reverted? I haven't yet figured the problem with my lab1 code and the autograder, very frustrating... 3c is working for me, but I had to use the not so optimum solution, ha anybody used reducebykey to do this? if yes, did you form pair of ((day,host), count) as a tuple?  3c is painful.

I have dayToHostPairTuple set as (1,(host1,1)), ....
Then dayGroupedHosts I grouped the hosts, the result is like (1,(host1,4) .....

Now I stuck at dayHostCount. How to get distinct hosts per day? Am I on the right track?

Thanks! Lab was partially complete, with local unit tests passing ...

NameError: name 'Test' is not defined
Yet no partial points were awarded.  I attempted multiple uploads ... removing and commenting out out syntax error of problem in subsequent sections - to no avail ...

Is there any (surefire) way to submit files to insure we get partial credit on labs when we don't have complete solution?  As it stands - I still have a zero on lab 1 :-(

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 294
    
    ^
SyntaxError: invalid syntax

 Even the most most basic transformations+action at 4f is throwing the following error:

Can someone help?

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-36-d01d6027a86e> in <module>()
      4 # print '\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15WordsAndCounts))
      5 
----> 6 print shakeWordsRDD.map(wordCount).top(10)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in top(self, num, key)
   1156             return heapq.nlargest(num, a + b, key=key)
   1157 
-> 1158         return self.mapPartitions(topIterator).reduce(merge)
   1159 
   1160     def takeOrdered(self, num, key=None):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 138, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 734, in func
    initial = next(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1153, in topIterator
    yield heapq.nlargest(num, iterator, key=key)
  File "/usr/lib/python2.7/heapq.py", line 464, in nlargest
    result = _nlargest(n, it)
  File "<ipython-input-26-1c02406389ab>", line 11, in wordCount
AttributeError: 'unicode' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

A screenshot from http://localhost:4040/stages/



 Hello,

Does it take lot of time for the first time to run the ipython notebook for the first time until it stores the log file in the cache memory?

My ipython is still taking taking more then 15 min and is keep running. Please suggest

Thanks
 Hi, I'm a bit confused. I did the map with going from a pair to a single value and I did the reduce. At the line where average is defined, I get a syntax error. I'm using float (UniqueWords) in the denominator. Is this the problems? Thanks, Vik In Lab 2, section 1(d), configuring initial RDD systems, there is a piece of code that's puzzling. 
parsed_logs = (sc
                   .textFile(logFile)
                   .map(parseApacheLogLine)
                   .cache())
Why is the call to spark context bounded in parenthesis? 
Subsequent calls also bound the RDDs, vide, 
access_logs = (parsed_logs.filter(lambda s: s[1] == 1).map(lambda s: s[0]).cache())
failed_logs = (parsed_logs.filter(lambda s: s[1] == 0).map(lambda s: s[0]))
Thanks for insights. 

 I got this error while submitting the job. Also, I don't see my jobs and stages on the website( 4040 portal). Can you advise what's wrong? Seems I need to do some clean up. Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 69.0 failed 1 times, most recent failure: Lost task 0.0 in stage 69.0 (TID 186, localhost): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed) I believe that where the slide on fullOuterJoin says:

For each element (k, v) in Y, resulting RDD…

that it should say (k, w) rather than (k, v).
 I was able to set up Virtual box and uploaded first python file . Second day , I wasn't able to start vagrant by using vagrant up . But it did work after running command "vagrant box remove sparkmooc/base" . But today , again I couldn't start vagrant . I tried all work arounds posted in this forum . It's not working anymore . Please help need to test lab 1

when I run vagrant up I see this ...

C:\edx>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Clearing any previously set forwarded ports...==> sparkvm: Clearing any previously set network interfaces...==> sparkvm: Preparing network interfaces based on configuration...    sparkvm: Adapter 1: nat==> sparkvm: Forwarding ports...    sparkvm: 8001 => 8001 (adapter 1)    sparkvm: 4040 => 4040 (adapter 1)    sparkvm: 22 => 2222 (adapter 1)==> sparkvm: Booting VM...==> sparkvm: Waiting for machine to boot. This may take a few minutes...The guest machine entered an invalid state while waiting for itto boot. Valid states are 'starting, running'. The machine is in the'poweroff' state. Please verify everything is configuredproperly and try again.
If the provider you're using has a GUI that comes with it,it is often helpful to open that and watch the machine, since theGUI often has more helpful error messages than Vagrant can retrieve.For example, if you're using VirtualBox, run `vagrant up` while theVirtualBox GUI is open. The exercise for 3a asks for "determine which endspoints did not have a 200 return code".

The default name for the displayed RDD is topTenErrURLs.  

Edit: The question is asking for endpoint but the variable is called URL Hi All,
 I thought deadline for lab submission is Friday midnight i.e. Friday 23:59:59 and
and the 3 day grace period ends at Monday midnight i.e. Monday 23:59:59

I just submitted my lab1 successfully little after 2330 New York time ( Eastern daylight saving time), but my 20 points were cut due to submission after grace period.

Can someone explain me please what are submission deadline and 3-day grace period deadline?

Thanks I submitted Lab 1 to the correct auto grader within the 3 day grace period but the auto grader took 20% off saying it was late. Lab 1 was due 6/12/15 at 8:00PM ET and I submitted 6/14/15 at 9:15PM ET. 

Pluralize and test (1b)-----------------------All tests passedApply makePlural to the base RDD(1c)------------------------------------All tests passedPass a lambda function to map (1d)----------------------------------All tests passedLength of each word (1e)------------------------All tests passedPair RDDs (1f)--------------All tests passedgroupByKey() approach (2a)--------------------------All tests passedUse groupByKey() to obtain the counts (2b)------------------------------------------All tests passedCounting using reduceByKey (2c)-------------------------------All tests passedAll together (2d)-----------------All tests passedUnique words (3a)-----------------All tests passedMean using reduce (3b)----------------------All tests passedwordCount function (4a)-----------------------All tests passedCapitalization and punctuation (4b)-----------------------------------All tests passedWords from lines (4d)---------------------All tests passedRemove empty elements (4e)--------------------------All tests passedCount the words (4f)--------------------All tests passed-- 16 cases passed (100.0%) --As for late submissions after 3 day grace period of the due date, you lose 20 points. Your final score is 80 Hi I submitted lab 1 on 14th Jun which was due on 12 Jun. I was graded on 80 % but I shoud be awarded 80 %. Can someone check Hi All,

        I am struck at 4e. getting 927631 value for 4e as well like 4d. Both tests passed in 4d. used split(' ') in 4d and got it correct. Please advise at the earliest.

shakespeareRDD.<FILL IN>---used for 4d

shakespeareWordsRDD.<FILL IN>--used for 4e.

return re.sub(<FILL IN>)-----------used the following for 4b
PLEASE DO NOT POST SOLUTIONS CODE

do I need to modify 4b to fix this issue. Please clarify


Thanks, Hi All,

Has the grace period for submitting lab1 crossed ?  Please clarify.



Thanks, Dear Edx / Professor,

I live in Boston and I submitted 3 days from due date - Friday.

Can you please fix it?

Thanks! In the Lecture 1 and Lecture 2, data engineering (for example ETL, data cleaning) is depicted as part of data science. In my opinion, data engineering and data science are two different subject areas. This course is heavily focused on data engineering not on data science. So when doing (3c)--finding unique counts of hosts broken out by day--I got the answer right (well, it passed all the checks...), but my answer relied more on Pythonic solutions--namely, using sets to get unique elements from a pyspark.ResultIterable.  But this seems like cheating to me, because no doubt a single-machine set wouldn't be feasible on truly big data, and I'm just wracking my brain to figure out a solution that scales and uses only RDD transformations.  Has anyone been able to do this and if so, could you give me a hint?  

My intermediate RDD looks like this:


dayToHostPairTuple = access_logs.map(lambda log: (log.date_time.day, log.host))

dayGroupedHosts = dayToHostPairTuple.groupByKey()

# dayHostCount = ???
Thanks! I think I'm going very much out of track, please can somebody explain what to do with 4e &4f. Your help will be appreciated. I needed some advice. This doesn't concern the labs directly. I wish to go through a log and filter out all that's relevant to me and then save the output to some other file. Right now what I am doing is using pyspark's filter method to get the relevant lines and then I wish to do some processing with it before I save it to a file. Should I use the map method to do this or something else. And also for saving should I again use spark's saveAs... method or python's inbuilt file management system( output.write('blah')) .The logfile is 12-13TB in size
Thanks :) Dear Staff,I am taking another class which also asks me to use vagrant to set up a space.I ran into the below problem:

“To fix this, modify your current projects Vagrantfile to use anotherport. Example, where '1234' would be replaced by a unique host port:
config.vm.network :forwarded_port, guest: 8000, host: 1234”

I would like to know1. Can I temperately shout down the sparkvm? if so, do I need to return back to the sparkvm directory? where will that be2. Can I spin up the sparkvm again when I am ready to return? How do that do that?

Thanks! I am trying to implement the GroupBy and map functions.  What is this, and how do I add the values in this object?  Is this what I'm suppose to do?  Is there a python document that will show me how to implement lambda functions on the value (i.e. a list) in the key value pair?  <-am I asking the right question? Thanks

<pyspark.resultiterable.ResultIterable object at 0xb0f13b6c> https://spark-summit.org/2015/Just in case anyone like to watch live streaming. Enjoy M.  Not sure if others are facing this problem. 

I have little time on the weekdays to work on class. Most of time is on weekends. As a result , a Friday deadline for submission is not optimal. 

Is their a way we can make this to be Sunday nights.

 Why so much emphases on regex exercises in spark class ?    I use R mostly for machine learning.  How would I use Spark to scale the algorithms I'm using in R (eg. Logistic regression, cart, random forest, etc.) to train and run models much quicker? Yes, increasing the CPUs slows things down.  There are a large number of old historical references that tell a story of the same effect, but not to this extent.  Also these are effects of old versions.

Decreasing the number to 4 improves things.

Perhaps this is simply an effect of not enough memory being dedicated to each spark worker?
BTW how to increase that?

Possibly this is simply a misconfiguration, here's it is.

    master.vm.provider :virtualbox do |v|
      v.name = master.vm.hostname.to_s
      v.customize ["modifyvm", :id, "--ioapic", "on"]
      v.memory = 49152
      v.cpus = 4
   end

Any possibilities? Can someone help in accessing the shakespeare.txt file. I have no clue as to where on the VM is this file located. Hello ,

I am unable to sort the RDD in the dayToHost count. I am only able to sort it only using the takeOrdered in the last line for dailyHostLists.
I am using groupByKey and mapValues before I perform reduceByKey and the takeOrdered to pass the test. Can anyone tell me if these a better way of perfoming sort on the RDD. I am not able to understand what to use to obtain the length of a word? I can get words count, but not character count. I'm stuck and deadline's already crossed. Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Unfortunately autograder is complaining about timeout. Some of the general guideline like avoiding collect() and groupByKey() could not be followed as Lab2 demands it. Any other pointer will be appreciated.
 I am getting a "invalid syntax" message in the below code ...what could be the problem.
Also, when I tried closing '(' before wordCounts with ')' after reduce(add) it gave different error..
 
# TODO: Replace <FILL IN> with appropriate codefrom operator import addtotalCount = (wordCounts              .map(lambda x,y:y)              .reduce(add)average =  lambda totalCount: float(totalCount) / float(uniqueWords)print totalCountprint round(average, 2)

Regards,
Venkat
 Why do I have to use the map function, instead of just using the values() function? For some reason, I have endpointSum.count() return 7688 rather 7689. I'm not entirely sure why this is happening. Has anyone else experienced this issue?

The topTenURLS test passes just fine. Hello,

When I run "Vagrant up" from the cmd prompt in Windows, the code is executed but it does not start the Oracle virtual Box. I am unable to access the local host as well. Please help me with this. Thank you so much!! In lab 1 4e i get the answer as 883271 instead of 882996.... pls help....... my 4d and 4e have worked fine
 When I implement my solution that works from (2d) All together change the RDD name without .collect(), I receive this error:

---> 13 print wordCount(wordsRDD).collect()

AttributeError: 'NoneType' object has no attribute 'collect'
Any suggestions on what this means? Thanks


  LAB 2 Part 1(b) is taking too long to complete execution. Am I doing something incorrect? How long should I wait before this step completes. Thanks i need to creat new machine due issue at existing machine . but shows like:
Bringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install... sparkvm: Box Provider: virtualbox sparkvm: Box Version: >= 0==> sparkvm: Loading metadata for box 'sparkmooc/base' sparkvm: URL: https://atlas.hashicorp.com/sparkmooc/base==> sparkvm: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox sparkvm: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box==> sparkvm: Successfully added box 'sparkmooc/base' (v0.0.7.1) for 'virtualbox'!==> sparkvm: Importing base box 'sparkmooc/base'...==> sparkvm: Matching MAC address for NAT networking...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...A VirtualBox machine with the name 'sparkvm' already exists.Please use another name or delete the machine with the existingname, and try again.

Thanks in advance  
I have 2 RDD's:

A = [88,75,36]
B= [8, 25, 9]

How do I divide them?
I would like to end up with
C = [11,3,4] Can somebody explain these two in details?

access_logs = (parsed_logs                         .filter(lambda s: s[1] == 1)                         .map(lambda s: s[0])
                         .cache())

failed_logs = (parsed_logs                      .filter(lambda s: s[1] == 0)                      .map(lambda s: s[0]))

 HI,

I downloaded the file link shared in the course-ware. However I am not able to upload this file on the VM using jupyter. 
While I am trying to upload I get a pop-up with error messages :

Cannot upload invalid Notebook
The error was: SyntaxError: Unexpected token <

Did anyone get this error?

Regards,
A

Thanks.
I saved as text file.
Uploaded at the text file and renamed it to remove ".txt" extension. it worked.

Thanks
 
I get the following error in 1b, please help


---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-3-8d8d6629d991> in <module>()
     32 
     33 
---> 34 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-3-8d8d6629d991> in parseLogs()
      9 def parseLogs():
     10     """ Read and parse log file """
---> 11     parsed_logs = (sc
     12                    .textFile(logFile)
     13                    .map(parseApacheLogLine)

NameError: global name 'sc' is not defined
 
I had first submitted Lab1 at 11:55pm on June 14th.  
Got 93% test cases passing because I'd only completed up to 4e.  
However, Grader penalized 20% and registered I'd only get 74 points.

I had resubmitted at 0:38am on June 15th.  
Got 100% test cases passing, and got penalized 20% to get 80 points.  
This was expected.

Shouldn't my final score for Lab1 be 93 points due to my early first submission? 
This is the grading convention I'm used to from Coursera. If I dont pay the $50 for the ID verified Certificate of Achievement ... do I still get the Basic Edx Certificate ??
Thanks Hi all,

Just spent time on 2c: Counting using reducedByKey.
I used the function "add" imported from "operator":
from operator import add and it passes

My question is how could we define a function which performs what add does??
I tried the following by was always getting an error:
wordCounts = wordPairs.reduceByKey(map(lambda (k,v): k, sum(v)))

Any suggestion on how i can resolve this? Could you please provide the URL to slides used during Week 3?

Thank you very much There is a lot said about ques 4 in this post and now there is a lot of confusion there. I request the TA or course instructor to kindly consolidate the post @575 and please remove redundancy I cannot understand why the following error is showing up..... Please help......
Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 27, in 
  File "", line 44
    text=text.replace(',','').replace('.','').replace('/','').replace(':','').replace(''','').replace('{','').replace('}','').replace('|','').replace('!','').replace('#','').replace('%','').replace('*','').replace(')','')
    text=text.replace('<','').replace('>','').replace('?','').replace(';','').replace('"','').replace('[','').replace(']','').replace('\','').replace('@','').replace('$','').replace('^','').replace('(','').replace('_','')
    text=text.replace('-','').replace('+','').replace('=','').replace('`','').replace('~','').replace('&','')
    return text
print removePunctuation('Hi, you!')
print removePunctuation(' No under_score!')


# In[60]:
                                                                                                                                                                                                                             
                                                                                                                                                                                                                             
                                                                                                             
               
                                   
                                           


        ^
SyntaxError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
EOF while scanning triple-quoted string literal

All tests passed
Apply makePlural to the base RDD(1c)
------------------------------------
All tests passed
Pass a lambda function to map (1d)
----------------------------------
All tests passed
Length of each word (1e)
------------------------
All tests passed
Pair RDDs (1f)
--------------
All tests passed
groupByKey() approach (2a)
--------------------------
All tests passed
Use groupByKey() to obtain the counts (2b)
------------------------------------------
All tests passed
Counting using reduceByKey (2c)
-------------------------------
All tests passed
All together (2d)
-----------------
All tests passed
Unique words (3a)
-----------------
All tests passed
Mean using reduce (3b)
----------------------
All tests passed
wordCount function (4a)
-----------------------
All tests passed
Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'removePunctuation' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'shakespeareWordCount' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'shakeWordCount' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'top15WordsAndCounts' is not defined

-- 12 cases passed (75.0%) --


As for late submissions after 3 day grace period of the due date, you lose 20 points.
 Your final score is 60
 
I'm just recording the time it takes me to finish these labs.  Anyone else who's recording please feel free to add your work time and previous programming experience. These labs are really fun (once the problem is figured out).

Previous Experience:

Java/Javascript/SQL
Software Engineer
No Python Experience




Lab 1: 4.5 hours
Lab 2: 6 hours (Exercise 3 is the hardest)



related: post @1456

 Hi,

Link in linkedin to connect and know about apache spark updates.

https://www.linkedin.com/grp/home?gid=7403611 Hi:
I am having problems launching the VM, normally I have to try a few times before the VM runs.. I open a cmd as administrator and type vagrant up and receive the following error:
Later on if I try to shut down the VM, vagrant halt instruction, I cannot stop the VM 
 
 
 
Microsoft Windows [Version 6.3.9600]
(c) 2013 Microsoft Corporation. All rights reserved.
 
C:\WINDOWS\system32>cd/
 
C:\>cd /users/JuanCarlos/myvagrant
 
C:\Users\JuanCarlos\myvagrant>vagrant up
C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/config.rb:83:in `bl
ock in missing_interpolation_argument_handler': missing interpolation argument :
vboxmanage in "Vagrant detected that VirtualBox appears installed on your system
,\nbut calls to detect the version are returning empty. This is often\nindicativ
e of installation issues with VirtualBox. Please verify\nthat VirtualBox is prop
erly installed. As a final verification,\nplease run the following command manua
lly and verify a version is\noutputted:\n\n%{vboxmanage} --version" ({:_key=>:vi
rtualbox_version_empty, :_namespace=>"vagrant.errors"} given) (I18n::MissingInte
rpolationArgument)
        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/interp
olate/ruby.rb:29:in `call'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/interp
olate/ruby.rb:29:in `block in interpolate_hash'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/interp
olate/ruby.rb:21:in `gsub'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/interp
olate/ruby.rb:21:in `interpolate_hash'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/interp
olate/ruby.rb:17:in `interpolate'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/backen
d/base.rb:153:in `interpolate'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n/backen
d/base.rb:41:in `translate'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n.rb:157
:in `block in translate'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n.rb:153
:in `catch'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/i18n-0.6.11/lib/i18n.rb:153
:in `translate'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/e
rrors.rb:103:in `translate_error'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/e
rrors.rb:72:in `initialize'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/provi
ders/virtualbox/driver/meta.rb:155:in `exception'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/provi
ders/virtualbox/driver/meta.rb:155:in `raise'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/provi
ders/virtualbox/driver/meta.rb:155:in `block in read_version'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/u
til/retryable.rb:17:in `retryable'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/provi
ders/virtualbox/driver/meta.rb:140:in `read_version'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/provi
ders/virtualbox/driver/meta.rb:38:in `initialize'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/provi
ders/virtualbox/provider.rb:47:in `new'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/provi
ders/virtualbox/provider.rb:47:in `machine_id_changed'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/provi
ders/virtualbox/provider.rb:27:in `initialize'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/m
achine.rb:129:in `new'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/m
achine.rb:129:in `initialize'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/v
agrantfile.rb:75:in `new'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/v
agrantfile.rb:75:in `machine'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/e
nvironment.rb:614:in `machine'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/p
lugin/v2/command.rb:168:in `block in with_target_vms'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/p
lugin/v2/command.rb:192:in `call'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/p
lugin/v2/command.rb:192:in `block in with_target_vms'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/p
lugin/v2/command.rb:174:in `each'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/p
lugin/v2/command.rb:174:in `with_target_vms'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/comma
nds/up/command.rb:74:in `block in execute'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/e
nvironment.rb:277:in `block (2 levels) in batch'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/e
nvironment.rb:275:in `tap'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/e
nvironment.rb:275:in `block in batch'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/e
nvironment.rb:274:in `synchronize'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/e
nvironment.rb:274:in `batch'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/comma
nds/up/command.rb:58:in `execute'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/c
li.rb:42:in `execute'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/e
nvironment.rb:301:in `cli'
        from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/bin/vagrant:1
74:in `<main>'
 
C:\Users\JuanCarlos\myvagrant>vagrant up
Bringing machine 'sparkvm' up with 'virtualbox' provider...
==> sparkvm: Checking if box 'sparkmooc/base' is up to date...
==> sparkvm: Clearing any previously set forwarded ports...
==> sparkvm: Clearing any previously set network interfaces...
==> sparkvm: Preparing network interfaces based on configuration...
    sparkvm: Adapter 1: nat
==> sparkvm: Forwarding ports...
    sparkvm: 8001 => 8001 (adapter 1)
    sparkvm: 4040 => 4040 (adapter 1)
    sparkvm: 22 => 2222 (adapter 1)
==> sparkvm: Booting VM...
==> sparkvm: Waiting for machine to boot. This may take a few minutes...
    sparkvm: SSH address: 127.0.0.1:2222
    sparkvm: SSH username: vagrant
    sparkvm: SSH auth method: private key
The guest machine entered an invalid state while waiting for it
to boot. Valid states are 'starting, running'. The machine is in the
'unknown' state. Please verify everything is configured
properly and try again.
 
If the provider you're using has a GUI that comes with it,
it is often helpful to open that and watch the machine, since the
GUI often has more helpful error messages than Vagrant can retrieve.
For example, if you're using VirtualBox, run `vagrant up` while the
VirtualBox GUI is open.
  May I suggest using some real life data as example, which could be slightly more intuitive, especially when it comes to order. 

>>> student = sc.parallelize([("1111", "John Smith"),("2222", "David Brown")])
>>> enroll = sc.parallelize([("1111", "history")])
>>> sorted(enroll.rightOuterJoin(student).collect())

[("1111", ("history", "John Smith")), ("2222", (None, "David Brown"))]
  Hi there,
I couldn't manage to find where is the link till the pdf files from the Lecture 5 and 6. If someone can tell me where they are would be a much of help
 I have tried multiple times to download the virtualbox, finally I succeeded in downloading it by waiting quite a while, however my Virtual Machine did not create or see the downloaded virtualbox, how can I make the Virtual Machine software see my downloaded file? I still couldn't start the course just because I have stuck with the download procedures of the software, looking forward to your help!


Cengizs-MacBook-Air:~ ccemaloglu$ vagrant box add sparkmooc/base --insecure
==> box: Loading metadata for box 'sparkmooc/base'
    box: URL: https://atlas.hashicorp.com/sparkmooc/base
==> box: Adding box 'sparkmooc/base' (v0.0.7.1) for provider: virtualbox
    box: Downloading: https://atlas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box
==> box: Box download is resuming from prior download progress
==> box: Successfully added box 'sparkmooc/base' (v0.0.7.1) for 'virtualbox'!


 Interesting blurb relevant to students of Spark.

http://bits.blogs.nytimes.com/2015/06/15/ibm-invests-to-help-open-source-big-data-software-and-itself/?ref=technology
 Hi All,

Ref:Excercise 3(b)-Test case fails as Unique hosts: 54500 but not 54507 .Is this correct? Hello,

I'm trying to understand how functions work in this lab and the output.
When i run (1a) no output, there is regex in (1a) which same name as (1c) .
When run (1b) the output depends on (1c), but in (1b) there is no variable or functinos name "APACHE_ACCESS_LOG_PATTERN"?!
What is parsed_logs for?!!
I'm stuck in (1c) because i don't understand what exactly output will be? 

Thanks Hello,

I submitted the setup before the deadline and got 100%.

Today I accidentally submitted the same doc again and got -20% for not respecting the deadlines as a result of these manipulations.

I don't think it should be like that, as I did respect the deadline and got 100% already.

Anyway, be careful :)

Thanks,
Maksym I wrote the following code
dayGroupedHosts = dayToHostPairTuple.groupByKey()

dayHostCount = dayGroupedHosts.map(lambda x: (x[0],len(x[1]))) dailyHosts = (dayHostCount.distinct()) dailyHostsList = dailyHosts.take(30)print 'Unique hosts per day: %s' % dailyHostsList

The test are failing. Can anyone provide some pointer as to where I am going wrong ? 1. Output of daysWithHosts[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]2. Output of hosts
[2582, 3222, 4190, 2502, 2537, 4106, 4406, 4317, 4523, 4346, 2864, 2650, 4454, 4214, 4340, 4385, 4168, 2550, 2560, 4134, 4456]
 
3. If I run the test case, then the output is:
1 test failed. incorrect days 1 test failed. incorrect hosts
 
I did simple transformations for dailyHosts inorder to get daysWithHosts and hosts. What wron I might have done?
Thanks in advance
 Hi ,

It is mentioned ,  Week 3 Lecture 5 slide no 36  , that Binary I/O is mush faster than text and I am wondering why.

Although the text file is larger than the binary file, It takes 1-6 seconds to read or write a binary file in comparison to 18-21 second for the text file.

here is my thoughts .
Is it  very expensive to parse the text to be viewed as raw text?

I noticed that reading a file without any computation is slower in Python than c++.
Does c++ has the same difference between Binary I/O and Text I/O?

Thanks,
Mostafa


 I search for this expressin using reg

127.0.0.1 - - [01/Aug/1995:00:00:01 -0400] "GET /images/launch-logo.gif HTTP/1.0" 200 1839

I literally looked for every thing including a pattern of " - - " all the way to the end looking for 3 and 4 digit numbers. But I get complete log file parsing failure. I must be totally off the search pattern here, isn't the above line what we look for. My text of \"\S+ \/\S+\s(\S+) gets the complete "GET...." section without the need for declaring 1.0 as \d\.\d. Could anyone please tell me why I am %100 off.

Actually now that I am running 1b I get this: I am sure we do not need to change 1b and just run it
Read 1043177 lines, successfully parsed 0 lines, failed to parse 1043177 lines

My test result for 1c
1 test failed. incorrect failed_logs.count()
1 test passed.
1 test failed. incorrect access_logs.count()
Thanks. Hi yesterday I saw test cases were passing today it is failing saying acess_logs not defined please guide

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-2-a826c6f17fdb> in <module>()
      2 # HINT: Do you recall the tips from (3a)? Each of these <FILL IN> could be an transformation or action.
      3 
----> 4 hosts = access_logs.map()
      5 
      6 uniqueHosts = 

NameError: name 'access_logs' is not defined

  Guide to build an environment and use it instead of using vagrant and vbox
Install jupyterGithub repo: https://github.com/jupyter/notebookDetailed Installation instructions: http://ipython.org/ipython-doc/stable/install/install.htmlInstall pyspark and configuring ipython to use ithttp://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/Now you're donethe next thing to do is to get the data we use in labsunfortunately I couldn't upload it.Getting the data shipped with the vmvagrant up
ssh -p 2222 vagrant@127.0.0.1 tar -czf data.tar.gz datascp -P 2222 vagrant@127.0.0.1:data.tar.gz .

the password is "vagrant"and then upload it I just tried to access the tree host by various methods, and the consensus is the same: the server at the address is not responding.

Just letting everyone know; if you do not have this problem, please comment below. Hi people,

I have written the <Fill in> code for Lab 2c as below:

wordCounts = wordPairs.reduceByKey(lambda word: word,len(word))print wordCounts.collect()The O/P should be[('cat', 2), ('elephant', 1), ('rat', 2)],I am getting below error---------------------------------------------------------------------------NameError                                 Traceback (most recent call last)<ipython-input-21-cbc8252a6235> in <module>()      1 # TODO: Replace <FILL IN> with appropriate code      2 # Note that reduceByKey takes in a function that accepts two values and returns a single value----> 3 wordCounts = wordPairs.reduceByKey(lambda word: word,len(word))      4 print wordCounts.collect()NameError: name 'wordPairs' is not definedPlease suggest how to declare 'wordPairs' Hello for some reason I have done the lab2 with all tests passed locally but when I submit the .py file I am getting (full output from grader):


Data cleaning (1c)------------------Traceback (most recent call last):  File "", line 22, in NameError: name 'Test' is not definedTraceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not definedTop ten error endpoints (3a)----------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not definedNumber of unique hosts (3b)---------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not definedNumber of unique daily hosts (3c)---------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not definedVisualizing unique daily hosts (3d)-----------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not definedAverage number of daily requests per hosts (3e)-----------------------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not definedAverage Daily Requests per Unique Host (3f)-------------------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not definedCounting 404 (4a)-----------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not definedListing 404 records (4b)------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'badUniqueEndpointsPick40' is not definedTop twenty 404 URLs (4c)------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not definedTop twenty-five 404 response code hosts (4d)--------------------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not defined404 response codes per day (4e)-------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not definedVisualizing the 404 Response Codes by Day (4f)----------------------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not definedFive dates for 404 requests (4g)--------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not definedHourly 404 response codes (4h)------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not definedVisualizing the 404 Response Codes by Hour (4i)-----------------------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'Test' is not defined-- 0 cases passed (0.0%) --



Any hint on what I have done wrong? 4c:print out a list of the top twenty endpoints that generate the most 404 errors.Why is the result a list of TUPLE consisting of endpoints and corresponding count, like [(u'/pub/winvn/readme.txt', 633), (u'/pub/winvn/release.txt', 494), etc.]According to the requirement, I think it should like  [u'/pub/winvn/readme.txt', u'/pub/winvn/release.txt', etc.]BTW, almost all strings show as u'abcded'. What's the meaning of the prefix u?4h:Hourly 404 Response Codes in decreasing orderBut why does the result look in an ascending order [(0, 175), (1, 171), (2, 422), (3, 272), (4, 102), (5, 95), etc.] Hi, I  would like to ask whether the request:"Please do not post your programming exercises in publicly visible repositories, such as GitHub."

is for the duration of the course, or even after the end of the course.

I would like to post my exercises, in a stripped down form (so it can't be searched by common text in the code files) on my GitHub Education repository after the course ends. I use it as my portfolio for employers.

Would this be a problem?Thank you,
Viktor Hi,

I have a question for you.

For how long do a broadcast variable keep in workers' memory? Is it deleted after driver execution end? Hi

Anyone encounter this error and resolve it on Windows 8.1? I have admin access and run the command prompt as administrator ... I have spent so much time trying to get this to work, inc setting a lot of permissions on the folders that I don't know what to do next.

How can I reinstall it? Simply by doing vagrant destroy and redoing the download?

Here is what happens after vagrant up ....

Microsoft Windows [Version 6.3.9600](c) 2013 Microsoft Corporation. All rights reserved.C:\WINDOWS\system32>cd..C:\Windows>cd..C:\>cd usersC:\Users>cd albatrossC:\Users\Albatross>cd myvagrantThe system cannot find the path specified.C:\Users\Albatross>cd my documentsC:\Users\Albatross\My Documents>cd myvagrantC:\Users\Albatross\My Documents\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/platform.rb:126:in `open': Permission denied - C:/Users/Albatross/MyDocuments (Errno::EACCES)        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/platform.rb:126:in `entries'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/platform.rb:126:in `block in fs_real_path'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/platform.rb:125:in `each'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/platform.rb:125:in `fs_real_path'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/synced_folders.rb:65:in `block (2 levels) in call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/synced_folders.rb:41:in `each'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/synced_folders.rb:41:in `block in call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/synced_folders.rb:40:in `each'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/synced_folders.rb:40:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/synced_folder_cleanup.rb:28:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/synced_folders/nfs/action_cleanup.rb:19:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/prepare_nfs_valid_ids.rb:12:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/handle_forwarded_port_collisions.rb:160:in `handle'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/handle_forwarded_port_collisions.rb:42:in `block in call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/environment.rb:516:in `lock'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/handle_forwarded_port_collisions.rb:41:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/prepare_forwarded_port_collision_params.rb:30:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/env_set.rb:19:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/provision.rb:80:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/clear_forwarded_ports.rb:15:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/set_name.rb:50:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/clean_machine_folder.rb:17:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/check_accessible.rb:18:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:95:in `block in finalize_action'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builder.rb:116:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `block in run'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/busy.rb:19:in `busy'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `run'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/call.rb:53:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:95:in `block in finalize_action'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builder.rb:116:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `block in run'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/busy.rb:19:in `busy'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `run'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/call.rb:53:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:95:in `block in finalize_action'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builder.rb:116:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `block in run'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/busy.rb:19:in `busy'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `run'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/call.rb:53:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/box_check_outdated.rb:68:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/config_validate.rb:25:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/check_virtualbox.rb:17:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:95:in `block in finalize_action'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:95:in `block in finalize_action'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builder.rb:116:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `block in run'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/busy.rb:19:in `busy'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `run'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/call.rb:53:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/config_validate.rb:25:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:95:in `block in finalize_action'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:95:in `block in finalize_action'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builder.rb:116:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `block in run'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/busy.rb:19:in `busy'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `run'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builtin/call.rb:53:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/plugins/providers/virtualbox/action/check_virtualbox.rb:17:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/warden.rb:34:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/builder.rb:116:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `block in run'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/util/busy.rb:19:in `busy'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/action/runner.rb:66:in `run'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/machine.rb:214:in `action_raw'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/machine.rb:191:in `block in action'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/environment.rb:516:in `lock'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/machine.rb:178:in `call'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/machine.rb:178:in `action'        from C:/Program Files (x86)/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/batch_action.rb:82:in `block (2 levels) in run'C:\Users\Albatross\My Documents\myvagrant>

Thanks I am almost ready to turn lab 1 in. Should be today or tomorrow. But I do have one observation.It seems that there are not enough examples to help one generalize and apply to solutions. Were it not for some of the collective effort of the students being available through Piazza it would be tougher to surmise the application of some of the concepts. The pedagogy of the course is good and the exercises build upon each other which is really the way application of the stuff becomes possible. But I think before this there should be some more general examples or references to materials containing them.Thanks everybody for all the help.
 
efundora Hello everybody,
i have just applied for  verified certificate and sent the photo of my ID but i'm from Egypt and my name written in ID in Arabic so i have to change it in edx to Arabic like the ID but i want my name in the certificate to be in English what can i do ?
Also i have another question about "ID Verification Status Pending" what time does it take in order to know if your application for the certificate accepted or not ?I have some worries if my application needs some modifications and they reply to me after 17th of June so i ask about the pending time ?
Thanks in advance. I assume that the collect operation runs by taking from every partition the iterable of the objects stored in them and it does it in asceding order (partition0.iterable, partiation1.iterable,...), but can we guarantee that collect run twice on the same RDD will preserve order? How about in limit cases when not all the partitions may be cached and some might need to be recomputed (or maybe a shuffle operation )?

I am curios about this answer since the template for lab2 relies on this.

Any thoughts? Hi,

I read in a "Learning Spark" book it's possible use Yarn as a cluster manager, it said something like the driver program requests cores from Yarn to execute the job operations. Could you explain me in more detail this interaction? Does Yarn give resources to drive program only or does it schedule the spark job execution too like it does with map-reduce jobs? REPL output:
>>> rdd1 = sc.parallelize(range(0,17), 4)
>>>
>>> rdd1.glom().collect()
[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15, 16]]
>>>
>>> rdd2 = sc.parallelize(range(20,37), 4)
>>>
>>> rdd2.glom().collect()
[[20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35, 36]]
>>>
>>> ziprdd = rdd1.zip(rdd2)
>>>
>>> ziprdd.glom().collect()
[[(0, 20), (1, 21), (2, 22), (3, 23)], [(4, 24), (5, 25), (6, 26), (7, 27)], [(8, 28), (9, 29), (10, 30), (11, 31)], [(12, 32), (13, 33), (14, 34), (15, 35), (16, 36)]]
>>>
>>> rdd2 = sc.parallelize(range(20,39), 4)
>>>
>>> rdd2.glom().collect()
[[20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35, 36, 37, 38]]
>>>
>>> ziprdd = rdd1.zip(rdd2)
>>>
>>> ziprdd.glom().collect()

# ValueError: Can not deserialize RDD with different number of items in pair: (1, 3)
I just want Spark to cut off the values 37 and 38, just like zip does in pure Python. Hi,

I tested my regex with various test sites and even with Python 2.7 against whole dataset and I didn't get any error.
But when I run it in notebook I get: see under
Should I reset some state in notebook or import it once again? Or is there some other posibility?

Thank you
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-6-053df1e09e83> in <module>()
      5 APACHE_ACCESS_LOG_PATTERN = '<valid regex>'
      6 
----> 7 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-3-2ae38335709c> in parseLogs()
     13                    .map(parseApacheLogLine)
     14                    .cache())
---> 15     print parsed_logs.count()
     16     access_logs = (parsed_logs
     17                    .filter(lambda s: s[1] == 1)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 60, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-1-0940d9fe8819>", line 48, in parseApacheLogLine
TypeError: int() argument must be a string or a number, not 'NoneType'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> If I cant answer all the question, should I send my answers? Does the system give me error ? 
Thanks
yasemin HI,
During the lab1 submission process yesterday, I made an error and used the Setup page to submit my lab1, overriding my previous score from the Setup lab.  

As result, the autograter penalized me late submission of 20 points, and changed my previous score of 100% to 80 %.  Can you please assist me in reversing this error? I submitted my setup lab on time in the first week. This was an error of assumption that all labs are submitted on the same page. I Should have read the bold text more carefully.
 
Thank you,
Warsame Can I confirm the deadline for labs 1 submission?

I perhaps incorrectly assumed the deadline was midnight on 15/06/15 (given the 3 day grace period from 12/06/15), but it looks like the deadline was 14/06/15 23:59:59? Hello,

is somebody here in the position to share her/his experience in choosing YARN over Mesos or vice versa for managing clusters?

I'm especially interested in which of the both causes less setup overhead and integrates more seamless with Spark.

The question arises b/c I would like to delve into setting up an own Spark cluster on EC2 or EMR for educational purposes and I would like to make things not more complex than necessary.

Kind Regards

Raffael Im doing below 

In [15]:

# TEST Apply makePlural to the base RDD(1c)

Test.assertEquals(pluralRDD.collect(), ['cats', 'elephants', 'rats', 'rats', 'cats'],

                  'incorrect values for pluralRDD')

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-15-f12b1024a961> in <module>()
      1 # TEST Apply makePlural to the base RDD(1c)
----> 2 Test.assertEquals(pluralRDD.collect(), ['cats', 'elephants', 'rats', 'rats', 'cats'],
      3                   'incorrect values for pluralRDD')

NameError: name 'Test' is not defined

Not sure what happened and why Test.    doesnt work.   I did relogin since it worked last time. COuld that be the issue? Do I need to download some kind of library?

Thanks P.S 
my 
print pluralRDD.collect()

did produce
 
['cats', 'elephants', 'rats', 'rats', 'cats']in case anyone was concerned but for some reason doesnt doesnt work. HI I am trying 
wordsRDD.map(lambda x: (x+",1")
but that just gets me 
'cat,1'
How do I get it to include round parantheses.   I included the parantheses in my code but it doesnt seem to catch it.  After spending the day I was down to Question 4 (i).  The page then became un responsive.  I have closed down vagrant and my mac book and restarted it and reloaded the page from the last check point.  If I try executing any of the cells I get errors,  I thought maybe I have to start re-executing from the 1st cell.   This got down as far as below (ie still part of the course & not my own work and I get the errors as at the bottom.  ie it does not seem to find even sc.  what is going on?  Please help.

import sysimport osfrom test_helper import Test
baseDir = os.path.join('data')inputPath = os.path.join('cs100', 'lab2', 'apache.access.log.PROJECT')logFile = os.path.join(baseDir, inputPath)
def parseLogs(): """ Read and parse log file """ parsed_logs = (sc .textFile(logFile) .map(parseApacheLogLine) .cache())
access_logs = (parsed_logs .filter(lambda s: s[1] == 1) .map(lambda s: s[0]) .cache())
failed_logs = (parsed_logs .filter(lambda s: s[1] == 0) .map(lambda s: s[0])) failed_logs_count = failed_logs.count() if failed_logs_count > 0: print 'Number of invalid logline: %d' % failed_logs.count() for line in failed_logs.take(20): print 'Invalid logline: %s' % line
print 'Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (parsed_logs.count(), access_logs.count(), failed_logs.count()) return parsed_logs, access_logs, failed_logs
parsed_logs, access_logs, failed_logs = parseLogs()
NameError                                 Traceback (most recent call last)
<ipython-input-3-8d8d6629d991> in <module>()
     32 
     33 
---> 34 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-3-8d8d6629d991> in parseLogs()
      9 def parseLogs():
     10     """ Read and parse log file """
---> 11     parsed_logs = (sc
     12                    .textFile(logFile)
     13                    .map(parseApacheLogLine)

NameError: global name 'sc' is not defined

 Hi I am having the following code to solve 3c it looks correct logically but dont know why if fails saying lambda can have only one value as input


dayToHostPairTuple = (access_logs.map(lambda log : (log.date_time.day,log.host) )).cache()dayGroupedHosts = dayToHostPairTuple.groupByKey().distinct()dayHostCount = dayGroupedHosts.cache()

dailyHosts = dayHostCount.reduce(lambda (day,host): (day,len(host)))dailyHostsList = dailyHosts.take(30)print 'Unique hosts per day: %s' % dailyHostsList Hello I need help, I have problems with the upload. I do not know the solution, thanks
Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 388
SyntaxError: Non-ASCII character '\xc2' in file /ok/submission.py on line 389, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pass a lambda function to map (1d)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Length of each word (1e)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Pair RDDs (1f)
--------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

groupByKey() approach (2a)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Use groupByKey() to obtain the counts (2b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting using reduceByKey (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Unique words (3a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Mean using reduce (3b)
----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

wordCount function (4a)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --


As for late submissions after 3 day grace period of the due date, you lose 20 points.
 Your final score is 0

 The assertEquals fails for the second evaluation in this part. The other two are successful. I have read several similar issues, but not the same.

I think I did everything right, but I might have misunderstood the required format of the result. Can anyone presume what might have gone wrong? I would like an answer without code to not violate the honor code. Thank you!

Here is, what I get and what it should be (looks the same to me):

Unique hosts per day: [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)]
Should be: [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)] hi,

In do not understand the part: 'and in decreasing order' from the request:
Using the RDD badRecords you cached in the part (4a) and by hour of the day and in decreasing order, create an RDD containing how many requests had a 404 return code for each hour of the day. Cache the resulting RDD hourRecordsSorted and print that as a list.
May someone can give me more detail about...
Thanks. I live in Monterrey, Mexico. If there is anyone nearby we could form a local spark group.

My linkedin profile:
https://mx.linkedin.com/pub/iván-venzor/32/227/643


Best regards,

Iván Venzor C. Hi All,
 In one recent post it was mentioned that lab and other course-ware are  released on every Saturday and indirectly said that students could immediately start working on the labs.
I think there is usually a strong connection between lectures and lab.
If that is the case, wouldn't it be ideal to go thru lectures first and then do the lab?

If I am interviewing a job candidate who brings in an edx certificate but does not demonstrate at least intermediate level knowledge of Spark development, I would not recommend to hire that candidate because that candidate most probably hacked the labs and got good grades on the course.
Thanks
 I can solve this exercise by using concepts used in earlier problems but thought of experimenting. Here is the code :

errDateCountPairTuple = badRecords.map(lambda x : (x.date_time.day,x.host))
errDateSum = errDateCountPairTuple.groupByKey().mapValues(lambda x : len(set(x[1])))print errDateSum.collect()

I get the following error. Please provide some pointers:

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-160-231e48d191e7> in <module>()
      4 
      5 errDateSum = errDateCountPairTuple.groupByKey().mapValues(lambda x : len(set(x[1])))
----> 6 print errDateSum.collect()
      7 #errDateSorted = errDateSum.sortByKey()
      8 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 393.0 failed 1 times, most recent failure: Lost task 0.0 in stage 393.0 (TID 797, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1806, in <lambda>
    map_values_fn = lambda (k, v): (k, f(v))
  File "<ipython-input-160-231e48d191e7>", line 5, in <lambda>
TypeError: 'ResultIterable' object does not support indexing

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
 When I am trying to execute the Part 3: Check plotting code I am not getting any plots with circles.

The code is also not giving any errors.

Let me know if you need any further details on my issue.

Thanks,
Arvind hi 

When performing any of these three operation Aggregation/Reduce/Fold on key-value pairs, I understood that a single key is aggregated/folded or reduced in one executor. Also it seems spark needs all of the pairs in single executor's memory to do the operation for the key. This can result in out of memory when for a particular key there are a large number of pairs.   Given the data and nature of aggregation it might be unavoidable that there are cases where certain keys have large pairs than others. So how can an out of memory occurrence be averted?  is it possible to use a partition-er that sends some pairs of the same key to one executor and others to another and then perform a second level of aggregation.  ?  

thanks
 Hello 

I had scored 100% in setup last week. Today mistakenly I submitted my lab2 on setup page which ranked me 80% considering I was submitting my work late and gave me 80%. Is it possible to revert my score. ? Please help. Is it possible to interact with the actual VM's file system?  I noticed that the 'shakespeare.txt' file seems to be in a local directory on the VM, however, when VirtualBox and vagrant launch, I don't seem to have a way to interact with the VM interactively. Thanks in advance. not able to figure out how to divide the daily total hosts with daily distinct hosts in the last fill in as i have listed the unique hosts daily as in 3c and also total hosts daily which was also straight forward but how to divide them using map or some other utility

Solved it...thanks Hi, I run my notebook, I pass all the tests but after completing and passing all the tests successfully autograder does not accepts my answers for 3C
I gives an error only for that question but it is correct. I Passed all the tests.
How can i solve this problem.
Thanks

 Enjoying this course tremendously and wish to continue on to the next course and complete the x-series.

However, the prerequisites are pretty vague for scalable machine learning.

My question is, assume I do well here, say 90-100% grade total. Should I be sufficiently prepared to complete Scalable Machine learning or is the a big skill-jump between the two courses?

Thanks http://www.ibmbigdatahub.com/blog/answers-more-your-burning-questions-about-spark As pointed out by several students, in Lab 2, exercise (4h) Hourly 404 Response Codes, the assertEquals test presents the hourly 404 response codes sorted in increasing order by hour (the first part of the tuple), and that's the natural way to present it of course, and that's what the autograder is looking for.

We have changed the word "decreasing" to "increasing" in the description for exercise 4h and released updated notebooks, version 1.0.1, in the repositories. But you don't need to get the new notebook in order to submit your answers, though you may want to change the text yourself also.

Sorry for the confusion!  We're increasing our corps of beta testers to avoid these issues in the future.  Thanks for your feedback.
#pin
 What is exactly is needed a return value stored inside uniqueWords?
I have done: 

uniqueWords = wordsCount.filter(lambda: aTuple: aTuple[1])
and it correctly returns elephant with 1 occurrence if I run collect on it. But the test fails?

Please help on this! Thanks
 If you're getting stuck with question 3e, you might want to look at this thread:

http://apache-spark-user-list.1001560.n3.nabble.com/Computing-mean-and-standard-deviation-by-key-td11192.html

It shows how to compute the conditional mean in a group by propagating a tuple of values (one corresponding to counts, another corresponding to sums). There's probably other ways of doing it, but that's how I got it to work. Hi I was playing around with the spark tutorial file and accidentaly deleted the second print statement in paragraph 41 

It starts with
print pairRDD.groupByKey().map(lambda (k, v)  
what is the code supposed to be after that? Please note that I tried closing it and restarting but it already saved it with the mistakes. Please note that I am not asking for solution to a homework. This is a tutorial. Thanks  Dear fellows,

Could you share if there is somebody from Uzbekistan?

Regards,
FUN lab 2 submission on autograder taking lot of time...i dont know why? Hi,

When is the next Spark class starting?

Thanks! I first submitted my assignment for lab1 before deadline and I obtained 81% (13 /16), as I answered only the questions up to 4d. Today I resubmitted a more complete version of my assignment (due to a miscalculation of the the extended deadline) and I was penalized by 20 points, ending up with a grade smaller than the initial one. Is it possible to discard the last submission and keep the initial grade?
Thank you.
 Yet another web based IDE that looks interesting:

http://blog.yhathq.com/posts/rodeo-0.4.html

To test drive it on the course VM, one can do the following:

1. Add the following line to the Vagrantfile to allow access to port 5000 :
    master.vm.network :forwarded_port, host: 5000, guest: 5000, auto_correct: true                 # Rodeo Notebook

2. Connect to the VM terminal using the jupyer console, or directly  
    vagrant ssh

3. Install rodeo
 pip install rodeo

4. Start rodeo:
export SPARK_HOME="/usr/local/bin/spark-1.3.1-bin-hadoop2.6" | rodeo . --host=0.0.0.0 --no-browser --pyspark

5. To view, browse to 
http://127.0.0.1:5000/# I have submitted Lab1 (late I know curses). Anyway I had all but the last question working and passing in PyNoteBook, but when I run in autograder I get this


Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 27, in 
  File "", line 34
    result = re.sub(r'[^\w\s']','',text)
                             ^
SyntaxError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
invalid syntax

All tests passed
Apply makePlural to the base RDD(1c)
------------------------------------
All tests passed
Pass a lambda function to map (1d)
----------------------------------
All tests passed
Length of each word (1e)
------------------------
All tests passed
Pair RDDs (1f)
--------------
All tests passed
groupByKey() approach (2a)
--------------------------
All tests passed
Use groupByKey() to obtain the counts (2b)
------------------------------------------
All tests passed
Counting using reduceByKey (2c)
-------------------------------
All tests passed
All together (2d)
-----------------
All tests passed
Unique words (3a)
-----------------
All tests passed
Mean using reduce (3b)
----------------------
All tests passed
wordCount function (4a)
-----------------------
All tests passed
Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'removePunctuation' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'shakespeareWordCount' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'shakeWordCount' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'top15WordsAndCounts' is not defined

-- 12 cases passed (75.0%) --Most of this was part of the skeleton stuff, so not my code at all. Like I say it works locally, what gives. Hi,

I am having some issues running first code of the second lab.

When attempting to run it. error says
ImportError                               Traceback (most recent call last)
<ipython-input-3-0940d9fe8819> in <module>()
      2 import datetime
      3 
----> 4 from pyspark.sql import Row
      5 
      6 month_map = {'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,

ImportError: No module named pyspark.sql

 

Do you have any idea why this may happen??

Thanks in advance,
Alvaro I want to download this file to my local box. Just so I can play with it with a smaller sample of it.
However, I tried to download it from the data folder. it's a html file.

Any hint would be helpful.

Thanks, Hi I am looking at the Tutorial ob paragraph [30] and it seems to be that the syntax for 
reduceByKey is 
print pairRDD.reduceByKey(add).collect()I am applying the same logic to lab1 exercise 
in paragraph [710   and I get the following error:

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-71-b33a21971056> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
      2 # Note that reduceByKey takes in a function that accepts two values and returns a single value
----> 3 wordCounts = wordPairs.reduceByKey(add).collect()
      4 print wordCounts.collect()

NameError: name 'add' is not defined
Any ideas what I am doing wrong.       'add' is not a variable so why should I have to define it? 



 Can someone provide hints for solving 3a for lab2? Stuck on it for quite a while. (Logic issue) Is it possible to cache/persist already-materialized RDDs in memory across jobs? Meaning, I want to keep the data in memory have job 1 end, then access the RDD (by its id?) in jobs 2,3,4 for example. It was my understanding that all RDDs are GC'd at job end. If it's already in memory, I don't want to persist in to disk just to read it again.

Cheers and thanks, Bob In lab 2 1(a) with function ' parse_apache_time(s)' what are we trying to acheive?
Here's the code:

def parse_apache_time(s):    """ Convert Apache time format into a Python datetime object    Args:        s (str): date and time in Apache time format    Returns:        datetime: datetime object (ignore timezone for now)    """    return datetime.datetime(int(s[7:11]),                             month_map[s[3:6]],                             int(s[0:2]),                             int(s[12:14]),                             int(s[15:17]),                             int(s[18:20])) Hello !
Is there any editing that we need to do on the first three groupings of code? I run 1b and this is the error that I get. Any help is appreciated.. 

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-5-8787f5be1803> in <module>()
      4 APACHE_ACCESS_LOG_PATTERN = '^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+)\s*(\S*)" (\d{3}) (\S+)'
      5 
----> 6 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-3-8d8d6629d991> in parseLogs()
     22                    .filter(lambda s: s[1] == 0)
     23                    .map(lambda s: s[0]))
---> 24     failed_logs_count = failed_logs.count()
     25     if failed_logs_count > 0:
     26         print 'Number of invalid logline: %d' % failed_logs.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 2, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-1-52e46b262bf4>", line 34, in parseApacheLogLine
NameError: global name 'r' is not defined

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

 Hi,

  I am getting this error - Any clue why?
ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused

It appears to go away after i restarted the VM, I got a response about invalid log lines when ran. On second run of the same cell above error appears again.
Why is Py4J not able to connect ?-----------------------I restarted the notebook again, the error vanished.



 Lab 2, Part 3a has the following assert statement.


Test.assertEquals(topTenErrURLs, [(u'/images/NASA-logosmall.gif', 8761), (u'/images/KSC-logosmall.gif', 7236), (u'/images/MOSAIC-logosmall.gif', 5197), (u'/images/USA-logosmall.gif', 5157), (u'/images/W|ORLD-logosmall.gif', 5020), (u'/images/ksclogo-medium.gif', 4728), (u'/history/apollo/images/apollo-logo1.gif', 2907), (u'/images/launch-logo.gif', 2811), (u'/', 2199), (u'/images/ksclogosmall.gif', 1622)], 'incorrect Top Ten failed URLs (topTenErrURLs)')

u'/images/W|ORLD-logosmall.gif'

Edit: This appears to be an errant character that I somehow entered. I checked the source and didn't see the issue.

Should be

u'/images/WORLD-logosmall.gif'

Correct? Notice the additional "|". 
In [8]:
















wordsRDD = sc.parallelize(wordsList, 4)
wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']
pluralRDD = wordsRDD.map(lambda word: [word, word+s])
# Print out the type of wordsRDD
print type(wordsRDD)
​
​

















---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-8-3a2b4aaf115b> in <module>()
----> 1 wordsRDD = sc.parallelize(wordsList, 4)
      2 wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']
      3 pluralRDD = wordsRDD.map(lambda word: [word, word+s])
      4 # Print out the type of wordsRDD
      5 print type(wordsRDD)

NameError: name 'wordsList' is not defined

need help!



 I have problems with switching to verified track.
I checked in all browsers where I could but nothing happens when I press the Government-Issued Photo ID or the Webcam buttons. All browsers are up-to-date, Chrome, Firefox, Safari. I tried on a Mac as well as on a Windows 7 PC. Both have web camera installed.
The deadline is closing and I would gladly pay for the verified track, but I cannot. This is a bit annoying. I am from Hungary, but that should not be an issue right?
Please help if you can.

Thanks,
Daniel Hello all,

Recently me and my friend (Saurabh) has posted an article on Introduction to Big Data with Apache Spark (Part-1) on http://rideondata.com/ based on the lecture notes of Week-1 and Week-2.

Wanted to share. It might be helpful for the students.

Thanks, Spark RDDs supports leftOuterJoin() and rightOuterJoin() but no left inner join. Left inner join is very common in traditional relational databases. Spark could support it as a matter of completeness...

Is there any particular reason why left inner join was left out of Spark? I just started a day ago and installed the software. However I had to re-install it. But now it doesn't function anymore an d the deadline for LAb 1 is there.Is there still some possibility to have it scored? Dear Anthony and others,

Is there are good report or paper you can recommend that does a survey of all in-memory streaming analytics platforms that are out there? Appreciate your input.

Cheers
Ram def parseLogs():
    """ Read and parse log file """
    parsed_logs = (sc
                   .textFile(logFile)
                   .map(parseApacheLogLine)
                   .cache())

    access_logs = (parsed_logs
                   .filter(lambda s: s[1] == 1)
                   .map(lambda s: s[0])
                   .cache())

    failed_logs = (parsed_logs
                   .filter(lambda s: s[1] == 0)
                   .map(lambda s: s[0]))

Looking at this code from part 1 of the lab, I understand how the parsed_logs is created (it is an an RDD containing items (Rows) with information corresponding to each part of the log entry. I do not understand how access_logs is created... specifically, I do not understand what the accessing of an item at index one of item the item "s" in the expression "lambda s: s[1] == 1" does... what item is accessed at index 0 and 1 of "s"? and what do the values they contain mean?

Because the parsed_logs is an RDD of Rows, I thought that you can access each item in a Row by using the notation rowObj.varToAccess, so the square bracket notation is also throwing me off.

Please help, and thanks in advance! Do we get half marks or is it all or none? 
 Hello, could someone give me some hints on what to fix?
running the notebook locally passes every test...

Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 27, in 
NameError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
global name 'string' is not defined

All tests passed
Apply makePlural to the base RDD(1c)
------------------------------------
All tests passed
Pass a lambda function to map (1d)
----------------------------------
All tests passed
Length of each word (1e)
------------------------
All tests passed
Pair RDDs (1f)
--------------
All tests passed
groupByKey() approach (2a)
--------------------------
All tests passed
Use groupByKey() to obtain the counts (2b)
------------------------------------------
All tests passed
Counting using reduceByKey (2c)
-------------------------------
All tests passed
All together (2d)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
TypeError: 'PipelinedRDD' object is not iterable

Unique words (3a)
-----------------
All tests passed
Mean using reduce (3b)
----------------------
All tests passed
wordCount function (4a)
-----------------------
All tests passed
Capitalization and punctuation (4b)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "", line 33, in removePunctuation
NameError: global name 'string' is not defined

Words from lines (4d)
---------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'shakespeareWordCount' is not defined

Remove empty elements (4e)
--------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'shakeWordCount' is not defined

Count the words (4f)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'top15WordsAndCounts' is not defined

-- 11 cases passed (68.0%) --
Thanks In
failed_logs_count = failed_logs.count()
    if failed_logs_count > 0:
        print 'Number of invalid logline: %d' % failed_logs.count()
what is the point of invoking in the modulo operation the count() method rather than using the failed_logs_count variable? As the code is written, there appears to be no reason for calculating failed_logs_count in the first place. When I executed Lab 2: 1b I received the following error messages:

---------------------------------------------------------------------------NameError                                 Traceback (most recent call last)<ipython-input-6-8d8d6629d991> in <module>()     32      33 ---> 34 parsed_logs, access_logs, failed_logs = parseLogs()<ipython-input-6-8d8d6629d991> in parseLogs()      9 def parseLogs():     10     """ Read and parse log file """---> 11     parsed_logs = (sc     12                    .textFile(logFile)     13                    .map(parseApacheLogLine)NameError: global name 'sc' is not defined
I did not modify or add or subtract anything from the code. I did execute both cells in 1a. What are some existing or emerging recommendation systems readily available preferably as open source? I have not yet figured out how to do indentation in Jupyter / iPython. 

Sometimes it works and sometimes not. 

I setup  indentation looks like it is in another cell that works, but it doesn't work in another and get
IndentationError: unexpected indent

What is the proper technique for indentation in Jupyter / iPython?

Should I just do line break and then use tab key (once, twice, three times?)Or should I use space key and do 4 spaces, or 8 or 12 ?




 As in the summary, I work during the weekday and don't have much time to do the lab (lost 20% of the mark for the first lab by submitting late), so it would be very helpful for me (or maybe some others) to change the deadline of the submission to Sunday.

Not sure if this is possible. In [47]:






# TODO: Replace <FILL IN> with appropriate code
wordCountsGrouped = wordsGrouped.<FILL IN>
print wordCountsGrouped.collect()




 Hi I am seriously stuck on this one. 
Are we supposed to write a code which takes each word and seperates it into a different line and then count the lines? 

are we supposed to use the logic on page 25/43 in lecture 4 handout? 
lines  =  
sc.textFile

print  lines.count() Really not sure what to do here. If someone can point me at the riight page at one of the notes or tutorial I would really appreciate it.  Suppose I have a RDD with tuples (1,google) (1,yahoo)(2,edx).


1) I want to groupbykey, I know that groupByKey() works, but how can I do it with reduceByKey() without changing RDD

2) After applying groupByKey() on above RDD ,Does the resulting RDD have (key,listofvalues) or (key,Iterabletolist)

3) if I want to apply map transformation on my RDD what is the correct input format i,ii or both correct
    i)  RDD.map(lambda a : (a[0],a[1]))
    ii) RDD.map(lambda (k,v) : (k,v))

4) I want to execute map transformation without further doing anything. i mean 

RDD.map(func) is the transformation,but it's not executed yet until i call some action and I don't want to do anything in my action except calling it for the purpose of executing map(). 
i know collect() is a way to do it , but not memory efficient

5) What is the best way to print RDD and look at its contents. Lack of debugging skills is making it difficult for me

Any help is appreciated

Thanks! Since groupByKey() has expensive shuffles, my question is: is it possible to solve (3c) without it?

I may be wrong but I think that every host value needs to be collected at a key-specific reduce stage so that a set() operation can remove duplicates before we count the final number of unique hosts - this would indicate that shuffling is necessary.

The best I can think of is to lessen the shuffling load (but not eliminate it) using maybe the combineByKey() operation so that map stage worker nodes can remove host duplicates as much as possible before the expensive shuffle.

Any better approaches? And in general, what are the indications that a problem cannot avoid an expensive groupByKey() shuffle?



 Hello Instructor,
it would be better to specify exact time like before 11:59 pm or 23:59 pm. It is really confusing that 3 day grace period with 00:00. I really thought 3 grace period means by 6/15 11:59 pm. I lost 20 points due to this confusing.

Thanks,
Sambi What should I do? I've tried twice, restart the computer, but cannot establish connection with vagrant. Help is highly appreciated.

Thanks, I completed the lab with all test passing in Databricks cloud. However, when I submit the .py to the grader all tests fail with
NameError: name 'Test' is not defined

Anybody else run into this? Hi there,

   I'm getting a dailyHostCount of 3186 for day 8. What I did to build dayToHostPairTuple was to first create an RDD with the key being the host and the value being the day, then I ReducedByKey simply discarding one of the values, then I did another map to revert key with value. GroupingByKey is getting that count for day 8.. Any clues ?

Regards,

Renato Silva Hi everyone,

I generated my dayHostCount by using groupByKey() and then doing map(lambda (a,b):(a,len(b)) 

and when I print out the elements the list looks great but when I try to use sorted() to order it I get the error.

TypeError: 'PipelinedRDD' object is not iterable
It's probably something simple but I've been working on it for way too long.

Thanks for your help def parseApacheLogLine(logline):    """ Parse a line in the Apache Common Log format    Args:        logline (str): a line of text in the Apache Common Log format    Returns:        tuple: either a dictionary containing the parts of the Apache Access Log and 1,               or the original invalid log line and 0    """    match = re.search(APACHE_ACCESS_LOG_PATTERN, logline)    if match is None:        return (logline, 0)    size_field = match.group(9)    if size_field == '-':        size = long(0)    else:        size = long(match.group(9))    return (Row(        host          = match.group(1),        client_identd = match.group(2),        user_id       = match.group(3),        date_time     = parse_apache_time(match.group(4)),        method        = match.group(5),        endpoint      = match.group(6),        protocol      = match.group(7),        response_code = int(match.group(8)),        content_size  = size    ), 1)

what does the function "Row" do exactly? I am not sure why I get the error in  4c

All tests for 4 b passed

# Just run this codeimport os.pathbaseDir = os.path.join('data')inputPath = os.path.join('cs100', 'lab1', 'shakespeare.txt')fileName = os.path.join(baseDir, inputPath)
shakespeareRDD = (sc .textFile(fileName, 8) .map(removePunctuation))
print '\n'.join(shakespeareRDD .zipWithIndex() # to (line, lineNum) .map(lambda (l, num): '{0}: {1}'.format(num, l)) # to 'lineNum: line' .take(15))

------Error below
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-35-24ba676fc75c> in <module>()
      9                   .map(removePunctuation))
     10 
---> 11 print '\n'.join(shakespeareRDD
     12                 .zipWithIndex()  # to (line, lineNum)
     13                 .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in zipWithIndex(self)
   1986         starts = [0]
   1987         if self.getNumPartitions() > 1:
-> 1988             nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
   1989             for i in range(len(nums) - 1):
   1990                 starts.append(starts[-1] + nums[i])

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 102, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <lambda>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <genexpr>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "<ipython-input-28-f75308094bb8>", line 17, in removePunctuation
TypeError: descriptor 'lower' requires a 'str' object but received a 'unicode'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

 Hi,  beyond the labs, how to use this course's vagrant virtual box environment to do my own big data problem?

Like how to upload/store/include my big dataset, how to create a sc variable.

 why is the lambda written as
    hostMoreThan10 = hostSum.filter(lambda s: s[1] > 10)
as opposed to earlier versions with 2 parameters
  hostMoreThan10 = hostSum.filter(lambda (x,y): y > 10)
 Hello all,

I have a quick question regarding the size of RDD cached in memory.
I read in a text file with size 60 MB and cached in a memory. But if I check the Spark web UI to see how much memory Spark used to store that RDD, it shows that Spark used ~600 MB. And I was wondering why there is so much discrepancy between the size of the actual file and the size of RDD of the file cached in memory.

Here is the brief explanation of what I did for the experiment:

sc.textFile(input_file)
    .repartition(12)
    .map(line => line.reverse)
    .persist(StorageLevel.MEMORY_ONLY)
    .saveAsTextFile(output_dir)

input_file contains words and each ward is separated by newline.


Thank you in advance. I got a RDD contains ((day, host),1) and reduce it and got the unique pair (day, 1) and reduce again to get the final unique count result, but I fount that I got almost right result but there are still some problems in my result eg. 21st day`s count.

Unique hosts per day: [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4404), (9, 4317), (10, 4522), (11, 4346), (12, 2863), (13, 2650), (14, 4454), (15, 4213), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2559), (21, 4133), (22, 4456)] I got the RDD of ((day,host), 1), then reduce it and got another RDD(day,1), and reduce it again to got my final unique count.But I fount there some problems in my result eg. 21st day`s count. The following is my result:

Unique hosts per day: [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4404), (9, 4317), (10, 4522), (11, 4346), (12, 2863), (13, 2650), (14, 4454), (15, 4213), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2559), (21, 4133), (22, 4456)] The Course Info page says:

The production of this course would not have been possible without the generous contribution of Databricks

I was wondering what has Databricks contributed towards this course.

I read Databricks is a company founded by the creators of Apache Spark, that aims to help clients with cloud-based big data processing using Spark.

So, its basically a company that lends server for processing big data. Right? But I was unclear how they have helped towards this course. I know the code that we use in the labs works on our own machine so how are they helping?

Of couse I am not taking an hit on Databricks by claiming they have contributed nothing. I just want to know how they have contributed.

Thank You I have an old machine with only 3GB of memory. I use Google Chrome as my browser and normally run it with seven or eight windows open, but for doing the labs in this course, when I'd like to run the VM and also have windows open to Jupyter and the lab's instructions, I figured it would be wise to close all except those two windows. Was this wise or unnecessary? How much memory does an open Chrome window normally use? I didn`t know what`s wrong with my logic. I can`t get the right answer.dayToHostPairTuple = access_logs.map(lambda log: ((int(str(log.date_time)[8:10]), log.host), 1))dayGroupedHosts = dayToHostPairTuple.reduceByKey(lambda x,y: x+y)dayHostCount = dayGroupedHosts.map(lambda x: (x[0][0], 1)).reduceByKey(lambda x,y: x+y)Unique hosts per day: [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4404), (9, 4317), (10, 4522), (11, 4346), (12, 2863), (13, 2650), (14, 4454), (15, 4213), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2559), (21, 4133), (22, 4456)] how can I test my code on small data set ?
each of the operations in 3c onwards is taking ages before I find a silly mistake in my code.

thanks Is there a way to start, save the lab and finish it another day?  but everytime I upload it it claims that its a ipython notebook file. How do I change In the exercise 3a of Lab2 I do:

1) filter by status > 200
2) create tuples with (endpoint, 1)
3) reduce with a + b

I get 12279 items, while the expectation is 7689.
Any mistake in my steps? thanks Hello everyone,

I would like to ask a question about regular expression. I check in Pythex the following regular expression 

127 -

With the pattern


^(\S+)  (\S+)

The result is

No matches

However the pattern 

^(\S+) 

The result is

127 -


Why the expression regular in lab 3 start with

^(\S) (\S+) (\S+)

The format of valid line is

127.0.0.1 - -

I think that with ^(\S) is enough

\S means  not whitespace character

Thanks in advance

Carlota Vina To Whom It May Concern

I submitted lab 1 three hours before the grace period was over.

However, when I checked my marks today it was marked as a late assignment.

I am wondering where I went wrong with the submission.

Could somebody clear up my confusion?

Thanks 
Looks like its already corrected @ the bottom of the lecture.

This slide needs to be corrected. I think it was created by cut and paste but forgotten to be edited fully.
 Now I feel although I can make all lab tests being passed, my solution may not be the best way.

Are we going to get lab explanation after lab deadline or after course end? I want compare my solution to standard solution (Or maybe instructor's explanation) for better understanding of labs.

If not, can someone provide me your way of improve your lab experience?

Thank you for your suggestion! One of the instructions for lab 2 is the following:

If you have any running notebooks they SHOULD BE shutdown.  Only ONE notebook should be run at a time.  Running notebooks have a green icon to the left of the notebook name and green text to the right of the screen that says "Running".  Shutdown running notebooks by clicking the checkbox next to the notebook and then clicking the orange "Shutdown" button.

What's the reasoning behind this? Is it purely for performance purposes or is there more to it? I have submitted lab1 late. after the penalty I still did well(80%), but I am not seeing the results posted on the progress section.
when would they get reflected ?  I would like to request the answer for the wordcount lab. Where do I find it?
Thanks Can the course staff please relax all the deadlines to the last day of the class? I maybe taking a two weeks vacation. This was planned before I joined the class. It's all auto graded, so it does not really matter as long as we all turn in by the last day of the class. This morning I found an email from the edX staff stating that something had gone wrong with Lab1 and that everyone who had submitted before Fri <specific time> should resubmit. So I did, and was rewarded with the 20% penalty for being late. My first submission had earned me the full 100 points.

Only then did I notice that the warning email was from another edX course I'm also taking. ;-(

So, it's entirely my own fault of course, but is there anything that can be done to restore the original result?

Thanks, Fred Tried for more than a day,but doesn't work.Please let me know if you find anything wrong

dayToHostPairTuple = access_logs.map(lambda log:(log.date_time.date,log.host)).distinct()

dayGroupedHosts = dayToHostPairTuple.groupByKey()

dayHostCount = dayGroupedHosts.mapValues(len)

dailyHosts = dayHostCount.sortByKey()

Advance thanks Hi - I was watching a streaming live big data event on Spark today in San Francisco, and there was some mention how Tachyon and Spark was used to increase a Hadoop query at Baidu from 1200 sec to 10 sec (300 sec with Spark only i.e. without Tachyon.)  Are you going to cover/introduce Tachyon in this mooc?

Can I integrate my Tachyon with my Spark VM after the class? 
......
topTenErrURLs = endpointSum.takeOrdered(10, key=lambda (a, b):-b)
print 'Top Ten failed URLs: %s' % topTenErrURLs
got this:
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-20-caf0f93337dd> in <module>()
......
---> 12 topTenErrURLs = endpointSum.takeOrdered(10, key=lambda (a, b):-b)
     13 print 'Top Ten failed URLs: %s' % topTenErrURLs 
......
endpointSum is the tuples of endpoints and their counts, right? For the below code i get a runtime error
dayToHostPairTuple = <EDIT>
dayGroupedHosts = <EDIT>
                                                 

dailyHosts = dayGroupedHosts.<EDIT>

#dailyHosts = dayHostCount
dailyHostsList = dailyHosts.take(30)
print 'Unique hosts per day: %s' % dailyHostsList
The run time error is:
RuntimeError: maximum recursion depth exceeded while pickling an object
 Dear all:

I've been able to successfully run the virtual machine (on win 7). When I go to http://localhost:8001, I will see the "juyper" page, and I can upload the Python code. However, nothing can execute, and I am faced with this error message:

A connection to the notebook server could not be established. The notebook will continue trying to reconnect, but until it does, you will NOT be able to run code. Check your network connection or notebook server configuration.

I have tried different ports, but with no luck. I am accessing the localhost from Internet Explorer.

Thanks!
-Nick This is my current setting. Is this good for optimal performance?


How do I decide the number of engines to use?
Also I noticed that this wasn't set when I booted the VM with `vagrant up` and my ipython notebook refused to execute till I set values here and started the clusters.. but I don't remember doing that for Lab 1. Is that weird?

EDIT: I know how to change the settings, don't know how to decide what number to use though. dayToHostPairTuple = access_logs.map(lambda log: (log.host,log.day),1)
dayGroupedHosts = dayToHostPairTuple.reduceByKey(lambda a,b:a+b)   # wrong heredayHostCount = dayGroupedHosts.map(lambda x: (x[0][0], 1)).reduceByKey(lambda x,y: x+y)

dailyHosts = (dayHostCount.sortByKey())     #wrong again I am afraid

dailyHostsList = dailyHosts.take(30)
print 'Unique hosts per day: %s' % dailyHostsList
---------------------------------------------------------------------------
Py4JError                                 Traceback (most recent call last)
<ipython-input-21-afc1961e7b39> in <module>()
      3 dayToHostPairTuple = access_logs.map(lambda log: (log.host,log.day),1)
      4 
----> 5 print dayToHostPairTuple.collect()
      6 
      7 dayGroupedHosts = dayToHostPairTuple.reduceByKey(lambda a,b:a+b)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _jrdd(self)
   2291                                              env, includes, self.preservesPartitioning,
   2292                                              self.ctx.pythonExec,
-> 2293                                              bvars, self.ctx._javaAccumulator)
   2294         self._jrdd_val = python_rdd.asJavaRDD()
   2295 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    699         answer = self._gateway_client.send_command(command)
    700         return_value = get_return_value(answer, self._gateway_client, None,
--> 701                 self._fqn)
    702 
    703         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    302                 raise Py4JError(
    303                     'An error occurred while calling {0}{1}{2}. Trace:\n{3}\n'.
--> 304                     format(target_id, '.', name, value))
    305         else:
    306             raise Py4JError(

Py4JError: An error occurred while calling None.org.apache.spark.api.python.PythonRDD. Trace:
py4j.Py4JException: Constructor org.apache.spark.api.python.PythonRDD([class org.apache.spark.api.python.PythonRDD, class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.Integer, class java.lang.String, class java.util.ArrayList, class org.apache.spark.Accumulator]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:184)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:202)
	at py4j.Gateway.invoke(Gateway.java:213)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)

 
 Hi 
I have set up the vagrant and virtualbox on a remote ubuntu server. The intention was to allocate more CPUs so that spark can actually do the stuff in parallel. Can someone help me with it. I think I need the following information

1. How to increase the number of cores/ram for the virtual box via command line. 
2. How to let sparkcontext know that I want to use more than 1 CPU. (Via the ipython Notebooks) 

Thanks 
Vikram Garg Can the course staff please release all the labs at once?? Some of use are taking vacations for longer than a week but don't want to miss the dealine as a result. Some Lab 2 tasks went really slow. Here is how to speed them up, if you have a modern CPU:

- VM must be in shutdown state
- Open "Oracle VM VirtualBox" GUI App
- Increase nr. of processors available to the VM

I increased RAM a bit (I don't think that is even necessary but I have 16GB of it so I don't care if the VM gets a little more) and the nr. of CPUs from 1 to 4 (that's the big change) and saw a big increase in speed in the iNotebook spark tasks.

I have not really looked and confirmed that the tasks are using the now 4 CPUs, I went with the clear and immediately obvious speed increase I saw "by feel". I tried to print succeed logs in question 1b:

for line in access_logs.take (10):
        print 'valid logline: %s' % line

But I met the error:

29 
     30     for line in access_logs.take (10):
---> 31         print 'valid logline: %s' % line
     32 
     33     print 'Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (parsed_logs.count(), access_logs.count(), failed_logs.count())

TypeError: not all arguments converted during string formatting

How could I fix this? Found this tool most useful.  Figure it may come in handy for others here as well. In counting the unique hosts, I use the function
len(uniqueHosts)
But the instructor seemed to look for another way, since his predefined code is: 
uniqueHostCount = uniqueHosts.
Anyone satisfy him, please share your way Hello all,
I was able to setup sparkR on windows 7 OS successfully. Please note its a standalone installation. I have posted the steps here on my blog (http://ow.ly/One4v). If anyone is interested, do take a look. Post a comment if you run into any problems, I will be happy to help.

Cheers
Ashish While doing 3e on lab2, I seem to be running out of memory.. how do I go about debugging this problem?

Will increasing the number of engines in the clusters help? What we can find from this chart? I understand the meaning of 2f, which help us or web administrators to find what contents are popular, how to cache those contents for better performance etc.., but I don't find good reason to just plot endpoints without special order in X-axis.

Any comments will be appreciated. Something went wrong while I was trying to do Lab 2 and my whole machine became unresponsive, with very slow responses from Chrome pages and other programs running on my machine. I tried to back out of everything, reboot my machine, and try again, but when I gave a "vagrant halt" command it said it couldn't because a process, pyspark, was using the VM. I tried to reach Jupyter and was told that the webpage wasn't available. A later "vagrant halt" command gave no response -- no error message -- so maybe I succeeded in shutting my VM down properly, but maybe not. So if the VM isn't closed properly -- say you just power down your real machine -- what needs to be done to put your real machine back in a safe configuration and restore your VM capacity? Hi all,

I spent quite some time solving the exercise 3(c) but still i am struggling.

Here is my code.

dayToHostPairTuple = access_logs.map(lambda log:(log.date_time.day,log.host))

# Grouping the hosts by daydayGroupedHosts = dayToHostPairTuple.groupByKey()


Thanks I modified the code a bit but still not through.

dayHostCount = dayGroupedHosts.map(lambda (x,y):(x,set(y)))dailyHosts = dayGroupedHosts.groupByKey().map(lambda (x,y):(x,len(y)))

dailyHostsList = dailyHosts.take(30)

When I execute this, I see the list of (1,1),(22,1),(8,1)....etc.

#Want unique hosts per day
dayHostCount = dayGroupedHosts.map(lambda (x,y):(x,y.distinct().count() ))

#dayHostCount.take(2) Here are my two quesitons:
(1) whether list object can be cached. If it can not be cached, what should I do.
(2) takeOrdered Operation need to assign a value to the first parameter.
If the number of entities in my list is less than the first parameter, does it affect my sorted result.

My process is describe as follows.
1. conduct the map operation to map data_time.day to one.
2. conduct the reduceByKey operation to sum up all days
Then I print the errDateSum. All are correct but they are not sorted.

3. Then I conduct the takeOrdered Operation and cache the errDateSum.
4. Then I assign the errDateSum to the errByDate

The system tell me I can not pass the second test. because 
AttributeError: 'list' object has no attribute 'is_cached'

 After looking up old posts i found out that the vagrant up error i am having is unique.  I need help on this as it is holding me back.
 Hello,
I've successfully completed all exercises from the second assignment but I'm stuck with exercise 4b.

Since I cannot post my code I will just explain my solution: I extracted from badRecords the endpoint field using a map function. Then I used the appropriate RDD method to remove all duplicates then I took 40 of those endpoints.

Here's the error I get
---------------------------------------------------------------------------Py4JJavaError Traceback (most recent call last)<ipython-input-68-5017cc3f5521> in <module>() 5 badUniqueEndpoints = badEndpoints.distinct() 6 ----> 7 badUniqueEndpointsPick40 = badUniqueEndpoints.take(40) 8 print '404 URLS: %s' % badUniqueEndpointsPick40/home/ubuntu/databricks/spark/python/pyspark/rdd.pyc in take(self, num) 1222  1223 p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))-> 1224 res = self.context.runJob(self, takeUpToNumLeft, p, True) 1225  1226 items += res/home/ubuntu/databricks/spark/python/pyspark/context.pyc in runJob(self, rdd, partitionFunc, partitions, allowLocal) 840 mappedRDD = rdd.mapPartitions(partitionFunc) 841 port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,--> 842 allowLocal) 843 return list(_load_from_socket(port, mappedRDD._jrdd_deserializer)) 844 /home/ubuntu/databricks/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args) 536 answer = self.gateway_client.send_command(command) 537 return_value = get_return_value(answer, self.gateway_client,--> 538 self.target_id, self.name) 539  540 for temp_arg in temp_args:/home/ubuntu/databricks/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 298 raise Py4JJavaError( 299 'An error occurred while calling {0}{1}{2}.\n'.--> 300 format(target_id, '.', name), value) 301 else: 302 raise Py4JError(Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 636.0 failed 4 times, most recent failure: Lost task 0.3 in stage 636.0 (TID 1016, ip-10-74-184-155.us-west-2.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last): File "/home/ubuntu/databricks/spark/python/pyspark/worker.py", line 101, in main process() File "/home/ubuntu/databricks/spark/python/pyspark/worker.py", line 96, in process serializer.dump_stream(func(split_index, iterator), outfile) File "/home/ubuntu/databricks/spark/python/pyspark/rdd.py", line 2250, in pipeline_func return func(split, prev_func(split, iterator)) File "/home/ubuntu/databricks/spark/python/pyspark/rdd.py", line 2250, in pipeline_func return func(split, prev_func(split, iterator)) File "/home/ubuntu/databricks/spark/python/pyspark/rdd.py", line 282, in func return f(iterator) File "/home/ubuntu/databricks/spark/python/pyspark/rdd.py", line 1705, in combineLocally merger.mergeValues(iterator) File "/home/ubuntu/databricks/spark/python/pyspark/shuffle.py", line 252, in mergeValues for k, v in iterator: File "<ipython-input-68-5017cc3f5521>", line 3, in <lambda>TypeError: 'unicode' object is not callable at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
 at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:64) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
 at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
 at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
 at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
 at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
 at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
 at scala.Option.foreach(Option.scala:236)
 at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
 at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
 at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
 at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) 


I'm using DataBricks cloud but, obviously, that's not the problem :)

Any hints would be much appreciated.


-- Update

My bad ... I actually tried to invoke endpoint as a method ... so stupid :)
 Just wondering if there are supplementary materials available for the concepts since, no offense to the instructors, the videos for most of the time just read out what's printed in the slides. For someone with a different background its really difficult to follow.

I understand that everything is "googlable", but curated materials would be awesome since the instructors knows best how to integrate deeper conceptual discussions with Spark. 
 not200 = access_logs.map(lambda log:log.response_code!=200).cache()print '%s'%not200.count()
I got 1043177.. Which is the total number of the log.. Is there anything wrong with the lambda function? In one of the first lecture videos it was mentioned that in big data, data is considered "cheap", so losing something like a single log entry out of billions would not be a big deal.

Also, in lecture 6, it is mentioned that for large databases usually have different demands regarding accuracy, consistency etc, and that some of the largest databases such as the yahoo click data do not care much about accuracy and consistency.
What if I have a use case where I have a lot of data, but it is still considered "expensive", meaning that I care a lot about accuracy and consistency?Is spark in particular, and the big data ecosystem in general (HDFS etc.) still considered a viable choice for this use case, or would I have to store the data offline and just transfer it to HDFS for analysis? Hello,
I've successfully completed lab2 but I had to submit two extra times in order to get 100% score. The autograder keep failing on optional exercises which I haven't completed. I think it should ignore optional exercises when they're incomplete.

Out of curiosity where's the documentation for databricks cloud plot? I'd like to complete those optional exercises in my spare time.

Have a nice day. Hi all,

So far I like the course very much. One thing that I think could be improved a bit is the lab download and submission process.I think it would be nice if the web interface of the VM would automatically download new lab worksheets, and have a button to submit the filled-out sheet to the autograder.

In the scala and reactive courses from EPFL, uploading to the autograder was automatic. You just had to configure credentials once, and then just execute a submit command.

"Downloading" from the virtual machine and then uploading to the autograder is a bit convoluted. I realise that this won't be possible for this iteration of the course, but maybe for the next time?

Cheers,

Rüdiger regex passed the (1c) and topten counted number but its wrong.
any hint , please.
 what's wrong with this code:

sortedByDay = allRequest.join(uniqueHosts).sortByKey()
avgDailyReqPerHost = (sortedByDay.map(lambda x:(x[0][0],x[1][0]/x[1][1]))).cache()
avgDailyReqPerHostList = avgDailyReqPerHost.take(30)

it throws error like:
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-95-a82f75be5878>", line 13, in <lambda>
TypeError: 'int' object has no attribute '__getitem__'And if I return a list use avgDailyReqPerHost = (sortedByDay.map(lambda x:x[1][0]/x[1][1])).cache()it is correct but it didn't map to the day. How to correct this? 


(1f) Pair RDDs  The next step in writing our word counting program is to create a new type of RDD, called a pair RDD. A pair RDD is an RDD where each element is a pair tuple (k, v) where k is the key and v is the value. In this example, we will create a pair consisting of ('<word>', 1) for each word element in the RDD. We can create the pair RDD using the map() transformation with a lambda() function to create a new RDD.




In [10]:
















# TODO: Replace <FILL IN> with appropriate code
wordPairs = wordsRDD.<Please do not post solutions It breaks the honor code>
print wordPairs.collect()

















[('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]








In [11]:
















# TEST Pair RDDs (1f)
Test.assertEquals(wordPairs.collect(),
                  [('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)],
                  'incorrect value for wordPairs')

















---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-11-cab99c06e7af> in <module>()
      1 # TEST Pair RDDs (1f)
----> 2 Test.assertEquals(wordPairs.collect(),
      3                   [('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)],
      4                   'incorrect value for wordPairs')

NameError: name 'Test' is not defined

 




 Hello Instructor,

I completed my lab2 exercise and it passed all tests. But the autograder shows errors in many places. Let me know the mistake. Thanks in advance.


Data cleaning (1c)------------------Traceback (most recent call last):  File "", line 27, in NameError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTIONname 'failed_logs' is not definedTraceback (most recent call last):  File "", line 1, in NameError: name 'failed_logs' is not definedTop ten error endpoints (3a)----------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'endpointSum' is not definedNumber of unique hosts (3b)---------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'uniqueHostCount' is not definedNumber of unique daily hosts (3c)---------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'dailyHosts' is not definedVisualizing unique daily hosts (3d)-----------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'daysWithHosts' is not definedAverage number of daily requests per hosts (3e)-----------------------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'avgDailyReqPerHostList' is not definedAverage Daily Requests per Unique Host (3f)-------------------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'daysWithAvg' is not definedCounting 404 (4a)-----------------Traceback (most recent call last):  File "", line 1, in NameError: name 'badRecords' is not definedListing 404 records (4b)------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'badUniqueEndpointsPick40' is not definedTop twenty 404 URLs (4c)------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'badEndpointsTop20' is not definedTop twenty-five 404 response code hosts (4d)--------------------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'errHostsTop25' is not defined404 response codes per day (4e)-------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'errByDate' is not definedVisualizing the 404 Response Codes by Day (4f)----------------------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'daysWithErrors404' is not definedFive dates for 404 requests (4g)--------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'topErrDate' is not definedHourly 404 response codes (4h)------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'errHourList' is not definedVisualizing the 404 Response Codes by Hour (4i)-----------------------------------------------Traceback (most recent call last):  File "", line 1, in NameError: name 'hoursWithErrors404' is not defined-- 0 cases passed (0.0%) -- I think I have the correct process for this exercise:

1) filter out lines not equal to 200
2) create a tuple pair (endpoint, 1) using map
3) reduce using a + b

but then for the final part "topTenErrURLs" I am not sure if I need to use collect(), take() or takeOrdered(). My computer is taking over 20 minutes to run this code (and it still is not finished yet) so I think there is something wrong with the code, maybe it is stuck in some erroneous loop. Can anyone help? Thank you **(3e) Exercise: Average Number of Daily Requests per Hosts**
Next, let's determine the average number of requests on a day-by-day basis.


We'd like a list by increasing day of the month and the associated average number of requests per host for that day.

To compute the average number of requests per host, get the total number of request across all hosts and divide that by the number of unique hosts.

The test says:

Test.assertEquals(avgDailyReqPerHostList, [(1, 13), (3, 12), (4, 14), (5, 12), (6, 12), (7, 13), (8, 13), (9, 14), (10, 13), (11, 14), (12, 13), (13, 13), (14, 13), (15, 13), (16, 13), (17, 13), (18, 13), (19, 12), (20, 12), (21, 13), (22, 12)], 'incorrect avgDailyReqPerHostList')

How is that an "average number of daily requests per hosts"?

We get plurar, singlular, and unclear wording.

Total requests across all hosts would be this:

totalRequests = (dayAndHostTuple .map(lambda dhkv: dhkv[1]) .reduce(lambda a,b: a+b) )

But I guess that what is meant is per day.

I also had a look at the mapValues() and zip things which make sense but the problem statement is quite confusing.

Comments welcome. RE: RegEx and Computational Efficiency
Hello!

I was wondering whether there are some general guidelines on Regex regarding computational efficiency:

E.g: I have successfully parsed the log files but the regex takes 2673 steps that actually requires more computation by the VM.
So, are there are some set rules about avoiding certain expressions operators for example? 
-What is the standard practice in Spark community with regard to the above? ...I guess that can have a significant impact on large files...

Regards,
 

The Lab 2 2f isnt a cell that I have changed as it was already filled in..so what is wrong here?Top Ten Endpoints: [(u'/images/NASA-logosmall.gif', 59666), (u'/images/KSC-logosmall.gif', 50420), (u'/images/MOSAIC-logosmall.gif', 43831), (u'/images/USA-logosmall.gif', 43604), (u'/images/WORLD-logosmall.gif', 43217), (u'/images/ksclogo-medium.gif', 41267), (u'/ksc.html', 28536), (u'/history/apollo/images/apollo-logo1.gif', 26766), (u'/images/launch-logo.gif', 24742), (u'/', 20184)]






---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-48-606dbe4c203b> in <module>()
      7 
      8 print 'Top Ten Endpoints: %s' % topEndpoints
----> 9 assert topEndpoints == [(u'/images/NASA-logosmall.gif', 59737), (u'/images/KSC-logosmall.gif', 50452), (u'/images/MOSAIC-logosmall.gif', 43890), (u'/images/USA-logosmall.gif', 43664), (u'/images/WORLD-logosmall.gif', 43277), (u'/images/ksclogo-medium.gif', 41336), (u'/ksc.html', 28582), (u'/history/apollo/images/apollo-logo1.gif', 26778), (u'/images/launch-logo.gif', 24755), (u'/', 20292)], 'incorrect Top Ten Endpoints'

AssertionError: incorrect Top Ten Endpoints

 and other unixy things

http://www.grymoire.com/Unix/Regular.html #### ** (2d) All together **#### The expert version of the code performs the `map()` to pair RDD, `reduceByKey()` transformation, and `collect` in one statement.


In [150]:


This is now resolved.
















# TODO: Replace <FILL IN> with appropriate code
wordCountsCollected = (wordsRDD.map(lambda k:(k,1)).reduceByKey(len)
                       .collect())
print wordCountsCollected

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-150-5f34149965c2> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
----> 2 wordCountsCollected = (wordsRDD.map(lambda k:(k,1)).reduceByKey(len)
      3                        .collect())
      4 print wordCountsCollected

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 166.0 failed 1 times, most recent failure: Lost task 2.0 in stage 166.0 (TID 390, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1714, in _mergeCombiners
    merger.mergeCombiners(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 289, in mergeCombiners
    d[k] = comb(d[k], v) if k in d else v
TypeError: len() takes exactly one argument (2 given)

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
 









 Dear all,
Lab 2 is the most time-consuming and thought-demanding so far. As of lab 1 and zero, I could finish all on a single seat. Now, I plan to do it day by day.
So, I need to save my work space, so I can come back when I have time (you know, life). But if I turn it off in the regular way - power off -  everything is lost. Since lab 2 make use of
cache() 
to put a RDD into memory for reuse, this comfortable feature would be missing if one power off the virtual machine. 
I tried to Save State but find it a bit awkward. 
EDIT: I don't use terminal and am famaliar with the GUI Dear Databricks sponsors and instructor,

Thanks for the great instruction of the course.

I have applied for the Databricks Cloud trial for the course, but unfortunately not selected as a trial user.
I would love to hereby express my willingness to participate the DBC trial program and promise to give feedback.

Thanks in advance.
 Hi all,

Im getting trouble in this exercise. I get the following partial result:

sortedByDay = [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)]

But considering uniqueHostCount = 54507 (as considered in 3b) i cant find the right solution

Im trying to map something like (a,b) in (a, b/uniqueHostCount)

Any tips?
Thx in advance ! I finished the lab2 exercises and i was able to run them one by one and all the test passed. Now when i submit the exercise i keep "MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION". I was wondering how do i run the whole script on my local to make sure that there are no compilation errors ? This error is preventing to pass any Test.assertEquals and Test.assertTrue and getting any credit for lab2.

Please help ! I'm solving Lab 2 Exercise 3a.For top10urls why can't we use top(10) Hi I think I accidentaly made some changes to section 1c. Can someone post me should be in paragraph [8]? Thanks  Hi TAs,

     I just submitted my lab 2 to the auto grader.  It passed all the tests within the lab and it's way before deadline.  I used this web site:

https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/courseware/d1f293d0cb53466dbb5c0cd81f55b45b/920d3370060540c8b21d56f05c64bdda/

    I got this message:

Compare with hash (2a)
----------------------
All tests passed

Compare lists (2b)
------------------
All tests passed
-- 2 cases passed (100.0%)--

As for late submissions after 3 day grace period of the due date, you lose 20 points. Your final score is 80

     I don't understand what this is about.  What late submission?  Can you please help?  Thanks!
 
dayToHostPairTuple = access_logs.map(lambda log: (log.date_time.day, log.host))
def mergeValue(a, b):
    a.add(b)
    return a
def mergeCombiner(a, b):
    return a | b
dayGroupedUniqueHosts = dayToHostPairTuple.combineByKey(set, mergeValue, mergeCombiner)
Can anyone explain why code above is not producing the correct output?I will not post working code, but if you group by key and then map the values of keys over set, then we get unique hosts. But the code above should also be correct. Hi,

Really stuck on this exercise.
For the first line, "dayToHostPairTuple", do I just use a map function and ((x.date_time, x.host), 1) ?

Second line, "dayGroupedHosts", should I group by key? I don't know how to order the hosts by day.Thanks! I simply have to ctrl run it but I get this? which I believe is the way we do 3(a).Thanks for all the feed backs.

AssertionError                            Traceback (most recent call last)
 in ()
      7 
      8 print 'Top Ten Endpoints: %s' % topEndpoints
----> 9 as sert topEndpoints == [(u'/images/NASA-logosmall.gif' ...................... https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html

"DataFrames go through the Catalyst optimizer, enabling optimized execution similar to that of Spark SQL queries. As we improve the Catalyst optimizer, the engine also becomes smarter, making applications faster with each new release of Spark"

Qs1- Does this mean where ever feasible we recommend to convert RDD's to DF for better performance? 
Qs2- Are there any specific scenarios(best practices) where RDD's excel in performance against DFs

Thanks Is this where we filter out endpoints that are not 200 through a map? I am getting an error with :
"topTenErrURLs" But I simply don't understand what it is or how to resolve and if the previous values are responsible. Any clarifications would be appreciated. Thanks Colleagues: I successfully reached 3c but when I submit my entries at 3b I find that somehow the "access_logs" RDD has become undefined. I could not have reached this position if access_logs had not been created correctly. What have I done wrong?




---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-1-04f8ddbc8270> in <module>()
      4 # you ensure the variables used in the next Test section are defined (ie. endpointSum, topTenErrURLs).
      5 
----> 6 not200 = access_logs.filter(lambda log: log.response_code !=200)
      7 
      8 endpointCountPairTuple = not200.map(lambda log: (log.endpoint, 1))

NameError: name 'access_logs' is not defined




 I am getting mad each and every test case takes so much time why is everything painfully slow my laptop have never been such a slow Hello,

I don't know where is wrong on my code, the result is showing "1042846":
I did same as in lab1 "Unique count word"

Unique hosts: 1042846Which is wrong.Thanks, I keep getting the daily digest emails of this class;  I track everything I need through the piazza portal and I don't need the emails.  How do I stop it?  Tried restarting the VM several times, deleted the existing notebook and downloaded a new one , and i am running only one notebook. the cells are not giving output (tried both ctrl+enter and shift+enter).Help please? Why cant the staff just extend all the deadline to the last day of the course? What's the problem with that??? I was able to submit the first lab correctly. I followed the same steps for lab2, but the tests are not executing, for example if I don't change the '<FILL IN>' sections and try to execute the code (shift + enter), I get no result (no error message, just a * beside the executed block). Does anyone have any ideas? Thank you positing solution code is a violation of honor code.  Based on this course and other literature, data frames are substantially faster than RDDs (especially in Python). Is there a reason why they aren't the underpinning of this course instead of RDDs?

Thanks!
Nosh Hello all! 

I am working on the lab, and I am stuck in 3a... 

I understand that the process is to create tuples with code with corresponding count, reduceByKey (code) and then use takeOrdered and figure out which 10 codes are the most used.

I am trying to implement this below, but I am getting cryptic error messages.

Please help by looking at my code and error message below, and many thanks in advance!
# TODO: Replace <FILL IN> with appropriate code
# HINT: Each of these <FILL IN> below could be completed with a single transformation or action.
# You are welcome to structure your solution in a different way, so long as
# you ensure the variables used in the next Test section are defined (ie. endpointSum, topTenErrURLs).

not200 = access_logs.filter(lambda x,: x != 200)

endpointCountPairTuple = not200.map(lambda x: (x,1))

endpointSum = endpointCountPairTuple.reduceByKey(lambda x,y : x+y)

topTenErrURLs = endpointSum.takeOrdered(10,key=lambda (code,count):-count).map(lambda (code,count): code)
print 'Top Ten failed URLs: %s' % topTenErrURLs


Error message:

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-44-4111367249f5> in <module>()
     10 endpointSum = endpointCountPairTuple.reduceByKey(lambda x,y : x+y)
     11 
---> 12 topTenErrURLs = endpointSum.takeOrdered(10,key=lambda (code,count):-count).map(lambda (code,count): code)
     13 print 'Top Ten failed URLs: %s' % topTenErrURLs

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in takeOrdered(self, num, key)
   1172             return heapq.nsmallest(num, a + b, key)
   1173 
-> 1174         return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
   1175 
   1176     def take(self, num):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 170, localhost): java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:196)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:108)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
 When I tried to execute the cell code I've got the following error:
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-3-8d8d6629d991> in <module>()
     32 
     33 
---> 34 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-3-8d8d6629d991> in parseLogs()
     22                    .filter(lambda s: s[1] == 0)
     23                    .map(lambda s: s[0]))
---> 24     failed_logs_count = failed_logs.count()
     25     if failed_logs_count > 0:
     26         print 'Number of invalid logline: %d' % failed_logs.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/vagrant/lab2/data/cs100/lab2/apache.access.log.PROJECT
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:203)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:57)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:57)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1512)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:813)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:374)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745) 

Even though I get the right RDD, the tests fail. I printed the RDD and it is exactly the same as what the test needs. Any thoughts ?

Thanks I think my code is right but still getting over 1000000
not200 = access_logs.filter(lambda log:log.response_code > 200)

The instructor in one of the logfile said maybe 1c is deleting the end points. What do the endpoint stand for and should look like as I think my code is right, unless I have to filter for something else. No Errors in Spark VM but at the grader I get this:(topErrDate is not part of question 4h -> it is about the Hour ?)Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect topErrDate

Hourly 404 response codes (4h) Are the Lab1 answers available or not? The links don't work. How to make connection with Jupyter? Please help!

Thanks, Is it necessary to use the same code sample given to us . Can we change the whole code in a cell without importing any libraries ?

Actually I did it and it has passed the test. Will it face anyproblem in autograder ? (wrong course --- deleted --- I'm taking too many at once...) Hi,

  I have completed labs 1 and 2 and wanted to check if the environment can be used for some experimental exercises I wanted to try out on a 1 Gb dataset. This involves finding similar sentences in a large corpus.

  Can this be safely tried in the environment setup on my machine without impacting the existing data and parameters? I have the following

dayAndHostTuple = access_logs.<FILL IN> POSTING SOLUTION CODE IS A VIOLATION OF THE HONOR CODE
groupedByDay = dayAndHostTuple.<FILL IN>
sortedByDay = groupedByDay.<FILL IN>

avgDailyReqPerHost = ?? how to calculate this
Also please let me know if I am on the right path to solve this problem. Thanks.

 In week 3 -> Lecture 5 -> Tab 10, at the end, in text, is said that "Text files typically contain lines of ASCII or Unicode". That sentence is "almost" correct...

I'm writing this here because encodings will bite every programmer at least once ...and possibly many times. It's almost unavoidable.

Also, there's a lot of confusion about encodings... and almost no one understands them really well.

You may read this "short" post in Piazza or you may read this complete post with more details by Joel Spolsky, one of the two founders of Stack Overflow: The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!), although, here are notes about "Windows Notepad" that are not there.

First, text files (or any file) are made of bytes.

ASCII is a mapping (encoding) from one character to one byte, using the common English characters (almost all the ones you find in a normal keyboard, including things such as "!#$%&~`").

Unicode is a "standard" and an encoding, to map from each character in (almost) every language in the world to a numerical identification (a number), each of those numbers is called a "code point". As each "code point" is an arbitrary number, it may be bigger than a single byte, so, Unicode is not really a text file encoding, but rather an "abstract" encoding that works with every language and isn't restricted to bytes.

UTF-8 (stands for Unicode Transform Format ...using groups of 8 bits, a.k.a. 1 byte) is an encoding, to map from each "code point" (character) in Unicode to one or more bytes in a file.

So, the sentence "Text files typically contain lines of ASCII or Unicode" could really be something like "Text files typically contain lines of ASCII or UTF-8".

But sadly, that's not the end of the story...

There are other encodings that map Unicode to "bytes" to store them in a file, such as UTF-16 and UCS, so, UTF-8 is not the only way to find "Unicode" in a file (and Windows Notepad will confuse you, I'll explain at the end).

Also, there are other encodings that are very popular, one is "iso-8859-1", also known as "Latin 1", also known as "Europa Western", also known as "Latin", and thanks to Microsoft, also misleadingly known as "ANSI" (that's how it's called in Windows Notepad), but "ANSI" should refer just to "ASCII".

The tricky thing is that "iso-8859-1" was made to be compatible with "ASCII", but having more characters (such as spanish "ñ"). Then came UTF-8, which was made to be almost compatible with "iso-8859-1" (and being compatible with "ASCII"). So, when you see that something like "El niño feliz" ("the happy kid" in spanish) that is shown as "El niÃ±o feliz", it probably means that the text is encoded in "UTF-8" but the program is decoding it as if it was "iso-8859-1", and when you see it like "El ni�o feliz" it probably means that the file is in "iso-8859-1" but the program is decoding it as "UTF-8". You can see just the ASCII characters right, because iso-8859-1 and UTF-8 are both compatible with ASCII, but once you start having other characters you can start having those problems.

Lastly, there's another coding, based on iso-8859-1, which is called "Windows-1252", it's the same as iso-8859-1 but uses some unused bytes to encode characters such as the Euro sign (€), which are not in the normal iso-8859-1. As of Windows 7, it's still the default encoding of files in the Operating System, despite of UTF-8 being the most complete and optimal encoding. In fact, in Notepad, the encoding "ANSI" actually refers to Windows-1252, not to iso-8859-1.

Lastly (this time for real), the program Notepad in Windows drives quite some confusion, when you are saving a file you can choose an encoding, but the names of the encodings are misleading, here's what they mean:

ANSI: means "Windows-1252", a variant of iso-8859-1 (Latin-1) with characters such as the euro sign.
Unicode: means "UCS", an encoding for Unicode using 2 bytes for each character, not very efficient. Also, if you open it with a different processor you may not know which of the 2 bytes comes first and have problems decoding it (even after knowing that it's UCS).
Unicode big endian: means UCS, but inverting the order of the 2 bytes, for the reasons explained above.
UTF-8: means UTF-8 (al least one is correct).

As a rule of thumb: if you can choose, always choose UTF-8.

I'm sorry to bore you with all this, but if you are a programmer, you should know it... and most of the programmers I know don't know it.

If you wanna know more, you can read the post by Joel Spolsky that I referenced earlier. And for more details about each encoding, Wikipedia. print shakespeareWordsRDD.top(5)print shakespeareWordCount
[u'zounds', u'zounds', u'zounds', u'zeal', u'youths']
122395
 I uploaded lab 1 yesterday. I came up with zero yet when I did all of assigment except a couple of items on number 4, the answers I provided had all passed. What is going on? I know I turned this in late but even so I should get the benefit of the right answers..Please advise. for this problem, I just need to filter those response code which equals to 404, and then map it, and finally reduce it. But I got 1201 404 URLs.

what is wrong with my code:

badRecords = (access_logs .filter(lambda log: log.response_code == 404) .map(lambda log: (log.endpoint, 1)) .reduceByKey(lambda a, b : a + b).cache()) 

Thanks.  Sorry, I didn't read 1c!!!
Number of invalid logline: 108
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:43:39 -0400] "GET / HTTP/1.0 " 200 7131
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:43:57 -0400] "GET /images/ksclogo-medium.gif HTTP/1.0 " 200 5866
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:07 -0400] "GET /images/NASA-logosmall.gif HTTP/1.0 " 200 786
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:11 -0400] "GET /images/MOSAIC-logosmall.gif HTTP/1.0 " 200 363
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:13 -0400] "GET /images/USA-logosmall.gif HTTP/1.0 " 200 234
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:15 -0400] "GET /images/WORLD-logosmall.gif HTTP/1.0 " 200 669
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:31 -0400] "GET /shuttle/countdown/ HTTP/1.0 " 200 4673
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:41 -0400] "GET /shuttle/missions/sts-69/count69.gif HTTP/1.0 " 200 46053
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:45:34 -0400] "GET /images/KSC-logosmall.gif HTTP/1.0 " 200 1204
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:45:46 -0400] "GET /cgi-bin/imagemap/countdown69?293,287 HTTP/1.0 " 302 85
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:45:48 -0400] "GET /htbin/cdt_main.pl HTTP/1.0 " 200 3714
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:45:52 -0400] "GET /shuttle/countdown/images/countclock.gif HTTP/1.0 " 200 13994
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:22 -0400] "GET / HTTP/1.0 " 200 7131
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:29 -0400] "GET /images/ksclogo-medium.gif HTTP/1.0 " 200 5866
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:35 -0400] "GET /images/NASA-logosmall.gif HTTP/1.0 " 200 786
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:37 -0400] "GET /images/MOSAIC-logosmall.gif HTTP/1.0 " 200 363
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:38 -0400] "GET /images/USA-logosmall.gif HTTP/1.0 " 200 234
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:40 -0400] "GET /images/WORLD-logosmall.gif HTTP/1.0 " 200 669
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:47:41 -0400] "GET /shuttle/missions/sts-70/mission-sts-70.html HTTP/1.0 " 200 20304
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:47:48 -0400] "GET /shuttle/countdown/count.html HTTP/1.0 " 200 73231
Read 1043177 lines, successfully parsed 1043069 lines, failed to parse 108 lines In lab2 3c I just for the sake of curiosity after solving replaced .day with .month in dayToHostPairtuple . Result what I got was "Unique hosts per day: [(8, 54507)]". So does 8 here tells us that month in log file was August or something else? I'm working on Lab 2  problems in parts 3 and 4.  If  I shut down and restart, I have to run the entire suite of part 1 and 2 first to set all the variables and definitions again.  Is there a way to speed this up? In the first (2a) example,  think I understand that reduce(lambda (a, b : a + b)) will successively take the value of the current record, and add it to the next record, but what happens when the lambda function gets to the last record and there is no next record (b) to add?  I would expect an out-of-range error, but don't get one. 









# TEST Counting 404 (4a)
Test.assertEquals(badRecords.count(), 6185, 'incorrect badRecords.count()')
Test.assertTrue(badRecords.is_cached, 'incorrect badRecords.is_cached')

















1 test passed.
1 test failed. Why incorrect badRecords.is_cached returns true in debugging? I see no badrecords being cached anywhere  in my code or before???



 Initially I programmed part (3c) using reduceByKey and a lambda function. The cell required almost 2 minutes to finish. After reading some post I modified the code to use groupByKey instead. Now the cell only requieres a few seconds to finish. The whole notebook runs in less that a minute!!

I guess that the overhead of calling Pyhton for each row is the cause of the timing difference.

Would this have happened if we were using Scala or Java? This afternoon I tried to start Lab2, on a Windows machine, but I was getting no output when running cells.
I added print statements just to be sure ... but no output.

So this evening I tried the same thing on another machine (OSX now) and I'm seeing exactly the same lack of output.
If I restart the kernel, some basic cells run OK, but then subsequent cells are non-responsive.
Any idea what's going on?

Restarting the kernel and then running a cell with "2+2" outputs 4 (!).
Running the cell which reads the data outputs nothing (despite an early print of inputPath ... before the read) and returning to "2+2" no longer works ...
 Anybody have idea that how to apply operations on dictionary to calculate the average. My  result of dictionary to list is given below
{1: (2582, 33996), 3: (3222, 41387), 4: (4190, 59554), 5: (2502, 31888), 6: (2537, 32416), 7: (4106, 57355), 8: (4406, 60142), 9: (4317, 60457), 10: (4523, 61245), 11: (4346, 61242), 12: (2864, 38070), 13: (2650, 36480), 14: (4454, 59873), 15: (4214, 58845), 16: (4340, 56651), 17: (4385, 58980), 18: (4168, 56244), 19: (2550, 32092), 20: (2560, 32963), 21: (4134, 55539), 22: (4456, 57758)}
Help me out and it will be highly appreciated .
 http://www.oreilly.com/data/sparkcert I've wasted 3 attempts in trying to submit the solution. The grader reports to me errors also in your code that I haven't changed.

It always seems to me that no file has been uploaded but what I do is to select the file and press the Check button. I've tried to save before but without result. I've always tried with 2 different browsers (Chromium and Firefox). What am I doing wrong?

Just a suggestion: Could you avoid to increase the number of attempts when you intercept that an empty solution has been submitted?

This is what the grader reports to me:



Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 22, in 
NameError: name 'Test' is not defined

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Top ten error endpoints (3a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Number of unique hosts (3b)
---------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting 404 (4a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Listing 404 records (4b)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badUniqueEndpointsPick40' is not defined

Top twenty 404 URLs (4c)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Top twenty-five 404 response code hosts (4d)
--------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

404 response codes per day (4e)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Five dates for 404 requests (4g)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Hourly 404 response codes (4h)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --





 Today when I tried to do the step 1 of the lab 2
"
Start the VM - To start the VM, from a DOS prompt (Windows) or Terminal (Mac/Linux), issue the command "vagrant up", while in the custom directory created for this course."
I got the error message :
The provider 'virtualbox' that was requested to back the machine'sparkvm' is reporting that it isn't usable on this system. Thereason is shown below:
VirtualBox is complaining that the kernel module is not loaded. Pleaserun `VBoxManage --version` or open the VirtualBox GUI to see the errormessage which should contain instructions on how to fix this error.

I run vagrant on Ubuntu 14.04. It was OK before.

 Hello everyone,
I keep getting this result in 4D.

Can anybody point me in the right direction?

I'm using a split added to a flat list for a mapper and I've tried different regular expressions (replace punctuations with an empty string then trim) all yielding the same result.

This also makes my other 3/4 tests (based on count) to fail.Thank you!


[u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds']
882996 Hi, I have a big problem in execution lab2 1b with error 
UnicodeEncodeError: 'decimal' codec can't encode character u'\ufffd' in position 4: invalid decimal Unicode string
I write python script to parse file with regular expression without problem, but running lab2 1b i have a mistake, any idea?

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-4-8d8d6629d991> in <module>()
     32 
     33 
---> 34 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-4-8d8d6629d991> in parseLogs()
     22                    .filter(lambda s: s[1] == 0)
     23                    .map(lambda s: s[0]))
---> 24     failed_logs_count = failed_logs.count()
     25     if failed_logs_count > 0:
     26         print 'Number of invalid logline: %d' % failed_logs.count()

........

  vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-1-0940d9fe8819>", line 39, in parseApacheLogLine
UnicodeEncodeError: 'decimal' codec can't encode character u'\ufffd' in position 4: invalid decimal Unicode string I recall reading somewhere that Spark would be taught in Scala, but it is nowhere to be found and the only 2 courses I am aware of is this one and the follow up on the XSeries. I also thought that this course originally would not offer a honor certificate, but the latest questions have seem to indicate so, so I may have missed the change of the logistics on this and the other course. Could someone please confirm?  While trying to validate the lab2, I used the lab1 form... It erased my lab1 100% score and when I resubmitted lab1, I lost 20 points due to late submission...

The submission should block when the high score is reached... Hi,

I am sorry for asking this question which may be too basic for this class, but I like to understand things in detail.
Can anyone please help me understand the concept of cores in a machine?

Eg: When run on laptop, Spark can still run in parallel mode by using the available number of cores in your machine.

What does core here mean? Processor? Hi, I start the Spark virtual machine directly from Virtualbox, not using "vagrant up"The default setup is that 2 Gb of memory is allocated to the virtual machine.
If I simply increase the maximal allocated memory in Virtualbox to 3 Gb or 4 Gb, will Spark be able to utilize the extra memory without any additional configurations? Not sure what happened, but everything is blank when I logged in today. Anyone saw the same thing or could help? Thanks in advance I'm having issues understanding how to make a field in the log entry optional. I'm getting 1709 errors related to the records that do not have a HTTP/1.0 field.  I'm trying to understand how to make this part of the reg ex optional.

Good Entry that I can parse:
local - - [24/Oct/1994:13:41:41 -0600] "GET 1.gif HTTP/1.0" 200 1210

Bad entry that fails since I can't make the missing field optionalpipe1.nyc.pipeline.com - - [01/Aug/1995:00:12:37 -0400] "GET /history/apollo/apollo-13/apollo-13-patch-small.gif" 200 12859

I way overthought this RegEx exercise. First, GREAT labs. I appreciate the progressive pedagogy.

Like others, I was struggling a bit (from the middle of lab two), wondering whether we should be using SQL based approaches. I could envision the answer from that direction, but was drawing a blank within the straight Spark approach. And, after all, SQL was discussed in the lecture, so perhaps this was the transition point. (Next time, the instructions could be a little clearer.)

But before jumping to SQL I was trying to create a new concatenated key (that I would break apart later) when I stumbled across an oblique sentence on StackOverflow and then it hit me, right between the eyes -- a key could be a list! (Hmmm, it would have been nice if the lecture had made that point.)

All of a sudden I was in a flashback to 1976 and a data languages course that included a LISP segment. Lists within lists within lists, The horrors of CONS, CAR, and CDR. The aggravation of matching open and close parens. ["My other CAR is a CDR."] And, I understood the difference between Spark's map and flatMap at a different level.

At any rate, perhaps this AHA! will resonate for a few others.

I have to admit, I didn't like LISP -- I never quite saw what it was good for except maybe AI. But from the Spark vantage it is clear, and it demonstrates another advantage of Spark over Hadoop. Of course in 1976, with punched cards on an IBM370, it was impossible to see 2015.

Best,
Tom I used reduceByKey to get the unique keys and used count() to tally up the total, i get 54393 as my final results instead of 54507, anyone got the same issue or knows how to resolve this? While working on 2nd lab, I made a mistake in a mapper's lambda, resulting in a long running Spark job. I wasn't able to properly kill that job, so I just had to wait until it finished. It actually bailed with an error.

Thereto, I configured Spark to enable the kill job link in Spark UI:

vagrant up  #make sure your VM is runningvagrant ssh #login into your VMsudo cp /usr/local/bin/spark-1.3.1-bin-hadoop2.6/conf/spark-defaults.conf.template /usr/local/bin/spark-1.3.1-bin-hadoop2.6/conf/spark-defaults.confsudo vi /usr/local/bin/spark-1.3.1-bin-hadoop2.6/conf/spark-defaults.conf

Add the following line to the configuration file

spark.ui.killEnabled               true

Save the file

:w!:q!

Restart your VM.

exitvagrant haltvagrant up

If you run some Spark job now and head over the SparkUI http://localhost:4040/, you'll find a small kill link behind the stages on the details page of you job. If you click that link, SparkUI will ask for confirmation to kill your job.

HTH!
-wim i obtained all the hosts using a simple map function
then I used reduceByKey() function to gather same hosts,
then i used count() to do the tally, 
i got 
54393 rather than 54507. this counting error seems to carry over to 3c, where the daily count for different hosts differ, either a FEW(as in the last digit) hosts more or less than the answer suggestsAnyone got the same issues or knows how to fix it? In Lab 2 and probably in subsequent labs, executing some of the cells in our IPython notebooks on my small/old machine takes significant time, so I'd like to avoid recomputing and like to have any necessary recomputing happen while I do something else.

When an IPython workbook is saved after some of its cells are executed, is just the webpage for the workbook saved, or are all the pySpark variable bindings, including RDDs, also saved? If the workbook is saved and Jupyter and the VM are shut down, will restarting the VM and Jupyter and opening the workbook restore the variable bindings, or must they all be recomputed? My guess is recomputed.

Is it possible to tell Jupyter to execute all the cells in a  workbook through a particular cell? My guess is yes; I'll look for it in Jupyter and post a follow-up note if I find it. sry didnt realize not to post code, removed code Hi there,
while trying to clean data, I am having some troubles:

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 146, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "", line 48, in parseApacheLogLine ValueError: invalid literal for int() with base 10: 'HTTP/1.0'
I am guessing that is due to the fact that I probably have some None types

part of my regex:
that is creating the None:
 "(\S+) (\/\S+|\/) ?(\S+)?\s+?(\S+)? ?"

Can someone point me in the right direction?

thanks
RF
 Hi,

I am confused how to find the number of args map takes. e.g. in 2b) it takes 1 arg (log) while in 2c) it takes 2 args(x,y)
2b)
responseCodeToCount = (access_logs                       .map(lambda log: (log.response_code, 1))                       .reduceByKey(lambda a, b : a + b)                       .cache())
2c)
racs = responseCodeToCount.map(lambda (x, y): (float(y) / count)).collect()

#pin Seems a few people got this on lab 2. I can't continue. I have the Ubuntu VM runing on MS WIndows 7, I have 8 Gig of RAM. It worked ok I got the regex correct. 
No errors in sparkVM logs. Everything worked fine until I re executed cells.
Halted vagrant and restarted.
So How do I continue?


ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused




---------------------------------------------------------------------------
Py4JNetworkError                          Traceback (most recent call last)
<ipython-input-39-8d8d6629d991> in <module>()
     32 
     33 
---> 34 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-39-8d8d6629d991> in parseLogs()
     10     """ Read and parse log file """
     11     parsed_logs = (sc
---> 12                    .textFile(logFile)
     13                    .map(parseApacheLogLine)
     14                    .cache())

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in textFile(self, name, minPartitions, use_unicode)
    375         [u'Hello world!']
    376         """
--> 377         minPartitions = minPartitions or min(self.defaultParallelism, 2)
    378         return RDD(self._jsc.textFile(name, minPartitions), self,
    379                    UTF8Deserializer(use_unicode))

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in defaultParallelism(self)
    283         reduce tasks)
    284         """
--> 285         return self._jsc.sc().defaultParallelism()
    286 
    287     @property

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    534             END_COMMAND_PART
    535 
--> 536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
    538                 self.target_id, self.name)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in send_command(self, command, retry)
    360          the Py4J protocol.
    361         """
--> 362         connection = self._get_connection()
    363         try:
    364             response = connection.send_command(command)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in _get_connection(self)
    316             connection = self.deque.pop()
    317         except Exception:
--> 318             connection = self._create_connection()
    319         return connection
    320 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in _create_connection(self)
    323         connection = GatewayConnection(self.address, self.port,
    324                 self.auto_close, self.gateway_property)
--> 325         connection.start()
    326         return connection
    327 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in start(self)
    430                 'server'
    431             logger.exception(msg)
--> 432             raise Py4JNetworkError(msg)
    433 
    434     def close(self):

Py4JNetworkError: An error occurred while trying to connect to the Java server



 Hi
I was expecting that this course will dive into Spark internals and show how some major APIs with spark are implemented, the streaming algorithms used and all. Can the instructors comment on this ? It will be nice to study the spark source and importantly, understand the streaming algorithms within. Any chance this will be taught ?

Thanks
 I've run all my cells and and everything passed the test but when i upload the file to the auto grader i get 0% and the following error for all the parts of the lab(ex 3 first cells):
Pluralize and test (1b)
-----------------------
Traceback (most recent call last):
  File "", line 22, in 
NameError: name 'Test' is not defined

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Apply makePlural to the base RDD(1c)
------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

This is very funny because same file i've submitted on sunday i did not get this error and got partial grade. I try submitting the same file today and got this error. Now my grade on the lab is 0 because i guess the last submission stays. Could you please let me know if you have any idea of what is going on. 
Thanks
 





All the above answers are matching but when I come to 2f it does not match. Where 
 am i doing mistake ?
Top Ten Endpoints: [(u'/images/NASA-logosmall.gif', 59666), (u'/images/KSC-logosmall.gif', 50420), (u'/images/MOSAIC-logosmall.gif', 43831), (u'/images/USA-logosmall.gif', 43604), (u'/images/WORLD-logosmall.gif', 43217), (u'/images/ksclogo-medium.gif', 41267), (u'/ksc.html', 28536), (u'/history/apollo/images/apollo-logo1.gif', 26766), (u'/images/launch-logo.gif', 24742), (u'/', 20182)]






---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-145-a8259819853c> in <module>()
      8 
      9 print 'Top Ten Endpoints: %s' % topEndpoints
---> 10 assert topEndpoints == [(u'/images/NASA-logosmall.gif', 59737), (u'/images/KSC-logosmall.gif', 50452), (u'/images/MOSAIC-logosmall.gif', 43890), (u'/images/USA-logosmall.gif', 43664), (u'/images/WORLD-logosmall.gif', 43277), (u'/images/ksclogo-medium.gif', 41336), (u'/ksc.html', 28582), (u'/history/apollo/images/apollo-logo1.gif', 26778), (u'/images/launch-logo.gif', 24755), (u'/', 20292), 'incorrect Top Ten Endpoints']

AssertionError: 
The code I am using 
# Top EndpointsendpointCounts = (access_logs                  .map(lambda log: (log.endpoint, 1))                  .reduceByKey(lambda a, b : a + b))topEndpoints = endpointCounts.takeOrdered(10, lambda s: -1 * s[1])





 Data mining application and system event log files typically includes which of the following actions?

Based on the answers (it took me 11 attempts to get checked), here is my suggestion for rephrasing this question.

Using data mining applications to analyze system event logs are useful for which of the follow (activities)?

Alternatively, if you're not emphasizing the application (Splunk), the following work.

Data-mining both application and system logs typically involves which of the following activities.

This is also a good question, but I personally feel that the answers would be different.  Ehh, whatever.

Anyways, I'm enjoying the course, and appreciate the great content even on its first run.

Ciao!

 I've filled in my code in the cell then i press shift+enter for it to compute and the little asterisk appears on the side and the browser tab title says busy, but long after the tab title has stopped saying busy, the asterisk is still there and no result is given and I cannot proceed without the result. I waited for 30 minutes yesterday and got tired. Powered off vagrant, started again today and still the same problem. I have tried restarting vagrant, no change.

Anyone else experiencing this?
Please help?


 Getting Below Error. Even though i tested my regex for individual log files for good and bad log files.but when i run i am getting below error

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-5-4ba6c68750e0> in <module>()
      5 
      6 print
----> 7 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-3-8d8d6629d991> in parseLogs()
     22                    .filter(lambda s: s[1] == 0)
     23                    .map(lambda s: s[0]))
---> 24     failed_logs_count = failed_logs.count()
     25     if failed_logs_count > 0:
     26         print 'Number of invalid logline: %d' % failed_logs.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 23, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-2-0940d9fe8819>", line 39, in parseApacheLogLine
TypeError: long() argument must be a string or a number, not 'NoneType'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
<p><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansiyellow"><span class="ansicyan"><span class="ansigreen"><span class="ansiyellow"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"> </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> I´ve managed to resolve lab2 3c, however I think I'm doing something wrong, as the next blocks fail to work. Could someone tell me what´s exactly expected on these variables (i.e. the definition) ? I´ve completed with my guesses. If anyone could check my definitions I would be thankful.

dayToHostPairTuple: rdd with N (day,host) tuples (where N = number of lines on the original log) containing dayGroupedHosts: rdd with (day,host) tuples without repeated entries (no more than one tuple for a day/host pairdayHostCount: the name count suggests it should be an integer, but it´s used as an RDD next, so alas, that´s not true. Perhaps it´s and RDD grouping the previous RDD by day, generating 21 tuples (day,[list_of_hosts]) ??dailyHosts: Looking at the following exercises, it looks like and RDD with 21 tuples of (day,number_of_unique_hosts_on_this_day). Is it ?dailyHostsList: If the previous is true, I have no idea what this is. But, looking at the tests, it seems to be exactly the same thing as dailyHosts

So.. If anyone could help me clarify those variables purposes I would be greateful.

Regards,

Renato Moutinho
 I know I am a bit late for lab1, but I have code that removes the punctuation for the last part of the lab. When I run it in ipython notebook or python in an IDE it works without throwing an error, but when i submit it to the checker it throws an error on my implementation and then fails out of the rest of the checks because the RDD is never created due to the error. 

I figured I should not post my code here to not give away answers, but I am not sure how to proceed. Any ideas? Thanks!  Hi,

I'm curious to know what is the optimum solution for Lab2-3c.  I have came up with two solutions, described as below:

1)
- Create a pair of all days and hosts, with duplicates removed
- Group the result by day
- Use map and lambda to create a pair of (day, and number of hosts for that day) using python's len() function
- sort, and print top 30

2)
- Create a pair of all days and hosts, with duplicates removed
- Use map and lambda to create a pairs of (day, 1)
- Use reduceByKey and perform a+b
- sort, and print top 30

*******

The first solution uses groupBy, and python's len() function, which i guess needs to run on the driver, and thus it is not optimum.
The second solution doesn't involve groupBy and instead relies on reduceByKey().
What about distinct() ?  Does it run on the driver?

What do you think?  What's your solution?
 Hello,

I used simple transformation with collect functions, but i didn't get any output.
 dailyHosts.<tranformation>.<collect function> 

Any hint will be great full.

Thanks, I'm running a little bit behind..  Not able to figure out where should I download the text file from and where to save it?

For this exercise. I downloaded a plain text file available at http://www.gutenberg.org/ebooks/100/  and saved it within data/cs100/lab1/shakespeare.txt


Following is the error I get. My removePunctuation function passed the test.. so wondering if anyone has comments? Thanks.




data/cs100/lab1/shakespeare.txt






---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-67-68dae6833a3a> in <module>()
      8                   .textFile(fileName, 8)
      9                   .map(removePunctuation))
---> 10 print '\n'.join(shakespeareRDD
     11                 .zipWithIndex()  # to (line, lineNum)
     12                 .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in zipWithIndex(self)
   1986         starts = [0]
   1987         if self.getNumPartitions() > 1:
-> 1988             nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
   1989             for i in range(len(nums) - 1):
   1990                 starts.append(starts[-1] + nums[i])

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 256.0 failed 1 times, most recent failure: Lost task 0.0 in stage 256.0 (TID 984, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <lambda>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <genexpr>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "<ipython-input-65-b244e58ba2e0>", line 23, in removePunctuation
IndexError: string index out of range

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

 

 Hello Professor/TAs,

I also have been experiencing problems with autograder for 3c. I'm passing all the tests in my IPython notebook. I'm passing all the tests in my IPython notebook for each cell. But in the autograder it says something wrong with dailyHosts.is_cache.
I've modified it multiple times where it passes all tests in notebook and have resubmitted but still something wrong. Is there something wrong with autograder for this question, or am I doing something wrong? Thanks!

Data cleaning (1c)
------------------
All tests passed
Top ten error endpoints (3a)
----------------------------
All tests passed
Number of unique hosts (3b)
---------------------------
All tests passed
Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect dailyHosts.is_cached

Visualizing unique daily hosts (3d)
-----------------------------------
All tests passed
Average number of daily requests per hosts (3e)
-----------------------------------------------
All tests passed
Average Daily Requests per Unique Host (3f)
-------------------------------------------
All tests passed
Counting 404 (4a)
-----------------
All tests passed
Listing 404 records (4b)
------------------------
All tests passed
Top twenty 404 URLs (4c)
------------------------
All tests passed
Top twenty-five 404 response code hosts (4d)
--------------------------------------------
All tests passed
404 response codes per day (4e)
-------------------------------
All tests passed
Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
All tests passed
Five dates for 404 requests (4g)
--------------------------------
All tests passed
Hourly 404 response codes (4h)
------------------------------
All tests passed
Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
All tests passed
-- 15 cases passed (93.0%) --

  hey I'm a bit confused on lab 2 problem 3b. So I can get the number of hosts.

hosts = access_logs.map(lambda x:x.host)

why cant len(set(hosts)?

hosts is just a list of hosts.

I did a quick test:

alist = len(set(hosts))print alist

but I get this error:

TypeError: 'PipelinedRDD' object is not iterable

Not sure what I'm doing wrong ^(\S+)\s+\S+\s+(?:\S+\s+)+\[([^]]+)\]\s"(\S*)\s?\s+([^"]*)"\s(\S+)\s(\S+)
passed on https://regex101.com/
but gives javascript error when running - why? I had a problem with lab submission stating that all my entries were incorrect and got a 0 (this was on Sunday). Monday afternoon comes along and I resubmit the same file and end up getting a 81 but then that became a 61 due to a penalty. Is there a way to fix this? I promise all I did was resubmit the same file 24 hours later and only then did it get graded. I tried
(int(str(log.date_time)[9:11])
But weirdly, I got only nine of them with wrong numbers of 404:
Top hours for 404 requests: [(0, 626), (1, 811), (2, 483), (3, 519), (4, 633), (5, 560), (6, 630), (7, 801), (8, 636), (9, 486)]
The hour starts from digits 9:
####day = 2 digits
####month = 3 letters
####year = 4 digits
####hour = 2 digits
####minute = 2 digits
####second = 2 digits
####zone = (+ | -) 4 digits
 I make the search and found this
URL REMOVED
Exactly the same plan and in IPython also. I guess this is from an on-campus Berkeley, from the instructor Prof Anthony 
http://amplab.github.io/datascience-sp14/
It's an Berkeley class from AMP Lab, where the instructor is working at. The lab must be the same! I don't see any videos but slides are OK.
As long as we have our feet wet in data, it would never be the same Why do you put a 'pass' statement at the end of some of the cells in Lab 2? I noticed that you include it in some cells but not others and am wondering what the reason is.  Just started on Lab2 and thought it would be nice to understand what the following actually means... given that I am not a web developer.

"
Note that log files contain information supplied directly by the client, without escaping. Therefore, it is possible for malicious clients to insert control-characters in the log files, so care must be taken in dealing with raw logs"
What does "without escaping" mean? what are control characters? and how would "clients" insert them into log files?

Many Thanks, 
below is the code for 2b when i treid to run:
#### Response Code to Count
responseCodeToCount = (access_logs
                       .<FILL IN> POSTING SOLUTION CODE IS A VIOLATION OF THE                       HONOR CODE
                       .cache())
responseCodeToCountList = responseCodeToCount.take(100)
print 'Found %d response codes' % len(responseCodeToCountList)
print 'Response Code Counts: %s' % responseCodeToCountList
assert len(responseCodeToCountList) == 7
assert sorted(responseCodeToCountList) == [(200, 940847), (302, 16244), (304, 79824), (403, 58), (404, 6185), (500, 2), (501, 17)]
I get below .not sure what to do wtih 2b. As output of 2b is required in 2c . please help
#### Response Code to Count
responseCodeToCount = (access_logs
                       .<FILL IN>
                       .cache())
responseCodeToCountList = responseCodeToCount.take(100)
print 'Found %d response codes' % len(responseCodeToCountList)
print 'Response Code Counts: %s' % responseCodeToCountList
assert len(responseCodeToCountList) == 7
assert sorted(responseCodeToCountList) == [(200, 940847), (302, 16244), (304, 79824), (403, 58), (404, 6185), (500, 2), (501, 17)]
 when try to test lab2 3a i am getting the following error


---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-1-2843066a2419> in <module>()
      4 # you ensure the variables used in the next Test section are defined (ie. endpointSum, topTenErrURLs).
      5 
----> 6 not200 = access_logs.map(lambda log:log.response_code!=200).cache()
      7 
      8 endpointCountPairTuple = not200.map(lambda endpoint: (log.endpoint, 1))

NameError: name 'access_logs' is not defined
 Hello, my solution passed all test, but grader don't accept it. I trying to copy-paste solutions to fresh notebook, but without any effect.

Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 22, in 
NameError: name 'Test' is not defined

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Top ten error endpoints (3a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Number of unique hosts (3b)
---------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting 404 (4a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Listing 404 records (4b)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badUniqueEndpointsPick40' is not defined

Top twenty 404 URLs (4c)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Top twenty-five 404 response code hosts (4d)
--------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

404 response codes per day (4e)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Five dates for 404 requests (4g)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Hourly 404 response codes (4h)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --

  I can no longer execute this part of the assignment. The text is all black, the comments are no longer italicized, and the "In [23]:" to the left of the cell has disappeared. How do I fix this? Will we be able to use the VM even after the course ends, for practice purpose? Hello everyone,

I work with  http://pythex.org/ and I check

Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:43:39 -0400]

with the expression regular

^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\]


But it's wrong. Why doesn't it match?

I think that the expression regular is right

Could somebody help me?

Thanks in advance

Carlota Vina



 Hi .. i am newbie to python coding, with have some programming experience. I see there were lot of things like functions, libraries etc being used with in the code.

I am able to pick up understanding the code  or when i try to add some functions to the code i am able to google things, though its not best way.

I feel crippled while coding, as there are lot with in python and i am not sure the best way to get used to these functionality. I understand that practice make us to get used to it.

My question in here is to pick the things, how do you folks get used to these functions/ librares  as there are lot to remember. Do you look for thing when they are needed to is th ere any other optimal way than remembering things ?

Any other Tips with respect to this is much appreciated.

Kindly let me know, as i wanted to pick my python skills and code on my own Is there any way i can get feel of running the commands, we are executing with in the notebook, on the VM server directly than using the workbook for practice.

I have experience working with VM with linux installed on it. But this oracle VM i am not sure what are install with in it and how to access the server console and how the work book is getting executed on the server.

Also can some one throw light on vagrant .... ? i know that it is masking thing and making us to run the notebook but not sure if that is correct. 

I ask these as in real world work place we might need to work on the server directly thank using such fancy utilities ..... and more over if we get an idea on backend things it might be more easy for troubleshooting too..

Kindly enlighten me on these things? Not sure how this happened, but for 3a, the content got rearranged and the test now shows up before the exercise. How do I fix this? I Have been trying to perfrom the Delta logic using Pyspark.
Below is the current scenario being used for delta (updates and inserts) in sql database.

File 1:
1,Spark,Berkeley
2,Mapreduce,Google

File 2:
1,Spark,AMP labs
2,Mapreduce,Google
3,Python,CWI

Now i want to compare the two files and return an rdd which should contain the below result:
1,Spark,AMP labs
2,Mapreduce,Google
3,Python,CWI

The updates and inserts should happen.

Please share your ideas if any.
Thanks in advance.

 Since Spark is gaining momentum all the components of Hadoop is trying to run with Spark instead of MR engine.

On the same note, I would like to know if anyone has worked with Hive on Spark.
If yes, could you please let me know the steps for the same?

Thanks. I am trying to install the VM in Windows 8.1. I installed the latest version of Vagrant and VirtualBox. However, I keep getting the errors:

sparkvm: Box Provider: virtualbox
sparkvm: Box Version: >= 0
The box 'sparkmooc/base' could not be found or
could not be accessed in the remote catalog. If this is a private
box on HashiCorp's Atlas, please verify you're logged in via
`vagrant login`. Also, please double-check the name. The expanded
URL and error message are shown below:
URL: ["https://atlas.hashicorp.com/sparkmooc/base"]
Error: Failed connect to atlas.hashicorp.com:443; No error
 
Please help.

P.S. Also I am using my system from behind a proxy. Hello Anthony,

This is one of the best course so far on MOOC platform. It has really made learning a reality on MOOC platform.

Do you have any plan to launch the part 2 of this course or any other similar course on edX in coming time?

Thanks and Regards,
Sanjay Gupta, PMP, CSM Hi,

I'm having difficulty to get the top 5 words. I've checked my answer for removepunctuation with other post and it looks pretty much the same.
[u'zounds', u'zounds', u'zounds', u'zeal', u'youths']

Can anyone point me to the right direction? I've been working on this particular question the whole night. 
Thanks 

 Hi,

I've answered the Lab-3e question, based on the following algorithm (not going to share the code):

1) Read access_log and use map and lambda to create a tuple of (x,1) , where x is the day number
    As you see, I don't care about the host values

2) ReduceByKey and a+b, this aggregates number of total hosts per day as (x,y)

3) Join the results to dailyHosts, which is calculated in the 3c and is cached, which results in (x, (y,z))

4) Use map and lambda, to calculate the average

This solution does not depend on host values, and doesn't use groupbyKey().

What's your solution? 

Sorry for redundant question.

print not200.count()print not200.take(1)
 
This gives me below result.
I spent so many times on this but cannot move forward.
I even try to fix 1c. Everytime I pass 1c but I don't know.
Could you help me to move forward.
102330
[Row(client_identd=u'-', content_size=0L, date_time=datetime.datetime(1995, 8, 1, 0, 0, 7), endpoint=u'/', host=u'uplherc.upl.com', method=u'GET', protocol=u'HTTP/1.0', response_code=304, user_id=u'-')]
 

 Hi - for Lab3e I'm using what I think is a simple sortByKey() and get the following attribute error.

Any clues - pretty sure my groupedByDay is a regular RDD that looks like
[(8, 60142), (12, 38070), (4, 59554)...etc]
ie number pairs created from a map and reduce operation.

Anyone else with the same issue?

---> 16 sortedByDay = groupedByDay.sortBykey()
     17 
     18 print sortedByDay.take(10)

AttributeError: 'PipelinedRDD' object has no attribute 'sortBykey' errDateCountPairTuple = badRecords.map(lambda a:(a.date_time.date,1))DerrDateSum = errDateCountPairTuple.reduceByKey(lambda a,b:a+b)errDateSorted = (DerrDateSum.sortByKey())


Py4JJavaError                             Traceback (most recent call last)
<ipython-input-165-5158bc22c5e7> in <module>()
      3 errDateCountPairTuple = badRecords.map(lambda a:(a.date_time.date,1))
      4 DerrDateSum = errDateCountPairTuple.reduceByKey(lambda a,b:a+b)
----> 5 errDateSorted = (DerrDateSum.sortByKey()).cache()
      6 
      7 errByDate = errDateSorted.takeOrdered(30)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sortByKey(self, ascending, numPartitions, keyfunc)
    584         # the key-space into bins such that the bins have roughly the same
    585         # number of (key, value) pairs falling into them
--> 586         rddSize = self.count()
    587         if not rddSize:
    588             return self  # empty RDD

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 494.0 failed 1 times, most recent failure: Lost task 0.0 in stage 494.0 (TID 1003, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 125, in dump_stream
    for obj in iterator:
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1636, in add_shuffle_key
    d = outputSerializer.dumps(buckets[split])
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 402, in dumps
    return cPickle.dumps(obj, 2)
TypeError: expected string or Unicode object, NoneType found

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:914)
	at scala.collection.Iterator$GroupedIterator.go(Iterator.scala:929)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:968)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:972)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:210)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) This class is kind of fun, thank you for putting it together. Questions:

Can you build an RDD in a distributed fashion, i.e. each worker reading from
a different file or a distributed file? Otherwise, reading a large file may be a) either prohibitively expensive or b) the driver may run out of memory.

Can we get some exercises using pandas, sql, accumulators, broadcast variables...

Can you teach a follow-up class with more advanced material (even if you charge a little bit, not much please) .

Thanks solved.
 I paid on June 17, is it too late for certificate? It shows successful payment. I'm too confusing about the deadline June 17 UTC00:00, it's the night of 17, or the night of 16?

Thank you.  Hi,

I am using http://www.pythonregex.com/ site to validate the logs.

Initially I got the error for following:

ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:43:39 -0400] "GET / HTTP/1.0 " 200 7131

using the pattern: ^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+)\s*(\S*)" (\d{3}) (\S+)

I know the problem is after 1.0 there is a space which is not taken care so I changed the pattern to:

^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+)\s*(\S*)(\s*)" (\d{3}) (\S+)

and the it is working for both the logs:

1. ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:43:39 -0400] "GET / HTTP/1.0" 200 7131
2. ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:43:39 -0400] "GET / HTTP/1.0 " 200 7131

Now when I change in the Notebook it is not working but working in http://www.pythonregex.com/.

Please help as I am struck and do not know what exactly is the problem.

Regards,
Mayank There is this "*" - believe it means it is running.

Why it is taking so long - it is a Win 7 with intel i5 Hi there,

I signed in recently, but I saw the course already started three weeks ago, so I wonder if it is worth going for it as I would like to get a certificate, given the first two labs are already finished. In case it is not, when does the next Spark course start?

Thank you Just a short note how to use pandas for plotting with spark. I'm sure that we will see similar use cases in future lectures, still this is a trick that I personally find very useful when I want to see results as fast as possible and don't want to play with matplotlib.

How to install pandas. Open virtual machine through virtualbox so you can see the shell.
sparkvm login: vagrant
password: vagrant
Now execute following command:
sudo apt-get install python-pandas

Use pandas for plotting. Go into lab2 and try this plotting code for the last task (4i):
import pandas
hourRecordsSorted.toDF().toPandas().plot(x='_1', y='_2') Is it ok not to use joins to do the assignments?
I completed the assignment without using joins. If joins were required to be used can someone give a hint where to use joins.(maybe after the deadline)
 Hello there,
I have been trying to upgrade to the verified certificate course but there seems to be issues with my bank and the transaction can't go through right now. I understand that today is the deadline for paying but I'm afraid. What happens if my bank can't resolve the issue today??
Do I lose out? Replace
plt.plot(daysWithHosts, hosts)
with
plt.bar(daysWithHosts, hosts)
To center the bars over the x-axis labels, make these tweaks:
plt.axis([min(daysWithHosts)-0.5, max(daysWithHosts)+0.5, \
     0, max(hosts)+500])
plt.bar(daysWithHosts, hosts, align='center')


 I have solved almost 80% of the issue!!! just now I have 
Number of invalid logline: 119
Which are like this:
Invalid logline: cs1-06.leh.ptd.net - - [01/Aug/1995:01:26:27 -0400] "GET /" 200 7280
Invalid logline: jurassic.usc.edu - - [01/Aug/1995:14:27:13 -0400] "GET / " 200 7280

So do I need to ommit HTTP/1.0 section or ignore it ? any hint.

Thanks for all suggestions, I have solved the problem.

As a hint make attention that some log lines don't have HTTP/1.0 section!!!

 Hi, 

I have a lot of error when i execute exercice 1b, the error is: 

Number of invalid logline: 108
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:43:39 -0400] "GET / HTTP/1.0 " 200 7131
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:43:57 -0400] "GET /images/ksclogo-medium.gif HTTP/1.0 " 200 5866
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:07 -0400] "GET /images/NASA-logosmall.gif HTTP/1.0 " 200 786
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:11 -0400] "GET /images/MOSAIC-logosmall.gif HTTP/1.0 " 200 363
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:13 -0400] "GET /images/USA-logosmall.gif HTTP/1.0 " 200 234
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:15 -0400] "GET /images/WORLD-logosmall.gif HTTP/1.0 " 200 669
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:31 -0400] "GET /shuttle/countdown/ HTTP/1.0 " 200 4673
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:44:41 -0400] "GET /shuttle/missions/sts-69/count69.gif HTTP/1.0 " 200 46053
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:45:34 -0400] "GET /images/KSC-logosmall.gif HTTP/1.0 " 200 1204
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:45:46 -0400] "GET /cgi-bin/imagemap/countdown69?293,287 HTTP/1.0 " 302 85
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:45:48 -0400] "GET /htbin/cdt_main.pl HTTP/1.0 " 200 3714
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:45:52 -0400] "GET /shuttle/countdown/images/countclock.gif HTTP/1.0 " 200 13994
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:22 -0400] "GET / HTTP/1.0 " 200 7131
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:29 -0400] "GET /images/ksclogo-medium.gif HTTP/1.0 " 200 5866
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:35 -0400] "GET /images/NASA-logosmall.gif HTTP/1.0 " 200 786
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:37 -0400] "GET /images/MOSAIC-logosmall.gif HTTP/1.0 " 200 363
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:38 -0400] "GET /images/USA-logosmall.gif HTTP/1.0 " 200 234
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:46:40 -0400] "GET /images/WORLD-logosmall.gif HTTP/1.0 " 200 669
Invalid logline: ix-li1-14.ix.netcom.com - - [08/Aug/1995:14:47:41 -0400] "GET /shuttle/missions/sts-70/mission-sts-70.html HTTP/1.0 " 200 20304
Invalid logline: ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:47:48 -0400] "GET /shuttle/countdown/count.html HTTP/1.0 " 200 73231
Read 1043177 lines, successfully parsed 1043069 lines, failed to parse 108 lines

How can i solve this? the next exercice fails too....

Thanks Hi,
sorry if my answer is not so interested, but I don't understand if I can modify the cell code of 3a exercise in this way


not200 = # Here I make two maps, one filter and one reduceByKey
endpointSum = not200 #Because the checking cell uses this variable

topTenErrURLs = not200.# Order
print 'Top Ten failed URLs: %s' % topTenErrURLs

I suppose yes, but could you confirm it?
Note: The result is correct

Thank you! Hi,

I am using the below filter functions.

uniqueHosts = hosts.filter(lambda s: s[1]<2)

it is returning [ ]. can anybody suggest how to correct it dayToHostPairTuple = access_logs.map(lambda d: (d.date_time.day,d.host)).distinct()

#dayGroupedHosts = dayToHostPairTuple.groupByKey().mapValues(lambda x: list(x))

am stuck here and am not sure if i should change the first 2 lines or move forward ...  I'm running these labs on complete rubbish hardware.

In order to get Lab2 to work I've had to disable the cache() in part 1b.
This all worked fine but many functions took a long time to execute and consequently my first submission to the autograder timed out. 

If you're forced down the same path don't forget to enable the cache() before submission.

The course is a real blast and I'm enjoying it!
 In 4(b) code- we are returning 40 distinct endpoints. The code is running successfully but the test case is failing. The test case is something like this-

badUniqueEndpointsSet40 = set(badUniqueEndpointsPick40)Test.assertEquals(len(badUniqueEndpointsSet40), 40, 'badUniqueEndpointsPick40 not distinct')

Now when I run this test then I get error-"TypeError: unhashable type: 'list'" at first line of test case. But if I comment the first line of test case then it is running.

4(b) code is returning 40 endpoints as a list and when we apply set on list in the test case then we get this error. What should be returned from the code in 4(b) so that test case passes. I am getting the below output in 3C which seems correct.

Unique hosts per day: [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)]

while testing it is giving the below output

1 test passed.
1 test passed.
1 test failed. incorrect dailyHosts.is_cached

can anybody suggest how to correct it.

 Is there any schedule to show the answers for labs?

It would be very interesting to compare my answers with the expected answers.

Thanks. On  (2a) Example: Content Size Statistics

I am getting:

TypeError                                 Traceback (most recent call last)
<ipython-input-9-2716495726a9> in <module>()
      2 content_sizes = access_logs.map(lambda log: log.content_size).cache()
      3 print 'Content Size Avg: %i, Min: %i, Max: %s' % (
----> 4     content_sizes.reduce(lambda a, b : a + b) / content_sizes.count(),
      5     content_sizes.min(),
      6     content_sizes.max())

TypeError: unsupported operand type(s) for /: 'unicode' and 'int'

Why is it occurring? How do I solve it? Hi could anyone help me out with how to create tuples by reading a file.
Example as below:

File:
1,a,Spark,AMP labs
2,b,Mapreduce,Google
3,c,Python,CWI

i want to read the file using sc,readFile.

And i want to create tuple in the below format.

( [((1,'a'),('Spark','Berkeley')),((2,'b'),('Mapreduce','Google')),((3,'c'),('Python','CWI'))] )

Please help. I need an answer regarding whether Apache Spark for real-time, enterprise-wide deployment uses Python or Scala/(or) Java? Should I believe that the choice of Python for this course has been borne out of the consideration to have a learner-friendly approach? Or there are pragmatic, serious Python-based Spark deployments in use by companies and research initiatives as well?
Also, if Java is a faster implementation, why aren't we learning that? Can we make a switch to it? If so,how?  
I am new to python, n i need some help with it ! How can i consider new line as also a seperator! its just ingnoring new line as seperator\
<img src="https://d1b10bmlvqabco.cloudfront.net/attach/i9esrcg0gpf8k/iagj82v7810cr/ib0s6ogniu1r/Capture.JPG" />
 I'd like to purchase one, but it seems there's no link for that, though deadline hasn't come (wed 23:59 utc) 
May i have some help please I've gotten 3c-3f wrong but autograder refused to grade me. Why?

Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect dailyHostsList

Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect hosts

Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect avgDailyReqPerHostList

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect avgs

 I'm trying to compile the exercise (1c) but, it throws an error. Any idea why ?

Thanks in advance ---------------------------------------------------------------------------NameError Traceback (most recent call last) in () 32  33 ---> 34 parsed_logs, access_logs, failed_logs = parseLogs() in parseLogs() 11 parsed_logs = (sc 12 .textFile(logFile)---> 13 .map(parseApacheLogLine) 14 .cache()) 15 NameError: global name 'parseApacheLogLine' is not defined Hi all,It takes forever to me to find out what is wrong with my 1c answer. Seems that I figured out the correct answer, but it doesn't matter how I replace the previous code, I'm always getting a syntax error. I don't understand why. Hopefully you can help me to figure this out. Here I'm posting the code with the original line (since we can't post answers), and it says there's a syntax error. Could you help me by telling what I'm doing wrong here? Sincere thanks!


# TODO: Replace <FILL IN> with appropriate code

# This was originally '^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+)\s*(\S*)" (\d{3}) (\S+)'
APACHE_ACCESS_LOG_PATTERN = ^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+)\s*(\S*)" (\d{3}) (\S+)

parsed_logs, access_logs, failed_logs = parseLogs()
  File "<ipython-input-47-a56afb10de10>", line 4
    APACHE_ACCESS_LOG_PATTERN = ^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+)\s*(\S*)" (\d{3}) (\S+)
                                ^
SyntaxError: invalid syntax
 In exercises (3c) and (4e) we need to map the badRecords RDD to a PairTuple. The exercise says
Since the log only covers a single month, you can ignore the month in your checks.
But I dont know how to map using date_time just using the day, i.e. on a per day basis. Any help would be much appreciated! I am getting the following error since yesterday, restarted the lab but same issue,

AttributeError                            Traceback (most recent call last)
<ipython-input-41-00040f2a6266> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
----> 2 shakespeareWordsRDD = shakespeareRDD.split('')
      3 shakespeareWordCount = shakespeareWordsRDD.count()
      4 print shakespeareWordsRDD.top(5)
      5 print shakespeareWordCount

AttributeError: 'PipelinedRDD' object has no attribute 'split' Hi,

If the log line doesn't contain HTTP/1.0 do I need to replace it with something or ignore it ?



Thanks in advance. Sorry... I solve it. I don't know how delete the thread.

Best Regards, I presume like in lab 1, whenever a cell is executed its output is seen beneath the cell window. But in lab 2 i am not able to view the output of the code in cell 1b. The print commands for

  print 'Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (parsed_logs.count(), access_logs.count(), failed_logs.count())

and yet another print 
print 'Number of invalid logline: %d' % failed_logs.count()

are not visible. Is it ok ?? 
 Hello!  In Lab 2 the plot in the example (2e) shows the Endpoint counts as bars, (or spikes).  But when I do the exercise in (3d) with Host counts per day, it looks like a line graph.  But the code structure for the plot functions looks the same.  Is this just an illusion that (2e) looks like bars/spikes because the x-axis is so dense, and it is really a line-graph just like (3d)?  My code in passes successfully in test environment but auto grader seem to miss the variable in question.

badRecords = (access_logs .filter(lambda log: log.response_code == 404).map(lambda log: (log.endpoint, 1))).cache()

Autograder says:

Counting 404 (4a)-----------------Traceback (most recent call last): File "", line 1, in NameError: name 'badRecords' is not defined

Any comments? the order of the input 47 and 48 in lab2 are incorrect which will result in failing the test when autograder checking the code. no matter your code is correct or not. I have answered the question successfully using:

errHostsCountPairTuple = (access_logs.filter(lambda log:...........
errHostsSum = errHostsCountPairTuple.groupBy.......
errHostsTop25 = errHostsSum.takeOrdered.......

Both tests passes successfully. However, autograder fails this by saying:
Top twenty-five 404 response code hosts (4d)--------------------------------------------Traceback (most recent call last): File "", line 1, in NameError: name 'errHostsTop25' is not defined

As such badRecords RDD doesn't contain 'host' information and it can only be extracted from access_logs. Not sure of any other way to extract it using badRecords.<Fill In>

Comments please. Hi All,

I'm having a blast with this course so far.. do need to learn few things though. Can someone please explain why [if] do we need the \s* in the regular expression provided in Lab2 (see image below). Why did we NOT need it elsewhere in the expression where there are spaces too? e.g. after the first (\S+) or the second etc??

many thanks,

 I tried \[([\w:/]+\s[+\-]\d{4})\] on regex101.com? Operating on data one at a time is terrible in terms of performance. Is it possible to aggregate data so each operation is done on arrays. So, for instance, we can write a
C++ hook that takes advantage of AVX and have a Python interface to interact with SPARK. (Without being very precise, I mean something like  reduceByKey(array, array))
Is Spark extensible (in a relatively easy way)? It would be great if this model can be 
applied to other environments where array operations are a must.
For instance, suppose I have a distributed panda. Can I get the entire chunk instead of one entry at a time? Hi,

For this exercise I am trying:

1) map to (x.date_time.day, x.uniqueHosts)
2) sortByKey
3) reduceByKey a + b

but I get an error on step 2: ValueError: too many values to unpack

Any advice? Thanks! Hi, Im having a problem, when tried to execute the1b cell it appears a [*] instead a number, I think is not being executed, I tried with execute all cells below, and its the same, if I continue to the next cell, nothing happens, I only get the [*]
What is causing this problem?
I did not have any problem with lab 1, everything was fine.

thanks Hello,

What i'm understand is we need something like <K1,V1(K2,V2)> for dayAndHostTuple.
K1 will be the Day and V1 will combine (K2"host",V2"count") , but i'm stuck in here how many transformers and actions we need on dayAndHostTuple.
When i print dayAndHostTuple to see <k,v> i didn't get any result.
Then we need average which will be K1 / V1.

Please help me. 
 Hi all!

My lab is running painfully slow.

I read on here that it is best to increase the number of cores in virtual box....

how should I do this?

I went to the virtualbox app, and clicked on settings, but the settings were "grayed out" and I could not change anything. Thanks in advance! It keeps on showing the PY4JJAVA error below which i cant solve. Please help. The code i wrote is given below
# TODO: Replace <FILL IN> with appropriate code
# HINT: Each of these <FILL IN> below could be completed with a single transformation or action.
# You are welcome to structure your solution in a different way, so long as
# you ensure the variables used in the next Test section are defined (ie. endpointSum, topTenErrURLs).

<Snip> Please do not post solutions, it breaks the honor code
 Hi All,
 I am trying to use this site to gain knowledge and work on 1c.

In the regular expression box, I put:
^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+)\s*(\S*)"

In the string box I put following which is initial part of good line example in 1C:

127.0.0.1 - - [01/Aug/1995:00:00:01 -0400] "GET /images/launch-logo.gif HTTP/1.0"

When I click on "Test Regex" box, I get following server error:
"Error: Server Error
The server encountered an error and could not complete your request.
Please try again in 30 seconds."

Please let me know what I am doing wrong.

Thanks in advance
 Looking for a hint on how to collect a list of the second elements from the following in ascending order of the first element.
[(8, 4406), (12, 2864), (4, 4190), (16, 4340), ...

I see it's a simple matter of map(lambda (x,y): (x)).takeOrdered(30) to get the first elements, in order of ascending values. But other than resorting to a set of list manipulations rather than using collect(), takeOrdered() ect. I'm at a loss. Thanks Seems that it takes about 10 minutes to run the parseLogs(). And so far I saw the notebook got disconnected a couple of times while running the parseLogs(). Since joins can be confusing when you start using them a lot, I like to think of them as Venn diagrams as outlined here: http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/

Hope this helps anyone! dayToHostPairTuple = access_logs.map(lambda log:((log.data_time.day, log.uniqueHosts)))

#dayToHostPairTuple.sortByKey(True)
print dayToHostPairTuple.take(1)

Now that I try to print out datToHostPairTuple is breaks down, and it was just fine. Restarting everything is taking me so long that I rather ask someone. Thanks for all the inputs. When I try to submit my notebook as a .py file, I get the error
NameError: name 'Test' is not defined
I checked we are importing Test in the code. Is there a problem in the autograder. In 3d, the exercise asks to generate two lists: one of keys and the other of values from an RDD containing (k, v) tuples. These two lists are obtained using map twice on provided functions. Does map guarantee the order of tuples passed to these functions? If not, the generated arrays may not be valid.
 In the course we've been learning how to do queries that are efficient and partition friendly, like map, filter and reduceByKey, specifically by focusing on the key part of the (key, value) tuple and trying to do transformations that don't need to cross partitions.

Does this sound right?  If so, are there similar guidelines should we use when using Spark SQL?  There isn't any key to focus on there.  How can we be sure our queries are efficient and partition-friendly with dataframe/schemaRDDs?  Or does Spark figure that out for us?   Hi
When I ran 1a and 1b this morning, I got a response. it showed the invalid lines. Now I do not see any response when I run the programs.  I have shutdown/ and ran it again too. Still no response. I am logged in

Any ideas?

Thanks

Ram Lab 2 was amazing. They are well prepared, and right on in terms of relevance.
I'm currently analyzing logs using Impala, HDFS for persistence, and Python scripts and Oozie for workflows.
Now, I can also analyze logs with Spark.

I look forward to the machine learning lab. Is it possible to publish lab 3 Friday evening?
Then I can work on it Saturday and/or Sunday, since it's one of my favorite topics.
 It does seem that the VM VirtualBox will go into meditation after a while.   Where's the setting for this pls?   It's just very frustrating that I'm in the middle of some coding and all of a sudden the machine stop working!!!
 http://www.oreilly.com/data/sparkcert I can't see why my logic is failing on this one:

first I create the (log.date_time.day, log.host) tuples
then groupByKey()
so I get a(day, (hosts)....) list
then I map over that and ask for a List[1].distinct() to get
(day, (unique hosts)...)
Then I map over that asking for a list of (day, len(unique hosts)...) 

When I run it it fails on 
dailyHosts.take(30) This gave me some challenges when working on question 3.

What is difference between using the notations for the mapping using lambda function below?

myRDD.map(lambda (a, b): (a, (b[0] / b[1])))   << this works

myRDD.map(lambda a, b: (a, (b[0] / b[1])))  << doesn't work

thx
 while trying for the fisrt time,vagrant Up command  fails with the following error..

Could anyone please help me on this error...I perfectly followed all the process for installation...but still am not able to get into virtual box....

Microsoft Windows [Version 6.1.7600]Copyright (c) 2009 Microsoft Corporation. All rights reserved.
C:\Users\NISHA>cd myvagrantThe system cannot find the path specified.
C:\Users\NISHA>cd macro
C:\Users\NISHA\macro>cd myvagrant
C:\Users\NISHA\macro\myvagrant>vagrant upC:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems/specification.rb:959:in `read': Permission denied - C:/HashiCorp/Vagrant/embedded/gems/specifications/hashicorp-checkpoint-0.1.4.gemspec (Errno::EACCES) from C:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems/specification.rb:959:in `load' from C:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems/specification.rb:644:in `block (2 levels) in each_spec' from C:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems/specification.rb:643:in `each' from C:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems/specification.rb:643:in `block in each_spec' from C:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems/specification.rb:642:in `each' from C:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems/specification.rb:642:in `each_spec' from C:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems/specification.rb:658:in `each_normal' from C:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems/specification.rb:669:in `_all' from C:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems/specification.rb:822:in `each' from C:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems/specification.rb:864:in `find' from C:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems/specification.rb:864:in `find_inactive_by_path' from C:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems.rb:175:in `try_activate' from C:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems/core_ext/kernel_require.rb:117:in `rescue in require' from C:/HashiCorp/Vagrant/embedded/lib/ruby/2.0.0/rubygems/core_ext/kernel_require.rb:124:in `require' from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/bundler.rb:6:in `<top (required)>' from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/pre-rubygems.rb:19:in `require_relative' from C:/HashiCorp/Vagrant/embedded/gems/gems/vagrant-1.7.2/lib/vagrant/pre-rubygems.rb:19:in `<main>'
C:\Users\NISHA\macro\myvagrant> Hi overall question about 4. I can get 4a to pass any number of ways but I was wondering if I need the cached RDD in 4a to contain all the information required for b-i.

Basically I'm not sure if I can make badrecords = access_logs.map(lambda log: log.response_code,1)

and do joins or if I'm meant to create it with numerous entries that have all the information required to do the rest of the questions?

Thanks Hi,

I am using the below code.

errDateSorted = errDateSum.sortByKey()

But it is giving me below error

----> 8 errDateSorted = errDateSum.sortByKey()
      9 
     10 errByDate = errDateSorted.cache().takeOrdered(30)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sortByKey(self, ascending, numPartitions, keyfunc)
    584         # the key-space into bins such that the bins have roughly the same
    585         # number of (key, value) pairs falling into them
--> 586         rddSize = self.count()
    587         if not rddSize:
    588             return self  # empty RDD

Can anybody suggest what I am doing wrong.
 In exercise 2e, we create and cache an RDD called endpoints. Then, in exercise 2f, we re-create the same RDD (per my understanding), call it endpointCounts, and use it to create the list topEndpoints. Why didn't we simply use the endpoints RDD, instead of endpointCounts? I actually tried the following code and got the very same results than the original script, albeit much faster (because of the cache):

topEndpoints = endpoints.takeOrdered(10, lambda s: -1 * s[1])
 Am I missing anything? 

127.0.0.1 - - [01/Aug/1995:00:00:01 -0400] "GET /images/launch-logo.gif HTTP/1.0" 200 1839
ix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:09 -0400] "GET /images/launch-logo.gif HTTP/1.0" 200 1713

Everything looks good except for spaces between the dashes? Has there been a modification made to the lab2 ipynb file linked on edx? this is because for Q3b, when I used map function to map the hosts and then used distinct(), the count() function output was 54393 rather than the correct 54507, i have similar problem with 3c where the I get the my dailyHostCount is correct but I get slightly different dailyHostList outputs for certain days (both up and downs).

I am coming to suspect that there has been some changes to the data provided rather than issues with my code. Any help is appreciated. Thank you I made a mistake and submitted lab2 using the lab1 page.
My previously submitted lab1 (100 points)  was erased.
What do I do to get my grade for lab1 back? I wanted to understand why we didn't use the parallelize method, in addition to textFile, to create the collections. I found this in the pySpark documentation about textFile:


The textFile method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 64MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.


So to clarify, does it mean that textFile automatically distribute the data, by default, based on the number of blocks? Hi, Im getting this error when executing the 2b example:


Found 7 response codes
Response Code Counts: [(200, 940742), (304, 79824), (404, 6185), (500, 2), (501, 17), (302, 16241), (403, 58)]




---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-17-79dc1579c7cd> in <module>()
      8 print 'Response Code Counts: %s' % responseCodeToCountList
      9 assert len(responseCodeToCountList) == 7
---> 10 assert sorted(responseCodeToCountList) == [(200, 940847), (302, 16244), (304, 79824), (403, 58), (404, 6185), (500, 2), (501, 17)]

AssertionError: 





 Hello Anthony,

I have submitted my complete lab 2 on tuesday.

Can you please provide me the link to download the same  Dataset which we have used for lab2. I will try to implement same in java?

thanks,
vibhor Hello,

Can someone please tell me why I'm getting this error with count() action. I solved this question but wanted to know the issue.

print wordCountsCollected.count()TypeError: count() takes exactly one argument (0 given)
Thanks I am getting incorrect sum for endpointSum and getting an incorrect for toptenErrURLs.

For not200: i used the filter syntax.   access_logs.filter(lambda log:log.response_code!=200)

For endpointCountPairTuple: I used map syntax.   not200.map(lambda log:(log.response_code,1))

For endpointSum: I used reduceByKey syntax.  endpointCountPairTuple.reduceByKey(lambda a,b:a+b)

For toptenErrURLs: I used the takeOrdered syntax.  endpointSum.takeOrdered(10,lambda s: -1 * s[1])

Could someone please help me out with this error? I'm finding it hard to visualize the data in between steps in lab 2.

What's the best and fastest way to view a snapshot of the data in order to check the last transformation?

For instance in Lab 2 (3c):

dayToHostPairTuple.top(5)
or  

dayToHostPairTuple.take(5)

take a long time to process. can you explain why we are using takeOrdered() with a reversal lambda to get top x results if we could use top()?
aren't these results the same? or is it just to make us remember that we can use takeOrdered() with a lambda? I just submitted the lab2 .py file and it was accepted. However, there may be different solutions to the same problem and I think I may have implemented a few items differently from the way it was presented on the course. For that reason I would like to know if we will have access to corrected labs at the final of the course.

Thanks in advance.

Best regards,
-Henrique Hello,

I used a method to develop avgDailyReqPerHostList that didn't fit the given variable names by the various " <FILL IN> " spots.  I made sure that my final variables/RDDs used the names that the Test routines were looking for, but will the AutoGrader flag this section as wrong because the solution does not look like the suggested structure of <FILL IN>s ?

 before joining the two datasets I had a sortByKey(), but the results after performing the necessary arithmetic to give the key value pair is again not sorted. Why is that?  it seems there is no need to do the sortByKey() prior to joining the tables. Hello TA's,

Its been almost 1 hour since I submitted my lab 2 file to the autograder. How do i resubmit the file? 
Do I just refresh the page and resubmit?

Please advise.

-- Update

when I refreshed the page ,it shows all tests passed, I got as below and when i clicked save , it is saying, i need to click check again. Do i have to since i already checked once? 
Or Is it okay if i check again??




Data cleaning (1c)
------------------
All tests passed
Top ten error endpoints (3a)
----------------------------
All tests passed
Number of unique hosts (3b)
---------------------------
All tests passed
Number of unique daily hosts (3c)
---------------------------------
All tests passed
Visualizing unique daily hosts (3d)
-----------------------------------
All tests passed
Average number of daily requests per hosts (3e)
-----------------------------------------------
All tests passed
Average Daily Requests per Unique Host (3f)
-------------------------------------------
All tests passed
Counting 404 (4a)
-----------------
All tests passed
Listing 404 records (4b)
------------------------
All tests passed
Top twenty 404 URLs (4c)
------------------------
All tests passed
Top twenty-five 404 response code hosts (4d)
--------------------------------------------
All tests passed
404 response codes per day (4e)
-------------------------------
All tests passed
Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
All tests passed
Five dates for 404 requests (4g)
--------------------------------
All tests passed
Hourly 404 response codes (4h)
------------------------------
All tests passed
Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
All tests passed
-- 16 cases passed (100.0%) --




Checkyour answer Saveyour answer 
You have used 1 of 10 submissions
 I got this from the current Spark Summit 2015 that's going on.  I found this PDF useful, without explanation, about how to think about and understand a lot of the API and available Spark functions for Python. Feel free to share it.visualapi.pdf When I run this (notice that it is incomplete, and I am using the print statement for debugging purposes), I get the error message below. Please help me understand as to what is happening. Any help is appreciated!

# TODO: Replace <FILL IN> with appropriate code

dayAndHostTuple = access_logs.map(lambda log: (log.date_time.day,log.host))

groupedByDay = dayAndHostTuple.groupByKey()

sortedByDay = groupedByDay.sortByKey(True)
print sortedByDay.take(1)
#avgDailyReqPerHost = (sortedByDay
#                     <FILL IN>
#                     )
#avgDailyReqPerHostList = avgDailyReqPerHost.take(30)
#print 'Average number of daily requests per Hosts is %s' % avgDailyReqPerHostList



---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-29-cc4b17480538> in <module>()
      5 groupedByDay = dayAndHostTuple.groupByKey()
      6 
----> 7 sortedByDay = groupedByDay.sortByKey(True)
      8 print sortedByDay.take(1)
      9 #avgDailyReqPerHost = (sortedByDay

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sortByKey(self, ascending, numPartitions, keyfunc)
    584         # the key-space into bins such that the bins have roughly the same
    585         # number of (key, value) pairs falling into them
--> 586         rddSize = self.count()
    587         if not rddSize:
    588             return self  # empty RDD

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 89.0 failed 1 times, most recent failure: Lost task 0.0 in stage 89.0 (TID 226, localhost): java.net.SocketException: Connection reset
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:118)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:159)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.io.FilterOutputStream.write(FilterOutputStream.java:97)
	at org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:409)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:421)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:421)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:421)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) This week we have seen that it is really easy to analyze logs using Spark. But, which are the possible applications of log analysis?

I can imagine that first benefit of this analysis is to improve a website: avoid 404 errors, improve most visites endpoints... But we can already do that with log analysis software without using Spark.

In addition to that, which are the possibilities? For example in online shops web sites, which are the possibilities of this analysis in order to improve the sales?

Any resource to learn about log analysis possibilities? once I have the sortedByDay RDD in the form 
[(1, 33996), (3, 41387), (4, 59554), ...etc],is it possible to only divide values (the second terms) by the elements of a list RDD which contains the same number of elements???Thank you in advance,Ale 
 for 3c, dayToHostPairTuple = access_logs.<FILL IN>
i can create a map to create tuple for day and host 
how can i group the hosts per day any hints? I had used the standard links provided many, many times. Now they do not connect.

sparkvm: SSH address: 127.0.0.1:2222
sparkvm: SSH username: vagrant
sparkvm: SSH auth method: private key


sparkvm: Forwarding ports...
sparkvm: 8001 => 8001 (adapter 1)
sparkvm: 4040 => 4040 (adapter 1)
sparkvm: 22 => 2222 (adapter 1)

EDIT:
Solved?
reload and such do not work.
I used vagrant destroy and the rebuild seemed to function though I thought I had done that(and a few other tricks) and it was still not connecting.
Oh well, hope it stays on. Would it be a good idea to clone mooc-setup from GitHub to be able to access the lab (and other data) files? 
Just wasn't sure of the differences in state between the mooc-setup github repo and the user content files for the labs (" https://raw.githubusercontent.com/spark-mooc/mooc-setup/master")

It would be easier to simply run git update from the terminal to pull the new labs instead of clicking and saving, etc The filter function takes the responses not equal to 200 and map and reduceByKey functions will determine the endpoints

not200 = access_logs.filter(lambda log: log.response_code != 200).map(lambda log: (log.endpoint, 1)).reduceByKey(lambda a, b : a + b)
print not200.count()

It gives me 7689 but errors for topTenErrURLs The graph generated by the 2e example code is interesting, but I thought it was more interesting to sort the data by the number of hits before plotting:

endpoints = (access_logs
             .map(lambda log: (log.endpoint, 1))
             .reduceByKey(lambda a, b : a + b)
             .map(lambda (k, v): (v, k))   # Adding this map
             .sortByKey()                  # and sort by key
             .map(lambda (v, k): (k, v))   # and remap
             .cache())
ends = endpoints.map(lambda (x, y): x).collect()
counts = endpoints.map(lambda (x, y): y).collect()

fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')
plt.axis([0, len(ends), 0, max(counts)])
plt.grid(b=True, which='major', axis='y')
plt.xlabel('Endpoints')
plt.ylabel('Number of Hits')
plt.plot(counts)
pass

There doesn't seem to be a way to sort-by-value, so I had to swap the position of the key/value in the tuple, sort, and then swap them back. I was new and joined late !! but my teachers really helped me alot, as We have a Big data Analysis Professor in our college he advised me to take up the course.The course is little bit tough but interesting.
Looking forward for successful completion :)
All the best to all  I couldn't figure out my mistake...but discovered after hours of total befuddlement, that my error  in Lab 2 4b was to put endpoints in my code rather than endpoint I am spending a lot of time re-running notebook 2 over and over again to avoid "variable not defined"  It takes a while for it to run so it is turning to be a significant waste of time.  It would be great if this is something that can be fixed going forward For 2(f), I'm getting the expected ordering, and all the counts are matching, except '/ksc.html'. The expected value is 28582, but I'm getting 28583. Has anybody else come across this? Is there an edge case I'm considering that should be ignored? There is a minor typo In the quiz on "Data Science Danger Zone" for Lecture 2.

URL:  https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/courseware/d1f293d0cb53466dbb5c0cd81f55b45b/3cf61a8718fe4ad5afcd8fb35ceabb6e/

Lecture 2 - 2nd Panel - Quiz Titled "Data Science Danger Zone"

last option "Can create what legitimate appearing analysis without any understanding of how they got there or what they have created"

On omitting "what," the sentence looks correct and conveys the correct idea.

It is also possible there is a missing phrase, such as "what seems like" "what might be considered"

 Nothing fancy: I used a map (lambda), a reduce(lambda) and a distinct for the three FILL INActually it's not the first section/exercise of this lab that gave me headaches:Already, I have spend about 7 hours to this lab (80% of time restarting kernel, restarting VM etc).My PC has Win7 with 4GB memory, VM (the course VM) with 1,8GB memory and 2 cores Why usually cache is using for example at 2e and above 2b examples but in other cases not like 2d. What is its uses is free to use wherever I mean or what is the best explanation  please Thanks.

PD: the tutorial says: 
                               cache()
                               Persists with the default storage level (MEMORY_ONLY_SER). Seriously. I can't believe I'm sitting here spending my free-time debugging someone else's regex. This has absolutely nothing to do with learning Spark. Hi vagrant doesn't seem to be booting ... 

In the middle of trying to finish the lab a couple days ago, the code wasn't running.  So I had to halt and reboot vagrant but I couldn't get it to reboot. 

Today I'm trying again with no luck.  Here is my screen shot.

Thanks

 From 1b, there are 108 response codes I could see.
But int 2b, just a handful of them do, less than 10 - what is going on - what am I missing?

How can I access the log file by way of an editor? Endpoints are URI's - eg /images/launch-logo.gif

Why the visual show numerals for Endpoints - if it were mapped, how was it done? What is wrong in it . i am struck here for long timePy4JJavaError                             Traceback (most recent call last)
<ipython-input-16-e987bcd72feb> in <module>()
     10 endpointSum = endpointCountPairTuple.reduceByKey(lambda a,b: a+b)
     11 
---> 12 topTenErrURLs = endpointSum.takeOrdered(10, lambda s: -1 * s[0])
     13 
     14 print 'Top Ten failed URLs: %s' % topTenErrURLs Hi

My lab2 was working fine today. But i am running into issue . and is taking more than 45 min which wasnt the case yesterday and worked fine.

Please help to resolve this. I am struggling with this instead of working towards assignment. I was able to get through 3c using the rudimentary transformations. 
In order to sort the values, I tried applying sorted() on the "dayHostCount" and "dailyHosts" which threw up an error saying about pipelined objects. It worked out when I did the same "sorted()" on the "dailyHostList".

dayHostCount = dayGroupedHosts.<FILL IN>
dailyHosts = dayHostCount.cache()
(was posting a part of the code solely to get my doubt clarified. please excuse this if it's not appropriate)

Someone please explain me when doesn't the sorted() or sort() functions work..? Faced the "pipelined error" many a times while using the sorted(). I have an error in sortByKey(), any ideas:

dayToHostPairTuple = <FILL IN> PLEASE DO NOT POST SOLUTIONS CODE AS IT IS A VIOLATION OF THE HONOR CODEdayGroupedHosts = dayToHostPairTuple.<FILL IN>dayHostCount = dayGroupedHosts.<FILL IN>dailyHosts = dayHostCount.collect().cache()              dailyHostsList = dailyHosts.take(30)print 'Unique hosts per day: %s' % dailyHostsList

The error message is
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-85-c223fd6c9b51> in <module>()
      5 dayGroupedHosts = dayToHostPairTuple.<FILL IN>
      6 
----> 7 dayHostCount = dayGroupedHosts.<FILL IN>
      8 dailyHosts = dayHostCount.collect().cache()
      9 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sortByKey(self, ascending, numPartitions, keyfunc)
    584         # the key-space into bins such that the bins have roughly the same
    585         # number of (key, value) pairs falling into them
--> 586         rddSize = self.count()
    587         if not rddSize:
    588             return self  # empty RDD Dear sirs:
I want to pay the Verified Certificate option to Wednesday June 17th
I am in other Time Zone, If possible do now?
thanks
Rodrigo https://aws.amazon.com/blogs/aws/new-apache-spark-on-amazon-emr/

They have an interesting example at the end that processes 79GB (uncompressed) DOT flight data to find:
the 10 airports with the most departuresthe most flight delays over 15 minutesthe most flight delays over 60 minutesthe most flight cancellationsthe number of flight cancellations by yearly quarterthe 10 most popular flight routes I get the unique hosts per day
[(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)]

how do i get the avg request,.. how can i achive with the above tuple? any hints

To compute the average number of requests per host, get the total number of request across all hosts and divide that by the number of unique hosts. Lab 2 notebook uploaded and started after vagrant up command.  Cells appear not to execute -- I don't get errors on tests, just nothing.  I had completed lab 1, but got this behavior when I got to lab 2.  Jupyter indicates notebook is running, but it doesn't actually appear to be.  Any thoughts? Hello everyone,

I would like to ask a question. In exercise 3a we have to get the data different than 200. But is it possible to put a condition in a function lambda and to get only the data different than 200?. I don't know how to write a condition in a map.

Thanks in advance

Carlota Vina I am just puzzle , how to solve this one
I have tried thisway
dayToHostPairTuple = access_logs.<FILL IN> PLEASE DO NOT POST SOLUTIONS
dayGroupedHosts = dayToHostPairTuple.<FILL IN>
dayHostCount = dayGroupedHosts.<FILL IN>
dailyHosts = (dayHostCount.cache())

please suggest/guid me hot to solve this on OK.  I was having an issue with out of memory in 3e so I changed tactics and used a different method in the first two steps.

1 - I created a tuple of the (day, hosts, 1) and then printed out a few of them to verify it is creating the data I am expecting.  Sample returned was right on the money.

2 - I then in the groupByDay section did a map and a reduce to take just the day and the 1 (dropped the host from the tuple) and reduced by adding up the 1's using a lambda function a+b.  Printed out the full set of data there and verified I have the values I am looking for and if I do the math on a few of the days dividing the resulting count by the unique hosts numbers for the same days out of 3c the first few check out as the correct/expected values for the averages.  

This is where I am stuck.  I can't seem to sort this RDD.  I get this error:
AttributeError: 'PipelinedRDD' object has no attribute 'sortedByKey'

Not sure why I cannot run the sortedByKey on this RDD.  

I can't for the life of me figure out how to get this sorted.

Any help is appreciated.  I am definitely stuck now and putting this down for tonight.

Update:
I made some slight modifications and now I have the list of days and total visits sorted.  Now I am running into an issue on how to take the day and total visits from this RDD and divide that number with the number of unique visits from the RDD in 3c.  

Any suggestions here?  Also, can anyone point out how to post replies?  I can only seem to edit my original post. Still stuck in 3a.

Now I passed endpointSum.count() = 7689 but not 10 urls are wrong.
My top 10s are.

Top Ten failed URLs: [(u'/cgi-bin/imagemap/countdown69?200,46', 1), (u'/cgi-bin/imagemap/countdown69?397,262', 1), (u'/cgi-bin/imagemap/fr?182,498', 1), (u'/facts/facts.html', 1), (u'/cgi-bin/imagemap/countdown69?60,190', 1), (u'/cgi-bin/imagemap/countdown69?58,216', 1), (u'/cgi-bin/imagemap/countdown69?340,293', 1), (u'/cgi-bin/imagemap/countdown69?410,288', 1), (u'/cgi-bin/imagemap/countdown69?256,204', 1), (u'/cgi-bin/imagemap/crew?197,246', 1)] After join when I try to mapavgDailyReqPerHost = sortedByDay.join(dailyHosts).reduceByKey(lambda (a,b): a, b[0]/b[1])

-> 11 avgDailyReqPerHost = sortedByDay.join(dailyHosts).reduceByKey(lambda (a,b): a, b[0]/b[1])
     12 avgDailyReqPerHostList = avgDailyReqPerHost.take(30)
     13 print 'Average number of daily requests per Hosts is %s' % avgDailyReqPerHostList

NameError: name 'b' is not defined Hi all,

I tried a regex on http://pythex.org/. it matched the two string both the invalid and valid row. 
The regex is 
(\w+\S+)[\.](\w+)[\.](\w+)[\.](\w+) [\- ]+ \[([\w:/]+\s[+\-]\d{4})\] "(\w+).*(\w*)" (\d{3}) (\S+)

This thrown an exception when used in spark this way

APACHE_ACCESS_LOG_PATTERN = '(\w+\S+)[\.](\w+)[\.](\w+)[\.](\w+) [\- ]+ \[([\w:/]+\s[+\-]\d{4})\] "(\w+).*(\w*)" (\d{3}) (\S+)'parsed_logs, access_logs, failed_logs = parseLogs()


The resulting exception is 
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-16-0faad6503d9b> in <module>()
      3 # This was originally '^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+)\s*(\S*)" (\d{3}) (\S+)'
      4 APACHE_ACCESS_LOG_PATTERN = '(\w+\S+)[\.](\w+)[\.](\w+)[\.](\w+) [\- ]+ \[([\w:/]+\s[+\-]\d{4})\] "(\w+).*(\w*)" (\d{3}) (\S+)'
----> 5 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-13-8d8d6629d991> in parseLogs()
     22                    .filter(lambda s: s[1] == 0)
     23                    .map(lambda s: s[0]))
---> 24     failed_logs_count = failed_logs.count()
     25     if failed_logs_count > 0:
     26         print 'Number of invalid logline: %d' % failed_logs.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 91, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-1-0940d9fe8819>", line 44, in parseApacheLogLine
  File "<ipython-input-1-0940d9fe8819>", line 16, in parse_apache_time
ValueError: invalid literal for int() with base 10: ''

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) Hi all, 
I am using a little bit old PC however there was not any problem during working with notebook1. Now, after running lab2 - 1b my machine is frozen... any idea how can i speed it up ?  filter(lambda log:log.response_code == 404).cache())
I used the above line of code in 4a,but it showed me a syntax error.What is going on?I have never used python before,so I can't figure this problem out.
 I tried to restart vagrant , but the system is still very very slow. Please advise.
Thanks.
 provide hint on lab2 1c
unable to complete next task unless we complete 1c I have the following process for 3c:

1) map to ((x.date_time.day, x.host),1))
2) reduce by key a + b
3) sort by key (k,v): (k,v)
4) to define daily hosts I only use collect().cache()
5) take(30)

But on step 5) when we use take(30) I get the error: 'list' object has no attribute 'take'

I think I am defining everything correctly, but I cant get around this error. What is the problem? Thanks! please explain (in details, if possible) the following function:

access_logs = parsed_logs.filter(lambda s: s[1] == 1).map(lambda s: s[0]).cashe())

in particular, I have problem understanding : map(lambda s: s[0]).

What is the sequence of events, from right to left or vise versa?

Thanks Hi,

I need some help in Lab (1f). I am stuck with this exercise. 

The exercise is:
The next step in writing our word counting program is to create a new type of RDD, called a pair RDD. A pair RDD is an RDD where each element is a pair tuple (k, v) where k is the key and v is the value. In this example, we will create a pair consisting of ('<word>', 1) for each word element in the RDD.
We can create the pair RDD using the map() transformation with a lambda() function to create a new RDD
# TODO: Replace <FILL IN> with appropriate code
wordPairs = wordsRDD.<FILL IN> print wordPairs.collect()I have created the following:# TODO: Replace <FILL IN> with appropriate code wordPairs = wordsRDD.map().groupByKey(lambda x: (x,1)) print wordPairs.collect()Not sure what am I doing wrong? I am not a Python or a Lambda programmer so I apologize if I am overlooking something small. In various places in Lab 2, such as constructing x and y coordinate lists for graphs, key and value extraction from tuples via lambda expressions could be easily be replaced by Spark API built-ins `rdd.keys()` and `rdd.values()`

Is there some reason to stick with the lambda-functions instead of something more [DRY](https://en.wikipedia.org/wiki/Don't_repeat_yourself) and less likely to be mistyped?


  I adore the way that the labs are made at its best to resemble what one would encounter in the real world. Just like they're taken straight out from industry and put into our hands, rough and need to be polished
I intend to take it further after this, dealing with real world data and take insights from it
How can we make a smooth switch from these labs into industry ? I mistakenly created an empty cell in Lab 2. How do I delete it? Due to some personal issues, I will miss two weeks of the course (end of june, beginning of july).
I do not need a certificate but I would like to finish the course.

What are my options?

e.g   Can I continue using the course material after the end Date? ( I can skip the Autograder if I need too.)

What will be left after the official end of the course? I have managed to prepare a joined RDD as follows:

((Key1, (Val11, Val12)), (Key2, (Val21, Val22)).....)

I want to perform operation to get resultant RDD as:

((Key1, Val11/Val12), (Key2, Val21/Val22) ....)

I have used .map(lambda(k, (v1, v2)): k, v1/v2) but it throws an error that v1 isn't defined.

Can anyone help me use correct command here? Splunk excels at machine data mining. And it has a very quick learning curve.
How is machine data mining better with Spark than Splunk other than Scalability? or its not? What are the following variables supposed to contain?
As I currently understand, it is:

dayToHostPairTuple :: Distinct Day -> Host map (Single Host, not a list)

dayGroupedHosts :: Day -> Host map (Here Host is a list)

dayHostCount :: Day -> Count of Hosts

dailyHosts :: Day -> Count of Hosts

2 Questions:
1) Can dayGroupedHosts be used without using groupBy since it is supposed to be avoided?
2) dayHostCount and dailyHosts seem same to me. What is the difference between the two? Hi,

I tried to launch the VM on Digital Ocean but I am getting the following error:
The guest machine entered an invalid state while waiting for itto boot. Valid states are 'starting, running'. The machine is in the'gurumeditation' state. Please verify everything is configuredproperly and try again.If the provider you're using has a GUI that comes with it,it is often helpful to open that and watch the machine, since theGUI often has more helpful error messages than Vagrant can retrieve.For example, if you're using VirtualBox, run `vagrant up` while theVirtualBox GUI is open.
Unfortunately, I dont have a GUI.

I VM configuration is:

CPU: 2
RAM: 4GB
SSD: 60 GB
OS: Fedora 22

Thanks, All of my tests pass when I run my code, but when I submitted it I got the errors below. I've gone through and verified that it works 3 times. I ran each cell in order before submitting, but the error message did not change. Can someone please help? The autograder response is below:


Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 27, in 
NameError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
name 'add' is not defined

All tests passed
Top ten error endpoints (3a)
----------------------------
All tests passed
Number of unique hosts (3b)
---------------------------
All tests passed
Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dailyHosts' is not defined

Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithHosts' is not defined

Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'avgDailyReqPerHostList' is not defined

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithAvg' is not defined

Counting 404 (4a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badRecords' is not defined

Listing 404 records (4b)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badUniqueEndpointsPick40' is not defined

Top twenty 404 URLs (4c)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badEndpointsTop20' is not defined

Top twenty-five 404 response code hosts (4d)
--------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errHostsTop25' is not defined

404 response codes per day (4e)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errByDate' is not defined

Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithErrors404' is not defined

Five dates for 404 requests (4g)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'topErrDate' is not defined

Hourly 404 response codes (4h)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errHourList' is not defined

Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'hoursWithErrors404' is not defined

-- 3 cases passed (18.0%) --


Your submission token id is 672747-cf729111357fe2141d8a69eec170578d:5b6ebb2287ac7a4117fe4051c3540565:ip-172-31-45-43
Please include this submission token id when you need support for your code submission. After reviewing I feel comfortable with the code but have received repeated timeouts. Need some kind of meaningful feedback to move forward, since there is the constraint of "filling in code".

673119-5ace6981036a59f14b6e60aedf246b81:0630b0e0055ff8ce2e33852158b52c50:ip-172-31-13-206 
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-37-86450af5c733> in <module>()
     10 endpointSum = endpointCountPairTuple.reduceByKey(lambda (x , y) :(x, sum(y)))
     11 
---> 12 topTenErrURLs = endpointSum.takeOrdered(10, lambda s: -1 * s[1])
     13 print 'Top Ten failed URLs: %s' % topTenErrURLs

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in takeOrdered(self, num, key)
   1172             return heapq.nsmallest(num, a + b, key)
   1173 
-> 1174         return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
   1175 
   1176     def take(self, num):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 67.0 failed 1 times, most recent failure: Lost task 0.0 in stage 67.0 (TID 188, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
  File "<ipython-input-37-86450af5c733>", line 8, in <lambda>
AttributeError: 'bool' object has no attribute 'endpoint'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

  hi,

I am using the below code 

# TODO: Replace <FILL IN> with appropriate code
dayAndHostTuple = access_logs.map(lambda log : (log.date_time.day,1)).reduceByKey(lambda a,b: a+b).distinct()
groupedByDay = dayAndHostTuple.join(dailyHosts)#print groupedByDay.collect()sortedByDay = groupedByDay.sortByKey()#print sortedByDay.collect()avgDailyReqPerHost = (sortedByDay .map(lambda a,b: (a, b[0]/b[1]))).cache()avgDailyReqPerHostList = avgDailyReqPerHost.takeOrdered(30, lambda s: -s[1])print 'Average number of daily requests per Hosts is %s' % avgDailyReqPerHostList

it is giving me below error

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-41-43a08fd91c89> in <module>()
     10                       .map(lambda a,b: (a, b[0]/b[1]))).cache()
     11 print avgDailyReqPerHost
---> 12 avgDailyReqPerHostList = avgDailyReqPerHost.takeOrdered(30, lambda s: -s[1])
     13 print 'Average number of daily requests per Hosts is %s' % avgDailyReqPerHostList

Can anybody suggest how to solve this # TODO: Replace <FILL IN> with appropriate code
pluralRDD = wordsRDD.map(<FILL IN>)
print pluralRDD.collect()
File "<ipython-input-16-e32b63862d41>", line 2
    pluralRDD = words.RDD.map(<FILL IN>)
                              ^
SyntaxError: invalid syntax
 Do you guys just extract everything straight from the start?   I found myself just extracting the response code, endpoints at the beginning, then I went back and extract response code, endpoints & host subsequently when I realize I need that further down.   Is there a more efficient way than just extracting EVERYTHING? Dear Course Staff,

Unfortunately, I've missed the deadline for signing up for the Verified Certificate option this week. Is it possible to extend it a bit or is there another way to apply for this option after the deadline?

Thanks,
Iurii Hi Guys,

I have been stuck on Lab 13(b) for some time. Any tips to proceed will help.

wordCounts has the following value. Any tip on what should I do to map this may help?

[('rat', 2), ('elephant', 1), ('cat', 2)]
# TODO: Replace <FILL IN> with appropriate code
from operator import add
totalCount = (wordCounts
              .map<FILL IN>
              .reduce<FILL IN>
average = totalCount / float(<FILL IN>)
print totalCount
print round(average, 2)
 
average = totalCount / float(<FILL IN>)
print totalCount
print round(average, 2) I would like to know if we have the replication factor settting for the RDD's.

I understand that Spark will recompute the RDD's if they are lost at any node.

So does it mean that we dont want the RDD's to be replicated?

Please clarify.

Thanks. I can not locate the Lab 2 IPython notebook, please help me locate it. Just to make sure, if I pass a distinct function on a tuple, it will distinct V value only, not Key?

RDD = access_logs.map(lambda x: (y, z)).distinct()

if I have (1, 2) (1, 4) (2, 3) (2, 3) (2, 4) I will have (1, 2) (1, 4) (2, 3) (2, 4) ?

Right?

Thanks Hi all,

In  Lab 2, 4c, how can I sort by value and not by key, in other words by the second element in the tuple not the key?

I have this:
badEndpointsSum = badEndpointsCountPairTuple.<FILL IN>


Any help is appreciated. I think the code is just fine
daysWithHosts = dailyHosts.map(lambda x,y:x).collect()

I have cached dailyHosts and it has the correct value but I get this 1 page of meaning less syntax  error which tells me nothing. Any thoughts, thanks 

The tester says name daysWithHosts not defined. Maybe my map fails some how.

It seems as collect() is the problem. I am trying to figure out if i can even run lab3 on my computer....

I thought that I would have to do access_logs.map(lambda log: (log.date_time.day, log.host)) and then group by day and then use len(set) but grouping them by day without using distinct does not seem to be an option: whem do dayToHostPairTuple =  access_logs.map(lambda log: (log.date_time.day, log.host)) then the test just hangs forever....-it has been at least 5 minutes the Python 2 is still greyed out....

If I do dayToHostPairTuple =  access_logs.map(lambda log: (log.date_time.day, log.host)).distinct() then my test will run faster, but fail later....

Please help - how long should grouping hosts by day take (without removing duplicates at first?). I already restarted vm, etc. Thank you.
 I think some students may think it take two long to study something a week and a week. When some have a time it may want study it one time.

Any body can take its own time to study course. But if you offer a whole course at one time will be good for somebody. Stuck on this.

I succeded in mapping the tuple (day, host), get the total of requests per day, sort it, I join both RDD (number of hosts and number of requests on a particular day) getting this format:

(1, (33996, 2582))
But I fail to calculate the average reducing the tuple(a, b) in a/b. Here's my line:

avgDailyReqPerHost = <FILL IN> PLEASE DO NOT POST SOLUTIONS
The collect function is for testing purposes only.

Any help? hI

I would like to know for verified track (paid),if grade would appear on the certificate

Thanks
 -- 5 cases passed (31.0%) --
Your submission token id is 678502-baf74de44542ef75ba0cb96c96fd17e1:e6bc3b47e9af28966ba93c4141609e57:ip-172-31-45-43i passed all level except 3e and 3f in spark_notebook.but grader result is 31.0% and it seems wrongi need an explanation , ty.
 Hi Guys,
Got stuck on this step. I believe that dayHostCount expects to have [(day, (host, host_count)), ..., ] structure, which would be used to count the unique hosts and also extract for the host names.

Any RDD method for this ? I can only think of the regular python way, which is way too clumsy to be used here.

Am I on the right track? Please, need your help.
Many thanks.


Update: go it to pass the tests. I am able to complete 4a , but not able to get 4b logic . can anyone explain a bit ? I got the answer 4079 which was wrong. 

Then the question turns out that what the definition of unique host is ? I thought the unique host is the host just appears once in the log file. Am I wrong ? 
 Hi,
I have submitted lab1 on 13th June during the grace period time. However, my score was 80% from autograder.
Not sure if it will be corrected later.

thanks,
Regards
Anil could someone please help me out with the error for the code below.
daysWithHosts = dailyHosts.<FILL IN>
hosts = dailyHosts.<FILL IN>

Edit - Posting code violates the Honor Code. In any case, Chris Sweeney's response below should help you understand what the issue was. given: 
Python List count() Method
Description

The method count() returns count of how many times obj occurs in list.
Syntax
Following is the syntax for count() method −
list.count(obj)
Parameters

obj -- This is the object to be counted in the list.

Return Value
This method returns count of how many times obj occurs in list.

Question:

What does it mean count() , another words,a  function with no object.
What gets returned?
how could we worked in Lab -1 with it? Here are the first five elements of my dayToHostPairTuple.  Why is a u attached to the hosts?[(14, u'204.174.82.76'), (15, u'192.112.22.106'), (7, u'gw1.cat.com'), (17, u'mac.100.39.arc.ab.ca'), (7, u'192.112.22.115')] Hi All,

Anyone's computer hanging while running the code for Lab 2 ?

When I see the RAM status it shows 85% used. I have only 2 gb RAM , is it because of that ?

Thanks, dayToHostPairTuple = access_logs.<FILL IN>dayGroupedHosts = <FILL IN>dayHostCount = <FILL IN>(lambda x, y: (x, set(y).count()))dailyHosts = <FILL IN>dailyHostsList = dailyHosts.collect()print 'Unique hosts per day: %s' % dailyHostsList

plz help. On Lab2 the Amazon ec2 t2.medium is just too slow and unresponsive to be practical.  It's horribly slow and completely undermines the thought process and wastes the hour or two which I have allocated to do this homework.  Cannot recommend using Amazon t2.medium, despite the instructors suggestions.

So much for The Cloud.  I am maybe going back to building a Linux box right on my desk, with the required OS version for the class virtualbox, that can run Spark fast enough to be at least somewhat useful for school assignments.  Too bad I don't actually have time for this excessive system admin work.

OK now back I go to my ipython notebook for Lab2, and see if the asterisks have disappeared yet. What:  t2.medium

Comments: Too slow and impractical for lab 2.  Recommend getting something faster than this. "THe guest machine entered an invalid state while waiting for it to boot. Valid states are starting, running. The machine is in the poweroff state. Please verify everything is configured properly and try again.

If the provider you're using has a GUI that comes with it is is often helpful to open that and watch the machine, since the GUI often has more helpful error messages than vagrant can retrieve. For example, if you're using VirtualBox, run vagrant up while the VirtualBox GUI is open."

I did everything just as it was in the video, I also ran cmd as administrator and that didn't help either! I can't set up my environment from the console, should I just go to the VirtualBox manager and right click the virtual machine and click start? it shows that the machine is in poweroff state

I tried to run the vm from the virtual box manager and it shows "unable to load R3 module C:\ProgramFiles\Oracle\VirtualBox/VBoxDD.DLL (VBoxDD) Using original  Rex (APACHE_ACCESS_LOG_PATTERN ), I got 108 invalid lies and in second step I changed Rex. This time it corrected all  previous invalid line 108 but showed all remaining lines as invalid. My question is , how to move in second step so that total invalid lines would be zero and local test 1c will pass.   I mean 
 Logfile(input) ->(orginal Rex) ->invalidLines(intput) -> (changeRx) -> ( invalidLine ==0). I think this is a kind of silly question but I am expecting some help. Thanks.   Hi,

What's the difference between .....(lambda a,b: a + b) and ......(lambda (a,b): b)  
Sometime I need to use parenthesis in the lambda and sometimes not, it's look like different So, I downloaded the lab2 ipython notebook to my
mooc-setup-master  folder and started vagrant with vagrant up.
But, I'm not able to see the lab 2 notebook there. 

I'm certain that vagrant is running in the right folder and that the file is saved in the same folder too. Did not see this problem with lab 1.

Could anyone suggest a solution ? I have tried restarting vagrant multiple number of times to no avail. Does VirtualBox actually recognize the multi-core nature of CPU?   Looking at the codes that we've been using for Lab1 & Lab2, it doesn't look obvious to me that it's indeed taking advantage of cluster/multi-core CPU computing.  

So IF the code is indeed running on single core AND IF there's a way to set the VirtualBox to access multi-core, then I would love to try & see the performance pick up by using the cluster mode... I am executing lab-2. There appears to be some problem with iPython notebook. Even after executing the code (using 'play' button), the notebook is not responding. The iPython notebook does show as "running", but it doesn't seem to be working.

Not sure what the problem is (I was able to run lab0 and lab1 successfully)

Any help is greatly appreciated.

Best

_________________
Update:

Thanks! I got my answer

It's taking a bit longer than usual because the file is large. It is working indeed.

Best 


I've tried
1. reboot my laptop
2. vagrant up/halt
3. manual restart
4. firefox/chrome

help...


-----------
(I don't know how to reply the instructors' answer directly, so post here)

What happens when you do manual restart?
I got 'Kernel starting, please wait', and then 'Dead kernel' again.
 I can get correct numbers but when I use sortByKey() and cache() together, dailyHosts.takeOrdered(30) yields an error.
Any hint where  the problem could be ?



Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 629.0 failed 1 times, most recent failure: Lost task 0.0 in stage 629.0 (TID 992, localhost): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:172)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:108)
	... 8 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) I'm getting a huge error for this code. any ideas?
For the dayAndHostTuple, I used map syntax with the tuple (log.date_time.day,1) in it along with reduceByKey(lambda a,b: a+b).distinct()
For the groupedByDay, I "join"ed dayAndHostTuple and dailyHosts
For the sortedByDay, I gave a sortByKey for groupedByDay
For the avgDailyReqPerHost I gave .map(lambda a,b: (a, b[0]/b[1]))


avgDailyReqPerHostList = avgDailyReqPerHost.take(30) I have submitted for autograder. I haven't had time to answer some other items; so I run another copy of the notebook so that no errors were flagged out.

But the autograder just seem to ignore all the later parts once I don't answer any of the lab tests in the upper parts.

I skipped 3(c) to 3(f); but I answered 4(a), 4(b), 4(c), 4(d), 4(g), 4(h) and tested OK in the IPython notebook

The autograder only seem to give credit only for 1(c), 3(a) and 3(b) and pretty much ignored the rest.

submission token id is 683293-ec9d4423e398840a568733c027e15b1b:8a053f9439b0163a2fbf4f8c2e11daf6:ip-172-31-47-248
 Hi There,
In exercise 3C I am getting this values:

21

Unique hosts per day: [(8, 4406), (12, 2864), (4, 4190), (16, 4340), (20, 2560), (1, 2582), (5, 2502), (9, 4317), (13, 2650), (17, 4385), (21, 4134), (22, 4456), (10, 4523), (18, 4168), (14, 4454), (6, 2537), (11, 4346), (15, 4214), (3, 3222), (19, 2550), (7, 4106)]




How can i sort it?

Thanks
RF



 I am using reduceByKey for avgDailyReqPerHost but not sure if I am right

reduceByKey(lambda a, b : (a+b)/uniqueHostCount) I am not sure if this would yield the correct result. It actually does not, but after groupByKey and sortByKey on a tuple,I thought this is how I would get the average. Thanks for all the inputs

I don't think my code is dividing by uniqueHostCount eventhough its value is 54507 I am getting.


Average number of daily requests per Hosts is [(1, 33996), (3, 41387), (4, 59554), (5, 31888), (6, 32416), (7, 57355), (8, 60142), (9, 60457), (10, 61245), (11, 61242), (12, 38070), Hi,
I am not passing the test because i think its not on the order it should be..

currently I am having this:
# TODO: Replace  with appropriate code daysWithHosts = sorted map lambda x  collect()hosts =  map lambda y collect() print daysWithHostsprint hosts[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
[4406, 2864, 4190, 4340, 2560, 2582, 2502, 4317, 2650, 4385, 4134, 4456, 4523, 4168, 4454, 2537, 4346, 4214, 3222, 2550, 4106]
can someone help out?

Thanks
RF i am not getting any error or result while result while running 3c
what could be the problem my code is below...

dayAndHostTuple = <FILL IN>
POSTING SOLUTIONS IS A VIOLATION OF THE HONOR CODE
print 'Average number of daily requests per Hosts is %s' % avgDailyReqPerHostList


I am getting below message when I executed my code

1 test failed. incorrect avgDailyReqPerHostList
1 test passed.

whats wrong with the above code? I have got in a pickle with a lab question and need to see the original question before I messed it up.
How is this done please?
Thanks I have accidentally deleted the 2 files present under the location /home/vagrant/data. Is there a way to obtain them. I was trying to sync my local directory with Vagrant. Stuck!
To get not200
    I filter log.response_code for != 200. Take(1) gives me this
[Row(client_identd=u'-', content_size=0L, date_time=datetime.datetime(1995, 8, 1, 0, 0, 7), endpoint=u'/', host=u'uplherc.upl.com', method=u'GET', protocol=u'HTTP/1.0', response_code=304, user_id=u'-')]
    next map the (endpoint,1)
print  not200 gives me
RDD at PythinRDD.scala:43 - what does this mean

Then for count I do (not200.endpoint,1)
sum(a,b:a+b)
print endpoint sum gives me 43

Basically all print statements give me 43. Also what is wrong with the statement below: Getting syntax error

topTenErrURLs = endpointSum.takeOrdered(10, lambda s:  s[1])

Obviously not right!

help

thanks



 I know we have not done this in class yet, but I would like to be able to create an RDD from a Microsoft Excel file (xlsx). I imagine this must be doable?? can someone please show an example or give me some pointers.

thanks, I'm pretty sure my dayToHostPairTuple is correct. Here is a snippet.  But the next step to make dayGroupedHosts I can't figure out. I would think groupByKey would work but that doesn not seem to work.
Any hints on how to get dayGroupedHosts if what you see here is my dayToHostPairTuple.
[(14, u'204.174.82.76'), (15, u'192.112.22.106'), (7, u'gw1.cat.com'), (17, u'mac.100.39.arc.ab.ca'), (7, u'192.112.22.115')] Does anyone know if a solution to lab 1 has been posted as the submission date has passed.
I think it would be beneficial for learning the things that you weren't able to solve correctly.
Maybe the instructors can comment.
 Is it okay to do this? I got the correct answer but I was wondering if there is another way...

badEndpointsSum = badEndpointsCountPairTuple.reduceByKey(lambda a,b:a+b) -> Swap the keys and values
badEndpointsTop20 = badEndpointsSum -> re-swap the keys and values What does it mean and how could I fix it:
AttributeError: 'int' object has no attribute 'endpoint'
Complete Trace follows:Py4JJavaError Traceback (most recent call last) <ipython-input-27-2c089497fb9e> in <module>()  10 endpointSum = endpointCountPairTuple.reduceByKey(lambda a, b: a + b)  11  ---> 12 topTenErrURLs = endpointSum.takeOrdered(10, key = lambda s: -1 * s[1])  13   14 print 'Top Ten failed URLs: %s' % topTenErrURLs /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in takeOrdered(self, num, key)  1172 return heapq.nsmallest(num, a + b, key)  1173  -> 1174 return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)  1175   1176 def take(self, num): /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)  737 yield reduce(f, iterator, initial)  738  --> 739 vals = self.mapPartitions(func).collect()  740 if vals:  741 return reduce(f, vals) /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)  711 """  712 with SCCallSiteSync(self.context) as css: --> 713 port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())  714 return list(_load_from_socket(port, self._jrdd_deserializer))  715  /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)  536 answer = self.gateway_client.send_command(command)  537 return_value = get_return_value(answer, self.gateway_client, --> 538 self.target_id, self.name)  539   540 for temp_arg in temp_args: /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)  298 raise Py4JJavaError(  299 'An error occurred while calling {0}{1}{2}.\n'. --> 300 format(target_id, '.', name), value)  301 else:  302 raise Py4JError( Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46.0 (TID 132, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last): File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main process() File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process serializer.dump_stream(func(split_index, iterator), outfile) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func return func(split, prev_func(split, iterator)) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func return func(split, prev_func(split, iterator)) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func return f(iterator) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally merger.mergeValues(iterator) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 252, in mergeValues for k, v in iterator: File "<ipython-input-27-2c089497fb9e>", line 8, in <lambda> AttributeError: 'int' object has no attribute 'endpoint' at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135) at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:64) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) 
  dayToHostPairTuple = access_logs.<FILL IN>
dayGroupedHosts = <FILL IN> POSTING SOLUTIONS IS A VIOLATION OF THE HONOR CODE
dailyHostsList = dailyHosts.take(30)
 
print 'Unique hosts per day: %s' % dailyHostsList



I am getting the following error
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-42-e70e1fef69d9> in <module>()
      7 dayHostCount = dayGroupedHosts.map(lambda (x, y): (x, len(set(y))))
      8 
----> 9 dailyHosts = dayHostCount.sortByKey()
     10 dailyHostsList = dailyHosts.take(30)
     11 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sortByKey(self, ascending, numPartitions, keyfunc)
    584         # the key-space into bins such that the bins have roughly the same
    585         # number of (key, value) pairs falling into them
--> 586         rddSize = self.count()
    587         if not rddSize:
    588             return self  # empty RDD

Please correct me where I am wrong?? I originally used a compound key design to solve lab2 ex 3c.

The code runs without error but the prints do not work.

Thus I cannot tell if the solution actually works, if I could get the print to work.

For example my first line of code is

r2 = access_logs.map( lambda x: ((x.date_time.day, x.host), 1) )

You see, I have a pair tuple which contains a pair tuple as the key.

Like I said it does not work (print).  An error is emitted from the following print statements, concerning the wrong number of arguments to lambda.  Of course, I did not use lambda in the print statements. It seems to be an implementation limitation of the top() or the count().

print r2.top(3)
print r2.count()

The textbook Learning Python never talks about it, nor does our instructor.

Has anyone else used a compound key design in their solution? Did you get it to print?


 What is this? Its there na, y this error? please help. Thank you
 Hi There,

I have an error when I try to use take,

for exemple in the lab 2, 3c, this line return me an error :
print dayHostCount.take(3)

any help appreciated ...
 Dear Course Staff,

Thank you for your great efforts on this valuable course. 
Kindly requesting you to open the lectures and labs for weeks four and five for the following reasons:
I live in Yemen which is currently unrunder the Saudi aggression and we don't have electricity for more than 84 days (all Republic of Yemen sink in dark) and its too hard to follow with this course because I need to spend time in internet cafe to download course materials.
Furthermore, Ramadhan (month) started today and the official working hours reduced because of people being fast during Ramadhan and that reduces my available online time in consequence.
It's really complicated situation and I hope that you cooperate with and open all the course materials for the coming weeks as soon as you can. 

My sincere appreciation in advanced   Hi everyone,

Sorry for the simple question. 

Since a couple of hour ago, any take (top or similar) hangs my notebook. The tab shows as (busy) for a while, then disappear and nothing happens. Any hint? Hello everybody.
I have got question, I looked at Spark notebook and I see how to setup it on my own, but I'm interested how to setup VM in similar way it was prepared in course. Could somebody provide me some hints, or maybe try to explain how vagrant is working here? I would like to prepare something similar for my own usage.
Thanks, Bartek. Hi,

I had issues with my vagrant so had started late with lab exercises. I got stucked in lab 1 problem 1f. When I'm running the code, I'm getting the following solution. But it failed in subsequent test. Not sure what's the issue. Can anybody suggest me the way or any hint without affecting the honor code.

print wordPairs.collect()
[('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]
Thanks in advance!
 I get the following counts with this code: 

not200 = (access_logs.filter(lambda log: (log.response_code != 200))) = 102330                       print not200.count()
endpointCountPairTuple = access_logs.map(lambda not200: (not200.endpoint, 1))        = 1043177
print  endpointCountPairTuple.count()

Not right.. Need help

Thanks My code:
dayToHostPairTuple = <FILL IN> POSTING SOLUTIONS IS AN HONOR CODE VIOLATION
print 'Unique hosts per day: %s' % dailyHostsList

I get error as folllows:
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-48-38e6d34e288d> in <module>()
      5 dayGroupedHosts = dayToHostPairTuple.groupByKey().mapValues(lambda x:list(x))
      6 dayHostCount = dayGroupedHosts.mapValues(lambda x:sum(x))
----> 7 dailyHosts = (dayHostCount
      8               .sortByKey()
      9               .cache())

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sortByKey(self, ascending, numPartitions, keyfunc)
    584         # the key-space into bins such that the bins have roughly the same
    585         # number of (key, value) pairs falling into them
--> 586         rddSize = self.count()
    587         if not rddSize:
    588             return self  # empty RDD

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 83.0 failed 1 times, most recent failure: Lost task 0.0 in stage 83.0 (TID 229, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <genexpr>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1806, in <lambda>
    map_values_fn = lambda (k, v): (k, f(v))
  File "<ipython-input-48-38e6d34e288d>", line 6, in <lambda>
TypeError: unsupported operand type(s) for +: 'int' and 'unicode'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

Whats wrong?I mean why error at daily hosts?
 Just curious, is it possible get less than 100% if all tests pass? Assuming the submission is not late. Hi guys,
I am really struggling with 3c.
I have dayToHostPairTuple = map....
and dayToHostPairTuple.reduceByKey using (day,ip-address) as key
which gives me tuples ((day,ip-address),count)

But I guess I have to reduce by day only, right?
How can I proceed from here?
Sorry, I am really stuck
[((14, u'204.174.82.76'), 9), ((15, u'192.112.22.106'), 5), ((7, u'gw1.cat.com'), 4), ((17, u'mac.100.39.arc.ab.ca'), 12), ((7, u'192.112.22.115'), 1), ((8, u'128.159.124.61'), 6), ((11, u'krimo.gme.usherb.ca'), 6), ((11, u'goofman.huntcol.edu'), 11), ((16, u'139.169.99.243'), 6), ((9, u'128.159.135.73'), 60)]			 why sometimes we can use access a key value parameters in a lambda function using 2 variables, and sometime just as an array ?

I tested code from the book "Learning Spark" in a Spark installation

the code was like

rdd.map( lambda x, y: y )

the same code does not work in Jupiter, it must be like
rdd.map( lambda x: x[ 1 ] )

is that a configuration in Jupiter or is something related to Python?
any clue? Hello, 
in the Lab 1 Part 2 problem description it says "
A naive approach would be to collect() all of the elements and count them in the driver program. While this approach could work for small datasets, we want an approach that will work for any size dataset including terabyte- or petabyte-sized datasets.
"
I am still not very clear on what I can be confident is executing via the driver program vs on workers. Can anyone provide the code that would be deemed the naive way so I can compare to the groupByKey() approach? 

Is there a way to see what code is being used in the driver program vs the workers after I run programs? Are function literals the only things passed to workers? if I say >>> rdd.collect() , will this not be passed to a worker since there is nothing inside of the parentheses? 

Thanks for any help!! 
 These are my top20 URLs: 
Top Twenty 404 URLs: [(u'/pub/winvn/readme.txt', 632), (u'/pub/winvn/release.txt', 494), (u'/shuttle/missions/STS-69/mission-STS-69.html', 431), (u'/images/nasa-logo.gif', 319), (u'/elv/DELTA/uncons.htm', 178), (u'/shuttle/missions/sts-68/ksc-upclose.gif', 156), (u'/history/apollo/sa-1/sa-1-patch-small.gif', 145), (u'/images/crawlerway-logo.gif', 120), (u'/://spacelink.msfc.nasa.gov', 117), (u'/history/apollo/pad-abort-test-1/pad-abort-test-1-patch-small.gif', 99), (u'/history/apollo/a-001/a-001-patch-small.gif', 95), (u'/images/Nasa-logo.gif', 85), (u'/shuttle/resources/orbiters/atlantis.gif', 64), (u'/history/apollo/images/little-joe.jpg', 62), (u'/images/lf-logo.gif', 57), (u'/shuttle/resources/orbiters/discovery.gif', 56), (u'/shuttle/resources/orbiters/challenger.gif', 54), (u'/robots.txt', 53), (u'/elv/new01.gif>', 43), (u'/pub', 36)]
As you can see, the result is almost identical to the Test.assertEquals values, but some are different by +/- 1 or 2. I can't figure out from the 1 million log entries where the problem is, and when I try it with a small test logfile it works correctly. I'm speculating that it might be related to the fact that I'm in the CET timezone and that datetime.day is giving a different result to the PST timezone? Any ideas? I finally finished lab 1.  I work for a living plus have family obligations.
I feel your 20% deduction is a bit severe.  
If the objective of the course is for people to learn and not drop out, for future courses, I suggest making the grace period one week and only 10% off.  
I will not be paying for the certification mainly for this reason. Am I wrong to start like this? 

badEndpointsCountPairTuple = badRecords.map(lambda a, (b, c): (b, c))

Being said that in my previous ex. my values were 404, (endpoint, 1)

Since I don't need anymore the 404, I map it like this : map(lambda 4, (b,c):(b,c)

To have tuples only and then reduce by key.

Since it returns me an error, I assume something is wrong...

Thanks for your help.

 I'm having some strange error while submitting the solution.

This is what I receive:

Data cleaning (1c) ------------------ Traceback (most recent call last): File "", line 27, in Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 71.0 failed 1 times, most recent failure: Lost task 0.0 in stage 71.0 (TID 179, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last): File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main process() File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process serializer.dump_stream(func(split_index, iterator), outfile) File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream vs = list(itertools.islice(iterator, batch)) TypeError:

And what is more worring exercise 4 a fails and then the rest of them seem to fail, but on my notebook they seem to work.

Counting 404 (4a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badRecords' is not defined

Listing 404 records (4b)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badUniqueEndpointsPick40' is not defined

What I'm doing wrong? I see many people struggling with later parts of lab2 due to an undetected error in part 1c. 

If (4b) had an extra test for the unique number of badEndpoints, it would be easier for students to self-diagnose presence of parsing errors. 

Test.assertEquals(6185, badEndpoints.count(), 'incorrect badEndpoints.count(), check your regex')
Test.assertEquals(1201, badUniqueEndpoints.count(), 'Wrong number of bad endpoints, check your regex')
Note: 
Some people seem to pass the lab if the following test is successful, but I double checked my lab and the value I get is 1201. I suppose it is possible to pass the lab with errors if the error does not impact the top 20 most common urls. 
Test.assertEquals(2661, badUniqueEndpoints.count(), 'Wrong number of bad endpoints, check your regex')

Note: The number of badEndpoints should be the same as the number of badRecords in part 4a. 
#pin
 I think I have the right number of days in the result but the contents have odd values, rather than a count.

I think it has to do with my use of groupByKey when calculating dayGroupedHosts, but I'm unclear about how to correct it. Any help or guidance would be appreciated.

Here's an example of the output of the Print - 
Unique hosts per day: [(1, <pyspark.resultiterable.ResultIterable object at 0xb0aeb5ac>), (3, <pyspark.resultiterable.ResultIterable object at 0xb0aeb68c>), (4, <pyspark.resultiterable.ResultIterable object at 0xb0aeb66c>), This last lab was tough! I had no clue how to proceed at 1c and some of the 3 exercises. But I made it! ...after putting in a lot of time. Thanks Prof! Nice challenges! Good Job!

PS. I am a Social Sciences guy with limited exposure to Python and zero exposure to Spark Sometimes I have to run many time that is not good approach in short time the machine from the beginner. Thanks ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused Hello,
I've submit my py file but autograde cannot complete. It is still running. Should i do something?

"Your files have been submitted. As soon as your submission is graded, this message will be replaced with the grader's feedback."

It just take time. I solved. When I submit I get error, says Your submission token id is 695050-e682b56d2a19745aa5e1ee2d7a11bd82:4cfcc0b42bd48904ed03c961cb44647c:ip-172-31-4-222

When I check I see "name 'sortedByDay' is not defined" message, which was defined. Then I look into code and saw that for short usage I skipped some of the variables you've defined. After using exactly that variables and resubmitting all passed.

As I understand, in labs, we have to use exact structure you've defined, right? The notebook displayed a messed-up screen for Lab 2 Q3b. This screen doesn't display the frame within which I can write my code and therefore I can't answer this question.

Unfortunately I can't paste a screen shot. I've tried inserting a file and inserting an image but when I paste the image it doesn't display.

The image file I have is tiff saved by using the MacOS grabber app.
 1(c) i have got 
Read 1043177 lines, successfully parsed 1043177 lines, failed to parse 0 lines
1 test passed.
1 test passed.
1 test passed.I ran rest of the code till 2(f) without errors, 2f threw me following error
Top Ten Endpoints: [(u'/images/NASA-logosmall.gif', 59666), (u'/images/KSC-logosmall.gif', 50420), (u'/images/MOSAIC-logosmall.gif', 43831), (u'/images/USA-logosmall.gif', 43604), (u'/images/WORLD-logosmall.gif', 43217), (u'/images/ksclogo-medium.gif', 41267), (u'/ksc.html', 28536), (u'/history/apollo/images/apollo-logo1.gif', 26766), (u'/images/launch-logo.gif', 24742), (u'/', 20182)]
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-18-606dbe4c203b> in <module>()
      7 
      8 print 'Top Ten Endpoints: %s' % topEndpoints
----> 9 assert topEndpoints == [(u'/images/NASA-logosmall.gif', 59737), (u'/images/KSC-logosmall.gif', 50452), (u'/images/MOSAIC-logosmall.gif', 43890), (u'/images/USA-logosmall.gif', 43664), (u'/images/WORLD-logosmall.gif', 43277), (u'/images/ksclogo-medium.gif', 41336), (u'/ksc.html', 28582), (u'/history/apollo/images/apollo-logo1.gif', 26778), (u'/images/launch-logo.gif', 24755), (u'/', 20292)], 'incorrect Top Ten Endpoints'

AssertionError: incorrect Top Ten Endpoints Please advise. Can not access the Jupyter web UI for running IPython notebooks

C:\myvagrant>vagrant upBringing machine 'default' up with 'virtualbox' provider...==> default: Checking if box 'sparkmooc/base' is up to date...==> default: Clearing any previously set forwarded ports...==> default: Clearing any previously set network interfaces...==> default: Preparing network interfaces based on configuration... default: Adapter 1: nat==> default: Forwarding ports... default: 22 => 2222 (adapter 1)==> default: Booting VM...==> default: Waiting for machine to boot. This may take a few minutes... default: SSH address: 127.0.0.1:2222 default: SSH username: vagrant default: SSH auth method: private key default: Warning: Connection timeout. Retrying...==> default: Machine booted and ready!==> default: Checking for guest additions in VM...==> default: Mounting shared folders... default: /vagrant => C:/Users/Antoni/myvagrant==> default: Machine already provisioned. Run `vagrant provision` or use the `--provision`==> default: to force provisioning. Provisioners marked to run always will still run.
C:\myvagrant>vagrant provision
C:\myvagrant>

Regards I dont know how to approach exactly this question, had tested some different forms, but still having a lot of errors, and sometimes when I tried to print the output nothing happens, the cell keeps processing too long!! more than 10 minutes, so I stop it.

So far this is what I have, dont know if Im going in the right direction, It does not give me any errors, but I can get output to have an idea of what is happening

dayToHostPairTuple = access_logs.<FILL IN> 
POSTING SOLUTIONS IS AN HONOR CODE VIOLATION
print dayHostCount.take(5)
 When I assign

part_of_access_logs=access_logs.top(1)

print part_of_access_logs

will print me this.

[Row(client_identd=u'-', content_size=3421948L, date_time=datetime.datetime(1995, 8, 3, 15, 51, 23), endpoint=u'/statistics/1995/Jul/Jul95_reverse_domains.html', host=u'6', method=u'GET', protocol=u'HTTP/1.0', response_code=200, user_id=u'-')]

I am assuming the host is '6' in this case.

so,

hosts = access_logs.map(lambda log:(log.host,1))

should give me something like [('z',1),('z',1)......,('6',1)....]

If I reduce it by key

uniqueHosts = hosts.reduceByKey(lambda a,b:a+b)

It returns me 

[(u'a', 29653), (u'e', 24371), (u'i', 3912), (u'm', 280277), (u'1', 37022), (u'y', 711), (u'5', 29062), (u'9', 26169), (u'u', 100554), (u'd', 222), (u'p', 22557), (u'h', 4572), (u'l', 16516), (u'0', 36002), (u'4', 35299), (u'x', 2073), (u'8', 25795), (u't', 90733), (u'c', 1037), (u'w', 314), (u'g', 8065), (u'k', 27758), (u's', 7430), (u'o', 3061), (u'3', 34801), (u'7', 28646), (u'b', 288), (u'f', 215), (u'v', 86679), (u'n', 353), (u'r', 11185), (u'j', 1), (u'6', 29666), (u'2', 35124), (u'z', 3054)]

You can see there are only 35 hosts I can get, so I think I am not quite understanding of what is unique Host in this case.

Anyone can explain a little bit more about it?

Thanks!! Lecture 4, Transformation video, filters examle at 3:23 :

comments = lines.filter(isComment)

Lecturer's text is "We then FILTER OUT lines that are comments ... what we say is if the line is a comment, we want it REMOVED from the RDD".

From my point of view the explanation text is wrong and totally opposite: apparently isComment function will return TRUE for comment lines, so the resulting RDD called 'comments' will contain COMMENT LINES ONLY. Not everything that left when the comments are removed, as stated in the explanation. Hence the RDD name: 'comments' :)

BTW, this explanation text obviously contradicts the result we see at the beginning of the same video:

rdd.filter(lambda x: x % 2 == 0)
RDD: [1, 2, 3, 4]  -> [2, 4] Since I cannot get the Notebook to run for Lab 2. I filled in an answer for 1c and sent the .py from the notebook
to the grader. 1c passed as I received some credit(there was a syntax error or some small message but passing).
So I then filled in some answers for a few parts of 3 and 4(not all of either).
I saved the notebook as .py again and resubmitted to the grader. 1c still receives credit but there is a larger message and then all other parts are not accepted as a proper answer.
I find it strange that for part 3a or 4b that I filled in there is a 'FunctionX' is not defined.
Maybe this can work and or it cannot but I thought the whole script would run through the grader and while I am not saying I should receive correct marks I do not understand why it does not see much of anything.

LONG PASTE AHEAD

Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 27, in 
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 93, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
  File "", line 160, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/sql/types.py", line 1211, in __getattr__
    idx = self.__FIELDS__.index(item)
ValueError: 'return_code' is not in list

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


All tests passed
Top ten error endpoints (3a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 923, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 713, in collect
    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 538, in __call__
    self.target_id, self.name)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/protocol.py", line 300, in get_return_value
    format(target_id, '.', name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 113, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
  File "", line 160, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/sql/types.py", line 1211, in __getattr__
    idx = self.__FIELDS__.index(item)
ValueError: 'return_code' is not in list

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)

[Stage 39:>                                                         (0 + 1) / 4]
[Stage 39:==============>                                           (1 + 1) / 4]
[Stage 39:=============================>                            (2 + 1) / 4]
[Stage 39:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 40:>                                                         (0 + 1) / 4]
[Stage 40:==============>                                           (1 + 1) / 4]
[Stage 40:=============================>                            (2 + 1) / 4]
[Stage 40:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 41:>                                                         (0 + 1) / 4]
[Stage 41:==============>                                           (1 + 1) / 4]
[Stage 41:=============================>                            (2 + 1) / 4]
[Stage 41:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 42:>                                                         (0 + 1) / 4]
[Stage 42:==============>                                           (1 + 1) / 4]
[Stage 42:=============================>                            (2 + 1) / 4]
[Stage 42:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 43:>                                                         (0 + 1) / 4]
[Stage 43:==============>                                           (1 + 1) / 4]
[Stage 43:=============================>                            (2 + 1) / 4]
[Stage 43:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 44:>                                                         (0 + 1) / 4]
[Stage 44:==============>                                           (1 + 1) / 4]
[Stage 44:=============================>                            (2 + 1) / 4]
[Stage 44:===========================================>              (3 + 1) / 4]
                                                                                
15/06/18 23:09:12 ERROR Executor: Exception in task 0.0 in stage 45.0 (TID 139)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
  File "", line 160, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/sql/types.py", line 1211, in __getattr__
    idx = self.__FIELDS__.index(item)
ValueError: 'return_code' is not in list

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/18 23:09:12 ERROR TaskSetManager: Task 0 in stage 45.0 failed 1 times; aborting job
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


Number of unique hosts (3b)
---------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueHostCount' is not defined

Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dailyHosts' is not defined

Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithHosts' is not defined

Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'avgDailyReqPerHostList' is not defined

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithAvg' is not defined

Counting 404 (4a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badRecords' is not defined

Listing 404 records (4b)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badUniqueEndpointsPick40' is not defined

Top twenty 404 URLs (4c)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badEndpointsTop20' is not defined

Top twenty-five 404 response code hosts (4d)
--------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errHostsTop25' is not defined

404 response codes per day (4e)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errByDate' is not defined

Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithErrors404' is not defined

Five dates for 404 requests (4g)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'topErrDate' is not defined

Hourly 404 response codes (4h)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errHourList' is not defined

Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'hoursWithErrors404' is not defined

-- 1 cases passed (6.0%) --
 I am stuck in  3(c) , I saw lot of post for this .. but confused .. please help
I used
1 access_logs.map(lambda log : ((date_time.day,host),1))
2 reduceByKey(lambda a,b:a )
3 distinct().count()
4dailyHosts = (dayHostCount.cache())

error

--------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-38-b1e15bf203c4> in <module>()
      5 dayGroupedHosts = dayToHostPairTuple.reduceByKey(lambda a,b:a )
      6 
----> 7 dayHostCount = dayGroupedHosts.distinct().count()
      8 
      9 dailyHosts = (dayHostCount.cache())

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

 
 I tried the following but it didn't seem to work, I could even look at the RDD:
tupleRDD.map(lambda x: (x[0], x[2])).reduceByKey(lambda a, b: a + b)

I was trying to get ((k1, k2),v) to (k1, vi+vj)
flatmap() did not work too.
What was I doing wrong?

Your help is highly appreciated. Why am I getting this:



---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-18-060319563142> in <module>()
      8 endpointCountPairTuple = not200.collect()
      9 
---> 10 endpointSum = endpointCountPairTuple.reduceByKey(lambda a,b: a+b)
     11 
     12 topTenErrURLs = endpointSum.takeOrdered(10, lambda s: -1 * s[0])

AttributeError: 'list' object has no attribute 'reduceByKey'




 http://blog.sunshineonacloudy.net/2015/06/introduction-to-apache-spark_64.html

From a talk I gave on spark for our local hadoop group,  here are some rough notes on

(i) a spark cluster-at-home setup using docker containers; and

(ii) spark performance on a cpu-bound task (calculating a sum of a billion-term series for π)

 This is the process I go through to get the response below:

Map, reduce, and distinctJoin and groupAnother map, then sortAnother reduce and cache

This is what my response back is:

Average number of daily requests per Hosts is [(8, 1), (16, 1), (1, 1), (17, 1), (9, 1), (10, 1), (18, 1), (11, 1), (19, 1), (3, 1), (12, 1), (4, 1), (20, 1), (21, 1), (5, 1), (13, 1), (22, 1), (14, 1), (6, 1), (15, 1), (7, 1)]
If I do the join later on, I get this:

Average number of daily requests per Hosts is [(8, (1, 4406)), (16, (1, 4340)), (1, (1, 2582)), (17, (1, 4385)), (9, (1, 4317)), (10, (1, 4523)), (18, (1, 4168)), (3, (1, 3222)), (11, (1, 4346)), (19, (1, 2550)), (12, (1, 2864)), (4, (1, 4190)), (20, (1, 2560)), (5, (1, 2502)), (13, (1, 2650)), (21, (1, 4134)), (22, (1, 4456)), (14, (1, 4454)), (6, (1, 2537)), (15, (1, 4214)), (7, (1, 4106))]

There has to be a piece somewhere that I'm missing, I just can't figure out where. Any guidance is greatly appreciated!
 
 HELP!

TAs, I finished my Lab2 and it was all successful, but I get an error when I put the .py file in the submission form. I can send you a PDF version of my work if needed.
Thanks!
Ashwin Hi all, 

I am stuck on lab2, 3e with partial success. Can you share some insights on the last two steps?

my sortedByDay is as follow:

[(1, (33996, 2582)), (3, (41387, 3222)), (4, (59554, 4190)), (5, (31888, 2502)), (6, (32416, 2537)), (7, (57355, 4106)), (8, (60142, 4406)), (9, (60457, 4317)), (10, (61245, 4523)), (11, (61242, 4346)), (12, (38070, 2864)), (13, (36480, 2650)), (14, (59873, 4454)), (15, (58845, 4214)), (16, (56651, 4340)), (17, (58980, 4385)), (18, (56244, 4168)), (19, (32092, 2550)), (20, (32963, 2560)), (21, (55539, 4134)), (22, (57758, 4456))]

So I know I might be close. But I am stuck on the lambda and takeOrdered()

I used:

map(lambda a, b: (a, b[0]/b[1])).cache()
takeOrdered(30)

And got error:

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-88-4ae813919d53> in <module>()
     16 avgDailyReqPerHost = sortedByDay.map(lambda a, b: (a, b[0]/b[1])).cache()
     17 
---> 18 avgDailyReqPerHostList = avgDailyReqPerHost.takeOrdered(30)

I saw some other posts about this, but I tried the suggested lambda and didn't work. 

Please share some hints if you know the solution. Thanks! 

 I have submitted my lab 2 solution twice and I am getting a ugly stack trace. However, all my test cases pass.

This is my submission token:

702921-107892ec70daa7e2d76f55b92716ef22:3b15fe4dcc05a3e559e9e964c9db2877:ip-172-31-47-248

Please help. 

Thanks,
Narendra

  Hi All,

When I run my code I am getting the following output :
[(8, <pyspark.resultiterable.ResultIterable object at 0xa21afe6c>), (12, <pyspark.resultiterable.ResultIterable object at 0xb204a54c>), (4, <pyspark.resultiterable.ResultIterable object at 0xb204afec>), (16, <pyspark.resultiterable.ResultIterable object at 0xa21af68c>)

can someone please tell me where I am going wrong? 

PS : My total count of dayHostcount is also right  Spark/Linux VM ran out of physical memory on Lab 2  and started thrashing disk - have observed via Win7 taskmanger - performance tab

Q: How much physical memory is required to finish the course?
( running on  olded pc only had 4gb on win 7/64 - was going to upgrade pc  anyway but now have excuse)

env spec
this is from the install software lab in week 1

Free disk space: 3.5 GB RAM memory: 2.5 GB (4+ GB preferred)Processor:  Any recent Intel or AMD multicore processor should be sufficient.
 Hello.

I have finished my lab 2, but when I went to post on autograder, it said:

Timeout error happened during grading. Please review your code to be more efficient and submit the code again.
I don't know where I need to improve my code, and where is taking real several time.

Could you help me?
Thank you in advance. Under (3a) there's a comment
#Data is just a normal Python list

But type(data) returns "xrange"
 So say your RDD has only 21 tuples.

wouldn't print yourRDD just work since it is so small?

when I do print yourRDD.take(21) it works perfectly. Now we have master Apache log file processing in Spark,
students may be interested in trying out their skills on another data set type

This is an interesting NZ national water quality samples set I have been using when studying R big data sets.
Thisa denormalised flat file of about 2 million water samples with GIS and time series data

Description of data
National indicator data for river condition in New Zealand - collected by Regional Councils and the National Institute of Water and Atmospheric Research (NIWA), collated and processed by NIWA and protected by copyright owned by the Ministry for the Environment on behalf of the Crown....
http://ei.niwa.co.nz/details/dc/00704a1b-f8f5-23be-af71-09f20b9b6c15

data file 1 ( same format as data file 2 )
http://docs.niwa.co.nz/dc/Master_WQ_query_North_Island.zip

data set 2 ( same format as data file 1 )
http://docs.niwa.co.nz/dc/Master_WQ_query_South_Island.zip
 I have:
_______________________________________________________________
# TODO: Replace <FILL IN> with appropriate code
dayToHostPairTuple = access_logs.<FILL IN>
dayGroupedHosts = dayToHostPairTuple.<FILL IN>
dayHostCount = dayGroupedHosts.<FILL IN>

dailyHosts = (dayHostCount.reduceByKey(lambda x,y: x+y).sortByKey().cache())
dailyHostsList = dailyHosts.take(30)print 'Unique hosts per day: %s' % dailyHostsList
_____________________________________________________

The first two tests passed
And the add  .cache()  is giving the following error for Test 3:



1 test passed.
1 test passed.
1 test failed. incorrect dailyHosts.is_cached

Where is my error? Pl help. Thanks For these two labs, the class provides a vagrant virtual machine with python notebook. I wonder will the class instruct us how to install and deploy Spark from the beginning i.e install and deploy Spark to a machine with fresh Operating System. Have followed all instructions before submitting. Deleted all extra cells, removed debug print statements, etc. Everything works on my notebook environment, I am guessing this is autograder is overloaded as many folks may be submitting. Can somebody please help? Error message below:

Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 705420-5c55c84185a603f52fb5ca2d309ac575:716f43edaeb1c53aa702b23764e7a9d9:ip-172-31-47-248
Please include this submission token id when you need support for your code submission.
Update: OK, just as I'd suspected as well as Jhon, the autograder was probably overloaded yesterday night. I re-submitted the exact same file again today morning and it worked! I made sure even before the first submission to only change the <FILL IN> parts. Any other cells I created temporarily was deleted prior to submission. I am guessing the autograder timeout param needs to be adjusted.
 Is spark documentation available offline? Hi,
Can someone please tell me how long it takes for the auto grader to score lab 2 hw?

Thanks. I used the same logic in 4d and 4c , 4c is running fine but 4d giving error

errHostsCountPairTuple = badRecords.map(lambda log :(log.hosts,1))
errHostsSum = errHostsCountPairTuple.reduceBykey(lambda a,b:a+b)

ERROR


---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-56-61ad314dcf56> in <module>()
     11 errHostsCountPairTuple = badRecords.map(lambda log :(log.hosts,1))
     12 
---> 13 errHostsSum = errHostsCountPairTuple.groupBykey(lambda a,b:a+b)
     14 
     15 errHostsTop25 = errHostsSum.takeOrdered(25,lambda s: -1 * s[1])

AttributeError: 'PipelinedRDD' object has no attribute 'groupBykey'

 
 Hi, 
Lab 2 - (3f) is producing the correct result (apparently) but it fails the test. Can you please have a look and see what I am doing wrong here? I am using key() and value()

 Hello,

Submitted lab2 but autogrador is still working. Waited for half to one hr but it is still spinning after i click on the check.

What should i do so that i wont miss the deadline.Can i send it across to TA so that they could upload /run against autogrador if its any issue

Thanks
 Hello everyone,

I would like to get two values access_logs with a map. By example 

dayhost = access_logs.map(lambda (hosts, time) : (hosts.host,hosts.date_time))

But, when I do a print

print dayhost.collect()

I have the error:

ValueError: too many values to unpack

 

I don't understand this message

Could somebody help me?

Thanks in advance

 Carlota Vina

 Most of the new class materials for this course are due on Fridays (for me anyways). However, I have to work during the weekdays, so I am often left scrambling to finish these labs near the end of the week. This is because, I spend most of the weekend trying to understand the course material (watching videos etc.). So, I never really get a weekend to attempt the assignments. I think a due date of Sunday, makes a lot more sense.  Admittedly I am a little late to the course but I have the VM going using Vagrant and am working through the First Lab. However, while I was looking through the lab I began to wonder how the spark context is always available to the Python Notebook. Seems like the version of Jupyter has been specficially configured for Spark on the VM.. Am I thinking about this right? Can anyone explain what is going on under the hood? Please help how to proceed 

1    badRecords.map(lambda log :(date_time.day,1))2  errDateCountPairTuple.reduceByKey(lambda a,b:a+b)3 (errDateSum.cache())
4 errByDate = errDateSorted.<FILL IN>  ????????print '404 Errors by day: %s' % errByDate

Is approach is correct? Please guide me what to do Your submission token id is 708387-396f0a4b358b3bcd9711092adccc8e69:9f997399b11370201a352c9ac61cc402:ip-172-31-47-249in jupyter all pass, but when I submit there is error. Please review. 
Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect hosts

 


In [48]:










# TEST Visualizing unique daily hosts (3d)
test_days = range(1, 23)
test_days.remove(2)
Test.assertEquals(daysWithHosts, test_days, 'incorrect days')
Test.assertEquals(hosts, [2582, 3222, 4190, 2502, 2537, 4106, 4406, 4317, 4523, 4346, 2864, 2650, 4454, 4214, 4340, 4385, 4168, 2550, 2560, 4134, 4456], 'incorrect hosts')















1 test passed.
1 test passed.



 getting error as follows.  is takeordered()  syntax wrong
can some one respond
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-113-907070bea831> in <module>()
     12 endpointSum = endpointCountPairTuple.reduceByKey(lambda x,y : x+y)
     13 
---> 14 topTenErrURLs = endpointSum.sortByKey(ascending=False).take(10)
     15 #print 'Top Ten failed URLs: %s' % topTenErrURLs

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sortByKey(self, ascending, numPartitions, keyfunc)
    584         # the key-space into bins such that the bins have roughly the same
    585         # number of (key, value) pairs falling into them
--> 586         rddSize = self.count()
    587         if not rddSize:
    588             return self  # empty RDD

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 366.0 failed 1 times, most recent failure: Lost task 0.0 in stage 366.0 (TID 1164, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
  File "<ipython-input-113-907070bea831>", line 7, in <lambda>
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/sql/types.py", line 1211, in __getattr__
    idx = self.__FIELDS__.index(item)
ValueError: 'responsecode' is not in list

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

 I am very annoyed by this. I have been working on Lab 2 question 3s for about 12 hours and finally gave up from 3e. I went to question 4 and solve I don't know... about four questions and after that I couldn't make it. BUT I definitely solved and passed the first several questions on the question 4. However, when I uploaded it to the grader it says

 hourCountPairTuple = badRecords.<Fill IN>  -> i am mapping to (log.date_time.day),1)
hourRecordsSum = hourCountPairTuple.<FILL IN> -> using reduce by key here using function lambda x,y:x+y
hourRecordsSorted = hourRecordsSum.<FILLIN> -> i am sorting the list here based on hour
errHourList = hourRecordsSorted.collect()print 'Top hours for 404 requests: %s' % errHourList

But i get answer as 
Top hours for 404 requests: [(1, 243), (3, 303), (4, 346), (5, 234), (6, 372), (7, 532), (8, 381), (9, 279), (10, 314), (11, 263), (12, 195), (13, 216), (14, 287), (15, 326), (16, 258), (17, 269), (18, 255), (19, 207), (20, 312), (21, 305), (22, 288)] The regular expression part was just thbe beginning of trouble.
Nice exercise though picking up on basics of  python programming too. good stuff  For the dayToHostPairTuple I mapped log.date_time.day, log.host.
I then group by key that should give me tupples of (<day>, <list of hosts>)
The next step I am trying to create the <day>, <count of the hosts for that day>
I am doing this by 

dayHostCount = dayGroupedHosts.map(lambda a,b: (a,len(b)))

Can someone please tell me what i'm doing wrong 

 I have checked for this info but didn´t find and exact answer about how parallelize works and the exact implications of the Databrics context.

1.- Exactly where is the code running? is it on our computers (virtual machines)? Is it running on the Databrics Cloud?
2.- When we parallelize, as in "xrangeRDD = sc.parallelize(data, 8)", Does it means that 8 cores/threads are at our disposal, if they run in our computer how are they provisioned?
3.- Why we do not parallelize in Lab2?
4.- I f we are working outside the Databrics context, are there any big differences about the previous questions?

Thanks so much, the course it´s being great
 can someone give the syntax of sortByKey()  
it is giving errors in lab2 3a, 3c,4c I am confused by the definition of each term. Please help
dayToHostPairTuple = <FILL IN> POSTING CODE IS AN HONOR CODE VIOLATIONdayGroupedHosts = <FILL IN>dayHostCount = <FILL IN>dailyHosts = (dayHostCount              <FILL IN>)dailyHostsList = dailyHosts.take(30)print 'Unique hosts per day: %s' % dailyHostsList


Thanks,

Vik  Hello Everyone, I have doubts regarding the 3c part of the lab. Details as follows : 

1. dayToHostPairTuple - An RDD of the form (day,host)
2. dayGroupedHosts - An RDD of the form (day,[host1, ...])
3. dayHostCount - Is this an RDD of the form (day, number of hosts per that day) ? 
4. dailyhosts - Is this an RDD of the form (day, number of unique hosts per that day) ? 
  submission token id is 713809Here is the output:Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 27, in 
NameError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
name 'end' is not defined

All tests passed
Top ten error endpoints (3a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'endpointSum' is not defined

Number of unique hosts (3b)
---------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueHostCount' is not defined

Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dailyHosts' is not defined

Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithHosts' is not defined

Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'avgDailyReqPerHostList' is not defined

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithAvg' is not defined

Counting 404 (4a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badRecords' is not defined

Listing 404 records (4b)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badUniqueEndpointsPick40' is not defined

Top twenty 404 URLs (4c)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badEndpointsTop20' is not defined

Top twenty-five 404 response code hosts (4d)
--------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errHostsTop25' is not defined

404 response codes per day (4e)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errByDate' is not defined

Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithErrors404' is not defined

Five dates for 404 requests (4g)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'topErrDate' is not defined

Hourly 404 response codes (4h)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errHourList' is not defined

Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'hoursWithErrors404' is not defined Hi Guys,

I am on Lab1 4d. I get the following result:
I have used a flatMap with a split(" ").
Not sure why I get the '}' it does not pass the unit test because of this.

If someone can help it will be great. Thanks


[u'}', u'}', u'zwaggerd', u'zounds', u'zounds']
928908
 Hi guys,
My code has passed all the tests in the notebook. But I have this problem with autograder. Below is my log. There is only one error which is :
name 'Test' is not defined
Here is the full log. Any suggestion is appreciated.

Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 447
    dayToHostPairTuple = access_logs.
                     ^
SyntaxError: invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Top ten error endpoints (3a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Number of unique hosts (3b)
---------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting 404 (4a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Listing 404 records (4b)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badUniqueEndpointsPick40' is not defined

Top twenty 404 URLs (4c)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Top twenty-five 404 response code hosts (4d)
--------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

404 response codes per day (4e)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Five dates for 404 requests (4g)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Hourly 404 response codes (4h)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --


Your submission token id is 714385-cc1bf380d203afd3a8f878a1e3c17436:1c7d5ac350e7f4a5181eea70b647849d:ip-172-31-45-43
Please include this submission token id when you need support for your code submission.

 Hello

The jupyter UI seemed to be working fine on my machine(OS X/Chrome),now when i start the vm and open localhost:8001i get the message "No data received / ERR_EMPTY_RESPONSE " , any suggestions to fix this ?Fixed it reinstalling the vagrant environment.  
<strong>program output:</strong> Unique hosts per day: [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4404), (9, 4317), (10, 4522), (11, 4346), (12, 2863), (13, 2650), (14, 4454), (15, 4213), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2559), (21, 4133), (22, 4456)]

<strong>Actual output</strong>: [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)]

There is slight mismatch.

<strong>Code:</strong>
 
Hello,

I have just encountered a problem with the autograder and I wish to share my solution.
Often during writing code, I make backups of the original code (with <FILL IN> and everything). I comment them out to make sure everything will run.

With all those backup comment, I submitted the code (lab2) and received 0% completion with a syntax error pointing to some lines which contain no code at all. I delete all the backup comments, resubmitted my code and got the correct grade.

Therefore I suspect there is a glitch inside the autograder that have something to do with comments.

Hope this help.

  Hi,

Do I have the right path for solving the 
(3c) Exercise: Number of Unique Daily Hosts
1. map
2. reduceByKey
3. sortBy
4. distinct
5. cache

Thanks in advance.

 Does someone know the the username and password of the Spark VM?


 I solved half part of the problem, and i get the avgDailyReqPerHostList in this form : 

 [(1, 13), (3, 12), (4, 14), (5, 12), (6, 12), (7, 13), (8, 13), (9, 14), (10, 13), (11, 14), (12, 13), (13, 13), (14, 13), (15, 13), (16, 13), (17, 13), (18, 13), (19, 12), (20, 12), (21, 13), (22, 12)

but i can't understand what i need for the avgDailyReqPerHost ?? I do not use this var for solve the other point ! I think is super easy task but i need a hint !  Greetings fellow students
I want to extract a pair tuple (day, host-name) from access_logs.

 I wrote following code:

date_time_host_list = access_logs.map( lambda log: ( log.date_time, log.host ) )
date_time_host_list.take(3)

I got following output
[(datetime.datetime(1995, 8, 1, 0, 0, 1), u'in24.inetnebr.com'), (datetime.datetime(1995, 8, 1, 0, 0, 7), u'uplherc.upl.com'), (datetime.datetime(1995, 8, 1, 0, 0, 8), u'uplherc.upl.com')]

I need to extract day and host from above. I can not figure out how to do this.
Any pointer to do this?
Thanks in advance

 Hi, 

First off, Thank you for a very educational and approachable program, especially for newcomers to python and spark like myself.  

For current week's submission, even though i have cleared all the tests on the notebook, upon submission none of the answers are reflecting as correct. I am not sure what the issue is, could you please take a look and let me know what i have done wrong. 

My token id for the submission is:
717011-cc4f7d5871f92edc960f21389eee7863:fbc8f9d8543a330a55c59ae6ec2218dc:ip-172-31-4-222

Some sample error messages:
Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 27, in 
NameError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
global name 'parseApacheLogLine' is not defined

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'failed_logs' is not defined

Top ten error endpoints (3a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'endpointSum' is not defined
 Hi, 

I have some problems with this exercice.... I took 3 days trying to do but I can not.. I have this error: 

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-18-8b3920fb089c> in <module>()
      6 #APACHE_ACCESS_LOG_PATTERN = '^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+)\s*(\S*)" (\d{3}) (\S+)'
      7 
----> 8 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-3-8d8d6629d991> in parseLogs()
     22                    .filter(lambda s: s[1] == 0)
     23                    .map(lambda s: s[0]))
---> 24     failed_logs_count = failed_logs.count()
     25     if failed_logs_count > 0:
     26         print 'Number of invalid logline: %d' % failed_logs.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 1 times, most recent failure: Lost task 0.0 in stage 44.0 (TID 147, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-1-0940d9fe8819>", line 44, in parseApacheLogLine
  File "<ipython-input-1-0940d9fe8819>", line 16, in parse_apache_time
ValueError: invalid literal for int() with base 10: ''

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

Any help?? 

Thanks Greetings all
 
I have the daysWithHosts list and the hosts RDD containing host count.  Printing out the first 23 elements of each returns:
daysWithHosts: [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)]hosts: [2582, 3222, 4190, 2502, 2537, 4106, 4406, 4317, 4523, 4346, 2864, 2650, 4454, 4214, 4340, 4385, 4168, 2550, 2560, 4134, 4456]

I did a direct comparison between the first 23 elements of hosts and the list given in the Tests.assertEquals function and it returns True.

However, both Tests.assertEquals functions fail.

I think I have the correct list and RDD but Test.assertEquals doesn't agree.

Any suggestions?

Thanks
Jim This is the error i get after the .py file submission : 

Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect dailyHosts.is_cached

Visualizing unique daily hosts (3d)

All the test in the notebook are passed and all the RDD cahed .... i think ! I post the .cache() function at the end of RDD's actions ! So i'm searching for help .... 

All the Submission Report Here : 

Data cleaning (1c)
------------------
All tests passed
Top ten error endpoints (3a)
----------------------------
All tests passed
Number of unique hosts (3b)
---------------------------
All tests passed
Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect dailyHosts.is_cached

Visualizing unique daily hosts (3d)
-----------------------------------
All tests passed
Average number of daily requests per hosts (3e)
-----------------------------------------------
All tests passed
Average Daily Requests per Unique Host (3f)
-------------------------------------------
All tests passed
Counting 404 (4a)
-----------------
All tests passed
Listing 404 records (4b)
------------------------
All tests passed
Top twenty 404 URLs (4c)
------------------------
All tests passed
Top twenty-five 404 response code hosts (4d)
--------------------------------------------
All tests passed
404 response codes per day (4e)
-------------------------------
All tests passed
Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
All tests passed
Five dates for 404 requests (4g)
--------------------------------
All tests passed
Hourly 404 response codes (4h)
------------------------------
All tests passed
Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
All tests passed
-- 15 cases passed (93.0%) --


Your submission token id is 718159-f652171c2922c23b558338d8f251dca9:e0fdd56b9d6c327832f78a46c96f1e19:ip-172-31-11-135
Please include this submission token id when you need support for your code submission. how to get [(1,2),......] from [(1,(4,2)),...........] ????
Plz any1 help Hi

Autograder ran for more than 10hrs after SEVERAL attempts uploading multiple times but its still SPINNING 

Please sugggest  Just curious how the code is processed on the Spark Servers.
Is Java byte code is created from Python code and executed on the server? Hello,

I'd like to ask if there is some way of configuring the spark VM that we got for this MOOC to make use of more than 1 core of my computer when doing computations. I  have almost completed 3c and I get the following results:
Unique hosts per day: [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4404), (9, 4317), (10, 4522), (11, 4346), (12, 2863), (13, 2650), (14, 4454), (15, 4213), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2559), (21, 4133), (22, 4456)]
The results are slightly off for some values (for instance (8, 4404) instead of (8, 4406)). The deviations are very small (maybe 1 or 2) but it's really frustrating. I have run the code the exercise several times and the results are always consistent so it must be linked to my code somehow... I have done the exercise in already 2 different ways but I always get this deviation.
I must say I am completely confused Edit: sorry, my mistake! The URLs are the same, but the numbers are not! So the assertion seems to be ok! Should have checked it better.

Sorry for any confusion/inconvenience I might have caused.

Moderators: please feel free to delete this note!

--- original note ---
Hello everybody,

I'm sure this must have been noted before, but I couldn' find any note about it, so let my apologise in advance for a possible double post ;-)

The problem: the assertion at the end of Exercise 3a (Lab 2) is wrong: the topTenErrURLs are checked against the top 10 URLs from Exercise 2f.

Could somebody from the course team please correct this as soon as possible? The autograder uses the same assertion.

Cheers, Christoph Has Lab2 submission passed???

 I completed and ran all the parts in the local system, it runs in no matter of time but when i submitted it for autograder it takes a lot of time. what should it do. deadline is approaching.

Thanks
Bibas  In the Lab 2 3b problem I arrived at a solution using the python built-in set() on a list of hosts rather than the RDD distinct(). It works, but I'm wondering if there may be performance penalties, or other reasons to use distinct() whenever possible? Like would set() force the execution into a single core? Not to sure why and how to apply join here. Lecture video is not clearing it up.

I have:
dayAndHostTuple = access_logs.map(lambda log:(log.date_time.day,log.host))

I am reading that I am suppose to create to RDD one for unique and one for all, and then join them but this line to fill in allows for one RDD to be created unless we just repeat for both but I doubt it

dayAndHostTuple = access_logs.map

Clarification would be appreciated. I am completely lost in this map function and don't understand what is needed from me.
How can I add something as an arg into the mapping function that is a mapping function itself??
And this is the only part of code that has to be changed. Please, help.
 responseCountPairTuple = access_logs.<FILL IN>                      print responseCountPairTuple.count()
not200=responseCountPairTuple.<FILL IN>print not200.take(5)

This gives all those that are not 200 tuples. 
How do i extract the endpoint from this

Edit - Please do not post Code as it is a violation of the Honor Code Data cleaning (1c)
------------------
All tests passed
Top ten error endpoints (3a)
----------------------------
All tests passed
Number of unique hosts (3b)
---------------------------
All tests passed
Number of unique daily hosts (3c)
---------------------------------
All tests passed
Visualizing unique daily hosts (3d)
-----------------------------------
All tests passed
Average number of daily requests per hosts (3e)
-----------------------------------------------
All tests passed
Average Daily Requests per Unique Host (3f)
-------------------------------------------
All tests passed
Counting 404 (4a)
-----------------
All tests passed
Listing 404 records (4b)
------------------------
All tests passed
Top twenty 404 URLs (4c)
------------------------
All tests passed
Top twenty-five 404 response code hosts (4d)
--------------------------------------------
All tests passed
404 response codes per day (4e)
-------------------------------
All tests passed
Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
All tests passed
Five dates for 404 requests (4g)
--------------------------------
All tests passed
Hourly 404 response codes (4h)
------------------------------
All tests passed
Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
All tests passed
-- 16 cases passed (100.0%) --
 So this is the last line and output. The output matches what is shown in the Tets. But
I am failing the middle test. It passes the other two tests?

print 'Unique hosts per day: %s' % dailyHostsList

Unique hosts per day: [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)]

1 test failed. incorrect dailyHostsList I did all the question except for 3e and 3f cause I couldn't figure them out yet (and probably never will).  I did do all the rest in 3 and 4 which tested fine.  When I upload the file again and test it, it still says I did not do any of 4.  It is a new file that I did a file->download-as->pyfile.py.  Is their an issue if you didn't complete 1 answer?  I am burning up submissions trying to get it to recognize my other answers Hi,

I'm a bit stuck. How can I only return a list of values ?

This is my output 

Thanks in advance

[(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)] Dear Mods - 

Given the difficulties people are having with the regex, I think the addition of the two tests I propose in @1971 will make the lab go more smoothly for many students. It is easy to make a subtle error in 1c that makes no noticeable difference until parts 3 and 4. At this point, students assume the issue is with their Spark transformations because their "regex passed 1c".

Unless this is a deliberate strategy help us practice identifying the difference between data errors and analysis errors, would you consider pinning that post to increase visibility for those struggling with lab2? I would much rather spend my time in this course figuring out the most efficient ways to solve data problems rather than trying to figure out why a count is off by 2. 

In a future version of the class, you could just include these tests in the notebook.

This has been a very interesting course so far and the labs are high quality!

Thanks,
Chris My question pertains to the internal computation in the following type of architecture. Let us assume that Spark is implemented as follows: Spark+Yarn+HDFS.  The HDFS contains unstructured data. I perform a SQL query using Spark SQL interface to select some information. 

How does the SQL query is translated internally to convert unstructured data to structured relational data and then SQL query is executed? Hi,
I did following things
1. with map function extract day and host
2. then get distinct  day,host combination

How can i count the hosts with reduce by key . If i used countByKey  it is giving list instead of RDD.

Please help me on count.

Thanks
Veerendra Hi every one,

I am crazy about this error which I get after executing ".take" on an RDD which has been previously mapped and filtered. Any hint? Thanks.

'tuple' object has no attribute 'response_code' I'll be in Italy next week taking a course on Machine learning. Ti will be hard to keep up with the course. I wonder if you are planing to start another version of this course anytime soon. Regards,


Victor received timeout error after submission. does anybody else face same issue?
Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 722124-41e2585d22983f25443101a2806ddcdc:ea4a24fdb8ee43a80df9a72d4c9a1d30:ip-172-31-4-221
Please include this submission token id when you need support for your code submission. When trying to run the second block of code, I get the following error. 

NameError                                 Traceback (most recent call last)
<ipython-input-11-8d8d6629d991> in <module>()
     32 
     33 
---> 34 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-11-8d8d6629d991> in parseLogs()
      9 def parseLogs():
     10     """ Read and parse log file """
---> 11     parsed_logs = (sc
     12                    .textFile(logFile)
     13                    .map(parseApacheLogLine)

NameError: global name 'sc' is not defined I got couple times with this issues of time out.
How can I fix this issue?
Timeout Error as below
Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 722791-11241085577545b5e678dbe8047c07d9:db203d1b5e4229b86fe88db145fd2cd6:ip-172-31-4-221
Please include this submission token id when you need support for your code submission. I cannot find the lab 2 at http://localhost:8001/..please help me find it. I am getting the following error:

Timeout error happened during grading. Please review your code to be more efficient and submit the code again

However, all my tests passed while I was doing it.

Can anybody point out why I am getting this error ?

Thanks When I click on 'file' and hover over 'Download as' there is no option to save as a .py file. I'm not sure why.  Has anyone run into this error? All the auto tests gave me "1 test passed". 
Timeout error happened during grading. Please review your code to be more efficient and submit the code again. Eventhough errDateSorted is sorted and cached from 4e when I try to extract the x from it for the plot it gets unsorted

map(lambda (x,y):x).collect()

[8, 12, 4, 16, 20, 1, 5, 9, 13, 17, 21, 22, 10, 18, 14, 6, 11, 15, 3, 19, 7]

This should not happen. Any thoughts? Thanks Here is my list of days as a list i used .collect()

Why does this fail this test?




1 test failed. incorrect days




[8, 12, 4, 16, 20, 1, 5, 9, 13, 17, 21, 22, 10, 18, 14, 6, 11, 15, 3, 19, 7]
 
I get this error when I try to upload either the tutorial or the lab1 .ipynb files. 

The error was: SyntaxError: JSON Parse error: Unexpected identifier "bplist00"

Any ideas?

Thanks Hello everyone,

I get  the following map in exercise 3c

[(22, [u'pprolo.tcn.net', u'pprolo.tcn.net', u'quoka.dotc.gov.au', u'du2-async-26.nmsu.edu']

I would like to ask a question. Is it possible to count the different values in 

[u'pprolo.tcn.net', u'pprolo.tcn.net', u'quoka.dotc.gov.au', u'du2-async-26.nmsu.edu']
Do I have to use a map with a lambda function?

Thanks in advance

Carlota Vina






 I'm having a problem with 3c. My code runs forever and I don't know what's wrong here. Could you give me any hints? I know we are not supposed to share codes, but I'm expecting some hits and then I'll remove this discussion.

dayToHostPairTuple = access_logs.map(lambda log:(log.date_time.day,log.host))dayGroupedHosts = dayToHostPairTuple.reduceByKey(lambda x, y: x + y)dayHostCount = dayGroupedHosts.map(lambda (x, y): (x, sum(y)))dailyHosts = (dayHostCount             .cache())dailyHostsList = dailyHosts.take(30)print 'Unique hosts per day: %s' % dailyHostsList
 I had to make an out of state trip from work for few days so I couldn't finish my lab 1 or start with Week 3 exercises. Can I still hand Lab 1 and Lab 2 by next week Wednesday? I just wanted to point out that the first quiz for Lecture 4 asks about properties of RDDs on the page BEFORE the properties of RDDs are discussed. That question should be on the following page. Hi all,

I was able to pass all tests before 3e. 

I'm doing 3e in this way.

To get dayAndHostTuple I used a map and lambda to get a tuple of day with 1 and used reduceByKey to add.

used join to get groupedByDay.

When I tried to print the result of the two operations above, neither result nor error, the notebook is very very long time to complete.

Can anybody help? Thanks. This is the code I'm using :
# Code removed Hi Guys!
There's a Whatsapp discussion group for this course. Anyone who wants to participate can Whatsapp me there numbers or reply to this message.

Happy Learning!
Anav Rastogi
(+91 9868010557)  Dear Vibhor, 

   Your summary of steps for 3(e) was cool. I had to spend almost 6 hours after that just for a simple reason. That is to find out a way to be able to access nested tuples. But i enjoyed the journey. 

The Staff, thank you for providing such a wonderful way of learning. Challenging yet enjoyable. Thanks Hi ,

Is there a way to know which part of my code creates a time out of the autograder.

Kind Regards,

Ruud Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 728932-dd2c4cabe561ef7dfa4075237c4f03d3:89750b6c4332ebdc33dd688baf575f21:ip-172-31-47-249
Please include this submission token id when you need support for your code submission. The lab is loaded and running. On my server, I have created folders for each week, and the .ipynb is also there. I therefore have created a sym link in each weekX folder named data --> /home/vagrant/data. This has worked fine in the past. Including today. 

Now, I am not sure what to run "ctrl+enter". 

1a) and 1b) do not seem to do anything, yet I have received the 104 failed lines. I have been working on the regular expression and therefore wanted to see a good one to make sure I did not cause issues. I do not get anything, (not even an error). I then reloaded web page to make sure I did not mess things up. That did not help. 

Thanks in advance for any help Hi,

I'm trying to resolve the exercise, and I'm getting an error that int is not callable. This happens from my first map operation, even when I'm commenting out the rest.

I map access_logs to a tupple, where the key is itself a tupple.
I do something like this, where I am changing now the name of the variables, in order to not give too much away. Hope it's ok:

access_logs.map(lambda log : ((log.a, log.b), c)) 

And when I try to print it out, I get the error message.
I print it out like this:

print dayToHostPairTuple.take(30)

Thanks for your help
Theo Here I get the date_time from the log.
Now I need to covert this using the parse date time function.
I can use the parse_apache_time in map function.
How do I access the fields month to proceed further.

thanks Hi Folks -

I've managed to get stuck on how to process the date_time data that results from processing the access_log.

After mapping a lambda to create a counter i get stuck on how to interpret the results. My line of thinking is that I need to have a unique count of the date, but how i count the date is difficult for me to get to. This is due to not knowing how to manipulate, via python, how to pick the correct set element. Can someone help out on what method to use?

The results of the map are below. Any help would be appreciated.

[(datetime.datetime(1995, 8, 12, 13, 18, 15), 1), (datetime.datetime(1995, 8, 4, 4, 44, 48), 2), (datetime.datetime(1995, 8, 18, 15, 53, 46), 2), (datetime.datetime(1995, 8, 14, 2, 0, 48), 1), (datetime.datetime(1995, 8, 22, 14, 10, 22), 1), (datetime.datetime(1995, 8, 3, 17, 18, 14), 1), (datetime.datetime(1995, 8, 3, 22, 21, 56), 1), (datetime.datetime(1995, 8, 17, 4, 36, 19), 1), (datetime.datetime(1995, 8, 15, 3, 1, 41), 1), (datetime.datetime(1995, 8, 12, 18, 12, 26), 1)]
 My expression is APACHE_ACCESS_LOG_PATTERN = <REDACTED>

I am struck at Lab 2 - 1(c) from last 1 hour. I am using a 16 GB RAM with 200 GB hard disk.Can anyone suggest ? I guess, there is nothing wrong in my expression. Please help. for 1c  lab2  I used the following 2 regex 

APACHE_ACCESS_LOG_PATTERN = <REDACTED>
or
APACHE_ACCESS_LOG_PATTERN = <REDACTED>

and I got the following output

Read 1043177 lines, successfully parsed 0 lines, failed to parse 1043177 lines

Only second passed first and third test failed. Please clarify what mistake i am making here.


Thanks
 
  Hi All,
 My initial steps as follows:

dayToHostPairTuple = access_logs.map( lambda log: (log.date_time.day, log.host))print dayToHostPairTuple.take(3)

dayGroupedHosts = dayToHostPairTuple.reduceByKey( lambda u, v: (u, set(v) ) )

dayHostCount = dayGroupedHosts.map( lambda a, b: ( a, len( b ) ) )print dayHostCount.take( 3 )

The first print statement prints fine:
[(1, u'in24.inetnebr.com'), (1, u'uplherc.upl.com'), (1, u'uplherc.upl.com')]
The second print statement gives error:
What I am doing wrong here?
Thanks

---> 21 print dayHostCount.take( 3 )
RuntimeError: maximum recursion depth exceeded while pickling an object

 I submitted the .py file but have not completed all the exercises.  Should it come back with any feed back? The other onlline python courses did comeback with partial results.

Also I thought I saw somewhere the submission time was extended to June 22nd?  Or is it still June 19th?

Thanks

Ram Hi. I am trying to run 3c and the code just doesn't finish executing. Stays [*] for a lot of time. I have been stuck on this one for half an hour now. I am running a powerful machine so I am not sure if I am doing something wrong or what. So let me try to break down what I am trying to achieve.

1. Make (day, host) pairs.
2. Group them together by day.
3. Make another RDD of (day, unique host count) pairs.
4. Sort it by days.

Is that on the right track?
 I firstly map log to a tuple x = (log.endpoint, log.response_code), then I filter out x[1] != 200.
Then I try to map (x[0],1) so that I can count the endpoint.
But I got the error message here.



File "<ipython-input-24-56ad1615cbf7>", line 10
    endpointCountPairTuple = not200.map(lambda x: (x[0], 1))
                         ^
SyntaxError: invalid syntax I tried to upload .py file to autogravder. All the tests in the VM passed.
Getting following error:
Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 730318-096914fda083cc4abd582179aff5f5f7:27a97b15a0305e23a4f9514a6f86450e:ip-172-31-40-128
Please include this submission token id when you need support for your code submission.Second time: Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 731206-439cf1511396dd507f81457594f5d404:27a97b15a0305e23a4f9514a6f86450e:ip-172-31-4-221
Please include this submission token id when you need support for your code submission. My entire code is working. I am just failing test case 2.

This is my output. Please help :  TEST Number of unique daily hosts (3c)Test.assertEquals(dailyHosts.count(), 21, 'incorrect dailyHosts.count()')Test.assertEquals(dailyHostsList, [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)], 'incorrect dailyHostsList')Test.assertTrue(dailyHosts.is_cached, 'incorrect dailyHosts.is_cached')
1 test passed.
1 test failed. incorrect dailyHostsList
1 test passed.
 can you give some hints in 4c.. i struct at printing the list Hi, 

All my tests pass locally when I test by first restarting kernel and then click on "Cell" --> "Run All". However when i submit to auto grader I get the following error message. Can you please advise.


Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 640
SyntaxError: Non-ASCII character '\xc2' in file /ok/submission.py on line 641, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Top ten error endpoints (3a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Number of unique hosts (3b)
---------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting 404 (4a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Listing 404 records (4b)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badUniqueEndpointsPick40' is not defined

Top twenty 404 URLs (4c)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Top twenty-five 404 response code hosts (4d)
--------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

404 response codes per day (4e)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Five dates for 404 requests (4g)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Hourly 404 response codes (4h)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --


Your submission token id is 731877-f2ecba9a267c91f78fa38eb8bd886066:951095551271d4620042fb7f69fcc12c:ip-172-31-40-128
Please include this submission token id when you need support for your code submission.




Thanks,
Giri

  Hi I am struggling with the following any help would be appreciated

# TODO: Replace <FILL IN> with appropriate codewordCountsGrouped = wordsGrouped.groupByKey().map(lambda key, value: key, sum(int(value)))
print wordCountsGrouped.collect() I passed 1(c) data cleaning without error:

Read 1043177 lines, successfully parsed 1043177 lines, failed to parse 0 lines

# TEST Data cleaning (1c) 
Test.assertEquals(failed_logs.count(), 0, 'incorrect failed_logs.count()') 
Test.assertEquals(parsed_logs.count(), 1043177 , 'incorrect parsed_logs.count()') 
Test.assertEquals(access_logs.count(), parsed_logs.count(), 'incorrect access_logs.count()') 
1 test passed. 
1 test passed. 
1 test passed.

But then when I go to 2f:











# Top Endpoints
endpointCounts = (access_logs
                  .map(lambda log: (log.endpoint, 1))
                  .reduceByKey(lambda a, b : a + b))
​
topEndpoints = endpointCounts.takeOrdered(10, lambda s: -1 * s[1])
​
print 'Top Ten Endpoints: %s' % topEndpoints
assert topEndpoints == [(u'/images/NASA-logosmall.gif', 59737), (u'/images/KSC-logosmall.gif', 50452), (u'/images/MOSAIC-logosmall.gif', 43890), (u'/images/USA-logosmall.gif', 43664), (u'/images/WORLD-logosmall.gif', 43277), (u'/images/ksclogo-medium.gif', 41336), (u'/ksc.html', 28582), (u'/history/apollo/images/apollo-logo1.gif', 26778), (u'/images/launch-logo.gif', 24755), (u'/', 20292)], 'incorrect Top Ten Endpoints'

















Top Ten Endpoints: [(u'/images/NASA-logosmall.gif', 59666), (u'/images/KSC-logosmall.gif', 50420), (u'/images/MOSAIC-logosmall.gif', 43831), (u'/images/USA-logosmall.gif', 43604), (u'/images/WORLD-logosmall.gif', 43217), (u'/images/ksclogo-medium.gif', 41267), (u'/ksc.html', 28536), (u'/history/apollo/images/apollo-logo1.gif', 26766), (u'/images/launch-logo.gif', 24742), (u'/', 20184)]






---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-12-606dbe4c203b> in <module>()
      7 
      8 print 'Top Ten Endpoints: %s' % topEndpoints
----> 9 assert topEndpoints == [(u'/images/NASA-logosmall.gif', 59737), (u'/images/KSC-logosmall.gif', 50452), (u'/images/MOSAIC-logosmall.gif', 43890), (u'/images/USA-logosmall.gif', 43664), (u'/images/WORLD-logosmall.gif', 43277), (u'/images/ksclogo-medium.gif', 41336), (u'/ksc.html', 28582), (u'/history/apollo/images/apollo-logo1.gif', 26778), (u'/images/launch-logo.gif', 24755), (u'/', 20292)], 'incorrect Top Ten Endpoints'

AssertionError: incorrect Top Ten Endpoints





What is the problem? I should have all the lines parsed correctly. Hi guys,
Can somebody help me with this error (I took good care to check I pass all tests in python notebook)
I get the following error:

Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 732424-624f1d4ad9ed66c08fe15343bdd69ef7:013af44b5be9bda09c1ecf974cc54c02:ip-172-31-32-106
Please include this submission token id when you need support for your code submission. How long should you wait for a cell to run? If it takes more then five minutes we should just stop it I assume?

Any print RDD.take(1) statement I have not been able to do. I wait five minutes then give up on it. I spent three hours trying to figure out 3b and getting error. It is more to do with Python coding issue then Spark. Would you please help in particular the python code inside the map.

# TODO: Replace <FILL IN> with appropriate codefrom operator import addtotalCount = (wordCounts .map(lambda L: [x[1] for x in L]) .reduce(add))average = totalCount / float(uniqueWords)print totalCountprint round(average, 2)print totalCount answer = dailyHosts.join(groupedByDay).cache()print answer.take(1)

If I run the first line it runs without error.

If I then attempt to print it, it will just run and run and not print, so I always kill it. Hi,
I got this error message while working on lab2 tonight. Does anyone know what to do now?

ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused I completed lab 2 but sending the results to the auto grader give some problems. The script runs fine in ipython notebook, but the auto grader gives the following log with errors:

Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 22, in 
AttributeError: 'PipelinedRDD' object has no attribute 'setFailFast'

Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

Top ten error endpoints (3a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

Number of unique hosts (3b)
---------------------------
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

Counting 404 (4a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

Listing 404 records (4b)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badUniqueEndpointsPick40' is not defined

Top twenty 404 URLs (4c)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

Top twenty-five 404 response code hosts (4d)
--------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

404 response codes per day (4e)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

Five dates for 404 requests (4g)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

Hourly 404 response codes (4h)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: 'PipelinedRDD' object has no attribute 'assertEquals'

-- 0 cases passed (0.0%) --


Your submission token id is 733145-c8ab98a4a4fcbdadfd45f51eddacb864:5448cc52edbc23e5b034c79dbdb2ec4a:ip-172-31-4-97
Please include this submission token id when you need support for your code submission. I already done with four submissions and still getting the same error . 
Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 733357-476037167a7f1abe383805f7d76e84cc:6423cd7ba5fe2f133d8a53dfee073dc0:ip-172-31-40-128
Please include this submission token id when you need support for your code submission. Please where is the data for lab 1 located and how large is it? What should I do? Everything run well on the Python Notebook and I review the code and it's pretty efficient.
Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 734387-63be4fe0aa05cc6c9d8b1bd21cdc7a9b:bfb17d77ca67333607eb87812b38e8f1:ip-172-31-41-110
Please include this submission token id when you need support for your code submission Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 734554-4a95aa4f9ad5451293fa65252f4cd9ed:ee439d5c027369849e68c947293de230:ip-172-31-40-128
Please include this submission token id when you need support for your code submission.
my code works perfectly in my computer all test are passed and everything but auto grade is timing out i dont want to run out of submissions.please help As a challenge to myself, I've downloaded and installed Spark locally running on my Windows machine.  Spark version 1.4.0 runs ok on my 64bit-16GB machine by doing bin\pyspark in the spark home directory.  I am able to use the SparkContext, sc and HiveContext, sqlContext.  I would like to bring up the Ipython notebook and run pyspark with it.  I have already installed Anaconda (64-bit) IP IPython (py2.7) notebook. and brought up the ipython notebook, working fine.

I did a set IPYTHON=1 then bin\pyspark  It was supposed to bring up the IPython, but nothing happened and the pyspark command line fired up instead.  

So what other settings do I have to have to bring up pyspark running on IPython on a local install.  Please help!

I'm running Windows 7 on 64bit machine with 16GB memory. I got two lists: sortedByDay and dailyHostsList.
I want to join them to make it like [(1, (33996,2000)), (2,(41387,3000))....]

I used 
(sortedByDay .join(dailyHostsList) .map(lambda (a, (b, c)): (a, b/c)))

But it resulted in the following error.





[(1, 33996), (3, 41387), (4, 59554), (5, 31888), (6, 32416), (7, 57355), (8, 60142), (9, 60457), (10, 61245), (11, 61242)]






---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-91-425e52887496> in <module>()
     10 print sortedByDay.take(10)
     11 avgDailyReqPerHost = (sortedByDay
---> 12                       .join(dailyHostsList)
     13                       .map(lambda (a, (b, c)): (a, b/c)))
     14 print avgDailyReqPerHost.take(5)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in join(self, other, numPartitions)
   1533         [('a', (1, 2)), ('a', (1, 3))]
   1534         """
-> 1535         return python_join(self, other, numPartitions)
   1536 
   1537     def leftOuterJoin(self, other, numPartitions=None):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/join.py in python_join(rdd, other, numPartitions)
     50                 wbuf.append(v)
     51         return [(v, w) for v in vbuf for w in wbuf]
---> 52     return _do_python_join(rdd, other, numPartitions, dispatch)
     53 
     54 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/join.py in _do_python_join(rdd, other, numPartitions, dispatch)
     37 def _do_python_join(rdd, other, numPartitions, dispatch):
     38     vs = rdd.mapValues(lambda v: (1, v))
---> 39     ws = other.mapValues(lambda v: (2, v))
     40     return vs.union(ws).groupByKey(numPartitions).flatMapValues(lambda x: dispatch(x.__iter__()))
     41 

AttributeError: 'list' object has no attribute 'mapValues'

 What should I do
I had time out error quite a lot of times during submissions....
I'm quite sure the last submissions is correct.

Is it possible to increase the number of allowed submissions? is this because I missed the deadline?

Your submission token id is 735112-243963be84c4e2705802eca5e1d9701d:7738d2dd80b677d78dd7ad7d2abbd22f:ip-172-31-41-110
Please include this submission token id when you need support for your code submission.
 Hi guys, 
My rdd looks like this:
[(28, u'194.166.2.31'), (1, u'ix-or10-06.ix.netcom.com'), (28, u'163.205.56.149'), (1, u'199.120.110.21'), (28, u'maynard.isi.uconn.edu')]
At this points, the tuples are unique.

Now, I want to reduce by key, counting the number of hosts.
But doing:
from operator import add
add_rdd = dayToHostPairTuple.reduceByKey(add)
just concatenates the values:
[(1, u'199.72.81.55unicomp6.unicomp.net199.120.110.21burger.letters.com199.120.110.21burger.letters.comburger.letters.com205.212.115.106d104.aa.net129.94.144.152unicomp6.uni....]
How can I sum them up?
Thanks  a lot




 groupBYKey method returns  (K, Iterable<v>) pairs.  In my mind it means that the v in this tupple is python list, right.  So what is the way to iterate through this list?   Thanks! still learning Python....   


<PLEASE DO NOT POST SOLUTIONS>

Homework reference ID is: 689218-968c2e5e960974e794a43ed8c0ce6173:9af9fbfe371d9adb8680b941348bcd60:ip-172-31-45-43 Every test passed in locally but got the following error in autograder




Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 22, in 
NameError: name 'Test' is not defined

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Top ten error endpoints (3a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Number of unique hosts (3b)
---------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting 404 (4a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Listing 404 records (4b)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badUniqueEndpointsPick40' is not defined

Top twenty 404 URLs (4c)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Top twenty-five 404 response code hosts (4d)
--------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

404 response codes per day (4e)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Five dates for 404 requests (4g)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Hourly 404 response codes (4h)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --


Your submission token id is 735847-7914a8f97387e79a4f16eb6c38282aeb:0546463f871ff5d09fc88d2bf3f863bc:ip-172-31-4-112
Please include this submission token id when you need support for your code submission.

 


 Timeout error happened during grading. Please review your code to be more efficient and submit the code again.
Your submission token id is 736215-71bbc085c64f1ad32ed685a0a1af719b:32915e2ae261e1ab516342d0d3689dcb:ip-172-31-40-128Please include this submission token id when you need support for your code submission. Hi there, 
I am getting this submission error for the second times. I did just follow the instructions to remove unnecessary print, library import etc. 

Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 736285-63c54acdb4a698f593eb671e07d07986:57415c3b8b996a0149c40b81b3405105:ip-172-31-41-110
Please include this submission token id when you need support for your code submission.
Please let me know what I should do.
Thanks!
 Hi,

For four times I got the message below, after submission .py file. Even, the grader its too slow and
the answer takes a long time.
Can anyone help me?

Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 736351-257d76d2b39bad74d62af2d5b989f394:3d2822d62244ed1b3c9b8b05935ec8e0:ip-172-31-47-249
Please include this submission token id when you need support for your code submission. I have reached till sorted tuple.Struck in last step...I have tried map with integer divide.


[(1, (33996, 2582)), (3, (41387, 3222)), (4, (59554, 4190)), (5, (31888, 2502)), (6, (32416, 2537)), (7, (57355, 4106)), (8, (60142, 4406)), (9, (60457, 4317)), (10, (61245, 4523)), (11, (61242, 4346)), (12, (38070, 2864)), (13, (36480, 2650)), (14, (59873, 4454)), (15, (58845, 4214)), (16, (56651, 4340)), (17, (58980, 4385)), (18, (56244, 4168)), (19, (32092, 2550)), (20, (32963, 2560)), (21, (55539, 4134)), (22, (57758, 4456))]
last step keeps failing 

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-95-d101326337cd> in <module>()
     12 print sortedByDay.collect()
     13 avgDailyReqPerHost = sortedByDay.map(lambda a,b: (a, b[0]//b[1]))
---> 14 print avgDailyReqPerHost.collect()
     15 avgDailyReqPerHostList = avgDailyReqPerHost.take(30)
     16 print 'Average number of daily requests per Hosts is %s' % avgDailyReqPerHostList

Can someone please help Please see @2396

If you are getting lab 2 autograder timeout errors, it means that you are doing something that is causing the autograder's driver or workers to run for a long time or generating excessive amounts of output. Our timeout is set for twice the time it takes our solution to run. Please make sure you do the following in your code:
Check that your change to the regular expression is a minor  change - no more than a 3 character changeRemove all extra cells you addedRemove all print statements that you addedRemove actions that call collect() on a very large RDD (such as the access_logs RDD)Add the tests in pinned post @1971

Then do the following:
Reboot your VM using the command: vagrant reloadIn Jupyter, select "Cell->Run all", and verify that your notebook passes all tests
#pin Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 27, in 
  File "", line 23
    errByDate = errDateSorted.
                              ^
SyntaxError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
invalid syntax

All tests passed
Top ten error endpoints (3a)
----------------------------
All tests passed
Number of unique hosts (3b)
---------------------------
All tests passed
Number of unique daily hosts (3c)
---------------------------------
All tests passed
Visualizing unique daily hosts (3d)
-----------------------------------
All tests passed
Average number of daily requests per hosts (3e)
-----------------------------------------------
All tests passed
Average Daily Requests per Unique Host (3f)
-------------------------------------------
All tests passed
Counting 404 (4a)
-----------------
All tests passed
Listing 404 records (4b)
------------------------
All tests passed
Top twenty 404 URLs (4c)
------------------------
All tests passed
Top twenty-five 404 response code hosts (4d)
--------------------------------------------
All tests passed
404 response codes per day (4e)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errByDate' is not defined

Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithErrors404' is not defined

Five dates for 404 requests (4g)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'topErrDate' is not defined

Hourly 404 response codes (4h)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errHourList' is not defined

Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'hoursWithErrors404' is not defined

-- 11 cases passed (68.0%) --


Your submission token id is 736847-1683ac36ba72f9ac8715366dbd64fafc:ee439d5c027369849e68c947293de230:ip-172-31-32-77
Please include this submission token id when you need support for your code submission.


auto grader giving me error in 1c but my computer shows no error ..please help (sorry if this is dumb)

our submission token id is 736847-1683ac36ba72f9ac8715366dbd64fafc:ee439d5c027369849e68c947293de230:ip-172-31-32-77
Please include this submission token id when you need support for your code submission.











# TEST Data cleaning (1c)
Test.assertEquals(failed_logs.count(), 0, 'incorrect failed_logs.count()')
Test.assertEquals(parsed_logs.count(), 1043177 , 'incorrect parsed_logs.count()')
Test.assertEquals(access_logs.count(), parsed_logs.count(), 'incorrect access_logs.count()')













1 test passed.
1 test passed.
1 test passed


 I have passed all cases except 3e and when submitted to autograder I got the following response, which indicate that the autograder doesn't recognize the test cases.
Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 15, in 
  File "/ok/submission.py", line 469
SyntaxError: Non-ASCII character '\xc3' in file /ok/submission.py on line 470, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Top ten error endpoints (3a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Number of unique hosts (3b)
---------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Counting 404 (4a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Listing 404 records (4b)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badUniqueEndpointsPick40' is not defined

Top twenty 404 URLs (4c)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Top twenty-five 404 response code hosts (4d)
--------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

404 response codes per day (4e)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Five dates for 404 requests (4g)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Hourly 404 response codes (4h)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'Test' is not defined

-- 0 cases passed (0.0%) --


Your submission token id is 737802-492f0045babfb214a65d44c2d7c8cec6:45c7773f046b96aef1d71024f627428a:ip-172-31-32-78
Please include this submission token id when you need support for your code submission.
 Hi, im very new on Phyton and im actually trying to get the language by taking this course, but im completely confused with this point. I have been trying to understand the different approaches from all the classmates on their different solutions but my feeling is that everytime i try to start understanding one, i get even more confused. 
I would really appreciate any guidance. I'm definitelly thinking on quitting here..... It is taking more than an hout to process the data. Guess Server is chocked. I have completed my work wating for submision I am getting the following output with a last line I do not understand, Pl help. The test fails because of this last line I think. All the output looks good except for this last line! with the result (I think) the test fails.  The second test passes:

[(1, 243), (3, 303), (4, 346), (5, 234), (6, 372), (7, 532), (8, 381), (9, 279), (10, 314), (11, 263), (12, 195), (13, 216), (14, 287), (15, 326), (16, 258), (17, 269), (18, 255), (19, 207), (20, 312), (21, 305), (22, 288)]
[(1, 243), (3, 303), (4, 346), (5, 234), (6, 372), (7, 532), (8, 381), (9, 279), (10, 314), (11, 263), (12, 195), (13, 216), (14, 287), (15, 326), (16, 258), (17, 269), (18, 255), (19, 207), (20, 312), (21, 305), (22, 288)]
404 Errors by day: PythonRDD[2945] at RDD at PythonRDD.scala:43___________________________________________1 test failed. incorrect errByDate
1 test passed. It seems that we have to use groupByKey (or an aggregateByKey that results in a lot of data movement, or distinct() that I assume has a lot of shuffling too) Are there any differences:
rdd.map() vs rdd.Map()

We used both in the lectures (slides 18, 19)? When compared to Apache Drill, how does Spark SQL performs? Do we have any comparison? After having unsuccessful submission I've restarted my computer open lab2 notebook and try to rerun the test. However exercise 2a is corrupted. It looks like this:

 ### **Part 2: Sample Analyses on the Web Server Log File** ####Now that we have an RDD containing the log file as a set of Row objects, we can perform various analyses. #### **(2a) Example: Content Size Statistics** ####Let's compute some statistics about the sizes of content being returned by the web server. In particular, we'd like to know what are the average, minimum, and maximum content sizes. ####We can compute the statistics by applying a `map` to the `access_logs` RDD. The `lambda` function we want for the map is to extract the `content_size` field from the RDD. The map produces a new RDD containing only the `content_sizes` (one element for each Row object in the `access_logs` RDD). To compute the minimum and maximum statistics, we can use [`min()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.min) and [`max()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.max) functions on the new RDD. We can compute the average statistic by using the [`reduce`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduce) function with a `lambda` function that sums the two inputs, which represent two elements from the new RDD that are being reduced together. The result of the `reduce()` is the total content size from the log and it is to be divided by the number of requests as determined using the [`count()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count) function on the new RDD.
When I run it and the other 2 exercises after that I received an error:

AssertionError                            Traceback (most recent call last)
<ipython-input-11-79dc1579c7cd> in <module>()
      8 print 'Response Code Counts: %s' % responseCodeToCountList
      9 assert len(responseCodeToCountList) == 7
---> 10 assert sorted(responseCodeToCountList) == [(200, 940847), (302, 16244), (304, 79824), (403, 58), (404, 6185), (500, 2), (501, 17)]

AssertionError:  To get the top 20 is it a spark takeOrdered kinda thing or a python kinda thing?

Thank you in advance.
 I submitted Lab 2 and even after 2 hours auto grader has not responded at all. It is still running. I'm running in more and more troubles. Test 3b for the number of the unique hosts I had it right, however without changing anything, rerunning it again I got a wrong number. Here is my code:

hosts = access_logs.<FILL IN> POSTING SOLUTIONS IS A VIOLATION OF THE HONOR CODE

uniqueHosts = hosts.<FILL IN>

uniqueHostCount = uniqueHosts.<FILL IN>

Please help! Thanks,
 Hi, 

When I was doing  the last Quiz in lecture 5.

Then answer given  said that "Writing a compressed file is slower than writing an uncompressed file" is true.

From the data given in the lecture, that is not true.

Could you explain why?

Thanks Hi, 

Just to make things crystal clear, can someone please help me understand the meaning of closure?

Is it like an instance of a function? So say if I call function X() 100 times, all the variables will be shipped from 
driver to that function X() on worker each time it is executed, but if I use broadcast variables they will be shipped
only once and resused. Am I correct?

So what I understand by closure is the scope of a function. Also, what's an external variable.

Detailed explanation will be really helpful. Thanks! Hi all
I was travelling the last week and I didn't finish the Lab 1 exercise, I am trying for a while to make some question to run and I don't know how to solve it. Can anyone help me? Thanks in advance.

The question is:
Use groupByKey() to generate a pair RDD of type ('word', iterator).
# TODO: Replace <FILL IN> with appropriate code
# Note that groupByKey requires no parameters
wordsGrouped = wordPairs.<FILL IN>
for key, value in wordsGrouped.collect():
    print '{0}: {1}'.format(key, list(value) Hi
I am able to get total number of request / Day, I also want to include number of unique hosts in that day how do I do that? QUESTION: How do course iPython notebooks import PySpark + SparkContext + RDD namespaces?

I am baffled by how the course-supplied notebooks have access to PySpark SparkContext and RDD without ever explicitly importing them. What am I missing? I've past all tests locally, removed the first line from the logo, have no extra comments or print line, but still receive the error. Please help! I spent 6 out of 10 submissions.
Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 469
SyntaxError: Non-ASCII character '\xe2' in file /ok/submission.py on line 470, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'failed_logs' is not defined

Top ten error endpoints (3a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'endpointSum' is not defined

Number of unique hosts (3b)
---------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueHostCount' is not defined

Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dailyHosts' is not defined

Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithHosts' is not defined

Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'avgDailyReqPerHostList' is not defined

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithAvg' is not defined

Counting 404 (4a)
-----------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badRecords' is not defined

Listing 404 records (4b)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badUniqueEndpointsPick40' is not defined

Top twenty 404 URLs (4c)
------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'badEndpointsTop20' is not defined

Top twenty-five 404 response code hosts (4d)
--------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errHostsTop25' is not defined

404 response codes per day (4e)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errByDate' is not defined

Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithErrors404' is not defined

Five dates for 404 requests (4g)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'topErrDate' is not defined

Hourly 404 response codes (4h)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errHourList' is not defined

Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'hoursWithErrors404' is not defined

-- 0 cases passed (0.0%) --


Your submission token id is 741134-4eb7a1cc124b70c7ff037e8a87af2d8f:45c7773f046b96aef1d71024f627428a:ip-172-31-34-22
Please include this submission token id when you need support for your code submission.
 I am not able cache the output as errByDate is python list



[(1, 243), (3, 303), (4, 346), (5, 234), (6, 372), (7, 532), (8, 381), (9, 279), (10, 314), (11, 263), (12, 195), (13, 216), (14, 287), (15, 326), (16, 258), (17, 269), (18, 255), (19, 207), (20, 312), (21, 305), (22, 288)]






---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-143-da9c49113da6> in <module>()
      8 print errDateSorted.collect()
      9 errByDate = errDateSorted.collect()
---> 10 errByDate.cache()
     11 print '404 Errors by day: %s' % errByDate

AttributeError: 'list' object has no attribute
can someone please help

 Was there a change to the hardware requirements for this class?  I specifically remember checking them at the beginning since I have a little older Macbook (OS X 10.6.8, 2 GB RAM, 2.13 GHz Intel Core 2 Duo).

After having trouble with Lab2 1(b) and reading about others having trouble with their computers freezing, I went back and checked the requirements again: 2.5GB Ram and OS x 10.9.5.  Did this change since the beginning of the class?  If not, I'm feeling kinda crazy. 

Am I out of luck for this course with my current hardware?

Thanks, Hi, Im stuck in this one, I have 
daysWithHosts with is fine , just take the key values of the dailyHosts RDD: dailyHosts.map(lambda (a,b):(a))
but with host Im trying to do the same but instead of taking the key values I get only the values so far:

host = dailyHosts.map(lambda (a,b):(b))
the output of this is: 

[<pyspark.resultiterable.ResultIterable object at 0xb0acf18c>, <pyspark.resultiterable.ResultIterable object at 0xb0acf14c>, <pyspark.resultiterable.ResultIterable object at 0xb0acf56c>, <pyspark.resultiterable.ResultIterable object at 0xb0dfe94c>, <pyspark.resultiterable.ResultIterable o
if I change to host = dailyHosts.map(lambda (a,b):list(b))
then the output will be:

[[u'128.159.124.61', u'carbon.cudenver.edu', u'slip59.ucs.orst.edu', u'www-d1.proxy.aol.com', u'liisi-1.cc.utu.fi', u'rpyne_intel.mobius.provo.novell.com', u'sappho2.u.washington.edu', u'166.103.167.14', u'saguache.slip.ucar.edu', u'catd1.catd.iastate.edu', u'dialup16.mn.interact.net', u'165.234.85.233', u'hsn30_33.hibo.no', u'annex-1-5.upstel.net', u'email.gmeds.com', u'titan02f', u'concept.break.com.au', u'sudial-119.syr.edu', u'hungerford.chch.cri.nz', u'128.217.61.118', u'lom000.wwa.com', u'128.159.154.148', u'apple.fujita.com', u'pm2_2.digital.net', u'fmvb.tksc.nasda.go.jp', u'gaschot.sep.bnl.gov', u'igate.uswest.com', u'130.225.84.229', u'holland.chem.sfu.ca', u'ix-san1-02.ix.netcom.com', u'131.110.51.57', u'www-b3.proxy.aol.com', u'mcbcm18.med.nyu.edu', u'ftmfl-8.gate.net', u'ad21-026.compuserve.com', u'203.250.146.72', u'vyger501.nando.net', u'151.124.1.21', u'walker.plh.af.mil', u'mariaellana.dfrc.nasa.gov', u'dd08-039.compuserve.com', u'axe3-ra.f-remote.cwru.edu', u'sydney2.world.net', u'ts02-ind-3.iquest.net', u'mac34.kip.apple.com', u'slip1-67.acs.ohio-state.edu', u'nts106.dialup.hawaii.edu', u'139.169.64.140', u'198.76.200.3', u'163.206.56.11', u'130.69.192.22', u'romeo.dialup.torino.alpcom.it', u'kel01019.direct.ca', u'163.205.154.12', u'ppp1-14.inre.asu.edu', u'gs.hip.cam.org', u'ftmfl-21.gate.net', u'maria.bga.com',
Im a little bit lost here!!
Any help will be appreciatted
thanks!
 Hello everybody,

My regex is working for all invalid logs, but when I try to run 1c I receive the error message below. I'm using a Mac and maybe I'm not executing the "Shift-Enter" thing as I should execute. 

Any help?

Thanks in advance.







Py4JJavaError                             Traceback (most recent call last)
<ipython-input-47-22a88b89ae27> in <module>()
      4 APACHE_ACCESS_LOG_PATTERN =  '***my_regex here***'
      5 
----> 6 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-46-8d8d6629d991> in parseLogs()
     22                    .filter(lambda s: s[1] == 0)
     23                    .map(lambda s: s[0]))
---> 24     failed_logs_count = failed_logs.count()
     25     if failed_logs_count > 0:
     26         print 'Number of invalid logline: %d' % failed_logs.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 86.0 failed 1 times, most recent failure: Lost task 0.0 in stage 86.0 (TID 277, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-44-0940d9fe8819>", line 35, in parseApacheLogLine
IndexError: no such group

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

<br /><br /><br />***************** next cell<br /><br /></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
<div class="input"> <div class="inner_cell"> <div class="input_area"> <div class="CodeMirror cm-s-ipython"> <div class="CodeMirror-scroll" tabindex="-1"> <div class="CodeMirror-sizer"> <div> <div class="CodeMirror-lines"> <div> <div class="CodeMirror-code">
<pre style="white-space: pre-wrap; word-wrap: break-word;"><span><span class="cm-comment"># TEST Data cleaning (1c)</span></span>
<span><span class="cm-variable">Test</span>.<span class="cm-variable">assertEquals</span>(<span class="cm-variable">failed_logs</span>.<span class="cm-variable">count</span>(), <span class="cm-number">0</span>, <span class="cm-string">'incorrect failed_logs.count()'</span>)</span>
<span><span class="cm-variable">Test</span>.<span class="cm-variable">assertEquals</span>(<span class="cm-variable">parsed_logs</span>.<span class="cm-variable">count</span>(), <span class="cm-number">1043177</span> , <span class="cm-string">'incorrect parsed_logs.count()'</span>)</span>
<span><span class="cm-variable">Test</span>.<span class="cm-variable">assertEquals</span>(<span class="cm-variable">access_logs</span>.<span class="cm-variable">count</span>(), <span class="cm-variable">parsed_logs</span>.<span class="cm-variable">count</span>(), <span class="cm-string">'incorrect access_logs.count()'</span>)</span>
</div> </div> </div> </div> </div> <div></div> </div> </div> </div> </div> </div> <div class="output_wrapper"> <div class="out_prompt_overlay prompt" title="click to scroll output; double click to hide"></div> <div class="output"> <div class="output_area"> <div class="prompt"></div> <div class="output_subarea output_text output_stream output_stdout">
<pre style="white-space: pre-wrap; word-wrap: break-word;">1 test failed. incorrect failed_logs.count()
1 test passed.
1 test failed. incorrect access_logs.count()
</div> </div> </div> </div>
<pre style="white-space: pre-wrap; word-wrap: break-word;"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansiyellow"><span class="ansicyan"><span class="ansigreen"><span class="ansiyellow"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><br /><br /></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
</div> </div> </div> </div> </div></div>
</div>
</div>
</div>
</div> Hello ,
I work with this list in exercie 3c


[(22, [u'pprolo.tcn.net', u'pprolo.tcn.net', u'qu


But I have repeat values in this list. I dont't know how to count the different values in this list.

Could somebody help me?

Thanks in advance

Carlota Vina I am different counts for: 
(u'/shuttle/missions/STS-69/mission-STS-69.html', 430)whereas in test it is:(u'/shuttle/missions/STS-69/mission-STS-69.html', 431)  Finally figured out 3e from the helpful hints throughout.

I sorted by the day then joined the dailyHosts with my sorted day list, then mapped that through to give me the average.

Doing this I lost my sorting from the sortedByDay and I'm not sure why.

I verified that sortedByDay was sorted.

Another thing I do not get is how come my sortedByDay list was "sorted" and dailyHosts was not sorted but it magically put the keys together so that the average could be calculated. can someone give the cause of error below



3c) Exercise: Number of Unique Daily Hosts For an advanced exercise, let's determine the number of unique hosts in the entire log on a day-by-day basis. This computation will give us counts of the number of unique daily hosts. We'd like a list sorted by increasing day of the month which includes the day of the month and the associated number of unique hosts for that day. Make sure you cache the resulting RDD dailyHosts so that we can reuse it in the next exercise.¶ Think about the steps that you need to perform to count the number of different hosts that make requests each day. Since the log only covers a single month, you can ignore the month.




In [384]:
















# TODO: Replace <FILL IN> with appropriate code
​<EDIT please do not post your answer>
















77506
77506
[(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)]






---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-384-c9b280a0578f> in <module>()
     14 dailyHosts = dayHostCount.takeOrdered(30,key = lambda s: 1*s[0])
     15 print '%s' %dailyHosts
---> 16 dailyHostsList = dailyHosts.take(30)
     17 print 'Unique hosts per day: %s' % dailyHostsList

AttributeError: 'list' object has no attribute 'take'









In [ ]:



















# TEST Number of unique daily hosts (3c)
Test.assertEquals(dailyHosts.count(), 21, 'i












 Hi all,
I posted during Wednesday and Thursday my work on Lab2, was graded, so everything seemed fine.
However on the last posting attempt, the grader timed out, and now the full work and credits for Lab2 does not appear on the progress tab.
It shows that Lab2 has not been posted at all.
How can I ammend that?
Thanks
H  What would be the definitions of the RDDs in this problem? 
 
 dayAndHostTuple -> An RDD of the type (day,host) ? 
 groupedByDay -> An RDD of the type (day, [list of hosts on that day]) ?
 sortedByDay -> An RDD of the type (day,[list of hosts on that day]) where the day is in ascending order? 
 avgDailyReqPerHost ??  Hi Everyone..

I am amazed to see the way this course is going.

Its simply awesome!!..

Special thanks to EDX team and Professor.

After finishing this lab, I can see what I have learnt.

@Professor, when you come to India, drop me a message. I will pick you..

And thank you soo much for this wonderful lab session.

All tests passed
-- 16 cases passed (100.0%) --


Your submission token id is 746911-0e6cffe4a640e3b2b09633573cfe2660:89750b6c4332ebdc33dd688baf575f21:ip-172-31-33-126
Please include this submission token id when you need support for your code submission. Hi all,

I just found solution for Lab2 3c, but I need to admit that i has been trying quite a lot of different ways, I had some difficulty, and Im sure that there are different ways to approach the problem,

Here is my approach with the sequence of operations, which seems to work fine

distinct(map(day,host)) map(day,1) reduceByKeysort

I will like to initiate a thread here to see different approaches for 3c resolution.

Can I get some feedback about my solution?
Which was your approach?
Which is the best way?

Thanks!
Oscar how to convert from RDD to list
can someone suggest tip for error in the following

dayToHostPairTuple = access_logs.map(lambda x:(x.date_time.day,x.host)).distinct() dayGroupedHosts = dayToHostPairTuple.map(lambda (x,y) : (x,1))
dayHostCount = dayGroupedHosts.reduceByKey(lambda x,y: x+y)
dailyHosts = dayHostCount.takeOrdered(key = lambda s: 1*s[0]) dailyHostsList = dailyHosts.take(30)
print 'Unique hosts per day: %s' % dailyHostsList Dear all,

I would like to share my approach with you to solve lab2 3C exercise, you only need:
1. map and distinct
2. map
3. reduce by key
4. cache

That's all.

Good luck.


Regards,
 Wahi HI,
       I am using the following code for this task but always end up with the bellow stack trace. I don't know what causing the issue. Can any one help me on this?
# TODO: Replace <FILL IN> with appropriate code
# HINT: Each of these <FILL IN> below could be completed with a single transformation or action.
# You are welcome to structure your solution in a different way, so long as
# you ensure the variables used in the next Test section are defined (ie. endpointSum, topTenErrURLs).
not200 = access_logs.filter(<EDIT no answer please>
endpointCountPairTuple = not200.map(lambda x:(not200.endpoint,1))



Stack Trace is looks like this

---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-14-e55d523bd357> in <module>()
      6 endpointCountPairTuple = not200.map(lambda x:(not200.endpoint,1))
      7 
----> 8 endpointSum = endpointCountPairTuple.reduceByKey(lambda a, b : a + b)
      9 topTenErrURLs = endpointSum.takeOrdered(10, lambda s: -1 * s[1])
     10 print 'Top Ten failed URLs: %s' % topTenErrURLs

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduceByKey(self, func, numPartitions)
   1480         [('a', 2), ('b', 1)]
   1481         """
-> 1482         return self.combineByKey(lambda x: x, func, func, numPartitions)
   1483 
   1484     def reduceByKeyLocally(self, func):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions)
   1691         """
   1692         if numPartitions is None:
-> 1693             numPartitions = self._defaultReducePartitions()
   1694 
   1695         serializer = self.ctx.serializer

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _defaultReducePartitions(self)
   2074             return self.ctx.defaultParallelism
   2075         else:
-> 2076             return self.getNumPartitions()
   2077 
   2078     def lookup(self, key):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in getNumPartitions(self)
    319         2
    320         """
--> 321         return self._jrdd.partitions().size()
    322 
    323     def filter(self, f):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _jrdd(self)
   2286         command = (self.func, profiler, self._prev_jrdd_deserializer,
   2287                    self._jrdd_deserializer)
-> 2288         pickled_cmd, bvars, env, includes = _prepare_for_python_RDD(self.ctx, command, self)
   2289         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(),
   2290                                              bytearray(pickled_cmd),

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _prepare_for_python_RDD(sc, command, obj)
   2204     # the serialized command will be compressed by broadcast
   2205     ser = CloudPickleSerializer()
-> 2206     pickled_command = ser.dumps(command)
   2207     if len(pickled_command) > (1 << 20):  # 1M
   2208         broadcast = sc.broadcast(pickled_command)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py in dumps(self, obj)
    409 
    410     def dumps(self, obj):
--> 411         return cloudpickle.dumps(obj, 2)
    412 
    413 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dumps(obj, protocol)
    814 
    815     cp = CloudPickler(file,protocol)
--> 816     cp.dump(obj)
    817 
    818     #print 'cloud dumped', str(obj), str(cp.modules)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dump(self, obj)
    131         self.inject_addons()
    132         try:
--> 133             return pickle.Pickler.dump(self, obj)
    134         except RuntimeError, e:
    135             if 'recursion' in e.args[0]:

/usr/lib/python2.7/pickle.pyc in dump(self, obj)
    222         if self.proto >= 2:
    223             self.write(PROTO + chr(self.proto))
--> 224         self.save(obj)
    225         self.write(STOP)
    226 

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    560         write(MARK)
    561         for element in obj:
--> 562             save(element)
    563 
    564         if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    631                 write(MARK)
    632                 for x in tmp:
--> 633                     save(x)
    634                 write(APPENDS)
    635             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    634                 write(APPENDS)
    635             elif n:
--> 636                 save(tmp[0])
    637                 write(APPEND)
    638             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    684                 k, v = tmp[0]
    685                 save(k)
--> 686                 save(v)
    687                 write(SETITEM)
    688             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    304             reduce = getattr(obj, "__reduce_ex__", None)
    305             if reduce:
--> 306                 rv = reduce(self.proto)
    307             else:
    308                 reduce = getattr(obj, "__reduce__", None)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in __getnewargs__(self)
    169         # This method is called when attempting to pickle an RDD, which is always an error:
    170         raise Exception(
--> 171             "It appears that you are attempting to broadcast an RDD or reference an RDD from an "
    172             "action or transformation. RDD transformations and actions can only be invoked by the "
    173             "driver, not inside of other transformations; for example, "

Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.
 I got this error while running 1c 
An error occurred while trying to connect to the Java server This is what I'm using to obtain the top 20 :badEndpointsTop20 = badEndpointsSum.takeOrdered(20, key = lambda x : 1 * x)
but my output is :




Top Twenty 404 URLs: [(u'/\x08\x08\x08', 1), (u'/%20history/apollo/apollo-13/apollo-13.html', 3), (u'/%3A//spacelink.msfc.nasa.gov', 26), (u'/%3A/spacelink.msfc.nasa.gov', 23), (u'/%3Aspacelink.msfc.nasa.gov', 1), (u'/%5B.pub', 3), (u'/%5B.pub.win3.winvn', 1), (u'/%7Emccoy/', 1), (u'/%7Emccoy/Icons/index.html', 1), (u'/*', 1), (u'/.../liftoff.html', 1), (u'/.ksc.nasa.gov/images/ksclogo.gif', 1), (u'/.nasa.gov/shuttle/', 1), (u'/.pub.win3', 1), (u'/.pub.win3.winvn', 2), (u'//ftp.amug.org/mirrors/info-mac/games/com', 1), (u'//naic.nasa.gov', 1), (u'//pub/postgres/postgres-v4r2/postgres.bugs', 1), (u'//www.msfc.nasa.gov', 1), (u'//zippo.com/pub/pictures/erotica/', 1)]



 I started working through Lab 2 this morning and it took me a good 4 hours to work through. I am writing notes on how to work through the exercises for people who, like me, started late. There is already a lot of hints and replies out there but most of them confused me, than helped me...Important Stuff (according to me)
Work through exercise 1 & 2 (no problems), then move on to #4. #3 is challenging and you might get struck. But once you are done with #4, which is mostly a breeze, #3 should appear simpler. In any case getting #4 first ensure you don't run out of time.

Let's run through the exercises now...
Problem 1
C)
As the pinned post from the faculty says, it will not take more than addition of 3 characters to work this out. Just goto http://pythex.org/, paste your bad log message and play with it, until it starts matching. Then you would know what to do. The bit to add is already present in the current regex!
Problem 3:A)
Straight forward. Remember the transformation you learned n used in Lab 1.response_code data type is an integer, not a string.Remember sorting working in a "reverse" style in take methods.
B)
Straightforward
C)
BIG Pain in the Neck.
Here's my take
From all the discussion, there are more than n ways to do this. I will talk about what I did.Don't put TOO much faith in the variable names already provided. They confused me quite a bit. Just work through what is needed.Forget GroupBy(), this is simpler than that.Use paired RDDs (obviously!).As somebody said in a post, you will have to use distinct(), two map calls, reduceByKey() and sortedByKey(True).Relax, work through this on paper and it should come to you.
D)
Easy-peasy. map and then collect one of the pair RDDs at a time. You will do this a lot in #4.
E)
Again, relax, this is much like #3.C.This will ultimately boil down to using a transformation that operates on 2 RDDs (one of them was calculated in #3.C). There is only one that we have learned: join.
  F) As D above.

#4)
These are easy, compared to #3.Remember to use distinct() where needed.Again, response_code is integer.takeOrdered() and sorkeyByKey() are your friends.

All the best! :) Looked at the doco for using cogroup to get a resulting RDD from two RDDs for each key and could only manage to get a list first and then put it back to an RDD.  It worked OK that way using collect since the list was small but just wanted to know if there is a better way to get the result in an RDD.   

this is a list example
http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cogroup
also 
https://spark.apache.org/docs/0.8.1/api/core/org/apache/spark/rdd/PairRDDFunctions.html

Thanks,
Stephanie

 This question is a general question. I am wondering how to launch Amazon EC2 (4 core linux with 8gb ram) running Spark ? Also I want to know the cost of transferring data between my local machine and EC2 cluster. 

I have searched the web and Amazon's docs, but they are useless. They are so cluttered with acronyms and information that after a while you will forget what it is that you want to do. 

If anybody has any idea, or knows any source, could you provide them here.

Thanks.
Tridib This is my output :

Unique hosts per day: [((1, u'128.126.216.37'), 10), ((1, u'128.135.36.35'), 12), ((1, u'128.138.169.91'), 15), ((1, u'128.138.169.94'), 3), ((1, u'128.149.109.74'), 6), ((1, u'128.158.20.67'), 16), ((1, u'128.158.28.33'), 6), ((1, u'128.158.36.4'), 53), ((1, u'128.158.37.244'), 7), ((1, u'128.158.42.141'), 30), ((1, u'128.158.42.193'), 9), ((1, u'128.158.45.18'), 49), ((1, u'128.158.49.61'), 6), ((1, u'128.158.50.129'), 15), ((1, u'128.158.53.223'), 16), ((1, u'128.158.54.58'), 5), ((1, u'128.158.55.116'), 6), ((1, u'128.158.56.155'), 24), ((1, u'128.158.66.97'), 3), ((1, u'128.159.105.240'), 14), ((1, u'128.159.111.138'), 12), ((1, u'128.159.111.141'), 8), ((1, u'128.159.111.174'), 12), ((1, u'128.159.111.23'), 20), ((1, u'128.159.112.24'), 10), ((1, u'128.159.115.22'), 1), ((1, u'128.159.117.33'), 6), ((1, u'128.159.121.108'), 12), ((1, u'128.159.121.64'), 33), ((1, u'128.159.122.107'), 10)]
can someone please help me ? Logging into the VM, I see messages that says 20 packages need to be updated with 11 of them security.  do I go ahead and do apt-get update or keep things as they are?  I don't want to hose the Spark env. Hi,

I get this error while booting my vagrant again

A VirtualBox machine with the name 'sparkvm' already exists.Please use another name or delete the machine with the existingname, and try again.

Can you please help me out?
Thanks,
Prateek Data cleaning (1c)
------------------
All tests passed
Top ten error endpoints (3a)
----------------------------
All tests passed
Number of unique hosts (3b)
---------------------------
All tests passed
Number of unique daily hosts (3c)
---------------------------------
All tests passed
Visualizing unique daily hosts (3d)
-----------------------------------
All tests passed
Average number of daily requests per hosts (3e)
-----------------------------------------------
All tests passed
Average Daily Requests per Unique Host (3f)
-------------------------------------------
All tests passed
Counting 404 (4a)
-----------------
All tests passed
Listing 404 records (4b)
------------------------
All tests passed
Top twenty 404 URLs (4c)
------------------------
All tests passed
Top twenty-five 404 response code hosts (4d)
--------------------------------------------
All tests passed
404 response codes per day (4e)
-------------------------------
All tests passed
Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
All tests passed
Five dates for 404 requests (4g)
--------------------------------
All tests passed
Hourly 404 response codes (4h)
------------------------------
All tests passed
Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
All tests passed
-- 16 cases passed (100.0%) -- I've completed all other tests including #4, but I'm stuck on 3(e).The code is use is :

sortedByDay = groupedByDay.reduceByKey(lambda (x,y): (float(y) / (log.host).count()).collect())
Can anyone point out the mistake ?

Please... i started a bit late. i am on lab 1 and stuck with problem 4d. i am trying to use lambda s=s.split() inside a map function but i am getting syntax error   and also  i think RDD.split() is not working with RDD . so is there any way to solve this problem . please guide me how to use map transformation in this case ? Just joined the class so I've been blitzing through the coursework but got stuck at question 4(d) because I was using 
`shakespeareRDD.flatMap(lambda x: x.split())` which splits according to any whitespace (newlines included)  as opposed to `shakespeareRDD.flatMap(lambda x: x.split(' '))`.

This would the filter in exercise 4e unnecessary. If the goal is to teach use of filter, I'd recommending tweaking the exercise to be something like count the number of times a word has the letter `q` instead. I have tried the below code.
dayToHostPairTuple = access_logs.map(lambda log: ((log.date_time.day, log.host),1))dayGroupedHosts = dayToHostPairTuple.reduceByKey(lambda a,b:a+b)dayHostCount = dayGroupedHosts.map(lambda a:(a[0],len(set(a[1]))))print dayHostCountnow, what method shall I call on dayHostCount to see the rdd. dayHostCount.collect() throws exception. Could anyone please help ? Is there any issues with the line : dayGroupedHosts.map(lambda a:(a[0],len(set(a[1]))))? ? I have cached:
errDateSorted = (errDateSum
                 .cache()
                 )

Both 4e and 4f tests pass but on autograder I get :
TestFailure: incorrect errDateSorted.is_cached

and everything else pass on autograder. Any ideas, thanks. what is to be done in 1a? I've followed the instructions precisely but spatkvm doesn't show up in the virtualbox, when i type "vagrant up" in terminal, it gives me that it's already running;
i tried "VBoxManage startvm "sparkvm" --type headless" and it gave me:Waiting for VM "sparkvm" to power on...VM "sparkvm" has been successfully started.but it's still not appearing in the virtualbox, what have i missed? "Your verification status is good until xx/06/2016"

What Significance of Certificate after the mentioned date ? I'm unable to complete Lab2 in time because the VM keeps slowing down the more cells I run. It's taking tens of minutes to get a result now and the only solution is to restart the VM. I've already given it extra RAM and CPU and this worked for a while but the slowdowns still occur. How are we expected to complete this course if everything is running so slowly due to the size of data, especially since I have to run all the cells up to the one I'm working on each time I start the VM! Why is it showing me Hadoop? Please check

screenshotlocalhost_4040_20150620_190622.png Hi All,
 This is my first MOOC course in my life. I had had a very good experience so far.
Prof. Anthony and his team have been working very hard to respond to student's questions and doubts.

I have following questions:
1. Prof. Anthony mentioned in one post that there might be self-paced version of this course. Will that be certified also?
When it is likely to start?

2. I am new to Spark and had beginner skill level for Python before the start of this course.
I have been spending most of my spare time working on the labs. There is no time right now to do extra studies in Spark.
I would like to go thru all the links and material provided in the lectures.
How long this course will remain open?

3. Will there be a Java version of this course in near future?
Python seems to be a quite good language, but I have invested a good number of years learning and practicing Java.

I would like to learn how to use Java in Spark world.

4. What is next after this course is completed?
Will there be an advanced level course in Spark?

5. How can I save my labs and solutions for future reference?
I looked at the python(.py) lab files saved so far. I can read thru them, but I would like to see the same browser display as we see under http://localhost:8001
==> open the ;lab file.
I would like to do this as on some of my computers, I may not have Oracle VM and vagrant installed.

I would like to know how to save this as a graphic image file.
I did save as a html file and opened in a Google chrome window but it would not render the same display as you opened the lab file in http://localhost:8001

Thanks in advance
 Hi All,

If I run the code with notebook/sparksvm, I can not run the code in 1b. I can even not start to solve the regex in 1c.

I do not have the most powerful Macbook (Air), but I do not know how to progress.

Best,
Wim hi,

when I do:
print badEndpointsSum.top(10)
I get output as follows:

[(633, u'/pub/winvn/readme.txt'), (494, u'/pub/winvn/release.txt'), (431, u'/shuttle/missions/STS-69/mission-STS-69.html'), (319, u'/images/nasa-logo.gif'), (178, u'/elv/DELTA/uncons.htm'), (156, u'/shuttle/missions/sts-68/ksc-upclose.gif'), (146, u'/history/apollo/sa-1/sa-1-patch-small.gif'), (120, u'/images/crawlerway-logo.gif'), (117, u'/://spacelink.msfc.nasa.gov'), (100, u'/history/apollo/pad-abort-test-1/pad-abort-test-1-patch-small.gif')]

but
when I do:
print badEndpointsSum.takeOrdered(6, key=lambda x: -x)

I get error in output:

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-87-86c0fb68f734> in <module>()
      5 badEndpointsSum = badEndpointsCountPairTuple.map(lambda (k,v):(v,k))
      6 print badEndpointsSum.top(10)
----> 7 print badEndpointsSum.takeOrdered(6, key=lambda x: -x)
      8 '''badEndpointsTop20 = badEndpointsSum
      9 print 'Top Twenty 404 URLs: %s' % badEndpointsTop20'''

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in takeOrdered(self, num, key)
   1172             return heapq.nsmallest(num, a + b, key)
   1173 
-> 1174         return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
   1175 
   1176     def take(self, num):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 382.0 failed 1 times, most recent failure: Lost task 0.0 in stage 382.0 (TID 899, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1174, in <lambda>
    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
  File "/usr/lib/python2.7/heapq.py", line 432, in nsmallest
    result = _nsmallest(n, it)
  File "<ipython-input-87-86c0fb68f734>", line 7, in <lambda>
TypeError: bad operand type for unary -: 'tuple'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)




whats wrong with takeOrdered bcoz it is the exact syntax from the docmentation.
 Good afternoon,

Just a suggestion for our "teachers team". Please consider that student like me don't have powerfull computer.
The main objective of this mooc is to learn Spark strategy to handle ways of computing metrics with data. To do so a minimalist quantity of data should have been sufficient .. don't you think so ?

Nethertheless, many thanks for the great quality of this mooc.

Regards

Benoit For code understanding purposes I am trying to log the successfully logged lines in 1b with the below code, but it fails when executed. I do not follow why does string formatting fail for the below code in python.

    failed_logs_count = failed_logs.count()    if failed_logs_count > 0:        print 'Number of invalid logline: %d' % failed_logs.count()        for line in failed_logs.take(20):            print 'Invalid logline: %s' % line     for myvar in access_logs.take(1):           print 'Invalid logline again: %s' % myvar            print 'Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (parsed_logs.count(), access_logs.count(), failed_logs.count())
I get an error with the message " not all arguments converted during string formatting" for the line that prints the access_log line. Any idea on what could I be missing? I'm trying to do 4e,
I'm applying a filter to shakespeareWordsRDD using the following lambda function - lambda x: x!=' '
when I test  lambda x: x==' ' it comes back as zero, suggesting that there are no ' ' entries.. What am I doing wrong? Has it got something to do with the u infront of entries? I have noticed that under the Course Info section the notice for this week (Week 4) is available. However on the Courseware section there is no sign of Week 4 and its material.

Will this be fixed soon? hallo,

on the course info page the "THIS WEEK'S TASKS" reports links to Lecture 7 and Lecture 8. Anyway if I click on them all I get is a "Page Not Found".  Moreover, if I go to "Courseware" section there is no hint of the new lectures, the last one is lecture number 6. Hi, 

I am very lost with this practice....i need one solution for 1c exercise... i check some solutions but any good.

Please help me.... 

Thanks I have some confusion for the code in 1b. As I understood from the earlier lectures that RDD is created by transformation followed by the actions and no transformation is done until an action is performed. So, here's the code snippet from 1b:
def parseLogs(): """ Read and parse log file """ parsed_logs = (sc .textFile(logFile) .map(parseApacheLogLine) .cache()) failed_logs = (parsed_logs .filter(lambda s: s[1] == 0) .map(lambda s: s[0])) failed_logs_count = failed_logs.count()return parsed_logs, access_logs, failed_logs

So, here I see that only transformation is performed then how are we getting results?
I know, I am missing something in the concept.Please help me in understanding.

Thanks I don't really understand what to do here. I have seen others talking about just using .join() in the final variable statement, but this isn't working for me. I have done the following:

EDIT Please do not post your answer

Now I want to join what I have in the sorted day variable to what I found in 3c. It's not working though. Really confused. I don't understand. I think the explanation paragraphs are a bit too vague for me sometimes. They way it's written, it almost sounds like it wants us to calculate the portion of the days hits and assign it to a host (almost like keys and values in a dictionary, but form looking at test cases, this is obviously not what they want). On the last statement, after the join procedure, I tried to map the length of host list and total hosts, but that didn't work. I got a pyspark error. 

EDIT
So I was able to figure it out, by using a mapping procedure and paying attention to the inner tuple structure. However, I ended up having to sort my rdd twice. Once before the join and once after. Why is this? I tend to do all my work for the week when I find a spare night, which is usually once every 10 days (or so).

How important are the deadlines? - In some MOOCs I've taken there is no deadline, in others there is a deadline (but in the final days of the course they open up everything and tell everyone they have an extension until <course end date> to finish everything with little-to-no penalty.

Just asking here so I don't miss out on the certificate!

Thanks Hi All,

I have joined sortedByDay with dailyHosts and getting the following result:

[(8, (60142, 4406)), (16, (56651, 4340)), (1, (33996, 2582)), (17, (58980, 4385)), (9, (60457, 4317)), (10, (61245, 4523)), (18, (56244, 4168)), (11, (61242, 4346)), (19, (32092, 2550)), (3, (41387, 3222)), (12, (38070, 2864)), (4, (59554, 4190)), (20, (32963, 2560)), (21, (55539, 4134)), (5, (31888, 2502)), (13, (36480, 2650)), (22, (57758, 4456)), (14, (59873, 4454)), (6, (32416, 2537)), (15, (58845, 4214)), (7, (57355, 4106))]
Now, I wish to divide the total number of requests per day/unique hosts per day and doing the following:
avgDailyReqPerHost = (sortedByDay                                       .join(dailyHosts).map(lambda (k,(v1,v2)):(k,(v2/v1))))

but I am not getting the expected result.

Please help. I passed all the tests in the notebook, but the submission has a weird error: 
All tests passed
Top twenty 404 URLs (4c)
------------------------
All tests passed
Top twenty-five 404 response code hosts (4d)
--------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
TypeError: object of type 'PipelinedRDD' has no len()

404 response codes per day (4e)
-------------------------------
All tests passed
Visualizing the 404 Response Codes by Day (4f)
---------------------------------------------- Hi Frnds,
I m facing issues in solving Lab 2 3c and 4e.
I got output as 21 but facing problems in getting the output in this format (1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406)
Can anyone help me out in solving this problem. 
Facing similar problem in 4e as well. Looks like a software/data issue is causing the checked "correct" answer to be incorrect. The "correct" answer is incorrect and does not match the short answer description. Lec7 has wrong answer for 9th submodule quiz Hello,

One of Sparks features that is distinct from MapReduce is to perform computations and analysis of data in-memory rather than reading and writing to disk each time. What i wanted to know is too take advantage of this feature of Spark, do we always have to call the method cache on each RDD created or does spark do this by default?

If spark does this by default, then when and why do we use the cache function? If not, then that means we always have to specify beforehand which RDD to cache and which do not need to be cached. Any specific scenarios where it is better to cache the RDD object and when not to?

Thank you in advance. This course is really enjoyable!!! :)
 dailyHosts = dayHostCount.sortByKey(True) - This is causing an error.

Can some one please reply on what is wrong here:

print '%s' % dayHostCount

I get this: No output



PythonRDD[334] at RDD at PythonRDD.scala:43


 I a getting this error "Py4JJavaError Traceback (most recent call last)"This is what I am doinghosts = access_logs.map(<EDIT>uniqueHosts = hosts.reduceByKey(<EDIT>uniqueHostCount = uniqueHosts.map(<EDIT>print 'Unique hosts: %d' % uniqueHostCount Folks,

I took this course because SPARK is a serious candidate to be deployed in our production as a Complex Event Processor. As exciting as SPARK is - It distributes work across multiple Worker Nodes, is fault tolerant, etc etc - the Driver Program is a single point of failure which is a potential risk.

Is there a way to mitigate this risk? If yes, can someone suggest as to how? At the very end of Lecture 8, Prof. Joseph says that the next course (Scalable Machine Learning) starts June 22nd (both audio and the slide). The PDF of the slides says June 29th for the start date so I guess the error was caught already. It's June 29th, right? Hi
I am sorry for asking this simple question, but i have no experience with Sql stuff. Can someone explain me what exactly is 
from pyspark.sql import Row
Whats the use of this class Row ? Can someone help me wrap my head around this class ?

Thanks In lab 2, I believe we had to deal with data that doesn't meet out specifications. The explanation on the "show answer" reflects that but the Correct option is "Adhoc modification to data" which seems incorrect. Should be "Data that doesn't meet the specification". Screenshot below
 Hi I have already registered for CS 100.1x and bought paid certificate. In order to get XSeries certificate do I need to register for CS 190.1X before June 22 or I can register any time? Please guide. Thanks in advance. I think the accepted answer here differs from the expected answer by the grader. 

In Lab 2, Apache web server log analysis, you had to deal with which of the following data storage problem?
The accepted answer was ad/hoc modifications to the data

But the answer given in the explanation, is different (and what I originally chose).  badEndpointsCountPairTuple = badRecords.map(lambda xxxxx) badEndpointsSum = badEndpointsCountPairTuple.reduceByKey(lambda xxxx)
print badEndpointsSum

and it results the following

I wonder if someone could suggest what did I do wrong?

PythonRDD[930] at RDD at PythonRDD.scala:43 Hi Instructor,
I was uploading the Lab 2 notebook in .py format.
But on uploading the book and checking the points, it shows nothing
Below is the pic of the same
I have wasted my 2 submission because of it.
Plz check and revert asap.

 So far in the exercises, we have always uses '\n' delimited lines as an atomic unit. But, a Record can span across multiple lines and a stream can contain multiple records.

Question: How do we split the Stream by these Records? For example, assume an Event Stream which contains multiple event types (event1, 2, ....), in JSON format, containing multiple keys and values. How do we build a Processor that accumulates events by type over a period of time? I have completed the lecture 7 quizzes, but could not find question 5. It seems to be missing.

PS. The Data Storage question has now appeared, but is giving an incorrect answer as its solution. Show answer reveals answer d is correct, but the grader marks c as correct What command do I use to halt the execution of a cell in my notebook that just seems stuck in a loop? All my tests before (4c) pass but when I execute the code in 4c I get:

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-44-809c7a081837> in <module>()
      9                   .textFile(fileName, 8)
     10                   .map(removePunctuation))
---> 11 print '\n'.join (shakespeareRDD
     12                 .zipWithIndex()  # to (line, lineNum)
     13                 .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'
I'm assuming nothing has to be done in (4C) since the comment says "just run this code"

Where am I going wrong?

I was using translate from the string module in python which was creating issues. It's fixed now vmerror.png

Please help me to solve this error. It is a blocker for me. Not able to solve the assignments due to this.
Please help

Regards,
Amita
 Hello. In one of the lectures it was suggested that we refrain from using groupByKey whenever possible (and use reduceByKey instead) because it causes to much data to be passed around the nodes which may be costly.

However, in many exercises (e.g. lab2 3c) the variable names for each step kind of suggest that we group the data in the RDDs (so I ended up using this). I think it is possible to solve many of those problems using reduce by key too.

Am I right in this conclusion or is using groupByKey also ok? Hi,

I'm really stuck on question 3e. I have completed all other exercises but I can't get my head around this one.

This is what I'm doing.

First I get the dayAndHostTuple using a map with the day and the host

Then I just use group by key: groupedByDay = dayAndHostTuple.groupByKey()

The I sort by Key and join with dailyHosts.

The last bit it was complicated but I think I got it right: sortedByDay.reduceByKey(lambda a,b: (a, b[0]/b[1]))

What am I doing wrong?

Thanks for your help! Hi everyone!  First post on Piazza for this class :) Sorry if this has already been asked, I searched and didn't see anything.

For 3e, in order to have access to the "number of unique hosts" for each did, did you need to use collectAsMap() on dailyHosts from 3c (or something similar)?

I know that my solution works, but I just want to make sure that it is efficient / makes sense. Hi,
I'm trying to find out what the regex should be.
right now i've lowercased the string and used the regex they've provided, but i can't figure out how to continue.

those are my result.
i only need an hint :)
['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '']
#pin Hi All,

I successfully completed my lab 2 tests. All test cases passed and scored 100% in first attempt.  I have reviewed all your valuable questions and comments. It was really helpful guys.Thank you all for continuous sharing on your thoughts and ideas

Thanks
Jey Hey, is there someone taking this course from Newcastle, UK? :)
 
Would you be interested to have a meetup to watch videos together or share our experience in this course? I would be interested :)
 
Enjoy! How does 0.98 (98%) probability came in rhine paradox ? I've noticed a huge slwodown in notebook performance today.  What use to take a few seconds takes minutes now.  has anyone else noticed this?  Is there any official explanation?  Perhaps because of the approaching final deadline for Lab 2?

I even shut down my notebook.  Turned vagrant off.  Rebooted, restarted vagrant and reloaded my notebook.  Now going thru it cell by cell and still seeing that massive slowdown.

Any advice? Hello,
Seems to be a type in 2(f). There's no  rec_b000hkgj8k_weights variable:
 
recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1]
idfsSmallWeights = idfsSmall.collectAsMap()
recb000hkgj8kWeights = tfidf(recb000hkgj8k, idfsSmallWeights)

print 'Amazon record "b000hkgj8k" has tokens and weights:\n%s' % rec_b000hkgj8k_weights

Also, the test for 2(f) is failing for me but I seem to only be off by a factor of 12 for each tf-idf value. Anyone else seeing this?
 string = string.<EDIT no answer please> print string return re.split(split_regex,string)

O/P
------------

a quick brown fox jumps over the lazy dog.
['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '']

Do I need to tweak the regex or the output array? There is a mistake in the quick question on the Data Storage page in Lecture 7, the answer marked correct doesn't match with the explanation.

 Does anyone else have an error in Lab3 second cell?

It's indicating the spark context (sc) has not been defined and I can't seem to find a place where it should be.
Are we expexted to define it ourselves or is something missing from the lab notebook? Hi all.
I found a couple of very nice youtube videos about Spark.

The first one is a training video from Databricks https://www.youtube.com/watch?v=7ooZ4S7Ay6Y
He gave very good overview of Spark system.

The second one is AMP camp 5 video and one of the talk was given by Ameet Talwalkar who will the teaching the follow up class "Scalable Machine Learning". You can also find the training material from the AMP camp 5 website http://ampcamp.berkeley.edu/5/

I went through the first hands-on exercise from the AMP camp 5 using jupyter notebook and posted it on my github https://github.com/briansp2020/AMPcamp5

Since we can't post solutions to the class assignment, I'm hoping it will be helpful for some of you who maybe having problems.

 I am getting this for my 3e:
Average number of daily requests per Hosts is [(1, 21), (3, 16), (4, 13), (5, 21), (6, 21), (7, 13), (8, 12), (9, 12), (10, 12), (11, 12), (12, 19), (13, 20), (14, 12), (15, 12), (16, 12), (17, 12), (18, 13), (19, 21), (20, 21), (21, 13), (22, 12)]
But the answer is:
[(1, 13), (3, 12), (4, 14), (5, 12), (6, 12), (7, 13), (8, 13), (9, 14), (10, 13), (11, 14), (12, 13), (13, 13), (14, 13), (15, 13), (16, 13), (17, 13), (18, 13), (19, 12), (20, 12), (21, 13), (22, 12)]
How could it be this close on averages? It doesn't seem right that my algorithm would be wrong and get answers this close. Maybe the server log has changed or something? Hi it seems data storage quiz answer is wrong. I choose correct answer but it is not and when we see answer it shows correct option

In Lab 2, Apache web server log analysis, you had to deal with which of the following data storage problem?

 I have
badRecords = (access_logs                       .map(lambda log: (log.response_code, 1))                       .reduceByKey(lambda a, b : a + b)).collect()

prints [(200, 940847), (304, 79824), (404, 6185), (500, 2), (501, 17), (302, 16244), (403, 58)]

I not sure how to get the 404 count out of the RDD.

 Couple clarifications as the "step by step" looks different to me than the theoretical set-up:

I think we're meant to:

create a list of unique tokens that exist across all records (ie documents?)for each unique token, count how many records (ie documents?) it appears in in the corpuscalculate the idf for each unique token by dividing the number of records it appears in by total records

I'm a bit confused because in the suggested steps to the function it seems to say we should:

create a list of unique tokens in a record (document)for each unique token in a record, count how many occurrences in a recordcompute the idf (but don't we have the wrong inputs?)

Is my thinking wrong on this somehow?  I am working through on the assumption the first set of bullet points is right, but then that seems like a trickier programming exercise than implied by the "fill ins".because we have to compare a uniqueTokens RDD with the corpus RDD.

Thoughts appreciated.
 Not that my grade matters too much, but I accidentally submitted lab 2 on the lab 1 submission site.  I had already submitted my lab 1 last week and passed all the tests fine.  Because I resubmitted my lab2, it took the place of my lab1 score (namely it gave me a 0% on lab 1).  I then resubmitted my lab1 python file, but this time it counted me for being late because it sees the current submission time.  Is there anyway to revert back to my previous submission that was on time? This point in the Data Delivery lecture is a little vague. What would be some examples of dependencies between a stream of data and its preprocessing? 2 (f) is looking for the solution: 

{'autocad': 33.33333333333333, 'autodesk': 8.333333333333332, 'courseware': 66.66666666666666, 'psg': 33.33333333333333, '2007': 3.5087719298245617, 'customizing': 16.666666666666664, 'interface': 3.0303030303030303}

My code provides:

{'autocad': 33.33333333333333, 'autodesk': 8.333333333333332, 'courseware': 66.66666666666666, 'psg': 33.33333333333333, '2007': 3.5, 'customizing': 16.666666666666664, 'interface': 3.0}
Anyone else encounter this problem? Any ideas for what might be causing this difference?
 I get an AttributeError: 'PipelinedRDD' object has not attribute 'flatmap'


The function countTokens() requires that amazonRecToToken consists of pairs like  (recordID, tokenizedValue).

As amazonSmall consists of pairs (recordID, Sentence), I use a lambda function which first applies tokenize() on the Sentence and thus get a list of tokens.  
Using list comprehensions on this list of tokens I can create a list of pairs consisting of (recordID, tokenizedValue) as required and tested in Python.

I don't understand why flatmap does not work on the list of pairs?
Thank you for help Hello, 
   my solution to test 2(f) seems correct to me, results are quite the same as the test hoever there are little discrepancies in decimal values for key 2007 and interface, I get the following values :
{'autocad': 33.33333333333333,'autodesk': 8.333333333333332,'courseware': 66.66666666666666, 'psg': 33.33333333333333, '2007': 3.5, 'customizing': 16.666666666666664, 'interface': 3.0}
but the test checks for these values
{'autocad': 33.33333333333333, 'autodesk': 8.333333333333332,'courseware': 66.66666666666666, 'psg': 33.33333333333333, '2007': 3.5087719298245617, 'customizing': 16.666666666666664,'interface': 3.0303030303030303}
Is there something wrong in my code or test values for 2007 and interface are not correct ?

Regards

Davide B.
 Let's say currently I have a 50GB csv file that I want to use as an initial source for ETL in PySpark. I also don't have Hadoop, as I run Spark as standalone locally.

Reading this file into memory takes a long time. Should I transform the file to some other format for future re-sourcing? If so, what format should I use? The lecture mentioned binary format is fast but there were no examples. Also, can Spark input a binary file?

Thanks!
Denis I've been trying to solve 2c unsuccessfully for a very long time. Any hints?

I can get the unique tokens, but I cannot proceed to the next step:

tokenCountPairTuple = uniqueTokens.<FILL IN>

I try an approach like this:

rdd1.map(lambda a: (a, rdd2.filter(lambda b: a in b))

but that ends up with the following error:

Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063. The lab says ..." Data files for this assignment are from the metric-learning project and can be found at:  cs100/lab3

But I went to that link and can not find cs100/lab3
 When I run the first few cells for lab 3 on DBC I get an error. There was this additional line "display(dbutils.fs.ls('/mnt/mooc-data/cs100/lab3'))" as well.  The ipython notebook works fine.
Any suggestions? 


Two problems in the assignment:

1) recb000hkgj8k are supposed to be tokens, instead it is a string
2) two variables: recb000hkgj8kWeights  !=  rec_b000hkgj8k_weights

==

"recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1]\n", "idfsSmallWeights = idfsSmall.collectAsMap()\n", "recb000hkgj8kWeights = tfidf(recb000hkgj8k, idfsSmallWeights)\n", "\n", "print 'Amazon record \"b000hkgj8k\" has tokens and weights:\\n%s' % rec_b000hkgj8k_weights" I've read every 3c post and I understand the logic but my lack of python knowledge is letting me down. My steps are:

map to an RDD of (day, host) pairsgroupByKey using day as the keymap where day is the key, and the values are a count of distinct valuessortByKey

The error seems to be at sortByKey, I can't call that method on the output of the last step?

I'm still new to python so a big problem is that I can't look at my variables along the way, I don't know how to convert them so I can see their contents. I try print and take and collect on the different variables created (dayToHostPairTuple, dayGroupedHosts, etc.) but it never works.

Can anyone help? Hi, 
I'm wondering how should I understand the output of function idfs in 2c of lab3 
it says RDD: a RDD of (record ID, IDF value)
but in the data set we have (as I understand it) something along the lines of ('recordID',['token1','token2'....]) and IDF makes only sense for tokens, not for recordIDs which are unique....
also, if tokenCountPairTuple should hold the number of records that each token is present in, wouldn't it be more reasonable to start with corpus RDD instead of uniqueTokens and then weirdly join to corpus ? My solution for 1C works in the iPython book but fails with a nasty error when submitted to the Autograder. Any suggestions?

Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 28, in 
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 59.0 failed 1 times, most recent failure: Lost task 0.0 in stage 59.0 (TID 157, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "", line 7, in 
ValueError: need more than 2 values to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


All tests passed
Top ten error endpoints (3a)
----------------------------
All tests passed
Number of unique hosts (3b)
---------------------------
All tests passed
Number of unique daily hosts (3c)
---------------------------------
All tests passed
Visualizing unique daily hosts (3d)
-----------------------------------
All tests passed
Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'avgDailyReqPerHostList' is not defined

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithAvg' is not defined

Counting 404 (4a)
-----------------
All tests passed
Listing 404 records (4b)
------------------------
All tests passed
Top twenty 404 URLs (4c)
------------------------
All tests passed
Top twenty-five 404 response code hosts (4d)
--------------------------------------------
All tests passed
404 response codes per day (4e)
-------------------------------
All tests passed
Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
All tests passed
Five dates for 404 requests (4g)
--------------------------------
All tests passed
Hourly 404 response codes (4h)
------------------------------
All tests passed
Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
All tests passed
-- 14 cases passed (87.0%) --


Your submission token id is 772765-11536c47e87fd1a35c7fb140b1413814:de82c8c907378f038f9b9150f9f0f777:ip-172-31-34-101
Please include this submission token id when you need support for your code submission. Hi All,

I created a collection of links about Python Lambdas. Hope it will be helpful for you as well. 

http://bit.ly/1GZXss8 

Cheers
Nitin Guys,

I am bit confused with this lab, what I'm trying is

dayAndHostTuple = access_logs.map(lambda log:(log.date_time.day,log.host))groupedByDay = dayAndHostTuple.groupByKey()sortedByDay = groupedByDay.sortByKey()
I think I am doing something wrong, could some one help please.

thanks
 Hello,

Just completed Lab 2, but I want to point out that it was extremely difficult to execute the exercises on the provided dataset due to its size. I do not own a good configuration (Currently at i5 (4 core, 2nd Gen), 4GB RAM) true, but I had to execute the entire lab multiple times from the start since my RAM kept on filling up requiring a hard restart. 

Executing each code exactly once was fine, but on executing each code multiple times (for trying out different ways to select the best path) the RAM usable was extremely high increasing incrementally for each execution.

Though the dataset wasn't very big (~100MB compressed as per the forum), However if the course staff from next time could provide 2 datasets (1 around 10MB and other 100MB or more) that will really help in execution and testing the code on the smaller dataset for further learning. The configuration of the Vagrant box was too less as well (maybe multiple versions again) 

I wonder if others too faced the same problem since my RAM usage does not go beyond 1000-1500 MB usually.

PS: Finally I gave up and bought a dedicated server from a cloud provider for few hours to complete the lab thus had to setup vagrant, virtualbox etc all over again. It was not fun to change 1 line of code to test something new and wait for 3-5 mins (if at all) for the output :) Hi all,
just submitted the 3rd lab. 
On my notebook no errors are detected.
When I submit I get an error from the autograder (see below) but the score given by the autograder is 100% and the completion of the lab is correctly reported on my progress page. Is this normal? At the end the autograder gives me the token id of the submission for when I will request support, I am avoiding to paste it here because my IP is part of the token.
Thanks!

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block
 ran my functions, even checked output with print statements, even though they should match the test output, receiving failure.  Here's my output:

print hosts.collect()
[2582, 3222, 4190, 2502, 2537, 4106, 4406, 4317, 4523, 4346, 2864, 2650, 4454, 4214, 4340, 4385, 4168, 2550, 2560, 4134, 4456]

print daysWithHosts.collect()
[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22] I'm guessing the variable idfs_small_weights used in the computeSimilarity function is a typo where idfsSmallWeights was meant?
(The camelcase version is used everywhere else). Just finished lab2, really enjoyed it!  I had first used sortBy(...) for the questions asking for top N. Unfortunately, the Spark  documentation is not clear on the second argument to top. Like takeOrdered, key which defaults to None can be a lambda making it possible to combine sortBy(...).take(n) to one operation

thanks for a great lab I have issue with notebook taking a long time to compute. Right now, I got stucked with 3c where I got no result.  How can make it run again? Here to compute crossSmall , can we use direct inbuild function of rdd even if its is not taught ?  Can anyone give any hints about how to deal with punctuation during tokenization?

I can get 1a and 1b, but it looks like it's dealing with punctuation differently than in the shakespeare assignment. e.g., it's clear that underscores are NOT to be removed, but ! are.  I may be missing it, but I don't see it clearly stated what we should do.

My answer in 1c is in the right ballpark, so I think it's just a matter of missing a character or two to remove. I am unable to get how to get "
number of true duplicate pairs in the trueDupsRDD dataset
"

Any hint would be great. Hi guys,

sorry for the silly question but I´m not good in python coding, at the first item, I did this:

not200 = access_logs.map(lambda x: (x.endpoint, x.response_code))

But I´m getting the following error:
 File "<ipython-input-31-ab99ba6fd5fe>", line 8, in <lambda> AttributeError: 'tuple' object has no attribute 'endpoint'
Could you help me understand what I´m doing wrong?

Thank you! When I submit lab 2, the autograder gives me a 
TestFailure: incorrect dailyHosts.is_cached

But dailyHosts _is_ cached, and the test passes on my VM when I rerun the notebook from scratch. Seems very strange!

Your submission token id is 775254-69936325530cdac7ae022f9a87d3e34f:5edc4a5d673995221d35cee880ac3eff:ip-172-31-36-16

  This question is mainly directed at our tutors/teachers.

I've done some other MOOCs where the organizers managed to get us free credits on AWS so that we could run/test our programs. Could you try and contact someone at AWS to see whether we could get a gift card that would enable us to get a small cluster running maybe just for a few hours? 

It could be something like what you guys did for AMPCAMP (http://ampcamp.berkeley.edu/big-data-mini-course/launching-a-bdas-cluster-on-ec2.html) and, if you run into cost issues, maybe only provide this to whoever gets past lab3 or something so you know we're serious about learning.

This would help us get a real world spark setup working since it's not easy to get a cluster unless it's on a cloud and many people (myself included) have not-so-powerful machines (4GB RAM) so we struggle a little bit on some of the more intensive exercises.

Anyway, thanks for pputting this course together, it's been great. So I did 1b and pass the first two tests, but my words are in a different order for the 3rd test.
I get this:
['brown', 'lazy', 'jumps', 'fox', 'dog', 'quick']
instead of: 
['quick', 'brown', 'fox' ,'jumps' ,'lazy', 'dog']

It seems to be happening when I convert back and forth between sets and lists - not sure where to look to fix it.

Well I used set operations to remove the stopwords...

Edit: Bah, figured it out. I am having troubles understanding what actually needs to be returned, since as the question says the token is a pair of (word,word TF-IDF), but the next question asks as to combine similar tokens, which I do not see how it could be reasonable that the same word might have similar TF-IDF in many documents. Should we keep just the word as a token?


"Create an invert function that given a pair of (ID/URL, TF-IDF weighted token vector), returns a list of pairs of (token, ID/URL). Recall that the TF-IDF weighted token vector is a Python dictionary with keys that are tokens and values that are weights." 
 Hi All,
I have a question to the instructors.I have joined this wonderful course somewhere between third and fourth week and I found bonus -20 pts in first week because I returned my homework after deadline which makes sense.I understand that on every my solution of task I will have bonus till fourth week ?

  
Im very stuck in 4a, my result is the same but not the correct answer:
 
badRecords = (access_logs.map(lambda log: (log.response_code == 404, 1)).reduceByKey(lambda a, b : a + b).cache())
badRecordsList = badRecords.take(10)
print badRecordsList
 
[(False, 1036992), (True, 6185)]
The true pair result is the one I want, but which will be the code to extract and get only the 6185??
the false pair are the request different from 404, and the true pair is the numbre of 404 tuples

can anybody point me in the rigth direction please?


 Hi, I was going through the Lab-2 -3D program for some reference purpose and was wondering how some lines of code worked there.

1) On what basis did the distinct() function work here to print out disparate tuples..? Is it based on the key(day), or value(host), or both?
2) Why couldn't we use reduceByKey in this program instead of doing groupByKey..?

Would really appreciate if someone could take the time to answer these. Thanks!

 I am not able to pass this test of 1a... not sure how to handle empty string... any help please!!!












# TEST Tokenize a String (1a)
Test.assertEquals(simpleTokenize(quickbrownfox),
                  ['a','quick','brown','fox','jumps','over','the','lazy','dog'],
                  'simpleTokenize should handle sample text')
Test.assertEquals(simpleTokenize(' '), [], 'simpleTokenize should handle empty string')
#print simpleTokenize(' ')
Test.assertEquals(simpleTokenize('!!!!123A/456_B/789C.123A'), ['123a','456_b','789c','123a'],
                  'simpleTokenize should handle puntuations and lowercase result')
Test.assertEquals(simpleTokenize('fox fox'), ['fox', 'fox'],
                  'simpleTokenize should not remove duplicates')

















1 test passed.
1 test failed. simpleTokenize should handle empty string
1 test passed.
1 test passed.



 Here is the token id for my submission:.  Here is my question.  I have completed until (3a).  Can I submit only those I have passed that is until 3a.  Should I delete the rest of the code after 3a which I have not done and resubmit?  When I submitted the whole file with only test passed till 3a I got the following error:
Timeout error happened during grading. Please review your code to be more efficient and submit the code again.


Your submission token id is 730894-15912b13441114d8b1329b5d35653c3c:77c0e561a2547f39634ce7032a974a36:ip-172-31-47-249Thanks
 Hi

I believe in Lecture 7 (section Data Storage) the quiz answer choice is incorrect.

======
Apache Web Server Log Analysis
(1/1 point)
In Lab 2, Apache web server log analysis, you had to deal with which of the following data storage problem?

========
The Answer the quiz accepts is "Ad-hoc modifications to the data"

However the explanation is correctly pointing to another answer choice. Could you check this and correct it if it is a mistake.

Thanks,
Sudhakar
 I would like to access the virtual machine sparkvm that we created. Then I can install other python packages to try them out. Is there a password for root?
Also how is that we type  127.0.0.1:8001 in the browser of the main (host) machine but the request is served by the virtual machine? So if I ping 127.0.0.1 from the host machine while the virtual machine is up who will reply? Is there an alternative to the cartesian (http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cartesian) method. It seems its extremely sow.
I'm worried the autograder might time out.

A simple google search for "spark cartesian slow" shows others having the same issue.

Are there any plans to optimize it or any alternatives? In test , we have one_, one_ two, so how to count tf?

Steps I took :
The steps your function should perform are:
Create an empty Python dictionary : createdFor each of the tokens in the input tokens list, count 1 for each occurance and add the token to the dictionary : trying to put one_,1    one_,1   two, 1.. which I guess is not possibleFor each of the tokens in the dictionary, divide the token's count by the total number of tokens in the input tokens list : division by num of token --------------------
 Is there any way by which we can apply map reduce to list?

Code : 

def tf(tokens): wordDict = {}
  tokens = map(lambda x : (x,1),tokens) It may help you tremendously to remember that set(list) will return a set of the unique members of that list.

 In the question below.

Recall that the IDF weight for a token, t, in a set of documents, U, is computed as follows:

Let N be the total number of documents in U.Find n(t), the number of documents in U that contain t.Then IDF(t) = N/n(t).

Document is referred to as each row for eg. in corpus

[('b000jz4hqo', ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund'])]
All the values in array refers to as document and each element is token right ?

Also for below documents IDF for each token will be as shown ?

d1: “new york times york post” 
 d2: “new york post”

new: 2 / 2
york: 3 / 2
times: 1 / 2
post: 2 / 2

Just want to clarify the formula.
 I've got following output from autograder. Is it mean that there is something wrong with 1a exercise? Locally everything passed.

Tokenize a String (1a)
----------------------

[Stage 60:=======================================>                (12 + 1) / 17]
[Stage 60:=================================================>      (15 + 1) / 17]
                                                                                

[Stage 64:====================================>                   (11 + 1) / 17]
[Stage 64:==============================================>         (14 + 1) / 17]
                                                                                

[Stage 73:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 87:====================================>                     (5 + 1) / 8]
[Stage 88:>                                                         (0 + 1) / 8]
[Stage 88:=======>                                                  (1 + 1) / 8]
[Stage 88:==============>                                           (2 + 1) / 8]
[Stage 88:=====================>                                    (3 + 1) / 8]
[Stage 88:=============================>                            (4 + 1) / 8]
[Stage 88:====================================>                     (5 + 1) / 8]
[Stage 88:===========================================>              (6 + 1) / 8]
[Stage 88:==================================================>       (7 + 1) / 8]
[Stage 89:>                                                         (0 + 1) / 8]
[Stage 89:=======>                                                  (1 + 1) / 8]
[Stage 89:==============>                                           (2 + 1) / 8]
[Stage 89:=====================>                                    (3 + 1) / 8]
[Stage 89:=============================>                            (4 + 1) / 8]
[Stage 89:====================================>                     (5 + 1) / 8]
[Stage 89:===========================================>              (6 + 1) / 8]
[Stage 89:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 92:>                                                         (0 + 1) / 8]
[Stage 92:=======>                                                  (1 + 1) / 8]
[Stage 92:==============>                                           (2 + 1) / 8]
[Stage 92:=====================>                                    (3 + 1) / 8]
[Stage 92:=============================>                            (4 + 1) / 8]
[Stage 92:====================================>                     (5 + 1) / 8]
[Stage 92:===========================================>              (6 + 1) / 8]
[Stage 92:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 97:>                                                         (0 + 1) / 8]
[Stage 97:=======>                                                  (1 + 1) / 8]
[Stage 97:==============>                                           (2 + 1) / 8]
[Stage 97:=====================>                                    (3 + 1) / 8]
[Stage 97:=============================>                            (4 + 1) / 8]
[Stage 97:====================================>                     (5 + 1) / 8]
[Stage 97:===========================================>              (6 + 1) / 8]
[Stage 97:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 100:>                                                        (0 + 1) / 8]
[Stage 100:=======>                                                 (1 + 1) / 8]
[Stage 100:==============>                                          (2 + 1) / 8]
[Stage 100:=====================>                                   (3 + 1) / 8]
[Stage 100:============================>                            (4 + 1) / 8]
[Stage 100:===================================>                     (5 + 1) / 8]
[Stage 100:==========================================>              (6 + 1) / 8]
[Stage 100:=================================================>       (7 + 1) / 8]
Your submission token id is 779005-d83307528be83d170860c2851a40a671:acdaea0acff612a5ee5d02dd45d69143:ip-172-31-32-219
Please include this submission token id when you need support for your code submission.
 Using 2d, I get :
AttributeError: 'list' object has no attribute 'map'
I have tried everything

 My all test are passing till 4b .

I want to ask what does one mean by "
For each of the Amazon and Google full datasets, create weight RDDs that map IDs/URLs to TF-IDF weighted token vectors
"
To my understanding tfidf function takes 

""" Compute TF-IDF Args: tokens (list of str): input list of tokens from tokenize idfs (dictionary): record to IDF value Returns: dictionary: a dictionary of records to TF-IDF values """

So output will be a dictionary. The how can we get "
that map IDs/URLs to TF-IDF weighted token vectors
"
 Yes, I'm behind on the lab assignments, but I'm not trying to get points, just trying to learn the material. 

I fundamentally understand what we are trying to do. I have a list of tuples in the wordCounts, with the second value being the word counts for the word. I'm trying to first change that list to a list of only values and then reduce on the values. The problem I'm having is rather stupid. I can't seem to get the syntax correct to map from a list of key values to a list of keys alone. 

My nonworking code for that section is below:

b = wordCounts.map(lambda a, b: b)
print b.collect()

When I call b.collect(), it throws an error, but when I don't call collect, there is no error. I also then can't figure out how to inspect the pipelinedRDD object effectively. 

The error is below - how would you recommend I debug this?

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-29-15cd38cd66c4> in <module>()
      5 
      6 b = wordCounts.map(lambda a, b: b)
----> 7 print b.collect()
      8 
      9 '''

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 41.0 failed 1 times, most recent failure: Lost task 2.0 in stage 41.0 (TID 110, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
TypeError: <lambda>() takes exactly 2 arguments (1 given)

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

  Hi all, 

I had a trouble in 3e in getting the average number, since I could not find a method to divide by the value of total daily request to the distinct daily requests. 

I have already submitted the Lab, getting 93%, since this part failed. Having spent quite a time on that, it would be nice to learn if anybody can guide how we can iteratively map RDD key, values' division by a tuples' values in incremental indices. 

Here's how I tried and what I got: (I know that the method is faulty, and the wrong result comes from doing the mapping for each for loop , 22 times, but as I said, I could not find a right method)

groupedByDay = <FILL IN>

POSTING CODE IS A VIOLATION OF THE HONOR CODE

#hosts is defined before as a list of unique daily hosts

avgDailyReqPerHostList = avgDailyReqPerHost.take(30)
print 'Average number of daily requests per Hosts is %s' % avgDailyReqPerHostList

#result : 
Average number of daily requests per Hosts is [(1, 7), (3, 9), (4, 13), (5, 7), (6, 7), (7, 12), (8, 13), (9, 13), (10, 13), (11, 13), (12, 8), (13, 8), (14, 13), (15, 13), (16, 12), (17, 13), (18, 12), (19, 7), (20, 7), (21, 12), (22, 12)]

 I noticed there is a typo in section 2c of lab3 within the verification cell:
Test.assertTrue(abs(tokenSmallestIdf[1] - 4.25531914894 < 0.0000000001),'incorrect smallest IDF value')
Note the closing parenthesis should be before the < sign:
Test.assertTrue(abs(tokenSmallestIdf[1] - 4.25531914894) < 0.0000000001,'incorrect smallest IDF value')
Although it's minor, the typo makes the assertion always true. I am getting the following error. Any clues, already spent a day on this.
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-26-ff4a4d5c7cc1> in <module>()
      7 dayHostCount = dayGroupedHosts.reduceByKey(lambda x,y: x+y)
      8 
----> 9 dailyHosts = dayHostCount.sortByKey()
     10 
     11 dailyHostsList = dailyHosts.take(30)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sortByKey(self, ascending, numPartitions, keyfunc)
    584         # the key-space into bins such that the bins have roughly the same
    585         # number of (key, value) pairs falling into them
--> 586         rddSize = self.count()
    587         if not rddSize:
    588             return self  # empty RDD

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """

dayToHostPairTuple = access_logs.<FILL IN> PLEASE DO NOT POST SOLUTIONS

dailyHostsList = dailyHosts.take(30)

print 'Unique hosts per day: %s' % dailyHostsList Hi,
 I have been trying out something with Spark at work.
 My requirement is:- I have a key (k1, k2)). I want to group it by K1 and sort it by K2, and apply a function to the values. Is this possible? If so, a little intro should be enough...

Thanks,
Venkat I am unable to understand what does "
Convert each collection into a broadcast variable, containing a dictionary of the norm of IDF weights for the full dataset
"
mean .

How is it related to statement above it ? Is it a typo that IDF-> TF-IDF ? Following is my solution, but some errors happen. Could anybody give me a hint how to fix it?

<EDIT - do not post solution>


errors:

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-47-48ae48ca43de> in <module>()
     26                 .cache())
     27 
---> 28 print similarities.first()
     29 
     30 def similar(amazonID, googleURL):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in first(self)
   1240         ValueError: RDD is empty
   1241         """
-> 1242         rs = self.take(1)
   1243         if rs:
   1244             return rs[0]

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 86.0 failed 1 times, most recent failure: Lost task 0.0 in stage 86.0 (TID 783, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-47-48ae48ca43de>", line 25, in <lambda>
  File "<ipython-input-47-48ae48ca43de>", line 21, in computeSimilarity
  File "<ipython-input-45-9ad39d1dfc19>", line 11, in cosineSimilarity
  File "<ipython-input-24-743747f3d9e1>", line 11, in tfidf
  File "<ipython-input-24-743747f3d9e1>", line 11, in <dictcomp>
KeyError: 'your'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
</span></span></span></span></span></span></span></span></span></span></span></span></span> Hello,
I calculated  dayToHostPairTuple, dayGroupedHosts , but I stuck dayHostCount.
My code are in below .What is the problem in dayHostCount? Can anyone help me?
Thanks 

dayToHostPairTuple = access_logs.<FILL IN> PLEASE DO NOT POST SOLUTIONS
 There has been a small fix in the Test for (2c)

Please get your new copy from https://raw.githubusercontent.com/spark-mooc/mooc-setup/master/lab3_text_analysis_and_entity_resolution_student.ipynb

And thank you for reporting this issue.

#pin Hi I was on Lab 2 3a and while I was debugging the code I had written the whole section of grey just disappeared now I can not see and can't complete this or move to the next section.Please can someone help explain what's going on? Description in answer says about "doesn't meet the specifications", while this answer isn't right according to the quiz:

 Lab3 part 2C need to fill in following definition, but I am confused by the tokenCountPairTuple &  tokenSumPairTuple. Just from the name, I don't know what is the difference of "Count" and "Sum" for an unique token. 

# TODO: Replace <FILL IN> with appropriate codedef idfs(corpus): """ Compute IDF Args: corpus (RDD): input corpus Returns: RDD: a RDD of (token, IDF value) """ N = <FILL IN> uniqueTokens = corpus.<FILL IN> tokenCountPairTuple = uniqueTokens.<FILL IN> tokenSumPairTuple = tokenCountPairTuple.<FILL IN> return (tokenSumPairTuple.<FILL IN>) In exercise 3c),  I am getting an error when computing the counts of hosts for a given day
using the RDD dayToHostPairTuple. Would appreciate some hints on this.

print dayToHostPairTuple.take(10) -->> This step is working fine
[(1, u'in24.inetnebr.com'), (1, u'uplherc.upl.com'), (1, u'uplherc.upl.com'), (1, u'uplherc.upl.com'), (1, u'uplherc.upl.com'), (1, u'ix-esc-ca2-07.ix.netcom.com'), (1, u'uplherc.upl.com'), (1, u'slppp6.intermind.net'), (1, u'piweba4y.prodigy.com'), (1, u'slppp6.intermind.net')]
The following lines gives error:
dayGroupedHosts = dayToHostPairTuple.groupByKey().map(lambda s: s[0], 1)  ==>> gives error. Note: Had previosuly tried same step with lambda a,b: a,1, but was getting a similar error.print type(dayGroupedHosts)print dayGroupedHosts.take(10)

---------------------------------------------------------------------------
Py4JError                                 Traceback (most recent call last)
<ipython-input-51-4d1fbf4e7f79> in <module>()
      7 dayGroupedHosts = dayToHostPairTuple.groupByKey().map(lambda s: s[0], 1)
      8 print type(dayGroupedHosts)
----> 9 print dayGroupedHosts.take(10)
     10 print dayGroupedHosts.count()
     11 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1192         """
   1193         items = []
-> 1194         totalParts = self._jrdd.partitions().size()
   1195         partsScanned = 0
   1196 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _jrdd(self)
   2291                                              env, includes, self.preservesPartitioning,
   2292                                              self.ctx.pythonExec,
-> 2293                                              bvars, self.ctx._javaAccumulator)
   2294         self._jrdd_val = python_rdd.asJavaRDD()
   2295 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    699         answer = self._gateway_client.send_command(command)
    700         return_value = get_return_value(answer, self._gateway_client, None,
--> 701                 self._fqn)
    702 

  703         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    302                 raise Py4JError(
    303                     'An error occurred while calling {0}{1}{2}. Trace:\n{3}\n'.
--> 304                     format(target_id, '.', name, value))
    305         else:
    306             raise Py4JError(

Py4JError: An error occurred while calling None.org.apache.spark.api.python.PythonRDD. Trace:
py4j.Py4JException: Constructor org.apache.spark.api.python.PythonRDD([class org.apache.spark.rdd.MapPartitionsRDD, class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.Integer, class java.lang.String, class java.util.ArrayList, class org.apache.spark.Accumulator]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:184)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:202)
	at py4j.Gateway.invoke(Gateway.java:213)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
  In lecture7 (Data Quality) in 9th step (Data Storage), there is a question for:
Apache Web Server Log Analysis
The correct answer must be the 4th item (data that doesn't meet the specifications you've assumed), but it accepts 3th item. Although the description of answer is correct.

Regards
 Why can't we do this?

amazonRecToToken = tokenize(amazonSmall)googleRecToToken = tokenize(googleSmall)

Any hints on these two two lines? Bit behind on lab2.

I am getting the below error in 1(b)

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-10-8d8d6629d991> in <module>()
     32 
     33 
---> 34 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-10-8d8d6629d991> in parseLogs()
      9 def parseLogs():
     10     """ Read and parse log file """
---> 11     parsed_logs = (sc
     12                    .textFile(logFile)
     13                    .map(parseApacheLogLine)

NameError: global name 'sc' is not defined I am not clear what is needed for the calculation of the value of idfs.

Here are the step values I see for from my data,
 my count of 'software' token from the union rdd passed is 185
there are  4772 unique tokens overall. 

From these how do I calculate the idfs ??

1.Let N be the total number of documents in U.
   Q: Is the value of N =4772? ..basically its not clear by document if it mean records 
2. Find n(t), the number of documents in U that contain t.
   Q: do we need to findout how many records contain the word 'software' ? ( cant think of a way to search in the rdd per record )


  def norm(a):
    """ Compute square root of the dot product
    Args:
        a (dictionary): a dictionary of record to value
    Returns:
        norm: a dictionary of tokens to its TF values

I believe the returns specification for norm is wrong.  Should say something like square root of the dot product of the dictionary and itself. for lab 1 (4d), here is my code:

shakespeareWordsRDD = shakespeareRDD.flatmap(lambda x: (x.split(' ')))shakespeareWordCount = shakespeareWordsRDD.count()print shakespeareWordsRDD.top(5)print shakespeareWordCount

But here is the error when executing it:

---------------------------------------------------------------------------AttributeError                            Traceback (most recent call last)<ipython-input-57-2b56d7fad931> in <module>()  1 # TODO: Replace <FILL IN> with appropriate code ----> 2 shakespeareWordsRDD = shakespeareRDD.flatmap(lambda x: (x.split(' ')))  3 shakespeareWordCount = shakespeareWordsRDD.count()  4 print shakespeareWordsRDD.top(5)  5 print shakespeareWordCount AttributeError: 'PipelinedRDD' object has no attribute 'flatmap' 
--------------------------------------

I don't know why 'flatmap' is not allowed while other responses saying 'flatmap' is ok and the reference manual says so too. Please help.
 Hi
I understand that unless and until i apply any action, RDD transformation functions won't change the state of RDD. It merely records what action needs to be taken in case any action method , like "count()" is called upon.
So, in my code, if i write the following line and press enter
parsed_logs = (sc.textFile(logFile).map(parseApacheLogLine))
what amount of memory is this object containing ? I think parsed_logs is instance of class RDD wich just contains what needs to be done if any action is called upon. So, if instead of writing it like above, if i write and enter
parsed_logs1 = (sc.textFile(logFile).map(parseApacheLogLine).cache())
But since i have not called any action yet, the state of parsed_logs1 and parsed_logs should remain the same or they refer the same object ? Is that understanding correct ?
What happens if i use
print sys.getsizeof(parsed_logs)print sys.getsizeof(access_logs)print sys.getsizeof(failed_logs)
When i executed the above commands, i got 28 for all of them. But didn't we cached the first two, so they should have higher memory as compared to failed_logs ? What is happening here ? I'm up to 4f, all tests passed so far on earlier steps.  I believe I have debugged my way through it correctly so that I have the correct function, and I'm running the full data set (instead of commenting out bits of it and running on test data only).  It's been running about 15 minutes so far, which seems long.  I can see from the tests in the next block that I'll end up running the function on a couple million entries, so I guess I shouldn't expect this to finish super quickly, but 15 minutes seems like it is probably too long.

How long should I expect this to take on a modern laptop?  If it has taken this long, is that a clear sign I am doing something incorrectly?  After how much time should I just stop it?  Is there even a way to stop it short of "vagrant halt" and then "vagrant up" and then re-running every single block in the worksheet?
 I used join on both full dataset, but I keep running into following exception, what should I do?

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 102.0 failed 1 times, most recent failure: Lost task 0.0 in stage 102.0 (TID 592, localhost): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:172)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:108)
	... 8 more What is the problem??? Please help.....
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-33-00e1612354e5> in <module>()
      6 
      7 dayHostCount = dayGroupedHosts.map(lambda x,y: (x, len(y)))
----> 8 dailyHosts = dayHostCount.sortByKey()
      9 dailyHostsList = dailyHosts.take(30)
     10 print 'Unique hosts per day: %s' % dailyHostsList

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sortByKey(self, ascending, numPartitions, keyfunc)
    584         # the key-space into bins such that the bins have roughly the same
    585         # number of (key, value) pairs falling into them
--> 586         rddSize = self.count()
    587         if not rddSize:
    588             return self  # empty RDD

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 77.0 failed 1 times, most recent failure: Lost task 0.0 in stage 77.0 (TID 222, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
  File "<ipython-input-33-00e1612354e5>", line 3, in <lambda>
TypeError: 'datetime.datetime' object has no attribute '__getitem__'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) I tried 3(c) a lot of times, it does not give any result. I tried restarting the kernel and the computer. What could be wrong ?  Hi,

I just found the course yesterday and have been working hard to catch up and just finish lab 1. How do I submit lab 1 result for the course.

I did everything myself and only get a hint for "flatMap" vs. "flatmap" error from piazza just now in lab 1 4e.

Thanks. What are best practices to store, retrieve and process time series (capital markets e.g. tick data) data in Spark?

From my initial digging:
 * For long term storage Parquet compressed files.
 * For near real-time Cassandra storage.

I want to keep 1 day data in-memory and looks like default open source Cassandra does not support it ...
Note: Parquet files on Tachyon is big overhead for latency and unacceptable number of jars required to access data. Not sure about AVRO and ability to access Tachyon's underlying ramdrive.
Please share you experiences. 

I would like to achieve something KISS for in-memory similar as kdb+ but with modern programming languages.
 
Thanks for all kind of ideas! My cosineSimilarity function seems to work but in 3c, i am getting a value that doesn't match the expected value (0.000303171940451).  I am assuming that the googleValue and amazonValue contain the entire strings needed to calculate the similarity. Is that assumption right?

Also, just to confirm, the google url talks about "diana ross" while the amazon one is a spiel about "adobe"?

Thanks in advance. 1 test passed.
1 test passed.
1 test failed. incorrect dailyHosts.is_cachedWhy the error if the two former tests pased?What does it mean?How can I correct it? Hi, 
   I followed all pre-requisites to notebook submission for lab 3, every test passed, no errors, when I submit the code, after a while I get the error below:
How can I try to understand the error to try to eventually fix the notebook ?What is the submission token id and how should I use it ?
Thanks
Davide B.

Tokenize a String (1a)
----------------------
[Stage 63:==============================> (9 + 1) / 17]
[Stage 63:==============================================> (14 + 1) / 17]
[Stage 67:================================> (10 + 1) / 17]
[Stage 67:==========================================> (13 + 1) / 17]
[Stage 67:====================================================> (16 + 1) / 17]
[Stage 76:===========================================> (6 + 1) / 8]
[Stage 79:====================================> (5 + 1) / 8]
[Stage 91:====================================> (5 + 1) / 8]
[Stage 92:> (0 + 1) / 8]
[Stage 92:=======> (1 + 1) / 8]
[Stage 92:==============> (2 + 1) / 8]
[Stage 92:=====================> (3 + 1) / 8]
[Stage 92:=============================> (4 + 1) / 8]
[Stage 92:====================================> (5 + 1) / 8]
[Stage 92:===========================================> (6 + 1) / 8]
[Stage 92:==================================================> (7 + 1) / 8]
[Stage 93:> (0 + 1) / 8]
[Stage 93:=======> (1 + 1) / 8]
[Stage 93:==============> (2 + 1) / 8]
[Stage 93:=====================> (3 + 1) / 8]
[Stage 93:=============================> (4 + 1) / 8]
[Stage 93:====================================> (5 + 1) / 8]
[Stage 93:===========================================> (6 + 1) / 8]
[Stage 93:==================================================> (7 + 1) / 8]
[Stage 96:> (0 + 1) / 8]
[Stage 96:=======> (1 + 1) / 8]
[Stage 96:==============> (2 + 1) / 8]
[Stage 96:=====================> (3 + 1) / 8]
[Stage 96:=============================> (4 + 1) / 8]
[Stage 96:====================================> (5 + 1) / 8]
[Stage 96:===========================================> (6 + 1) / 8]
[Stage 96:==================================================> (7 + 1) / 8]
[Stage 101:> (0 + 1) / 8]
[Stage 101:=======> (1 + 1) / 8]
[Stage 101:==============> (2 + 1) / 8]
[Stage 101:=====================> (3 + 1) / 8]
[Stage 101:============================> (4 + 1) / 8]
[Stage 101:===================================> (5 + 1) / 8]
[Stage 101:==========================================> (6 + 1) / 8]
[Stage 101:=================================================> (7 + 1) / 8]
[Stage 104:> (0 + 1) / 8]
[Stage 104:=======> (1 + 1) / 8]
[Stage 104:==============> (2 + 1) / 8]
[Stage 104:=====================> (3 + 1) / 8]
[Stage 104:============================> (4 + 1) / 8]
[Stage 104:===================================> (5 + 1) / 8]
[Stage 104:==========================================> (6 + 1) / 8]
[Stage 104:=================================================> (7 + 1) / 8]
[Stage 107:> (0 + 1) / 8]
[Stage 107:=======> (1 + 1) / 8]
[Stage 107:==============> (2 + 1) / 8]
[Stage 107:=====================> (3 + 1) / 8]
[Stage 107:============================> (4 + 1) / 8]
[Stage 107:===================================> (5 + 1) / 8]
[Stage 107:==========================================> (6 + 1) / 8]
Your submission token id is 791727-4368a890d80eee220a34719c667c59c5:f1a611bf69bdf82135836032690dc99c:ip-172-31-41-172
Please include this submission token id when you need support for your code submission.
 Hello,

I think some clarification on how the resulting RDD's look like on section 4 would be good, in other sections there are some examples of how the records looks like but i don't think its clear for section 4, ex: Section 4b: amazonWeightsRDD, not sure what needs to be done here.

Any pointers? hi there, can i download the virtual box image with wget and put it in .vagrant file, my internet speed is bit slow and can run with vagrant up.... While I was able to resolve 4b but didn't follow why doesn't the below code does not get unique bad end points. In the first step I get a list and in the next I convert it to a set.

badEndpoints = badRecords.<FILL IN> POST RESULTS NOT CODE

Calling count() on both the RDD's gives me the same value. Shouldn't the set function remove duplicates from the list? RE 3e lst stepI have created the attached list using join and assumed a lambda (x,y): x , y[0] / y[1] would produce a list of averages ... BUT " NameError: name 'y' is not defined"Q: What type is this RDD list belowQ: how do i access the values in the sublist in a lambda or def functionstandard RDD join [(1, (33996, 2582)), (3, (41387, 3222)), (4, (59554, 4190)), (5, (31888, 2502)), (6, (32416, 2537)), (7, (57355, 4106)), (8, (60142, 4406)), (9, (60457, 4317)), (10, (61245, 4523)), (11, (61242, 4346)), (12, (38070, 2864)), (13, (36480, 2650)), (14, (59873, 4454)), (15, (58845, 4214)), (16, (56651, 4340)), (17, (58980, 4385)), (18, (56244, 4168)), (19, (32092, 2550)), (20, (32963, 2560)), (21, (55539, 4134)), (22, (57758, 4456))]  Can anyone please help me with the definitions of the RDDs in task 2c? 

1. uniqueTokens = RDD of the format ([list of unique tokens in document 1],[list of unique tokens in document 2]...)

2. tokenCountPairTuple - ??

3. tokenSumPairTuple - ?? 

As I read in another discussion is tokenCountPairTuple an RDD of the type ('unique_token',1) -> if yes, how do I extract each token from an RDD which contains lists of tokens?? 

Thank you After the first submission i got 94% correct.
For the second submission the result has been (with no score at all).

793665-fc278e283cab0699fe720ede8818ed0b:89a4b76de340810c257bcbd34c6b4b0d
is it correct to assume that the autograder currently has some problems? -- this message can be reproduced
see e.g. 

795705-febddbfd7b051e048681e54c8b109c1d:89a4b76de340810c257bcbd34c6b4b0d
----
Tokenize a String (1a)
----------------------

[Stage 60:==========================>                              (8 + 1) / 17]
[Stage 60:=======================================>                (12 + 1) / 17]
[Stage 60:==============================================>         (14 + 1) / 17]
                                                                                

[Stage 64:==============================>                          (9 + 1) / 17]
[Stage 64:====================================>                   (11 + 1) / 17]
[Stage 64:==============================================>         (14 + 1) / 17]
                                                                                

[Stage 72:===========================================>              (6 + 1) / 8]
                                                                                

[Stage 73:=============================>                            (4 + 1) / 8]
[Stage 73:===========================================>              (6 + 1) / 8]
[Stage 73:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 75:====================================>                     (5 + 1) / 8]
[Stage 75:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 76:=====================>                                    (3 + 1) / 8]
[Stage 76:====================================>                     (5 + 1) / 8]
[Stage 76:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 88:=====================>                                    (3 + 1) / 8]
[Stage 88:====================================>                     (5 + 1) / 8]
[Stage 88:===========================================>              (6 + 1) / 8]
[Stage 89:>                                                         (0 + 1) / 8]
[Stage 89:=======>                                                  (1 + 1) / 8]
[Stage 89:==============>                                           (2 + 1) / 8]
[Stage 89:=====================>                                    (3 + 1) / 8]
[Stage 89:=============================>                            (4 + 1) / 8]
[Stage 89:====================================>                     (5 + 1) / 8]
[Stage 89:===========================================>              (6 + 1) / 8]
[Stage 89:==================================================>       (7 + 1) / 8]
[Stage 90:>                                                         (0 + 1) / 8]
[Stage 90:=======>                                                  (1 + 1) / 8]
[Stage 90:==============>                                           (2 + 1) / 8]
[Stage 90:=====================>                                    (3 + 1) / 8]
[Stage 90:=============================>                            (4 + 1) / 8]
[Stage 90:====================================>                     (5 + 1) / 8]
[Stage 90:===========================================>              (6 + 1) / 8]
[Stage 90:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 93:>                                                         (0 + 1) / 8]
[Stage 93:=======>                                                  (1 + 1) / 8]
[Stage 93:==============>                                           (2 + 1) / 8]
[Stage 93:=====================>                                    (3 + 1) / 8]
[Stage 93:=============================>                            (4 + 1) / 8]
[Stage 93:====================================>                     (5 + 1) / 8]
[Stage 93:===========================================>              (6 + 1) / 8]
[Stage 93:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 98:>                                                         (0 + 1) / 8]
[Stage 98:=======>                                                  (1 + 1) / 8]
[Stage 98:==============>                                           (2 + 1) / 8]
[Stage 98:=====================>                                    (3 + 1) / 8]
[Stage 98:=============================>                            (4 + 1) / 8]
[Stage 98:====================================>                     (5 + 1) / 8]
[Stage 98:===========================================>              (6 + 1) / 8]
[Stage 98:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 101:>                                                        (0 + 1) / 8]
[Stage 101:=======>                                                 (1 + 1) / 8]
[Stage 101:==============>                                          (2 + 1) / 8]
[Stage 101:=====================>                                   (3 + 1) / 8]
[Stage 101:============================>                            (4 + 1) / 8]
[Stage 101:===================================>                     (5 + 1) / 8]
[Stage 101:==========================================>              (6 + 1) / 8]
[Stage 101:=================================================>       (7 + 1) / 8]
                                                                                

[Stage 104:>                                                        (0 + 1) / 8]
[Stage 104:=======>                                                 (1 + 1) / 8]
[Stage 104:==============>                                          (2 + 1) / 8]
[Stage 104:=====================>                                   (3 + 1) / 8]
[Stage 104:============================>                            (4 + 1) / 8]
[Stage 104:===================================>                     (5 + 1) / 8]
[Stage 104:==========================================>              (6 + 1) / 8]
[Stage 104:=================================================>       (7 + 1) / 8]
                                                                                

[Stage 107:=======>                                                 (1 + 1) / 8]
[Stage 107:==============>                                          (2 + 1) / 8]
[Stage 107:=====================>                                   (3 + 1) / 8]
[Stage 107:============================>                            (4 + 1) / 8]
[Stage 107:===================================>                     (5 + 1) / 8]
[Stage 107:==========================================>              (6 + 1) / 8]
[Stage 107:=================================================>       (7 + 1) / 8]
---------

 Basically, I join both reverted full datasets, then do swap, and group them by key, however, the result got, 4381424 differs lot from the given, 2441100. there should be missing steps. I will appreciate if someone could give some hints.  After thinking for hours, I found I've mistaken N as #tokens, and it actually is #documents.

 
I have no idea why i am getting wrong number of counts. Is there anything wrong with my tokenize method ?

There are 200511 tokens in the combined datasets
 Lab 3 mentions:
For each of the unique tokens, count how many times it appears in the document and then compute the IDF for that token: N/n(t)

This is incorrect, we need to count how many times the token appears in the corpus (only counting at most one occurrence per document), not in the document.

Another way to phrase it is the number of documents in the corpus the document appears in - a definition which is used when explaining IDF. Hey all ,

How can I get access to the just date in log.date_time.
I Tried using log.date_time[9:10] considering the python format is 1234-12-xx (YYYY-MM-DD) 

Manish Hello,

I have been able to remove stopwords using different approaches. However, I get the resulting list in unordered form and have a following test failed:

Test.assertEquals(tokenize(quickbrownfox), ['quick','brown','fox','jumps','lazy','dog'],                    'tokenize should handle sample text') 
My result is:
['brown', 'lazy', 'jumps', 'fox', 'dog', 'quick']
I am not sure that it is correct in this case for bag of words - we are not sorting or something.  Can you give a hint?
P.S. I used '-' between two sets. The exercise seems to be straightforward which is why I don't understand where I am going wrong.

As explained in other places here, documents count equals the corpus RDD count, yet only if I feed in a N value 94 times larger than that, do I get values that pass the last test.

The code for the IDF calculations seems completely transparent:

Getting the uniqueTokens is simple with flatMap and the cartesian product of this with the corpus makes for the n(t) count.  One reduceByKey later we can then calculate N/n(t).

So why are my IDF values of by a factor of 94?  Have been puzzling over this for hours without seeing the light. In this section, I was asked to make a corpus by combining two RDD objects.

I know I can join two RDDs with the .join() function. However, I don't know in which way I can implement a "union" operation. Is there any hint for this?  I have submitted the results of the Lab 1 on time (with allowed delay without penalty, i.e. within grace period) and to the best of my understading I have received grade 100.
  After the deadline I have mistakenly submitted the solution once again and I now I have (with penalty) only 80.

As for late submissions after 3 day grace period of the due date, you lose 20 points.
 Your final score is 80
Questions:A) Why the initial grade (submitted on time) was discarded and replaced by the grade with penalty?B) Can this be fixed? In the lecture, the probability that at least one subject in 1000 will guess correctly was given as 98%, which I assume was calculated by 1000 / 1024. However, the probability that all 1000 subjects guess wrong would be (1023 / 1024) ^ 1000 = 38%. Therefore, the probability that at least one of them guess correctly would be 100% - 38% = 62%. What do you guys think? Is there a way to save these links without clicking on individuals for future references. Thanks i found 24320 tokens in the combined datasets.i put two constraints in tokenize() functions, 's not in stopwords' and 's != '' '
and then i added one more constraint as 'len(s) > 1'.then i got 23616.
did i do right move ? and is there any more specific constraints that i should apply into tokenize() function.
ty for youre attention. (RESOLVED)

Hello!

def parseData(filename):
[...]
 return (sc
 .textFile(filename, 4, 0)
 .map(parseDatafileLine)
 .cache()) # (1)

def loadData(path):
 [...]
 filename = os.path.join(baseDir, inputPath, path)
 raw = parseData(filename).cache() # (2)
[...]
 return valid
Why is the RDD returned by parseData() cached again in loadData()?

Is that necessary?

Kind Regards

Raffael The path says data/cs100/lab3, but where can I find the data folder? Thank you very much for your help. seems like some initialized coding in lab2 exercise is a bit daunting for a naive person like me. whats your views to handle this ? well i have prior programming experience in R and python (not so much) and a good data analysis knowledge but spark seems to be a hard nut to crack for me . please guide how to build a strong foundation in spark or is it better to leave ? i did well in lab 1 :) Hello!

I don't understand what I am doing wrong.
In "(1c) Tokenizing the small datasets" I got the result of 33414 tokens. My code did very simple things:1) Map tokenize function for every amazonRecToToken and googleRecToToken => (key, <iterator object with tokens>)
2) countTokens returns count only <iterator object with tokens>'s.

My tokenize functions passed all test and did very simple things too:
1) Split given string by given regex with re.split()
2) map (python map!) lower() function to every element from in after splitting list
3) filtering empty strings and strings from stopwords list.

Everything looks like good.
Any ideas about my misunderstanding?

And it is look like my tokenizer works good. I hot correct result with it for "(1d) Amazon record with the most tokens" The autograder gave the following error message for my submission. Please note that I ran successfully all cells in the notebook.
Tokenize a String (1a)
----------------------

[Stage 60:==========================================>             (13 + 1) / 17]
[Stage 60:====================================================>   (16 + 1) / 17]
                                                                                

[Stage 64:====================================>                   (11 + 1) / 17]
[Stage 64:=================================================>      (15 + 1) / 17]
                                                                                

[Stage 73:===========================================>              (6 + 1) / 8]
                                                                                

[Stage 77:=============================>                            (2 + 1) / 4]
[Stage 77:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 79:=============================>                            (2 + 1) / 4]
                                                                                

[Stage 81:==============>                                           (1 + 1) / 4]
[Stage 81:=============================>                            (2 + 1) / 4]
[Stage 81:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 83:=============================>                            (2 + 1) / 4]
[Stage 83:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 87:==================================================>       (7 + 1) / 8]
[Stage 88:>                                                         (0 + 1) / 8]
[Stage 88:=======>                                                  (1 + 1) / 8]
[Stage 88:==============>                                           (2 + 1) / 8]
[Stage 88:=====================>                                    (3 + 1) / 8]
[Stage 88:=============================>                            (4 + 1) / 8]
[Stage 88:====================================>                     (5 + 1) / 8]
[Stage 88:===========================================>              (6 + 1) / 8]
[Stage 88:==================================================>       (7 + 1) / 8]
[Stage 89:>                                                         (0 + 1) / 8]
[Stage 89:=======>                                                  (1 + 1) / 8]
[Stage 89:==============>                                           (2 + 1) / 8]
[Stage 89:=====================>                                    (3 + 1) / 8]
[Stage 89:=============================>                            (4 + 1) / 8]
[Stage 89:====================================>                     (5 + 1) / 8]
[Stage 89:===========================================>              (6 + 1) / 8]
[Stage 89:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 92:>                                                         (0 + 1) / 8]
[Stage 92:=======>                                                  (1 + 1) / 8]
[Stage 92:==============>                                           (2 + 1) / 8]
[Stage 92:=====================>                                    (3 + 1) / 8]
[Stage 92:=============================>                            (4 + 1) / 8]
[Stage 92:====================================>                     (5 + 1) / 8]
[Stage 92:===========================================>              (6 + 1) / 8]
[Stage 92:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 93:=============================>                            (2 + 1) / 4]
[Stage 93:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 97:>                                                         (0 + 1) / 8]
[Stage 97:>                                                         (0 + 1) / 8]
[Stage 97:>                                                         (0 + 1) / 8]
[Stage 97:=======>                                                  (1 + 1) / 8]
[Stage 97:=======>                                                  (1 + 1) / 8]
[Stage 97:=======>                                                  (1 + 1) / 8]
[Stage 97:==============>                                           (2 + 1) / 8]
[Stage 97:==============>                                           (2 + 1) / 8]
[Stage 97:==============>                                           (2 + 1) / 8]
[Stage 97:=====================>                                    (3 + 1) / 8]
Your submission token id is 798385-ef53a7dc58ddb0147ee55b7fa1514f11:024ca0df3671f4a9cef9d909eb54a5a2:ip-172-31-39-99
Please include this submission token id when you need support for your code submission.Can you please look into this and let me know if there is indeed some error on my part and how i can proceed? Hi,

some feedback for the lab 3

a) I enjoyed this lab.
b) Learned a lot about Python data structures.
c) Learned a bit new things about spark (that's OK:  I finally understand that you do need much more Python than I expected).
d) Well designed lab which incrementally introduces new ideas and develops a framework.

BUT after 7/10 submissions which I list here I'm kind of frustrated;

Submission 1: 94% - but that was OK, as I did not solved 4e and 4f at this point
Submission 2: All tasks solved (at least on my VM) --> cryptic message (see @2291) with no grade  --> fallback to 0%
Submission 3: All tasks solved (at least on my VM) --> cryptic message with no grade --> 0%
Submission 4: All tasks solved (at least on my VM) --> statement that the run time was too long (OK - but I think it would be nice to get at least partial grades, this should not be a binary decision) -> 0%
Submission 5: All tasks solved (at least on my VM) / optimized --> cryptic message with no grade -> 0%
Submission 6: All tasks solved (at least on my VM) / optimized --> not feedback at all (>1h) -> no change
Submission 7: resubmit / optimized --> 100%

So the solution of the lab 3 takes me about 4h (mainly due to understanding Python and the tasks at hand).
Getting the grade took me about 3 additional hours.

So I would like to propose:
a) Please check the cryptic message
b) Please don't count submissions with no feedback
c) It would be great if the "timeout" won't yield to a binary decision, i.e. for me it looks like that 1-4d are short runners and 4e/4f take a little more time. 4e/4f or 5 might be responsible for an overall timeout although the results for 1-4d might be gradable!?
d) I also have the feeling that the "timeout" is a little restrictive, i.e. my "long runner" took about 5 minutes on my virtual machine, the "short runner" took about 3 minutes, so this difference yields already to a kick out from the autograder, it might be a good idea to relax this time constraint for lab 3 a little bit!

Thank's a lot 
John

 I passed succesfully the 4a, but dont know if badRecordsList has the correct output for the next problem, cos I had some issues in 4b and dont know if its because the rdd is not in correct format, I have this:

badRecordsList = badRecords.collect()

print badRecordsList

[(Row(client_identd=u'-', content_size=0L, date_time=datetime.datetime(1995, 8, 1, 0, 7, 33), endpoint=u'/shuttle/resources/orbiters/discovery.gif', host=u'js002.cc.utsunomiya-u.ac.jp', method=u'GET', protocol=u'HTTP/1.0', response_code=404, user_id=u'-'), 1), (Row(client_identd=u'-', content_size=0L, date_time=datetime.datetime(1995, 8, 1, 0, 28, 41), endpoint=u'/pub/winvn/release.txt', host=u'tia1.eskimo.com', method=u'GET', protocol=u'HTTP/1.0', response_code=404, user_id=u'-'), 1), (Row(client_identd=u'-', content_size=0L, date_time=datetime.datetime(1995, 8, 1, 0, 50, 12), endpoint=u'/www/software/winvn/winvn.html', host=u'grimnet23.idirect.com', method=u'GET', protocol=u'HTTP/1.0', response_code=404, user_id=u'-'), 1), (Row(client_identd=u'-', content_size=0L, date_time=datetime.datetime(1995, 8, 1, 1, 4, 54), endpoint=u'/history/history.htm', host=u'miriworld.its.unimelb.edu.au', method=u'GET', protocol=u'HTTP/1.0', response_code=404, user_id=u'-'), 1), (Row(client_identd=u'-', content_size=0L, date_time=datetime.datetime(1995, 8, 1, 1, 5, 14), endpoint=u'/elv/DELTA/uncons.htm', host=u'ras38.srv.net', method=u'GET', protocol=u'HTTP/1.0', response_code=404, user_id=u'-'), 1), (Row(client_identd=u'-', content_size=0L, date_time=datetime.datetime(1995, 8, 1, 1, 17, 38), endpoint=u'/sts-71/launch/', host=u'cs1-06.leh.ptd.net', method=u'GET', protocol=u'', response_code=404, user_id=u'-'), 1), (Row(client_identd=u'-', content_size=0L, date_time=datetime.datetime(1995, 8, 1, 1, 33, 2), endpoint=u'/history/apollo/apollo-13.html', host=u'dialip-24.athenet.net', method=u'GET', protocol=u'HTTP/1.0', response_code=404, user_id=u'-'), 1), 

Is this output ok?
Cos in 4b when I tried this:
badEndpoints = badRecords.map(lambda log: (log.endpoint,1))

I get this error: 
AttributeError: 'tuple' object has no attribute 'endpoint'
thanks!
 Hi, my last submission is

Your submission token id is 798149-5f44755a58946211c21030b1b868bcd5:5c95ff422aea5a63362f2473a2faaf40:ip-172-31-41-171

the result is 

All tests passed
Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect dailyHosts.is_cached

but this is not correct, because I have cached this rdd, in fact, all test are ok in my exercise.

Please, could someone tell me what can happen? I don't want to do more submission, because the result is always the same
 Due to the downloading speed, I downloaded the package.box directly on my windows. Then I ran command "vagrant box add --name sparkvm  "package.box" ", and an error appeared "The box failed to unpackage properly. Please verify that the box file you're trying to add is not corrupted and try again." What was the matter?Anybody can help me?Thanks I submitted my lab2 assignment, but autograder is taking hours to run (2.5 at the moment, still running).
I tried again locally, I do not have regex problem or additional print.

The problem is that I could not even retry to submit.

Update: I forced a new resubmit and it ran ok in 10 min. With the same file. So the grader could have problem. I've hit a wall with my solution for 4f. It's computing a value that seems ok but does not pass the test:

(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), 7.249921151292871e-06)

Now should I compute the norms for:
amazonWeightsBroadcast.value[amazonRec].values()
or
[amazonWeightsBroadcast.value[amazonRec][t] for t in tokens]
?

For the second case I get the same weights for amazonWeights and googleWeights which results in a cosineSimilarity of 1.0.
amazonWeights = [('data', 18.504032258064516), ('complete', 7.725589225589226), ('includes', 13.657738095238095), ('software', 2.472521551724138)]
googleWeights = [('data', 18.504032258064516), ('complete', 7.725589225589226), ('includes', 13.657738095238095), ('software', 2.472521551724138)]
Should the weights be the same ?

Can anyone provide a hint what I'm doing wrong? All,

I have just enrolled myself in the Berkeley X: CS100.1x Introduction to Big Data with Apache Spark

Obviously, I am behind schedule but I am doing my best to catch up.

I have just submitted my 1st assignment just today 22nd June 2015 while it was due 9th June.

My question is: will I be able to get my assignments checked and also if I can keep on catching up while being behind schedule, will I be able to get a certificate of completion at the end ?

Thanks
Ramy Amer Hi I am running into memory issues and system gets hanged after 1c. I have 2gb ram, and cannot increase it. Can I run the program on Databricks cloud, as mentioned in other posts, and then download the python notebook and submit? If yes, please let me know how. I failed to answer one item correctly and scored 93 out of 100 on lab 1. If I now fix the remaining item, does it make sense to re-submit after the deadline? Would I be penalized and be left with 80 points, or are my 93 points from the first attempt still valid? Hello!
I have successfully finished this section, but I don't like my solution because I used python tricks to get list of tokens and remove duplicates.
So to implement it via pyspark only, I need to reverse groupByKey(). So let's say I have this rdd:
[(1, ['a', 'b', 'c']), (2, ['d', 'e'])]
and I want to get:
[(1,'a'), (1, 'b'), (1, 'c'), (2, 'd'), (2, 'e')]
How can I do that?
Thanks. I get the b000jz4hqo count instead of the b000o24l3q count.. I ve sorted the list by descending but still the  b000jz4hqo
appears first....?? Hi,

first of all congratulations for the course and the lab3 is amazing. I've submitted my lab3 work something like 30 mins ago and the autograder has not returned the result yet. This reduces the time that I can use to improve my solution since my spare time is a limited resource. Isn't there anything that can be done? repl.it  - Quick way to test Python commands, regular expressions

http://repl.it/languages/Python3 (RESOLVED)

Hello,

idfs() is provided the following dataset:

amazonRecToToken.union(googleRecToToken)

But this dataset is just a simple list of (ID, [tokens]). To calculate the IDF I need to know in how many documents a token occured. But this information is no longer available - only implicitely as Google-IDs are URLs and start with "http"!?

If the argument to idfs() would be provided as a two dimensional array like this:

[[(ID,[tokens]), ...],[(ID,[tokens]), ...]]

and the sub-arrays represent different documents - then the information for n(t) would be preserved.

Or is there a flaw in my logic?

Kind Regards

Raffael Stuck in seemingly straightforward 3b. I cannot get 0.0577 but 0.981 as the answer.

I am sure I passed 3a.

Here are what what these variables return for debugging:

w1, w2, dotprod(w1, w2), norm(w1), norm(w2), cossim(w1, w2) are respectively



({'photoshop': 22.22222222222222, 'adobe': 8.333333333333334}, {'illustrator': 50.0, 'adobe': 8.333333333333334}, 1180.5555555555557, 23.73334373699314, 50.68968775248516, 0.9813137496771572)

 These are the stopwords: set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'with', u'had', u'should', u'to', u'only', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'did', u'these', u't', u'each', u'where', u'because', u'doing', u'theirs', u'some', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'below', u'does', u'above', u'between', u'she', u'be', u'we', u'after', u'here', u'hers', u'by', u'on', u'about', u'of', u'against', u's', u'or', u'own', u'into', u'yourself', u'down', u'your', u'from', u'her', u'whom', u'there', u'been', u'few', u'too', u'themselves', u'was', u'until', u'more', u'himself', u'that', u'but', u'off', u'herself', u'than', u'those', u'he', u'me', u'myself', u'this', u'up', u'will', u'while', u'can', u'were', u'my', u'and', u'then', u'is', u'in', u'am', u'it', u'an', u'as', u'itself', u'at', u'have', u'further', u'their', u'if', u'again', u'no', u'when', u'same', u'any', u'how', u'other', u'which', u'you', u'who', u'most', u'such', u'why', u'a', u'don', u'i', u'having', u'so', u'the', u'yours', u'once'])

Should the 'u' precede every stopword? And why is it being added? What are the differences among:

xyz = abc_logs.map(lambda log: log.xyz)
xyz = abc_logs.map(lambda log: log.xyz).cache(0
xyz = abc_logs.map(lambda log: log.xyz).collect()

Appreciate your help to clarify. (please note, I read lectures, and did labs, still, I have the question for clarification)
 There are minor typos in labs 3

like in 2c:
replace assert as
Test.assertTrue(abs(tokenSmallestIdf[1] - 4.25531914894) < 0.0000000001, 'incorrect smallest IDF value')

in 2f:
replace : recb000hkgj8kweights
with: rec_b000hkgj8k_weights

If anyone find anything else please update this thread. Hi,

I'm currently stuck at trying to create the RDD crossSmall.
Any hits at which built in functions I should be using? And I'm guessing it should be an RDD with length len(googleSmall)*len(amazonSmall)?

Thanks Hi,

I am currently in 3a. 
I did the following 
not200 = access_logs.<checked for not 200 >
#not200.count()
endpointCountPairTuple = not200.<created a tuple here >
endpointSum = endpointCountPairTuple.<reduce the key and added the value>

topTenErrURLs = endpointSum.<Problem is here>
print 'Top Ten failed URLs: %s' % topTenErrURLs
What should I do working with takeOrdered is giving me error
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-60-6570f725d6ba> in <module>()
     10 endpointSum = endpointCountPairTuple.reduceByKey(lambda a,b : a + b)
     11 
---> 12 topTenErrURLs = endpointSum.take(10)
     13 print 'Top Ten failed URLs: %s' % topTenErrURLs

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4J


Thanks for the help! Getting "Saving disabled" error. The .txt file is fine.
 The question says that amazonRecToToken is a pair of record ID and tokens. But through 1c and using flatmap it is a list now as follows:

'clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund', 'ca', 'international', 'arcserve', 'lap', 'desktop', 'oem', '30pk', 'oem', 'arcserve', 'backup', 'v11', '1 .........................

Unless my approach on1c is wrong. this is what I am inputting into findBiggestRecord() which is not right, amazonSmall probably would have worked out better. Where am I confused? Thanks Hi,
I am a bit stuck in exercise 1C.
I've an output of the content of the RDD amazonRecToToken.
An example is as follows:
('b000jz4hqo', 'clickart 950 000 - premier image pack (dvd-rom)  "broderbund"'), ('b0006zf55o', 'ca international - arcserve lap/desktop oem 30pk "oem arcserve backup v11.1 win 30u for laptops and desktops" "computer associates"')
 Now I am a bit confuse what we should do. Should we for each tuple count the number of values ? So in my example I have 2 id values, and each text item being a token ?
Should we also count the stop words ? If I apply tokenize on the RDD I get the following error: 

'tuple' object has no attribute 'lower'
Am I missing something in this exercise ?

Thanks for any assistance The instructions say to do this: 

Create a new sims RDD from the similaritiesBroadcast RDD, where each element consists of a pair of the form ("AmazonID GoogleURL", cosineSimilarityScore). An example entry from sims is: ('b000bi7uqshttp://www.google.com/base/feeds/snippets/18403148885652932189', 0.40202896125621296)

The exisiting format of the record in similaritiesBroadcast is: 'googleURL','amazonID',cosine scoreThe expectations for the following steps are: 'amazonID GoogleURL',cosine scoreWhen I do a map like this: map(lambda record: (record[1] record[0],record[2]))I get invalid syntax.
I think I missed a step.
Any guidance on how to reformat the record for similaritiesBroadcast to sims?
 I have hit a road block on Lab 3 3c.

I'm getting this error when trying to apply my computeSimilarity() function to each record in crossSmall. I'm trying to do this by mapping a lambda function on crossSmall. Error:

Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
 
The computeSimilarity() function works by itself if I feed it just one record from crossSmall in the form of ((id,string),(id,string)). For that test, I retrieved a record using "record = crossSmall.collect()[1]" and then running computeSimilarity(record). It returns (id1,id2,cs) as it should.

I've spent about 2 hours investing what could be going on here, alternatives and trying to understand more about broadcast variables. At this point, I think I've just confused myself further. 

Can anyone point in an appropriate place to read more on this, or a subtle nudge in the right direction? To be clear, I'm not looking for the answer. Submission seems to get rejected at the first cell (Tokens 1a) without giving me any sort of feedback except that it failed. The code passes all tests in the local VM.

TokenID: 
803790-a11d733827a72ba00bc547f273587802:8b35c807bcbea555ba4ea7f69b122d0b:ip-172-31-41-172

Thanks
 My configuration like this -  Virtualbox  ( Vagrant ) -  Ubuntu 14.10 ( from tutorial setup )  -  Virtual CPU  2 core
-  Memory  - 2048MB.-  Error:  Spark Java schedule error   
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-4-8d8d6629d991> in <module>()
     32 
     33 
---> 34 parsed_logs, access_logs, failed_logs = parseLogs()

<ipython-input-4-8d8d6629d991> in parseLogs()
     22                    .filter(lambda s: s[1] == 0)
     23                    .map(lambda s: s[0]))
---> 24     failed_logs_count = failed_logs.count()
     25     if failed_logs_count > 0:
     26         print 'Number of invalid logline: %d' % failed_logs.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 6.0 failed 1 times, most recent failure: Lost task 3.0 in stage 6.0 (TID 24, localhost): java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:196)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.DataInputStream.readFully(DataInputStream.java:195)
	at java.io.DataInputStream.readFully(DataInputStream.java:169)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:111)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

 Not sure if I understood the requirement. Please help

<EDIT - please do no post your solution> I am doing lab 2, 3e and in trying to figure out whether DayToHost"Tuple" is a tuple, I tried .collect at the end and got Kernal Died.... will restart.  Would have expected a simple out of memory error. 


In "(1c) Tokenizing the small datasets" I got the result of 22566 tokens. I did the following
1) Map tokenize function for every values2) countTokens returns count of the tokens. 



 I have no prior Python experience and am facing a lot of difficulties because I don't know what is acceptable syntax and what I can do with Python.

I tried: return [x for x in simpleTokenize(string) if for y in stopwords  if x == y]

but this is invalid syntax. I experimented with filter() and reordering x and y in list comprehensions but to no avail. Any hints? The web page https://www.edx.org/xseries doesn't specify that the courses had to be taken Verified for the Big Data XSeries Certificate, though for other XSeries Certificates it is specified.

The web page says that taking the course Verified is "generally" required and that "requirements vary for each institution", but since the Big Data XSeries Certificate made no such statement I had thought that was not required for this.

Thank you for your review and clarification.

 Hi , I submitted my code , but it gave me error messagae that 

Your submission token id is something . Please include this submission token id when you need support for your code submission.

All test are passing on vm  
I can see it ran 1a but nothing more is displayed. What to do ?  Hallo, I have been working in this point por hours and nothing works, any clue about how to create the pair (recordId, tokenizedValue).

for the moment  I created the map for amazonRecToToken and googleRecToToken with the function tokenize directly without creating the pair.

Loreto Hello
Right now I'm  in lab 2 3e I did the following code

dayAndHostTuple = access_logs.map(lambda log:(log.date_time.day, log.host)).map(lambda k:(k[0],len(set(k[1])))).cache()

groupedByDay = dayAndHostTuple.groupByKey()

sortedByDay = groupedByDay.sortedByKey()
But it gives error in   groupedByDay.sortedByKey()
how can I sorted by key? Can anyone help me? 
Thanks I have my answer in the format

[(token1, weight1), (token2, weight2)]

[('aided', 400), ('precise', 100), ('duplex', 400), ('dance', 400), ('steve', 200), ('timecode', 400), ('verses', 400), ('battle', 100), ('compact', 80), ('content', 23)] 
but .count() in the final bit gives me:
TypeError: count() takes exactly one argument (0 given)
 
Edit: ah I shouldn't have collect() where I did.

Thanks :D Would professors please help us providing some hints or providing some solutions to 3 c)? we are stuck and spending so much time on this problem and cannot move forward. Appreciate your help.

In an university setting, you can get some help in coding from class mates. Now that we are here in the discussion group and as no one is allowed to share codes, we are simply discussing how to do it and stuck for a long time.

Appreciate help in Lab 2 3(c).

Questions are:
Will the output of:
Option 1:
print dayToHostPairTuple.take(3) = 
[(1, u'in24.inetnebr.com'), (1, u'uplherc.upl.com'), (1, u'uplherc.upl.com')]
Option 2:


[((1, u'in24.inetnebr.com'), 1), ((1, u'uplherc.upl.com'), 1), ((1, u'uplherc.upl.com'), 1)]

For both options, how to find dayGroupedHosts?
Some suggested to use set(). But, in problem 3(b), we used distinct(). Thus, I think, we should use distinct() here. Then, we do not know how do the distinct here.
Appreciate help here showing some code for dayGroupedHosts for both options 1 or options 2.



 Hi there

Got a stats question from lecture 8

What is the distribution governing the frequency in a term in a document and the website visits? The Poisson or the Zipf?

p16  says
Poisson: distribution of counts that occur at a certain “rate”
» Observed frequency of a given term in a corpus
» Number of visits to web site in a fixed time interval

p17 says
Zipf/Pareto/Yule distributions:» Govern frequencies of different terms in a document, or web site visits

Is there any difference in the situations described on pag 16 and p17?

Thanks
Giacomo I have already used 2 submissions on this error.
All of my tests passes and I have not added extra print lines.
Previous attempt shown similar error.
Data cleaning (1c)
------------------
----------------------------------------
Exception happened during processing of request from ('127.0.0.1', 51286)
Your submission token id is 807618-33c4180bcc9e50768b4010584951ea3a:541488f8c1382b8cf80574429a90e82b:ip-172-31-47-110
Please include this submission token id when you need support for your code submission. So we also need to fix the regular expression even though there is not <fill in> there I assume? In this lab, we first compute idfsFull, which consists of idf weights. But we are asked to recompute idfsFullWeights. What is the difference between these two?

Thanks
---
idfsFull = idfs(fullCorpusRDD)
# Recompute IDFs for full dataset#idfsFullWeights = <FILL IN>#idfsFullBroadcast = <FILL IN>

 All the tests pass locally but I get the following output from the Autograder and no points on the Progress page. How am I to interpret this ouput?

Data cleaning (1c)
------------------

[Stage 0:>                                                          (0 + 0) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:>                                                          (0 + 2) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:============================================>              (3 + 1) / 4]
                                                                                

[Stage 1:>                                                          (0 + 1) / 4]
[Stage 1:==============>                                            (1 + 1) / 4]
[Stage 1:=============================>                             (2 + 1) / 4]
[Stage 1:============================================>              (3 + 1) / 4]
                                                                                

[Stage 2:>                                                          (0 + 1) / 4]
[Stage 2:==============>                                            (1 + 1) / 4]
[Stage 2:=============================>                             (2 + 1) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
                                                                                

[Stage 3:>                                                          (0 + 1) / 4]
[Stage 3:==============>                                            (1 + 1) / 4]
[Stage 3:=============================>                             (2 + 1) / 4]
[Stage 3:============================================>              (3 + 1) / 4]
                                                                                

[Stage 4:>                                                          (0 + 1) / 4]
[Stage 4:==============>                                            (1 + 1) / 4]
[Stage 4:=============================>                             (2 + 1) / 4]
[Stage 4:============================================>              (3 + 1) / 4]
                                                                                

[Stage 5:>                                                          (0 + 1) / 4]
[Stage 5:==============>                                            (1 + 1) / 4]
[Stage 5:=============================>                             (2 + 1) / 4]
[Stage 5:============================================>              (3 + 1) / 4]
                                                                                

[Stage 6:>                                                          (0 + 1) / 4]
[Stage 6:==============>                                            (1 + 1) / 4]
[Stage 6:=============================>                             (2 + 1) / 4]
[Stage 6:============================================>              (3 + 1) / 4]
                                                                                

[Stage 7:>                                                          (0 + 1) / 4]
[Stage 7:==============>                                            (1 + 1) / 4]
[Stage 7:=============================>                             (2 + 1) / 4]
[Stage 7:============================================>              (3 + 1) / 4]
                                                                                

[Stage 8:>                                                          (0 + 1) / 4]
[Stage 8:==============>                                            (1 + 1) / 4]
[Stage 8:=============================>                             (2 + 1) / 4]
[Stage 8:============================================>              (3 + 1) / 4]
                                                                                

[Stage 9:=============================>                             (2 + 2) / 4]
                                                                                

[Stage 10:=============================>                            (2 + 1) / 4]
[Stage 10:=============================>                            (2 + 2) / 4]
                                                                                

[Stage 11:=============================>                            (2 + 1) / 4]
                                                                                

[Stage 12:>                                                         (0 + 1) / 4]
[Stage 12:>                                                         (0 + 2) / 4]
[Stage 12:==============>                                           (1 + 1) / 4]
[Stage 12:=============================>                            (2 + 1) / 4]
[Stage 12:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 18:>                                                         (0 + 1) / 4]
[Stage 18:==============>                                           (1 + 1) / 4]
[Stage 18:=============================>                            (2 + 1) / 4]
[Stage 18:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 21:>                                                         (0 + 1) / 4]
[Stage 21:==============>                                           (1 + 1) / 4]
[Stage 21:=============================>                            (2 + 1) / 4]
[Stage 21:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 23:>                                                         (0 + 1) / 4]
[Stage 23:==============>                                           (1 + 1) / 4]
[Stage 23:=============================>                            (2 + 1) / 4]
[Stage 23:===========================================>              (3 + 1) / 4]
[Stage 24:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 27:>                                                         (0 + 1) / 4]
[Stage 27:==============>                                           (1 + 1) / 4]
[Stage 27:=============================>                            (2 + 1) / 4]
[Stage 27:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 29:>                                                         (0 + 1) / 4]
[Stage 29:>                                                         (0 + 2) / 4]
[Stage 29:==============>                                           (1 + 1) / 4]
[Stage 29:=============================>                            (2 + 1) / 4]
[Stage 29:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 33:>                                                         (0 + 1) / 4]
[Stage 33:==============>                                           (1 + 1) / 4]
[Stage 33:==============>                                           (1 + 2) / 4]
[Stage 33:=============================>                            (2 + 1) / 4]
Your submission token id is 808140-92cd0f92aa7dcfb16a67932d874aed28:53a901d7ca42ab2dc40fa667438d462c:ip-172-31-34-104
Please include this submission token id when you need support for your code submission. A few suggestions for next version of this course
1. you should have mentioned Snappy compression which is very common in big data 
2. did not mention about splittability which can affect which compression algorithm to use
3. did not mention binary file layouts, like parquet/orc/seqfile etc
4. binary layout can affect compression ratio and throuputness, eg. seqfile with snappy will have in most cases much smaller compression ratio than the same data set in parquet because of columnar file layout - similar data groupped together, better compression.
5. other I think important point, e.g. if a compression algorithm like snappy is not splittable, but with some file layouts like parquet allows dataset to be "splittable" anyway, because parquet internally is split into multiple rowchunks etc.
Hope this helps. Hi guys,

I am having some troubles to process and RDD with structure (a, (b, c)), I would like to aggregate by c argument using a lambda function, how could I do that?

Thanks. I am unable to get the correct average similarity score for the non-duplicate entries.

In order to obtain the nonDupsRDD, I use the join operator that only selects the elements in the sims RDD that are not in the goldStandard (name of operator excluded to not give away too much information). Afterwards, I use the same operations to calculate the average score as used for the duplicates, except for that a count of the non-duplicates is used. In stead of the required average score of 0.00123476304656 I get 0.00219507005447.

The first two tests pass, whilst the third does not. Any hints as to what detail I'm missing here? It's unbelievable that lab2 3c took ten hours of work and finally ended up with such a simple solution. I almost gave up. But I don't. LOL Hi All ,
when i trying to submit my code in lab 2 i got this error i think it's not timeout error as the response after few minutes.
Thanks in advance


Data cleaning (1c)
------------------

[Stage 0:>                                                          (0 + 0) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:============================================>              (3 + 1) / 4]
                                                                                

[Stage 1:>                                                          (0 + 1) / 4]
[Stage 1:>                                                          (0 + 2) / 4]
[Stage 1:==============>                                            (1 + 1) / 4]
[Stage 1:=============================>                             (2 + 1) / 4]
[Stage 1:============================================>              (3 + 1) / 4]
                                                                                

[Stage 2:>                                                          (0 + 1) / 4]
[Stage 2:==============>                                            (1 + 1) / 4]
[Stage 2:=============================>                             (2 + 1) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
[Stage 2:===========================================================(4 + 0) / 4]
                                                                                

[Stage 3:>                                                          (0 + 1) / 4]
[Stage 3:==============>                                            (1 + 1) / 4]
[Stage 3:=============================>                             (2 + 1) / 4]
[Stage 3:============================================>              (3 + 1) / 4]
                                                                                

[Stage 4:>                                                          (0 + 1) / 4]
[Stage 4:==============>                                            (1 + 1) / 4]
[Stage 4:=============================>                             (2 + 1) / 4]
[Stage 4:============================================>              (3 + 1) / 4]
                                                                                

[Stage 5:>                                                          (0 + 1) / 4]
[Stage 5:==============>                                            (1 + 1) / 4]
[Stage 5:=============================>                             (2 + 1) / 4]
[Stage 5:============================================>              (3 + 1) / 4]
                                                                                

[Stage 6:>                                                          (0 + 1) / 4]
[Stage 6:>                                                          (0 + 2) / 4]
[Stage 6:==============>                                            (1 + 1) / 4]
[Stage 6:=============================>                             (2 + 1) / 4]
[Stage 6:============================================>              (3 + 1) / 4]
                                                                                

[Stage 7:>                                                          (0 + 1) / 4]
[Stage 7:==============>                                            (1 + 1) / 4]
[Stage 7:=============================>                             (2 + 1) / 4]
[Stage 7:============================================>              (3 + 1) / 4]
                                                                                

[Stage 8:>                                                          (0 + 1) / 4]
[Stage 8:==============>                                            (1 + 1) / 4]
[Stage 8:=============================>                             (2 + 1) / 4]
[Stage 8:============================================>              (3 + 1) / 4]
                                                                                

[Stage 9:=============================>                             (2 + 1) / 4]
[Stage 9:============================================>              (3 + 1) / 4]
                                                                                

[Stage 10:==============>                                           (1 + 1) / 4]
[Stage 10:=============================>                            (2 + 1) / 4]
[Stage 10:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 11:==============>                                           (1 + 1) / 4]
[Stage 11:=============================>                            (2 + 1) / 4]
[Stage 11:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 12:>                                                         (0 + 1) / 4]
[Stage 12:>                                                         (0 + 2) / 4]
[Stage 12:==============>                                           (1 + 1) / 4]
[Stage 12:=============================>                            (2 + 1) / 4]
[Stage 12:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 18:>                                                         (0 + 1) / 4]
[Stage 18:>                                                         (0 + 2) / 4]
[Stage 18:==============>                                           (1 + 1) / 4]
[Stage 18:=============================>                            (2 + 1) / 4]
[Stage 18:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 21:>                                                         (0 + 1) / 4]
[Stage 21:==============>                                           (1 + 1) / 4]
[Stage 21:=============================>                            (2 + 1) / 4]
[Stage 21:===========================================>              (3 + 1) / 4]
[Stage 21:==========================================================(4 + 0) / 4]
                                                                                

[Stage 23:>                                                         (0 + 1) / 4]
Your submission token id is 808372-db8b8b069dc03aa3ec490f0c97002d27:fcac3d9f21ecb4a9fbb21001ecfae498:ip-172-31-41-171
Please include this submission token id when you need support for your code submission.
 Dear TA,

I submitted my lab3 python file one hour ago, but the grading job seems still running and there is still no feedback yet.

Can you please tell me the normal time of running lab 3? Should I wait for more time or something weird has already happened.

Thanks! 

Best regards,
Zhe Sun I have tried running lab2_apache_log_student ipython notebook on my computer for a dozen times and 4 submissions on edx. But the autograder keeps giving a signal that I am wrong only at 3c

Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect dailyHosts.is_cached
Tell me if I am wrong.

My dailyHosts:
dailyHosts = (dayHostCount<FILL IN> POSTING SOLUTIONS IS AN HONOR CODE VIOLATION
print dailyHosts.is_cached # this is removed, but it returns True

My 3rd & 4th submission token ids:
Your submission token id is 807068-055746822a40f6f08db5ef9e7e751536:602d9c6ba0cac047aed16e876f5bf75e:ip-172-31-39-99

Your submission token id is 804080-4f1f94ff7328f664fb2ce1cb099e603d:602d9c6ba0cac047aed16e876f5bf75e:ip-172-31-39-98 I've tried submitting my solution twice and get the following output each time. Does this just mean that the grader timed out? When I run locally, everything runs in around ten minutes. I checked for prints, collects, etc. Any help interpreting the grader's output would be appreciated.

Tokenize a String (1a)
----------------------

[Stage 60:======>                                                  (2 + 1) / 17]
[Stage 60:==========>                                              (3 + 1) / 17]
[Stage 60:=============>                                           (4 + 1) / 17]
[Stage 60:================>                                        (5 + 1) / 17]
[Stage 60:====================>                                    (6 + 1) / 17]
[Stage 60:=======================>                                 (7 + 1) / 17]
[Stage 60:==========================>                              (8 + 1) / 17]
[Stage 60:==============================>                          (9 + 1) / 17]
[Stage 60:================================>                       (10 + 1) / 17]
[Stage 60:====================================>                   (11 + 1) / 17]
[Stage 60:=======================================>                (12 + 1) / 17]
[Stage 60:==========================================>             (13 + 1) / 17]
[Stage 60:==============================================>         (14 + 1) / 17]
[Stage 60:=================================================>      (15 + 1) / 17]
[Stage 60:====================================================>   (16 + 1) / 17]
[Stage 61:==========================>                              (8 + 1) / 17]
[Stage 61:====================================>                   (11 + 1) / 17]
[Stage 61:==============================================>         (14 + 1) / 17]
                                                                                

[Stage 63:=============>                                           (4 + 1) / 17]
[Stage 63:================>                                        (5 + 2) / 17]
[Stage 63:=======================>                                 (7 + 1) / 17]
[Stage 63:================================>                       (10 + 1) / 17]
[Stage 63:=======================================>                (12 + 1) / 17]
[Stage 63:==============================================>         (14 + 1) / 17]
[Stage 63:=================================================>      (15 + 0) / 17]
[Stage 63:====================================================>   (16 + 1) / 17]
                                                                                

[Stage 64:======>                                                  (2 + 1) / 17]
[Stage 64:==========>                                              (3 + 1) / 17]
[Stage 64:=============>                                           (4 + 1) / 17]
[Stage 64:================>                                        (5 + 1) / 17]
[Stage 64:====================>                                    (6 + 1) / 17]
[Stage 64:=======================>                                 (7 + 1) / 17]
[Stage 64:==========================>                              (8 + 1) / 17]
[Stage 64:==============================>                          (9 + 1) / 17]
[Stage 64:================================>                       (10 + 1) / 17]
[Stage 64:====================================>                   (11 + 1) / 17]
[Stage 64:=======================================>                (12 + 1) / 17]
[Stage 64:==========================================>             (13 + 1) / 17]
[Stage 64:==============================================>         (14 + 1) / 17]
[Stage 64:=================================================>      (15 + 1) / 17]
[Stage 64:====================================================>   (16 + 1) / 17]
[Stage 65:==========>                                              (3 + 1) / 17]
[Stage 65:=============>                                           (4 + 1) / 17]
[Stage 65:====================>                                    (6 + 1) / 17]
[Stage 65:=======================>                                 (7 + 1) / 17]
[Stage 65:==========================>                              (8 + 1) / 17]
[Stage 65:==============================>                          (9 + 1) / 17]
[Stage 65:================================>                       (10 + 1) / 17]
[Stage 65:====================================>                   (11 + 1) / 17]
[Stage 65:=======================================>                (12 + 1) / 17]
[Stage 65:==============================================>         (14 + 1) / 17]
[Stage 65:=================================================>      (15 + 1) / 17]
                                                                                

[Stage 67:=============>                                           (4 + 1) / 17]
[Stage 67:====================>                                    (6 + 1) / 17]
[Stage 67:==========================>                              (8 + 1) / 17]
[Stage 67:==============================>                          (9 + 1) / 17]
[Stage 67:================================>                       (10 + 1) / 17]
[Stage 67:=======================================>                (12 + 1) / 17]
[Stage 67:==============================================>         (14 + 1) / 17]
[Stage 67:=================================================>      (15 + 1) / 17]
[Stage 67:====================================================>   (16 + 1) / 17]
                                                                                

[Stage 68:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 70:==============>                                           (1 + 1) / 4]
[Stage 70:=============================>                            (2 + 1) / 4]
[Stage 70:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 71:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 72:=======>                                                  (1 + 1) / 8]
[Stage 72:==============>                                           (2 + 1) / 8]
[Stage 72:=====================>                                    (3 + 1) / 8]
[Stage 72:=============================>                            (4 + 1) / 8]
[Stage 72:====================================>                     (5 + 1) / 8]
[Stage 72:===========================================>              (6 + 1) / 8]
[Stage 72:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 73:=======>                                                  (1 + 1) / 8]
[Stage 73:==============>                                           (2 + 1) / 8]
[Stage 73:=====================>                                    (3 + 1) / 8]
[Stage 73:=============================>                            (4 + 1) / 8]
[Stage 73:====================================>                     (5 + 1) / 8]
[Stage 73:===========================================>              (6 + 1) / 8]
[Stage 73:==================================================>       (7 + 1) / 8]
[Stage 74:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 76:===========================================>              (6 + 1) / 8]
                                                                                

[Stage 77:==============>                                           (1 + 1) / 4]
[Stage 77:=============================>                            (2 + 1) / 4]
[Stage 77:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 78:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 79:==============>                                           (1 + 1) / 4]
[Stage 79:=============================>                            (2 + 1) / 4]
[Stage 79:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 80:=============================>                            (2 + 1) / 4]
[Stage 80:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 81:==============>                                           (1 + 1) / 4]
[Stage 81:=============================>                            (2 + 1) / 4]
[Stage 81:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 82:=============================>                            (2 + 1) / 4]
[Stage 82:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 83:==============>                                           (1 + 1) / 4]
[Stage 83:=============================>                            (2 + 1) / 4]
[Stage 83:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 84:==============>                                           (1 + 1) / 4]
[Stage 84:=============================>                            (2 + 1) / 4]
[Stage 84:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 87:>                                                         (0 + 1) / 8]
[Stage 87:=======>                                                  (1 + 1) / 8]
[Stage 87:==============>                                           (2 + 1) / 8]
[Stage 87:=====================>                                    (3 + 1) / 8]
[Stage 87:=============================>                            (4 + 1) / 8]
[Stage 87:====================================>                     (5 + 1) / 8]
[Stage 87:===========================================>              (6 + 1) / 8]
[Stage 87:==================================================>       (7 + 1) / 8]
[Stage 88:>                                                         (0 + 1) / 8]
[Stage 88:=======>                                                  (1 + 1) / 8]
[Stage 88:==============>                                           (2 + 1) / 8]
[Stage 88:=====================>                                    (3 + 1) / 8]
[Stage 88:=============================>                            (4 + 1) / 8]
[Stage 88:====================================>                     (5 + 1) / 8]
[Stage 88:===========================================>              (6 + 1) / 8]
[Stage 88:==================================================>       (7 + 1) / 8]
[Stage 89:>                                                         (0 + 1) / 8]
[Stage 89:=======>                                                  (1 + 1) / 8]
[Stage 89:==============>                                           (2 + 1) / 8]
[Stage 89:=====================>                                    (3 + 1) / 8]
Your submission token id is 809242-6cc71465f564e8e81d8ddfc230fcc48e:3d51d320db39b58551a91d9bb1642f3f:ip-172-31-41-172
Please include this submission token id when you need support for your code submission. My submission is seen like this. Is it error , i couldn't understand.

Data cleaning (1c)
------------------

[Stage 0:>                                                          (0 + 0) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:============================================>              (3 + 1) / 4]
                                                                                

[Stage 1:>                                                          (0 + 1) / 4]
[Stage 1:==============>                                            (1 + 1) / 4]
[Stage 1:=============================>                             (2 + 1) / 4]
[Stage 1:============================================>              (3 + 1) / 4]
                                                                                

[Stage 2:>                                                          (0 + 1) / 4]
[Stage 2:==============>                                            (1 + 1) / 4]
[Stage 2:=============================>                             (2 + 1) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
                                                                                

[Stage 3:>                                                          (0 + 1) / 4]
[Stage 3:==============>                                            (1 + 1) / 4]
[Stage 3:=============================>                             (2 + 1) / 4]
[Stage 3:============================================>              (3 + 1) / 4]
                                                                                

[Stage 4:>                                                          (0 + 1) / 4]
[Stage 4:==============>                                            (1 + 1) / 4]
[Stage 4:=============================>                             (2 + 1) / 4]
[Stage 4:============================================>              (3 + 1) / 4]
                                                                                

[Stage 5:>                                                          (0 + 1) / 4]
[Stage 5:==============>                                            (1 + 1) / 4]
[Stage 5:=============================>                             (2 + 1) / 4]
[Stage 5:============================================>              (3 + 1) / 4]
[Stage 5:===========================================================(4 + 0) / 4]
                                                                                

[Stage 6:>                                                          (0 + 1) / 4]
[Stage 6:==============>                                            (1 + 1) / 4]
[Stage 6:=============================>                             (2 + 1) / 4]
[Stage 6:============================================>              (3 + 1) / 4]
                                                                                

[Stage 7:>                                                          (0 + 1) / 4]
[Stage 7:==============>                                            (1 + 1) / 4]
[Stage 7:=============================>                             (2 + 1) / 4]
[Stage 7:============================================>              (3 + 1) / 4]
                                                                                

[Stage 8:>                                                          (0 + 1) / 4]
[Stage 8:==============>                                            (1 + 1) / 4]
[Stage 8:=============================>                             (2 + 1) / 4]
[Stage 8:=============================>                             (2 + 2) / 4]
[Stage 8:============================================>              (3 + 1) / 4]
                                                                                

[Stage 9:=============================>                             (2 + 1) / 4]
[Stage 9:===========================================================(4 + 0) / 4]
                                                                                

[Stage 10:=============================>                            (2 + 1) / 4]
[Stage 10:=============================>                            (2 + 2) / 4]
                                                                                

[Stage 11:=============================>                            (2 + 1) / 4]
[Stage 11:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 12:>                                                         (0 + 1) / 4]
[Stage 12:==============>                                           (1 + 1) / 4]
[Stage 12:=============================>                            (2 + 1) / 4]
[Stage 12:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 18:>                                                         (0 + 1) / 4]
[Stage 18:==============>                                           (1 + 1) / 4]
[Stage 18:=============================>                            (2 + 1) / 4]
[Stage 18:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 21:>                                                         (0 + 1) / 4]
[Stage 21:==============>                                           (1 + 1) / 4]
[Stage 21:=============================>                            (2 + 1) / 4]
Your submission token id is 809660-708924be6559093ca6a6afff8697ae16:7fec67e0e74f34dbfb0dbc4cd900625a:ip-172-31-34-104
Please include this submission token id when you need support for your code submission. I was able to have all tests passed in the notebooks.
When running the AutoGrader, I got the following:

Data cleaning (1c)
------------------

[Stage 0:>                                                          (0 + 0) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:============================================>              (3 + 1) / 4]
                                                                                

[Stage 1:>                                                          (0 + 1) / 4]
[Stage 1:==============>                                            (1 + 1) / 4]
[Stage 1:=============================>                             (2 + 1) / 4]
[Stage 1:============================================>              (3 + 1) / 4]
                                                                                

[Stage 2:>                                                          (0 + 1) / 4]
[Stage 2:==============>                                            (1 + 1) / 4]
[Stage 2:=============================>                             (2 + 1) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
                                                                                

[Stage 3:>                                                          (0 + 1) / 4]
[Stage 3:==============>                                            (1 + 1) / 4]
[Stage 3:=============================>                             (2 + 1) / 4]
[Stage 3:============================================>              (3 + 1) / 4]
                                                                                

[Stage 4:>                                                          (0 + 1) / 4]
[Stage 4:==============>                                            (1 + 1) / 4]
[Stage 4:=============================>                             (2 + 1) / 4]
[Stage 4:============================================>              (3 + 1) / 4]
                                                                                

[Stage 5:>                                                          (0 + 1) / 4]
[Stage 5:==============>                                            (1 + 1) / 4]
[Stage 5:=============================>                             (2 + 1) / 4]
[Stage 5:============================================>              (3 + 1) / 4]
                                                                                

[Stage 6:>                                                          (0 + 1) / 4]
[Stage 6:==============>                                            (1 + 1) / 4]
[Stage 6:=============================>                             (2 + 1) / 4]
[Stage 6:============================================>              (3 + 1) / 4]
                                                                                

[Stage 7:>                                                          (0 + 1) / 4]
[Stage 7:>                                                          (0 + 2) / 4]
[Stage 7:==============>                                            (1 + 1) / 4]
[Stage 7:=============================>                             (2 + 1) / 4]
[Stage 7:=============================>                             (2 + 2) / 4]
[Stage 7:============================================>              (3 + 1) / 4]
                                                                                

[Stage 8:>                                                          (0 + 1) / 4]
[Stage 8:==============>                                            (1 + 1) / 4]
[Stage 8:=============================>                             (2 + 1) / 4]
[Stage 8:============================================>              (3 + 1) / 4]
                                                                                

[Stage 9:=============================>                             (2 + 1) / 4]
                                                                                

[Stage 10:==============>                                           (1 + 1) / 4]
[Stage 10:=============================>                            (2 + 1) / 4]
[Stage 10:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 11:==============>                                           (1 + 1) / 4]
[Stage 11:=============================>                            (2 + 1) / 4]
                                                                                

[Stage 12:>                                                         (0 + 0) / 4]
[Stage 12:>                                                         (0 + 1) / 4]
[Stage 12:==============>                                           (1 + 1) / 4]
[Stage 12:=============================>                            (2 + 1) / 4]
[Stage 12:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 18:>                                                         (0 + 1) / 4]
[Stage 18:==============>                                           (1 + 1) / 4]
[Stage 18:=============================>                            (2 + 1) / 4]
[Stage 18:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 21:>                                                         (0 + 1) / 4]
[Stage 21:==============>                                           (1 + 1) / 4]
[Stage 21:==============>                                           (1 + 2) / 4]
[Stage 21:=============================>                            (2 + 1) / 4]
Your submission token id is 810152-552a463901f665b8622029d26a9f6b05:a027f58a46c9151215302df91067a009:ip-172-31-47-110
Please include this submission token id when you need support for your code submission. I have been attempting to solve the regular expression (first one) for a while, I always get all of the lines parsed. and no problems until 2f with the assertion. One of the listings /ksc.html, I have 5 more matches of the group. I have even created a smaller file by grepping for that (/ksc.html) and redirecting to the new file. I tested locally using a tool that I own RegExBuddy and the group is correct. To allow for any possible problem, I relaxed my capture groups to be lazy instead of greedy. My having 5 more matches for the group suggested that mine were too greedy. 

I have installed spark locally (win 7 64 bit), but can not integrate with my IDE (Intellij) to write scritps that have access to sc and can run. I am out of ideas at this time. My latest regex is

^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+?) (\S*?:*\/\/*[^"\s]*?)\s*?"*(\s*?[^"\s]*?)\s*" (\S+) (\S+)

Looking for a suggestionss Hi, I have submitted my assignment ID:

810933-0fcb43802b30d548e6ff283645ad8236:f69ffd086bb88cb1090e5381dfab7eb5:ip-172-31-34-104
Problem is, can't understand the feedback I'm getting, which is something like:

Tokenize a String (1a)
----------------------

[Stage 60:======>                                                  (2 + 1) / 17](...)[Stage 92:=============================>                            (4 + 1) / 8]

Is there any chance someone has a clue on what's going on? Did it timeout? If so, how can I improve it? It is implemented trivially via a normal Python function.

Thanks in advance. I was wondering - does the order of the words is important in working with the bags of words?
The thing is that I have assumed that the easiest way to filter out the stopwords would be to convert the tokenized list to set and take the difference of the sets. The autograder however expects a list. I can convert it back to list, but it seems that the resulting order will be wrong. I suppose I will need to go with some less efficient option to satisfy the autograder, but it made me thinking - do we really need to have the ordered list if we are working with bags of words in real live situations? Hey everyone, in case this is useful, this my go-to regex tester! It's very visual and easy to use!

https://www.regex101.com/#python With just a few hours left for grace period to end this is a strange "bug" but it seems some of my classmates here have also faced it ... please suggest how to fix ...
Data cleaning (1c)
------------------

[Stage 0:>                                                          (0 + 0) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:============================================>              (3 + 1) / 4]
                                                                                

[Stage 1:>                                                          (0 + 1) / 4]
[Stage 1:==============>                                            (1 + 1) / 4]
[Stage 1:=============================>                             (2 + 1) / 4]
[Stage 1:============================================>              (3 + 1) / 4]
                                                                                

[Stage 2:>                                                          (0 + 1) / 4]
[Stage 2:>                                                          (0 + 2) / 4]
[Stage 2:==============>                                            (1 + 1) / 4]
[Stage 2:=============================>                             (2 + 1) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
                                                                                

[Stage 3:>                                                          (0 + 1) / 4]
[Stage 3:==============>                                            (1 + 1) / 4]
[Stage 3:=============================>                             (2 + 1) / 4]
[Stage 3:============================================>              (3 + 1) / 4]
                                                                                

[Stage 4:>                                                          (0 + 1) / 4]
[Stage 4:==============>                                            (1 + 1) / 4]
[Stage 4:=============================>                             (2 + 1) / 4]
[Stage 4:============================================>              (3 + 1) / 4]
                                                                                

[Stage 5:>                                                          (0 + 1) / 4]
[Stage 5:==============>                                            (1 + 1) / 4]
[Stage 5:=============================>                             (2 + 1) / 4]
[Stage 5:============================================>              (3 + 1) / 4]
                                                                                

[Stage 6:>                                                          (0 + 1) / 4]
[Stage 6:==============>                                            (1 + 1) / 4]
[Stage 6:=============================>                             (2 + 1) / 4]
[Stage 6:============================================>              (3 + 1) / 4]
                                                                                

[Stage 7:>                                                          (0 + 1) / 4]
[Stage 7:==============>                                            (1 + 1) / 4]
[Stage 7:=============================>                             (2 + 1) / 4]
[Stage 7:============================================>              (3 + 1) / 4]
                                                                                

[Stage 8:>                                                          (0 + 1) / 4]
[Stage 8:==============>                                            (1 + 1) / 4]
[Stage 8:=============================>                             (2 + 1) / 4]
[Stage 8:============================================>              (3 + 1) / 4]
                                                                                

[Stage 9:=============================>                             (2 + 1) / 4]
[Stage 9:============================================>              (3 + 1) / 4]
                                                                                

[Stage 10:==============>                                           (1 + 2) / 4]
[Stage 10:=============================>                            (2 + 1) / 4]
[Stage 10:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 11:==============>                                           (1 + 1) / 4]
[Stage 11:=============================>                            (2 + 1) / 4]
[Stage 11:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 12:>                                                         (0 + 1) / 4]
[Stage 12:>                                                         (0 + 2) / 4]
[Stage 12:==============>                                           (1 + 1) / 4]
[Stage 12:=============================>                            (2 + 1) / 4]
[Stage 12:=============================>                            (2 + 2) / 4]
[Stage 12:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 18:>                                                         (0 + 1) / 4]
[Stage 18:==============>                                           (1 + 1) / 4]
Your submission token id is 810646-db564d0014b0d902271b2827ee381131:95b48e66c062b85817ebb00f7a435495:ip-172-31-47-52
Please include this submission token id when you need support for your code submission.

DID NOT GET ANYTHING ON PROGRESS Hi, when I submit lab3 I get the following output, my notebook is fine,  not sure what is the problem. Thanks.
Tokenize a String (1a)
----------------------

[Stage 60:=============>                                           (4 + 1) / 17]
[Stage 60:====================>                                    (6 + 1) / 17]
[Stage 60:=======================>                                 (7 + 1) / 17]
[Stage 60:==============================>                          (9 + 1) / 17]
[Stage 60:====================================>                   (11 + 1) / 17]
[Stage 60:==========================================>             (13 + 1) / 17]
[Stage 60:=================================================>      (15 + 1) / 17]
[Stage 61:====================================>                   (11 + 1) / 17]
[Stage 61:==============================================>         (14 + 1) / 17]
                                                                                

[Stage 63:====================================>                   (11 + 1) / 17]
[Stage 63:=================================================>      (15 + 1) / 17]
                                                                                

[Stage 64:================>                                        (5 + 1) / 17]
[Stage 64:=======================>                                 (7 + 1) / 17]
[Stage 64:==============================>                          (9 + 1) / 17]
[Stage 64:====================================>                   (11 + 1) / 17]
[Stage 64:=======================================>                (12 + 1) / 17]
[Stage 64:==========================================>             (13 + 1) / 17]
[Stage 64:=================================================>      (15 + 1) / 17]
[Stage 65:>                                                        (0 + 0) / 17]
[Stage 65:==========================>                              (8 + 1) / 17]
[Stage 65:====================================>                   (11 + 1) / 17]
[Stage 65:==============================================>         (14 + 1) / 17]
[Stage 65:========================================================(17 + 0) / 17]
                                                                                

[Stage 67:==========================>                              (8 + 1) / 17]
[Stage 67:====================================>                   (11 + 2) / 17]
[Stage 67:=================================================>      (15 + 1) / 17]
                                                                                

[Stage 72:=====================>                                    (3 + 1) / 8]
[Stage 72:=============================>                            (4 + 1) / 8]
[Stage 72:===========================================>              (6 + 1) / 8]
[Stage 72:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 73:==============>                                           (2 + 1) / 8]
[Stage 73:=====================>                                    (3 + 1) / 8]
[Stage 73:=============================>                            (4 + 1) / 8]
[Stage 73:====================================>                     (5 + 1) / 8]
[Stage 73:===========================================>              (6 + 1) / 8]
[Stage 73:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 77:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 79:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 81:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 83:=============================>                            (2 + 1) / 4]
[Stage 83:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 84:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 87:==============>                                           (2 + 1) / 8]
[Stage 87:=====================>                                    (3 + 1) / 8]
[Stage 87:=============================>                            (4 + 1) / 8]
[Stage 87:====================================>                     (5 + 1) / 8]
[Stage 87:===========================================>              (6 + 1) / 8]
[Stage 87:==================================================>       (7 + 1) / 8]
[Stage 88:>                                                         (0 + 1) / 8]
[Stage 88:=======>                                                  (1 + 1) / 8]
[Stage 88:==============>                                           (2 + 1) / 8]
[Stage 88:=====================>                                    (3 + 1) / 8]
[Stage 88:=============================>                            (4 + 1) / 8]
[Stage 88:====================================>                     (5 + 1) / 8]
[Stage 88:===========================================>              (6 + 1) / 8]
[Stage 88:==================================================>       (7 + 1) / 8]
[Stage 89:>                                                         (0 + 1) / 8]
[Stage 89:=======>                                                  (1 + 1) / 8]
[Stage 89:==============>                                           (2 + 1) / 8]
[Stage 89:=====================>                                    (3 + 1) / 8]
[Stage 89:=============================>                            (4 + 1) / 8]
[Stage 89:====================================>                     (5 + 1) / 8]
[Stage 89:===========================================>              (6 + 1) / 8]
[Stage 89:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 92:>                                                         (0 + 1) / 8]
[Stage 92:=======>                                                  (1 + 1) / 8]
[Stage 92:==============>                                           (2 + 1) / 8]
[Stage 92:=====================>                                    (3 + 1) / 8]
[Stage 92:=============================>                            (4 + 1) / 8]
[Stage 92:====================================>                     (5 + 1) / 8]
[Stage 92:===========================================>              (6 + 1) / 8]
[Stage 92:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 93:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 97:>                                                         (0 + 1) / 8]
[Stage 97:=======>                                                  (1 + 1) / 8]
[Stage 97:==============>                                           (2 + 1) / 8]
[Stage 97:=====================>                                    (3 + 1) / 8]
[Stage 97:=============================>                            (4 + 1) / 8]
[Stage 97:====================================>                     (5 + 1) / 8]
[Stage 97:===========================================>              (6 + 1) / 8]
[Stage 97:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 100:>                                                        (0 + 1) / 8]
[Stage 100:=======>                                                 (1 + 1) / 8]
Your submission token id is 811395-13ef619bf3685440aec66679359a3762:c1ff3d396e42897e4ee6290330a65120:ip-172-31-41-171
Please include this submission token id when you need support for your code submission.
 when I convert the files to python I know where they are saved on the file systems

I also want to make sure I back up the work I did in my notebooks. I know where I uploaded them from how ever when I save the file time stamps do not change? Any idea where the saved version are being saved?

thanks

Andy Hi,

I submitted my solution to Lab 2 about an hour ago and it's still grading. Is there a way to check the status of the grader? I'm afraid if something's wrong.

Thanks.
 Does anyone know how one can set up a cluster for apache spark ?
How the vm in the tutorial is created ? I have imported the python file for the lab 2 after verifying that all the test case has passed but when i upload the file in Auto grader, it doesnt give me the feedback and its processing for ages. Neither does it gives any timeout... its been more than 5 hours now that after i uploaded the python file to auto grader but no results yet.

Can someone please help here ? Since i am left with few hours for the grace period to get over, would appreciate an asap reply .. I've tried to submit lab 2 for 2 times and it just spinning and nothing comes up. Been spinning now for more than 1 hour. Any advice would be greatly appreciated. 
 

As in the explanation, the answer should be the last option. Hi,
Im submitting mi tasks and im getting an error in the autograder:

Traceback (most recent call last):  File "", line 2, in   File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals    cls.assertTrue(var == val, msg)  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue    raise TestFailure(msg)TestFailure: incorrect value for wordCounts

This is the output of this exercise, it says: 1 test passed 

# TODO: Replace <FILL IN> with appropriate code
wordCountsCollected = (wordsRDD
                       .map(lambda k: (k,1))
                       .reduceByKey(<CODE>)
                       .collect())
print wordCountsCollected[('rat', 2), ('elephant', 1), ('cat', 2)]

What can be wrong  ?

I'm getting a loop when i submit the task, I do not know that is wrong, and the attempts are running out 

Thanks in advance  I had an overall progress of around 46-49 % in lab 2 exercise and now when I tried to upload the remaining solution today, I got timeout error and my score become 36 % !

what to do now? I have 2 more submissions remaining in lab 2 Have submitted 4 times now, below are the messages, can you please help:

Your submission token id is 814139-d8c2bea68407756d8a2e9c95c1c7f14d:1c9553a937a19e7aefaa68cbf3abe16d:ip-172-31-47-15
Please include this submission token id when you need support for your code submission.

Exception happened during processing of request from ('127.0.0.1', 40268)
Your submission token id is 813050-90c30719b6f8432bf4621cca96f0b13e:1c9553a937a19e7aefaa68cbf3abe16d:ip-172-31-47-110
Please include this submission token id when you need support for your code submission.

Your submission token id is 810508-cdbc6cae4f4dac377eacafa581ac7fb1:1c9553a937a19e7aefaa68cbf3abe16d:ip-172-31-41-172
Please include this submission token id when you need support for your code submission.



Your submission token id is 811366-931d9b58a24245a3216b1beb9ff926d3:1c9553a937a19e7aefaa68cbf3abe16d:ip-172-31-39-97
Please include this submission token id when you need support for your code submission.
 Anyone a clue that this means? Is doesn't say right/wrong at the end.

Data cleaning (1c)
------------------

[Stage 0:>                                                          (0 + 0) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:==============>                                            (1 + 2) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:============================================>              (3 + 1) / 4]
                                                                                

[Stage 1:>                                                          (0 + 1) / 4]
[Stage 1:==============>                                            (1 + 1) / 4]
[Stage 1:=============================>                             (2 + 1) / 4]
[Stage 1:============================================>              (3 + 1) / 4]
                                                                                

[Stage 2:>                                                          (0 + 1) / 4]
[Stage 2:==============>                                            (1 + 1) / 4]
[Stage 2:=============================>                             (2 + 1) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
                                                                                

[Stage 3:>                                                          (0 + 1) / 4]
[Stage 3:==============>                                            (1 + 1) / 4]
[Stage 3:=============================>                             (2 + 1) / 4]
[Stage 3:============================================>              (3 + 1) / 4]
                                                                                

[Stage 4:>                                                          (0 + 1) / 4]
[Stage 4:==============>                                            (1 + 1) / 4]
[Stage 4:=============================>                             (2 + 1) / 4]
[Stage 4:============================================>              (3 + 1) / 4]
                                                                                

[Stage 5:>                                                          (0 + 1) / 4]
[Stage 5:==============>                                            (1 + 1) / 4]
[Stage 5:=============================>                             (2 + 1) / 4]
[Stage 5:============================================>              (3 + 1) / 4]
                                                                                

[Stage 6:>                                                          (0 + 1) / 4]
[Stage 6:==============>                                            (1 + 1) / 4]
[Stage 6:=============================>                             (2 + 1) / 4]
[Stage 6:============================================>              (3 + 1) / 4]
                                                                                

[Stage 7:>                                                          (0 + 1) / 4]
[Stage 7:==============>                                            (1 + 1) / 4]
[Stage 7:=============================>                             (2 + 1) / 4]
[Stage 7:============================================>              (3 + 1) / 4]
                                                                                

[Stage 8:>                                                          (0 + 1) / 4]
[Stage 8:==============>                                            (1 + 1) / 4]
[Stage 8:=============================>                             (2 + 1) / 4]
[Stage 8:============================================>              (3 + 1) / 4]
                                                                                

[Stage 9:=============================>                             (2 + 1) / 4]
                                                                                

[Stage 10:=============================>                            (2 + 1) / 4]
[Stage 10:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 11:=============================>                            (2 + 1) / 4]
[Stage 11:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 12:>                                                         (0 + 1) / 4]
[Stage 12:==============>                                           (1 + 1) / 4]
[Stage 12:=============================>                            (2 + 1) / 4]
[Stage 12:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 18:>                                                         (0 + 1) / 4]
[Stage 18:==============>                                           (1 + 1) / 4]
[Stage 18:=============================>                            (2 + 1) / 4]
[Stage 18:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 21:>                                                         (0 + 1) / 4]
[Stage 21:==============>                                           (1 + 1) / 4]
[Stage 21:=============================>                            (2 + 1) / 4]
[Stage 21:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 23:>                                                         (0 + 1) / 4]
[Stage 23:==============>                                           (1 + 1) / 4]
[Stage 23:=============================>                            (2 + 1) / 4]
[Stage 23:===========================================>              (3 + 1) / 4]
[Stage 24:=============================>                            (2 + 2) / 4]
                                                                                

[Stage 27:>                                                         (0 + 1) / 4]
[Stage 27:==============>                                           (1 + 1) / 4]
[Stage 27:=============================>                            (2 + 1) / 4]
Your submission token id is 808241-bb2ec59ea6286f030c7a36e50742f285:206c2781c86d2ad8abbe8e0a2acc7dca:ip-172-31-47-110
Please include this submission token id when you need support for your code submission
 I've downloaded the notebook and renamed it lab-2 and copied over all my answers.
I've double and triple checked that my lab has no print statements that I've added.
All tests pass in my notebook locally.

I've tried submitting 4 times and am having no luck.  I'm trying to make the deadline for the grace period so I don't lose any points, but not sure what I can do at this point... any help?

Data cleaning (1c)
------------------

[Stage 0:>                                                          (0 + 0) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:>                                                          (0 + 2) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:============================================>              (3 + 1) / 4]
                                                                                

[Stage 1:>                                                          (0 + 1) / 4]
[Stage 1:==============>                                            (1 + 1) / 4]
[Stage 1:=============================>                             (2 + 1) / 4]
[Stage 1:============================================>              (3 + 1) / 4]
                                                                                

[Stage 2:>                                                          (0 + 1) / 4]
[Stage 2:==============>                                            (1 + 1) / 4]
[Stage 2:=============================>                             (2 + 1) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
                                                                                

[Stage 3:>                                                          (0 + 1) / 4]
[Stage 3:==============>                                            (1 + 1) / 4]
[Stage 3:=============================>                             (2 + 1) / 4]
[Stage 3:============================================>              (3 + 1) / 4]
                                                                                

[Stage 4:>                                                          (0 + 1) / 4]
[Stage 4:==============>                                            (1 + 1) / 4]
[Stage 4:=============================>                             (2 + 1) / 4]
[Stage 4:============================================>              (3 + 1) / 4]
                                                                                

[Stage 5:>                                                          (0 + 1) / 4]
[Stage 5:==============>                                            (1 + 1) / 4]
[Stage 5:=============================>                             (2 + 1) / 4]
[Stage 5:============================================>              (3 + 1) / 4]
                                                                                

[Stage 6:>                                                          (0 + 1) / 4]
[Stage 6:==============>                                            (1 + 1) / 4]
[Stage 6:=============================>                             (2 + 1) / 4]
[Stage 6:============================================>              (3 + 1) / 4]
                                                                                

[Stage 7:>                                                          (0 + 1) / 4]
[Stage 7:>                                                          (0 + 2) / 4]
[Stage 7:==============>                                            (1 + 1) / 4]
[Stage 7:=============================>                             (2 + 1) / 4]
[Stage 7:============================================>              (3 + 1) / 4]
                                                                                

[Stage 8:>                                                          (0 + 1) / 4]
[Stage 8:==============>                                            (1 + 1) / 4]
[Stage 8:=============================>                             (2 + 1) / 4]
[Stage 8:============================================>              (3 + 1) / 4]
                                                                                

[Stage 12:>                                                         (0 + 1) / 4]
[Stage 12:==============>                                           (1 + 1) / 4]
[Stage 12:=============================>                            (2 + 1) / 4]
[Stage 12:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 18:>                                                         (0 + 1) / 4]
[Stage 18:==============>                                           (1 + 1) / 4]
[Stage 18:=============================>                            (2 + 1) / 4]
[Stage 18:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 21:>                                                         (0 + 1) / 4]
[Stage 21:==============>                                           (1 + 1) / 4]
[Stage 21:=============================>                            (2 + 1) / 4]
[Stage 21:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 23:>                                                         (0 + 1) / 4]
[Stage 23:==============>                                           (1 + 1) / 4]
[Stage 23:=============================>                            (2 + 1) / 4]
[Stage 23:===========================================>              (3 + 1) / 4]
[Stage 24:=============================>                            (2 + 1) / 4]
[Stage 24:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 27:>                                                         (0 + 1) / 4]
[Stage 27:==============>                                           (1 + 1) / 4]
[Stage 27:=============================>                            (2 + 1) / 4]
Your submission token id is 813400-9b161e88481a99504ea26d461e1f9315:6c5837e2c5e1d1c8396c021862db4ff8:ip-172-31-39-98
Please include this submission token id when you need support for your code submission. I get the foolowing results in my test:

Amazon record "b000hkgj8k" has tokens and weights:
{'autocad': 33.33333333333333, 'autodesk': 8.333333333333332, 'courseware': 66.66666666666666, 'psg': 33.33333333333333, '2007': 3.5, 'customizing': 16.666666666666664, 'interface': 3.0}The difference from the test is on the following elements:'interface' 3.0 vs. 3.0303030303030303'2007' 3.5 vs. 3.5087719298245617I don't really understand where my mistake is (if any), any clue? I'm have submitted the a task on time(Week 2, Lab 1) but, when i was presenting the Week 3 Lab 2, i made a mistake. i uploaded Lab 2 in Lab 1 in , and it changed mi score to 0.

When i try to submit the task again, it takes me out 20 points because i have exceeded the time. Is there any way to recover the first score again ?
I'm submitting the task again but is throws errors i don't know why, and, sometimes it gets into a infinite loop.
I got just one more attempt because i have wasted all the attempts trying to solve this issues

Thanks in advance for any help you can provide All of my tests pass, but when I save the notebook as a .py file and upload it to the autograder it spins for a long time only to tell me that everything failed.   I removed any extraneous cells, and I see in another thread that I am supposed to remove the print statement?  Is this all print statements, including the ones that were built into the lab? I have no problem getting the result with regular Python code, but since I'm only a very very casual Python user I'm wondering if this can be done more efficiently?

I currently have TWO for loops: first to iterate over the tokens array to count the words into a dictionary, and then another one to iterate over the dictionary to calculate the TF.

Can this be done differently?
 
My answer for biggestRecordAmazon is:

[('b000o24l3q', 1547)]

but <strong>len</strong>(biggestRecordAmazon[0][1]) is needed for the Test.

Test.assertEquals(len(biggestRecordAmazon[0][1]), 1547, 'incorrect len for biggestRecordAmazon')

DO i need to recreate 1547 elements in a list and put that into biggestRecordAmazon[0][1] just to pass the test ? 

>>i did by the way, but a waste of time it is .. << I had 87% when I submitted this Lab2 results last time.

Now, I see we have an issue with the Autograder: Will the time for submission be extended?

Data cleaning (1c)
------------------

[Stage 0:>                                                          (0 + 0) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:>                                                          (0 + 2) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:============================================>              (3 + 1) / 4]
                                                                                

[Stage 1:>                                                          (0 + 1) / 4]
[Stage 1:==============>                                            (1 + 1) / 4]
[Stage 1:==============>                                            (1 + 2) / 4]
[Stage 1:=============================>                             (2 + 1) / 4]
[Stage 1:============================================>              (3 + 1) / 4]
                                                                                

[Stage 2:>                                                          (0 + 1) / 4]
[Stage 2:==============>                                            (1 + 1) / 4]
[Stage 2:=============================>                             (2 + 1) / 4]
[Stage 2:=============================>                             (2 + 2) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
                                                                                

[Stage 3:>                                                          (0 + 1) / 4]
[Stage 3:==============>                                            (1 + 1) / 4]
[Stage 3:=============================>                             (2 + 1) / 4]
[Stage 3:============================================>              (3 + 1) / 4]
                                                                                

[Stage 4:>                                                          (0 + 1) / 4]
[Stage 4:==============>                                            (1 + 1) / 4]
[Stage 4:=============================>                             (2 + 1) / 4]
[Stage 4:============================================>              (3 + 1) / 4]
                                                                                

[Stage 5:>                                                          (0 + 1) / 4]
[Stage 5:>                                                          (0 + 2) / 4]
[Stage 5:==============>                                            (1 + 1) / 4]
[Stage 5:=============================>                             (2 + 1) / 4]
[Stage 5:============================================>              (3 + 1) / 4]
                                                                                

[Stage 6:>                                                          (0 + 1) / 4]
[Stage 6:==============>                                            (1 + 1) / 4]
[Stage 6:=============================>                             (2 + 1) / 4]
[Stage 6:============================================>              (3 + 1) / 4]
                                                                                

[Stage 7:>                                                          (0 + 1) / 4]
[Stage 7:==============>                                            (1 + 1) / 4]
[Stage 7:=============================>                             (2 + 1) / 4]
[Stage 7:============================================>              (3 + 1) / 4]
                                                                                

[Stage 8:>                                                          (0 + 1) / 4]
[Stage 8:==============>                                            (1 + 1) / 4]
[Stage 8:=============================>                             (2 + 1) / 4]
[Stage 8:============================================>              (3 + 1) / 4]
                                                                                

[Stage 9:=============================>                             (2 + 1) / 4]
[Stage 9:============================================>              (3 + 1) / 4]
                                                                                

[Stage 10:=============================>                            (2 + 1) / 4]
                                                                                

[Stage 11:==============>                                           (1 + 1) / 4]
[Stage 11:=============================>                            (2 + 1) / 4]
[Stage 11:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 12:>                                                         (0 + 1) / 4]
[Stage 12:==============>                                           (1 + 1) / 4]
[Stage 12:=============================>                            (2 + 1) / 4]
[Stage 12:=============================>                            (2 + 2) / 4]
[Stage 12:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 18:>                                                         (0 + 1) / 4]
[Stage 18:==============>                                           (1 + 1) / 4]
[Stage 18:=============================>                            (2 + 1) / 4]
[Stage 18:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 20:==========================================================(4 + 0) / 4]
                                                                                

[Stage 21:>                                                         (0 + 1) / 4]
[Stage 21:==============>                                           (1 + 1) / 4]
[Stage 21:=============================>                            (2 + 1) / 4]
[Stage 21:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 23:>                                                         (0 + 1) / 4]
[Stage 23:==============>                                           (1 + 1) / 4]
Your submission token id is 814567-fc090ae8b3af3a7cbad67c36957b6a29:c768b7d9c2da2060766fdc022b9d053e:ip-172-31-47-14
Please include this submission token id when you need support for your code submission. I did the following to calculate the TF 

tfs = dict([w, s.count(w)/len(s)*1.0] for w in s)

It seemed it wont work, but it worked when I count the frequency. Any idea?

Thank you.  Dear instructors,

please provide a grace period extension, since the autograder is acting terribly (today at least).
I don't know if the autograder is just overwhelmed by our requests or something else, but please see what you can do with it.

You can see dozens of posts recording absolutely correct submissions being denied because of unexplained reasons.

My already 3rd submission:
- has NO extra print statements
- has NO extra cells added
- has NO collect actions on large RDDs, in fact no other than the necessary ones
- has a proper regex set up at the beginning - only added "\s*" at one point in the string

The first 2 submissions went around with cryptic messages (not timeout; but that "Data cleaning (1c) with some Stages..."), while the 3rd one is still submitting after more than half an hour now.

Please upvote this post so the instructors see it.

Cheers,
Damir My function in 4b has passed the tests. Still in 4c i am getting the following error.
Please help. Stuck since ages
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-199-cbd912244396> in <module>()
      8                   .textFile(fileName, 8)
      9                   .map(removePunctuation))
---> 10 print '\n'.join(shakespeareRDD
     11                 .zipWithIndex()  # to (line, lineNum)
     12                 .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in zipWithIndex(self)
   1986         starts = [0]
   1987         if self.getNumPartitions() > 1:
-> 1988             nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
   1989             for i in range(len(nums) - 1):
   1990                 starts.append(starts[-1] + nums[i])

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 141.0 failed 1 times, most recent failure: Lost task 0.0 in stage 141.0 (TID 364, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <lambda>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1988, in <genexpr>
    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
  File "<ipython-input-197-ac41dd362a27>", line 20, in removePunctuation
IndexError: string index out of range

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
 In creating the uniqueTokens RDD, is this one list of all unique tokens in the corpus?  Or is it an RDD of lists of unique tokens in each document (list of lists)?

I think the "not a pair RDD" is throwing me off.   Hey, I am trying and struggeling for a long time now on 4 d)... I do not really know how to approach this right now. Any tips, hint and clarification would be awesome! 
Thanks Hello I need help, I have this:

<Please Don't post Code. It is a violation of the Honor Code>

print 'Unique hosts: %d' % uniqueHostCount

Unique hosts: 1043177

I do not know continue, thanks The autograder is running normally again and we have spun up many more instances to clear the backlog of submissions. Thank you for being patient with us while we diagnosed the problems.

We have extended the Lab 2 deadline with grace period to June 23 00:00 UTC
------------------------------------------------------------------------------------------------------------------------------------------
[ORIGINAL POST]

We're currently investigating a problem with the autograder. We'll update you when it is back online. Thanks for your patience!

Please do not add to this post - we are aware of the error messages that are being returned and we are rolling out fixes.

Regarding extensions: as was mentioned in this week's update email, you can turn in all of the labs and setup exercises on the last day of the course and still receive a passing grade.
 
edX certificates do not include grades or scores. 
 
 Hello,

I have successfully completed all sections except 3(e) and 3(f). And check it on my local machine by using auto run option but it hanged/stopped before 3(e). I then continue running cell by cell and it seems fine.

But when I upload the same file for auto grading it is taking more than 2 hours and still it is running. Please suggest me how to proceed further because I am already approaching the deadline. 

I also tried after removing all the spaces. But issue does not resolved.

Is there any way to send to the same file to TA team so, they can help me out. or is it mandatory to complete all exercise for lab2 ??.

Please suggest me the earliest as deadline approaching fast.  Your submission token id is 816974-7b45d73c1c2cae2bb9fbc9ea4ae602f7:09dcf07911cca739cb8a2af08a032081:ip-172-31-41-172
Please include this submission token id when you need support for your code submission. Hi,
I cannot complete lab3 3C, I always obtain this error:
It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.

I made some tests and looks like that workers cannot access to 'cosineSimilarity' function.
Any hints?Thanks
 It passes all tests in notebook. 
Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 28, in 
NameError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
name 'b' is not defined Isn't the test case incorrect? Why does it use double indexing if the expected result is a tuple of (ID, tokens)?

I guess it expects a list in reverse sorted order, but if that's the case, then I think the function is poorly named.  Here are a few tips that helped me pass Lab3 (2c):

- use float when needed
- map lambda l to (l,1.0) not (l, 1)- read other postings related to the lab
- use print statements after each line of code to debug

This should help you get
- ('software': 4.25531914894) not ('software': 4.0)
- and it could also help you with Lab3 (2f)
 
# TEST Identify common tokens from the full dataset (4f)
similarityTest = similaritiesFullRDD.filter(lambda ((aID, gURL), cs): aID == 'b00005lzly' and gURL == 'http://www.google.com/base/feeds/snippets/13823221823254120257').collect()
Test.assertEquals(len(similarityTest), 1, 'incorrect len(similarityTest)')
Test.assertTrue(abs(similarityTest[0][1] - 4.286548414e-06) < 0.000000000001, 'incorrect similarityTest fastCosineSimilarity')
Test.assertEquals(similaritiesFullRDD.count(), 2441100, 'incorrect similaritiesFullRDD.count()')
print similarityTest[0][1]
and the result is : how can i fix the bug
1 test passed.
1 test failed. incorrect similarityTest fastCosineSimilarity
1 test passed.
0.0 Hello,

I would like to see a list of the questions or answers I was involved in.

I cannot find a way to do that - but given that this is a usual feature, I assume it is possible somehow.

Kind Regards

Raffael Ugh, I'm getting really lost here in lab3 2c.  the instructions tell us to first "
"Create an RDD (not a pair RDD) containing the unique tokens from each document in the input corpus."
Then the next step is: 
"For each of the unique tokens, count how many times it appears in the document and then compute the IDF for that token: N/n(t)"
But this suggests two problems.  
1. First: the hints we're given from the answer 
tokenCountPairTuple = uniqueTokens.<FILL IN>
suggest that we ought to get the counts per document by applying some function to the uniqueTokens list.  But that's impossible, if it's just a list of unique tokens, not a list of tuples, then the count information is lost by the time we've created uniqueTokens.  Unless there's some way to access information in other RDDs while applying a function to uniqueTokens?

2. Also, "how many times it appears in the document" isn't even an element of the IDF formula.  So why would we use it to calculate the IDF?

The only guess I have here is that the second instruction re: "count how many times it appears in the document" is wrong, and what we're really supposed to be getting is "count how many documents it appears in, in the corpus."  Is that right? In 4d I am getting a word count of 927633 against expected word count of 927631
And in 4e I am getting a word count of 883016 against expected word count of 882996.
I think my logic for removePunctuation and wordCount is almost correct but fails for some special case.
Is there any way I can know where my code fails? My output from the printf statement is correct:

['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']

but the first of the following tests marks it as a failure.

Same problem with the third test.  The output from it is
['123a', '456_b', '789c', '123a']
but the third test fails.

What am I missing here?  I suspect this is a problem with Pythons dynamic typing.

Edit:  I just ran the input block and test block again and all tests passed. I was wondering if we could use the "partial" function from functools in Lab3.

Thanks I keep getting this error How can it be resolved?
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-31-31d711ee5531> in <module>()
      7 dayHostCount = dayGroupedHosts.map(lambda ((k1,k2),v):(k1,v))
      8 
----> 9 dailyHosts = (dayHostCount.sortByKey()).cache()
     10 dailyHostsList = dailyHosts.take(30)
     11 print 'Unique hosts per day: %s' % dailyHostsList

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sortByKey(self, ascending, numPartitions, keyfunc)
    584         # the key-space into bins such that the bins have roughly the same
    585         # number of (key, value) pairs falling into them
--> 586         rddSize = self.count()
    587         if not rddSize:
    588             return self  # empty RDD Hi!The process of submit .py file is looping.Is there a problem on the server?Thanks! Cheers,

 Is 4f expected to run fast because we are using fastcosinesimilarity()? Mine has been running for a minute or more. Looking for opinions on if this is norm or I have done something wrong. 
Thanks
 Hi,

After submitting the file, I have received the below response (after 60 min).

Data cleaning (1c)
------------------

[Stage 0:>                                                          (0 + 0) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:==============>                                            (1 + 2) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:============================================>              (3 + 1) / 4]
                                                                                

[Stage 1:>                                                          (0 + 1) / 4]
[Stage 1:>                                                          (0 + 2) / 4]
[Stage 1:==============>                                            (1 + 1) / 4]
[Stage 1:=============================>                             (2 + 1) / 4]
[Stage 1:============================================>              (3 + 1) / 4]
                                                                                

[Stage 2:>                                                          (0 + 1) / 4]
[Stage 2:==============>                                            (1 + 1) / 4]
[Stage 2:==============>                                            (1 + 2) / 4]
[Stage 2:=============================>                             (2 + 1) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
                                                                                

[Stage 3:>                                                          (0 + 1) / 4]
[Stage 3:==============>                                            (1 + 1) / 4]
[Stage 3:=============================>                             (2 + 1) / 4]
[Stage 3:============================================>              (3 + 1) / 4]
                                                                                

[Stage 4:>                                                          (0 + 1) / 4]
[Stage 4:==============>                                            (1 + 1) / 4]
[Stage 4:=============================>                             (2 + 1) / 4]
[Stage 4:============================================>              (3 + 1) / 4]
                                                                                

[Stage 5:>                                                          (0 + 1) / 4]
[Stage 5:==============>                                            (1 + 1) / 4]
[Stage 5:=============================>                             (2 + 1) / 4]
[Stage 5:============================================>              (3 + 1) / 4]
                                                                                

[Stage 6:>                                                          (0 + 1) / 4]
[Stage 6:==============>                                            (1 + 1) / 4]
[Stage 6:=============================>                             (2 + 1) / 4]
[Stage 6:============================================>              (3 + 1) / 4]
                                                                                

[Stage 7:>                                                          (0 + 1) / 4]
[Stage 7:>                                                          (0 + 2) / 4]
[Stage 7:==============>                                            (1 + 1) / 4]
[Stage 7:=============================>                             (2 + 1) / 4]
[Stage 7:============================================>              (3 + 1) / 4]
                                                                                

[Stage 8:>                                                          (0 + 1) / 4]
[Stage 8:>                                                          (0 + 2) / 4]
[Stage 8:==============>                                            (1 + 1) / 4]
[Stage 8:=============================>                             (2 + 1) / 4]
[Stage 8:============================================>              (3 + 1) / 4]
                                                                                

[Stage 9:=============================>                             (2 + 1) / 4]
[Stage 9:=============================>                             (2 + 2) / 4]
                                                                                

[Stage 10:=============================>                            (2 + 1) / 4]
[Stage 10:=============================>                            (2 + 2) / 4]
                                                                                

[Stage 11:==============>                                           (1 + 2) / 4]
[Stage 11:=============================>                            (2 + 1) / 4]
                                                                                

[Stage 12:>                                                         (0 + 1) / 4]
[Stage 12:>                                                         (0 + 2) / 4]
[Stage 12:==============>                                           (1 + 1) / 4]
[Stage 12:=============================>                            (2 + 1) / 4]
[Stage 12:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 18:>                                                         (0 + 1) / 4]
[Stage 18:==============>                                           (1 + 1) / 4]
[Stage 18:=============================>                            (2 + 1) / 4]
[Stage 18:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 21:>                                                         (0 + 1) / 4]
[Stage 21:==============>                                           (1 + 1) / 4]
[Stage 21:==============>                                           (1 + 2) / 4]
[Stage 21:=============================>                            (2 + 1) / 4]
[Stage 21:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 23:>                                                         (0 + 1) / 4]
[Stage 23:==============>                                           (1 + 1) / 4]
Your submission token id is 819380-41f860609d1ff78be195de1d775b72e7:e378397d537a993b79bfaa19e08b143a:ip-172-31-34-104
Please include this submission token id when you need support for your code submission.


What does this mean? Do I need to resubmit again?

Please clarify.

 I have solved the issue below, just by optimizing my regex expression. 
I suggest in future notebooks to add the timing constraint to the tests in the notebook.
It is very frustrating to pass all tests locally and fail remotely to a hidden constraint.

best regards
Rod Senra
Dear Staff,
I have burnt 5 of my 10 attempts trying to submit lab 2.
All tests are passing.
I have removed all print statements from the code.
Could you give some insight about what should I do ?
Below follows the error code presented by the submission tool.

Data cleaning (1c)
------------------
....
[Stage 20:>                                                         (0 + 1) / 4]
Your submission token id is 820307-93e76bc9e8642d9111cfa2f1d5f1be84:08300f4b07a6990558091b510fd746a1:ip-172-31-39-97
Please include this submission token id when you need support for your code submission. sortedByDay = groupedByDay.sortByKey()

avgDailyReqPerHost = (sortedByDay.join(dayHostCount).map(lambda (a, (b, c)): (a, b/c)).sortByKey().cache() )
Average number of daily requests per Hosts is [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)]
i am not getting what to do ? Please Help I submitted lab 2 today but I received no response for more than 1 hour. I submitted again, but still no response. I have already tried multiple times with no response.

Also, I submitted lab 2 yesterday without the some of the questions answered. I received 75%, but after my lab 2 submission attempt today, I do not see that grade for lab2. Would submitting again remove the previous lab score?

Can one of the TAs look into it?

Thanks  I have been waiting for 1.5 hours. After I submitted my Lab 3 solutions It has been 1.5 hours but it is still working :(
I tried second time but it is still working to grade.
Whats wrong?
I have lost 7 points because of the autograder for Lab 2 but this time i event can not submit it!
 Hi

After first submission of lab2 I got grade 87

Then I fixed my bags and submitted again lab2 ( changes in 3 lines , the changes pass test successfully)

Never get grade again , so I had to submitted another time or two , same thing - did not receive grade only flowing print :

Data cleaning (1c)------------------
[Stage 0:> (0 + 1) / 4][Stage 0:> (0 + 1) / 4][Stage 0:> (0 + 2) / 4][Stage 0:==============> (1 + 1) / 4][Stage 0:==============> (1 + 1) / 4][Stage 0:=============================> (2 + 1) / 4][Stage 0:=============================> (2 + 1) / 4][Stage 0:============================================> (3 + 1) / 4]
[Stage 1:> (0 + 1) / 4][Stage 1:==============> (1 + 1) / 4][Stage 1:=============================> (2 + 1) / 4][Stage 1:============================================> (3 + 1) / 4]
[Stage 2:> (0 + 1) / 4][Stage 2:==============> (1 + 1) / 4][Stage 2:=============================> (2 + 1) / 4][Stage 2:=============================> (2 + 2) / 4][Stage 2:============================================> (3 + 1) / 4]
[Stage 3:> (0 + 1) / 4][Stage 3:==============> (1 + 1) / 4][Stage 3:=============================> (2 + 1) / 4][Stage 3:============================================> (3 + 1) / 4]
[Stage 4:> (0 + 1) / 4][Stage 4:==============> (1 + 1) / 4][Stage 4:=============================> (2 + 1) / 4][Stage 4:============================================> (3 + 1) / 4]
[Stage 5:> (0 + 1) / 4][Stage 5:==============> (1 + 1) / 4][Stage 5:=============================> (2 + 1) / 4][Stage 5:============================================> (3 + 1) / 4]
[Stage 6:> (0 + 1) / 4][Stage 6:==============> (1 + 1) / 4][Stage 6:=============================> (2 + 1) / 4][Stage 6:============================================> (3 + 1) / 4]
[Stage 7:> (0 + 1) / 4][Stage 7:==============> (1 + 1) / 4][Stage 7:=============================> (2 + 1) / 4][Stage 7:============================================> (3 + 1) / 4]
[Stage 8:> (0 + 1) / 4][Stage 8:==============> (1 + 1) / 4][Stage 8:=============================> (2 + 1) / 4][Stage 8:============================================> (3 + 1) / 4]
[Stage 12:> (0 + 1) / 4][Stage 12:==============> (1 + 1) / 4][Stage 12:=============================> (2 + 1) / 4][Stage 12:===========================================> (3 + 1) / 4][Stage 12:==========================================================(4 + 0) / 4]
[Stage 15:======================================> (2 + 1) / 3]
[Stage 17:===========================================> (3 + 1) / 4]
[Stage 18:> (0 + 1) / 4][Stage 18:==============> (1 + 1) / 4][Stage 18:=============================> (2 + 1) / 4][Stage 18:===========================================> (3 + 1) / 4]
[Stage 21:> (0 + 1) / 4][Stage 21:==============> (1 + 1) / 4]Your submission token id is 819833-b0429f7c76cf17d5e0c45671972a7ca6:e90314fc192cd58b98ac3765415c7f73:ip-172-31-39-99Please include this submission token id when you need support for your code submission

Please check
Thanks Hello,
Here is what I want to do: 

I have RDD =  [ (a,1), (a,2), (a,3), (b,1)]
I want to process RDD to get: [(a, [1,2,3]), (b,1)]
groupbykey returns iterable. 
Dont know how to use reduce by key. When used like this: reduceByKey(lambda a,b: (a,b)), I get nested lists. 
How do I get all the values for the same key in a list without having to worry about iterables. 
Alternatively, once I get an iterable, how do I use that to merely get a list of values?
Any help is appreciated. 
Thanks
 Hey guys!

Can anyone help me how to return just the day from the date_time at the RDD?

Any help would be very appreciated. This cracked me up in question 3e:

Combine the sims RDD with the goldStandard RDD by creating a new trueDupsRDD RDD that has the just the cosine similarity scores for those "AmazonID GoogleURL" pairs that appear in both the sims RDD and goldStandard RDD. Hint: you can do this using the join() transformation.

Anyone that already made it this far in Lab 3 without help definitely doesn't need this hint! hourCountPairTuple = badRecords.map(lambda log: (log.date_time.hour, 1))hourRecordsSum = hourCountPairTuple.reduceByKey(lambda a, b : a + b).collect()hourRecordsSorted = sc.parallelize(hourRecordsSum)hourRecordsSorted = sorted(hourRecordsSum)hourRecordsSorted = sc.parallelize(hourRecordsSum)errHourList = hourRecordsSorted.takeOrdered(24, lambda s: 1 * s[0])hourRecordsSorted= sc.parallelize(errHourList)hourRecordsSorted.cache() 

I have some (my) code above and I am having to  switch between RDD and List.
The above code does not see to be proper or efficient.
Can you let me know a better way to do it so that I do not have to switch between list and RDDs
Thanks! I'm completely new to python. Any hints on this?

Thanks. Hi ,

All my test pass on local machine but i am getting same error as before. This is after autograder was restarted.

Tokenize a String (1a)
----------------------

[Stage 62:====================================>                   (11 + 1) / 17]
[Stage 62:==============================================>         (14 + 1) / 17]
                                                                                

[Stage 66:================================>                       (10 + 1) / 17]
[Stage 66:==========================================>             (13 + 1) / 17]
                                                                                

[Stage 89:=============================>                            (4 + 1) / 8]
[Stage 89:==================================================>       (7 + 1) / 8]
[Stage 90:>                                                         (0 + 1) / 8]
[Stage 90:=======>                                                  (1 + 1) / 8]
[Stage 90:==============>                                           (2 + 1) / 8]
[Stage 90:=====================>                                    (3 + 1) / 8]
[Stage 90:=============================>                            (4 + 1) / 8]
[Stage 90:====================================>                     (5 + 1) / 8]
[Stage 90:===========================================>              (6 + 1) / 8]
[Stage 90:==================================================>       (7 + 1) / 8]
[Stage 91:>                                                         (0 + 1) / 8]
[Stage 91:=======>                                                  (1 + 1) / 8]
[Stage 91:==============>                                           (2 + 1) / 8]
[Stage 91:=====================>                                    (3 + 1) / 8]
[Stage 91:=============================>                            (4 + 1) / 8]
[Stage 91:====================================>                     (5 + 1) / 8]
[Stage 91:===========================================>              (6 + 1) / 8]
[Stage 91:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 94:>                                                         (0 + 1) / 8]
[Stage 94:=======>                                                  (1 + 1) / 8]
[Stage 94:==============>                                           (2 + 1) / 8]
[Stage 94:=====================>                                    (3 + 1) / 8]
[Stage 94:=============================>                            (4 + 1) / 8]
[Stage 94:====================================>                     (5 + 1) / 8]
[Stage 94:===========================================>              (6 + 1) / 8]
[Stage 94:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 99:>                                                         (0 + 1) / 8]
[Stage 99:=======>                                                  (1 + 1) / 8]
[Stage 99:==============>                                           (2 + 1) / 8]
[Stage 99:=====================>                                    (3 + 1) / 8]
[Stage 99:=============================>                            (4 + 1) / 8]
[Stage 99:====================================>                     (5 + 1) / 8]
[Stage 99:===========================================>              (6 + 1) / 8]
[Stage 99:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 102:>                                                        (0 + 1) / 8]
[Stage 102:=======>                                                 (1 + 1) / 8]
[Stage 102:==============>                                          (2 + 1) / 8]
[Stage 102:=====================>                                   (3 + 1) / 8]
[Stage 102:============================>                            (4 + 1) / 8]
[Stage 102:===================================>                     (5 + 1) / 8]
[Stage 102:==========================================>              (6 + 1) / 8]
[Stage 102:=================================================>       (7 + 1) / 8]
                                                                                

[Stage 105:>                                                        (0 + 1) / 8]
[Stage 105:=======>                                                 (1 + 1) / 8]
[Stage 105:==============>                                          (2 + 1) / 8]
[Stage 105:=====================>                                   (3 + 1) / 8]
[Stage 105:============================>                            (4 + 1) / 8]
[Stage 105:===================================>                     (5 + 1) / 8]
[Stage 105:==========================================>              (6 + 1) / 8]
[Stage 105:=================================================>       (7 + 1) / 8]
                                                                                

[Stage 108:=======>                                                 (1 + 1) / 8]
Your submission token id is 

Please help me !  Here I am using "collect()" function to get all data from "vendorRDD" . Is it fine ? I see in submission notes that , "
Do not use collect() actions to collect all data from an RDD.
"

.
Then how can i get list from vendorRDD ?  Need a guidance on 3C.
I get the rows with date_time
I sort with datetime as key by sortByKEY
The use reduceBykey a+b to sum by datetime to groupby hosts
I get 54507  records

how do I get for one day
Should I use filter, if so how

Thanks 
I want to log timestamp of every element of the RDD so i have  assigned the MSGid to every elemnt inside RDD,and increamented it.(static variable).
My code is giving distinct Msgid in local mode but in cluster mode this value is duplicated every 30-40 count.
Please help !!
//public static long msgId=0;
   newinputStream.foreachRDD(new Function2<JavaRDD<String>, Time, Void>() {
            @Override
            public Void call(JavaRDD<String> v1, Time v2) throws Exception {
                for(String s:v1.collect()) {

spoutlog.batchLogwriter(System.currentTimeMillis(), "spout-MSGID," + msgeditor.getMessageId(s));
//                System.out.println(msgeditor.getMessageId(s));
                }
                return null;
            }
        });

// (Is it being updated in parallel,yes ?? how to avoid it) I got this:
There are 400 tokens in the combined datasets
And my code is that:
amazonRecToToken = <REDACTED> POSTING SOLUTIONS IS A VIOLATION OF THE HONOR CODE

def countTokens(vendorRDD):
    return vendorRDD.count()

totalTokens = countTokens(amazonRecToToken) + countTokens(googleRecToToken)
print 'There are %s tokens in the combined datasets' % totalTokens
What I'm doing wrong?

Thanks I submitted Lab2 but it's still spinning from last half an hour.
My submission contained correct answers for question 1 to 3 in the labs. 
Instructor or any other staff please help. I submitted a lab3. All local test passed but edx test just stopped job at stage 111 without much of a feedback. My assumption with distributed data is that we don't assume it is stored in sorted fashion (i.e. sorting is only carried out when we query it).

However Lab 2, 3(c) and later require that sortByKey() be performed.

How useful or common is this is the real world? I submitted Lab 2 exercise for grading and there is no result even after 2 hrs. Is it normal? can I have hints about this? Thanks a lot I'm trying to resolve lab2- 3c and, as someone suggested, using 'distinct' function. Problem is that after applying 'distinct' to an RDD I cannot use any 'take' or 'sortByKey' function on the new RDD. It always rises the same error: 
unhashable type: 'list'I'm frustrated about this problem. Please, could anyone help me? I accidentally submitted lab 2 to the setup lab autograder and it erased my previous score.  I submitted lab 2 to the correct grader, then re-submitted the setup lab to the setup grader , but now I have a late score for the seup lab.  I think if you make a mistake like this it should not erase your previous score.  I'm completely stuck on lab2 3b for the uniqueHostCount. I so far have tried count(), distinct().count() and countByValue() but without success and it is beginning to fill like I am blindly guessing?

Any pointers?

Thanks in advance

 
Hello,
This error seems to be in code not written by me. The autograder passed all the tests except for this exception. Please help. 

Thanks


 I've been running with 2GB of base memory in the VM, which was fine for labs 1 and 2. Lab 3 is clearly more demanding. Problem 4e ran successfully about half the times I tried it and crashed with a memory error the other half. Problem 5a (which is entirely Berkeley-supplied code) fails reproducibly.

I increased the base memory to 4GB and everything ran successfully to completion. Students with memory-limited hardware may have a problem with part 5. 

ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
 Dear Course Staff,

        Thank you so much for this great class. And I am an newer for this field and have learned a lot from it.
         So I was wondering if there is any resources (or like small projects from real problems) that I could get more practice after this class.  Thank you so much.


Cheers 

      Hi,

I would like to take the next course as well (scalable machine learning) to obtain the Xseries certificate but I will be absent from 20/07- 05/08, I will therefore miss out on a number of deadlines. Is it possible to submit the tasks after 05/08 and still obtain the certificate albeit with a late submission penalty? I just want to know whether I should pay or not.. Lab 3 is time consuming. It took about 8 hours for me. Most of the time is spent in 'variable' names and "comments" that I felt were confusing. 

As is usual now, lot of my time is spent on understanding pyspark methods, its return values and how to get it to do what I want done. 

Several errors related to out of memory and kernel restarts. In all those cases, restart kernel and run all cells to current cell. Some errors got fixed by restarting kernel. 

Autograder continues to have issues. 

Positive side, this is a very practical lab. The cosine similarity is very interesting and is probably used by several internet companies. Now I know how to implement CS, which feels good. 

 I never had this problem with the previous two labs. I finished lab3 and passed the tests in the notebook, however, I am unable to save it as a python file. Whenever I try Download as -> Python, I get a *.txt file instead. 

I tried to submit that *.txt file renamed to *.py, but the autograder gave me 0/100.

Here is the submission number:

Your submission token id is 830128-6c8dd89d77b0eddf56c5ff270137d183:2832ebfe6e36e337177812c5cf404851:ip-172-31-11-135
Please include this submission token id when you need support for your code submission. Hi,
I'm having some troubles with understanding 4e and getting the correct results.

I'm not sure about the purpose of commonTokens and how to actually produce it according to the count.
A normal inverted index would have a single entry for each token. However, if I try to do this as
commonTokens = (amazonInvPairsRDD                .join(googleInvPairsRDD)                .groupByKey()                .cache())
it gives 6601 tokens only. If I omit the groupByKey, and in this case I'm not sure what this represents, it gets 5966373 which is also far from the required result in the test.

Can someone shed some light on this please? I am trying to submit my lab 3.

I get following error message

Tokenize a String (1a)----------------------[Stage 60:==========================================>             (13 + 1) / 17]                                                                                [Stage 64:==========================================>             (13 + 1) / 17]                                                                                [Stage 87:====================================>                     (5 + 1) / 8][Stage 88:>                                                         (0 + 1) / 8][Stage 88:=======>                                                  (1 + 1) / 8][Stage 88:==============>                                           (2 + 1) / 8][Stage 88:=====================>                                    (3 + 1) / 8][Stage 88:=============================>                            (4 + 1) / 8][Stage 88:====================================>                     (5 + 1) / 8][Stage 88:===========================================>              (6 + 1) / 8][Stage 88:==================================================>       (7 + 1) / 8][Stage 89:>                                                         (0 + 1) / 8][Stage 89:=======>                                                  (1 + 1) / 8][Stage 89:==============>                                           (2 + 1) / 8][Stage 89:=====================>                                    (3 + 1) / 8][Stage 89:=============================>                            (4 + 1) / 8][Stage 89:====================================>                     (5 + 1) / 8][Stage 89:===========================================>              (6 + 1) / 8][Stage 89:==================================================>       (7 + 1) / 8]                                                                                [Stage 92:>                                                         (0 + 1) / 8][Stage 92:=======>                                                  (1 + 1) / 8][Stage 92:==============>                                           (2 + 1) / 8][Stage 92:=====================>                                    (3 + 1) / 8][Stage 92:=============================>                            (4 + 1) / 8][Stage 92:====================================>                     (5 + 1) / 8][Stage 92:===========================================>              (6 + 1) / 8][Stage 92:==================================================>       (7 + 1) / 8]                                                                                [Stage 97:>                                                         (0 + 1) / 8][Stage 97:=======>                                                  (1 + 1) / 8][Stage 97:==============>                                           (2 + 1) / 8][Stage 97:=====================>                                    (3 + 1) / 8][Stage 97:=============================>                            (4 + 1) / 8][Stage 97:====================================>                     (5 + 1) / 8][Stage 97:===========================================>              (6 + 1) / 8][Stage 97:==================================================>       (7 + 1) / 8]                                                                                [Stage 100:>                                                        (0 + 1) / 8][Stage 100:=======>                                                 (1 + 1) / 8][Stage 100:==============>                                          (2 + 1) / 8][Stage 100:=====================>                                   (3 + 1) / 8][Stage 100:============================>                            (4 + 1) / 8][Stage 100:===================================>                     (5 + 1) / 8][Stage 100:==========================================>              (6 + 1) / 8][Stage 100:=================================================>       (7 + 1) / 8]                                                                                [Stage 103:>                                                        (0 + 1) / 8][Stage 103:=======>                                                 (1 + 1) / 8][Stage 103:==============>                                          (2 + 1) / 8][Stage 103:=====================>                                   (3 + 1) / 8][Stage 103:============================>                            (4 + 1) / 8][Stage 103:===================================>                     (5 + 1) / 8][Stage 103:==========================================>              (6 + 1) / 8][Stage 103:=================================================>       (7 + 1) / 8]Your submission token id is 829780-9416cb819a3d27f16fcf77411b3ef4c7:f1279c2dca1d40b7828b7009bd3f75b8:ip-172-31-39-83Please include this submission token id when you need support for your code submission.

All my tests have passed locally. Please help!

Thanks
 Hello,

I uploaded Lab 2 into Lab 1 submission by accident and as a result lost my Lab 1 score.
I re-uploaded Lab 1 results and got 80% (due to late submission).

Is it possible to correct this as I originally submitted Lab 1 on time (and was awarded 100% at the time).

(unfortunately, I don't have submission ID for my first submission as everything went smoothly so I didn't anticipate any problems...)

Kind regards,
 My fourth submission just failed.  Seriously, these submissions that fail to grade, that don't even time out or return any grade, shouldn't be counted.  I've had three of these failures and one that failed with even less output.  @2396 says the autograder is fixed, which is why I'm posting this.

Tokenize a String (1a)
----------------------

[Stage 60:======>                                                  (2 + 1) / 17]
[Stage 60:==========>                                              (3 + 1) / 17]
[Stage 60:=============>                                           (4 + 1) / 17]
[Stage 60:================>                                        (5 + 1) / 17]
[Stage 60:====================>                                    (6 + 1) / 17]
[Stage 60:=======================>                                 (7 + 1) / 17]
[Stage 60:==========================>                              (8 + 1) / 17]
[Stage 60:==============================>                          (9 + 1) / 17]
[Stage 60:================================>                       (10 + 1) / 17]
[Stage 60:====================================>                   (11 + 1) / 17]
[Stage 60:=======================================>                (12 + 1) / 17]
[Stage 60:==========================================>             (13 + 1) / 17]
[Stage 60:=================================================>      (15 + 1) / 17]
[Stage 61:==============================>                          (9 + 1) / 17]
[Stage 61:=======================================>                (12 + 1) / 17]
[Stage 61:==============================================>         (14 + 1) / 17]
                                                                                

[Stage 63:==============================>                          (9 + 1) / 17]
[Stage 63:=======================================>                (12 + 1) / 17]
[Stage 63:==========================================>             (13 + 1) / 17]
[Stage 63:==============================================>         (14 + 1) / 17]
[Stage 63:=================================================>      (15 + 1) / 17]
                                                                                

[Stage 64:===>                                                     (1 + 1) / 17]
[Stage 64:======>                                                  (2 + 1) / 17]
[Stage 64:==========>                                              (3 + 1) / 17]
[Stage 64:=============>                                           (4 + 1) / 17]
[Stage 64:================>                                        (5 + 1) / 17]
[Stage 64:====================>                                    (6 + 1) / 17]
[Stage 64:=======================>                                 (7 + 1) / 17]
[Stage 64:==========================>                              (8 + 1) / 17]
[Stage 64:==============================>                          (9 + 1) / 17]
[Stage 64:================================>                       (10 + 1) / 17]
[Stage 64:====================================>                   (11 + 1) / 17]
[Stage 64:=======================================>                (12 + 1) / 17]
[Stage 64:==============================================>         (14 + 1) / 17]
[Stage 64:=================================================>      (15 + 1) / 17]
[Stage 64:====================================================>   (16 + 1) / 17]
                                                                                

[Stage 67:==========>                                              (3 + 1) / 17]
[Stage 67:=============>                                           (4 + 1) / 17]
[Stage 67:================>                                        (5 + 1) / 17]
[Stage 67:====================>                                    (6 + 1) / 17]
[Stage 67:==========================>                              (8 + 1) / 17]
[Stage 67:==============================>                          (9 + 1) / 17]
[Stage 67:================================>                       (10 + 1) / 17]
[Stage 67:====================================>                   (11 + 1) / 17]
[Stage 67:=======================================>                (12 + 2) / 17]
[Stage 67:==============================================>         (14 + 1) / 17]
[Stage 67:=================================================>      (15 + 1) / 17]
                                                                                

[Stage 69:======>                                                  (2 + 1) / 17]
[Stage 69:==========>                                              (3 + 1) / 17]
[Stage 69:=============>                                           (4 + 1) / 17]
[Stage 69:================>                                        (5 + 1) / 17]
[Stage 69:====================>                                    (6 + 1) / 17]
[Stage 69:=======================>                                 (7 + 1) / 17]
[Stage 69:==========================>                              (8 + 1) / 17]
[Stage 69:==============================>                          (9 + 1) / 17]
[Stage 69:================================>                       (10 + 1) / 17]
[Stage 69:====================================>                   (11 + 1) / 17]
[Stage 69:=======================================>                (12 + 1) / 17]
[Stage 69:==========================================>             (13 + 1) / 17]
[Stage 69:==============================================>         (14 + 1) / 17]
[Stage 69:=================================================>      (15 + 1) / 17]
                                                                                

[Stage 70:=============================>                            (2 + 1) / 4]
[Stage 70:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 72:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 73:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 74:=======>                                                  (1 + 1) / 8]
[Stage 74:==============>                                           (2 + 1) / 8]
[Stage 74:=====================>                                    (3 + 1) / 8]
[Stage 74:=============================>                            (4 + 1) / 8]
[Stage 74:====================================>                     (5 + 1) / 8]
[Stage 74:===========================================>              (6 + 1) / 8]
[Stage 74:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 75:>                                                         (0 + 1) / 8]
[Stage 75:=======>                                                  (1 + 1) / 8]
[Stage 75:==============>                                           (2 + 1) / 8]
[Stage 75:=====================>                                    (3 + 1) / 8]
[Stage 75:=============================>                            (4 + 1) / 8]
[Stage 75:====================================>                     (5 + 1) / 8]
[Stage 75:===========================================>              (6 + 1) / 8]
[Stage 75:==================================================>       (7 + 1) / 8]
[Stage 76:====================================>                     (5 + 1) / 8]
[Stage 76:===========================================>              (6 + 1) / 8]
[Stage 76:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 78:====================================>                     (5 + 1) / 8]
[Stage 78:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 81:===========================================>              (6 + 1) / 8]
                                                                                

[Stage 82:==============>                                           (1 + 1) / 4]
[Stage 82:=============================>                            (2 + 1) / 4]
[Stage 82:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 83:>                                                         (0 + 0) / 4]
[Stage 83:=============================>                            (2 + 1) / 4]
[Stage 83:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 84:==============>                                           (1 + 1) / 4]
[Stage 84:=============================>                            (2 + 1) / 4]
[Stage 84:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 85:==============>                                           (1 + 2) / 4]
[Stage 85:=============================>                            (2 + 1) / 4]
[Stage 85:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 86:==============>                                           (1 + 1) / 4]
[Stage 86:=============================>                            (2 + 1) / 4]
[Stage 86:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 87:>                                                         (0 + 0) / 4]
[Stage 87:=============================>                            (2 + 1) / 4]
[Stage 87:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 88:>                                                         (0 + 0) / 4]
[Stage 88:>                                                         (0 + 1) / 4]
[Stage 88:==============>                                           (1 + 1) / 4]
[Stage 88:=============================>                            (2 + 1) / 4]
[Stage 88:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 89:==============>                                           (1 + 1) / 4]
[Stage 89:=============================>                            (2 + 1) / 4]
[Stage 89:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 90:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 92:>                                                         (0 + 1) / 8]
[Stage 92:=======>                                                  (1 + 1) / 8]
[Stage 92:==============>                                           (2 + 1) / 8]
[Stage 92:=====================>                                    (3 + 1) / 8]
[Stage 92:=============================>                            (4 + 1) / 8]
[Stage 92:====================================>                     (5 + 1) / 8]
[Stage 92:===========================================>              (6 + 1) / 8]
[Stage 92:==================================================>       (7 + 1) / 8]
[Stage 93:>                                                         (0 + 0) / 8]
[Stage 93:>                                                         (0 + 1) / 8]
[Stage 93:=======>                                                  (1 + 1) / 8]
[Stage 93:==============>                                           (2 + 1) / 8]
[Stage 93:=====================>                                    (3 + 1) / 8]
[Stage 93:=============================>                            (4 + 1) / 8]
[Stage 93:====================================>                     (5 + 1) / 8]
[Stage 93:====================================>                     (5 + 1) / 8]
[Stage 93:===========================================>              (6 + 1) / 8]
[Stage 93:==================================================>       (7 + 1) / 8]
[Stage 94:>                                                         (0 + 1) / 8]
Your submission token id is 830244-2c7da6f6114cddc8c5b6a55c8e21253c:815ebf2555a0a31c2b9e1eec995476ce:ip-172-31-36-185
Please include this submission token id when you need support for your code submission. I am still having grading difficulties with Lab 2. I have tried the various steps outlined in other posts to no avail. I am likely doing something dumb, but I don't know what. I'd appreciate any help the instructors can provide.

My submission token is 

830043-80b14fdc3337adb12c83e0ea73a86968:3f9f1ec275cf147a1dad2f15c1d47dd3:ip-172-31-36-180											 Is this a python question or a spark question? I generally get confused when to use python and when to use spark.  Is there suppose to be any RDD stuff in this? Or is this just python stuff for this problem? submission token id is. All tests run fine and pass locally. Now I am gonna pass the deadline, what should I do?? Grading continues for more than an hour or two for lab2. "Your files have been submitted. As soon as..."
Should I wait a day? Or more?
It passed all tests, I exported .py and submitted. 
What else should I do? Hi,

My tests passed locally but the autograder gave me zero points for lab 3.... 

Please help. I spent a lot of time on the lab and deserve more than zero! :)

I checked for collect() and I do not have extra ones than the ones used in the original notebook.

I do not understand what these messages mean:

Tokenize a String (1a)
----------------------

[Stage 66:==========>                                              (3 + 1) / 17]
[Stage 66:=============>                                           (4 + 1) / 17]
[Stage 66:================>                                        (5 + 1) / 17]
[Stage 66:================>                                        (5 + 2) / 17]
[Stage 66:=======================>                                 (7 + 1) / 17]
[Stage 66:==========================>                              (8 + 1) / 17]
[Stage 66:==============================>                          (9 + 1) / 17]
[Stage 66:================================>                       (10 + 1) / 17]
[Stage 66:====================================>                   (11 + 1) / 17]
[Stage 66:=======================================>                (12 + 1) / 17]
[Stage 66:==========================================>             (13 + 1) / 17]
[Stage 66:==============================================>         (14 + 1) / 17]
[Stage 66:=================================================>      (15 + 1) / 17]
[Stage 66:====================================================>   (16 + 1) / 17]
[Stage 67:====================>                                    (6 + 1) / 17]
[Stage 67:=======================>                                 (7 + 1) / 17]
[Stage 67:==============================>                          (9 + 1) / 17]
[Stage 67:================================>                       (10 + 1) / 17]
[Stage 67:=======================================>                (12 + 1) / 17]
[Stage 67:==============================================>         (14 + 1) / 17]
[Stage 67:====================================================>   (16 + 1) / 17]
                                                                                

[Stage 69:================>                                        (5 + 1) / 17]
[Stage 69:=======================>                                 (7 + 1) / 17]
[Stage 69:==========================>                              (8 + 1) / 17]
[Stage 69:================================>                       (10 + 1) / 17]
[Stage 69:=======================================>                (12 + 1) / 17]
[Stage 69:==============================================>         (14 + 1) / 17]
[Stage 69:====================================================>   (16 + 1) / 17]
                                                                                

[Stage 70:==========>                                              (3 + 1) / 17]
[Stage 70:=============>                                           (4 + 1) / 17]
[Stage 70:================>                                        (5 + 1) / 17]
[Stage 70:====================>                                    (6 + 1) / 17]
[Stage 70:=======================>                                 (7 + 1) / 17]
[Stage 70:==========================>                              (8 + 1) / 17]
[Stage 70:==============================>                          (9 + 1) / 17]
[Stage 70:====================================>                   (11 + 1) / 17]
[Stage 70:=======================================>                (12 + 1) / 17]
[Stage 70:==========================================>             (13 + 1) / 17]
[Stage 70:=================================================>      (15 + 1) / 17]
[Stage 71:>                                                        (0 + 0) / 17]
[Stage 71:==========>                                              (3 + 1) / 17]
[Stage 71:=============>                                           (4 + 1) / 17]
[Stage 71:====================>                                    (6 + 1) / 17]
[Stage 71:====================>                                    (6 + 2) / 17]
[Stage 71:==========================>                              (8 + 1) / 17]
[Stage 71:================================>                       (10 + 1) / 17]
[Stage 71:================================>                       (10 + 2) / 17]
[Stage 71:=======================================>                (12 + 1) / 17]
[Stage 71:==============================================>         (14 + 1) / 17]
[Stage 71:====================================================>   (16 + 1) / 17]
                                                                                

[Stage 73:====================>                                    (6 + 1) / 17]
[Stage 73:==========================>                              (8 + 1) / 17]
[Stage 73:================================>                       (10 + 1) / 17]
[Stage 73:=======================================>                (12 + 1) / 17]
[Stage 73:==============================================>         (14 + 1) / 17]
                                                                                

[Stage 74:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 78:==============>                                           (2 + 1) / 8]
[Stage 78:=====================>                                    (3 + 1) / 8]
[Stage 78:=============================>                            (4 + 1) / 8]
[Stage 78:====================================>                     (5 + 1) / 8]
[Stage 78:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 79:=======>                                                  (1 + 1) / 8]
[Stage 79:==============>                                           (2 + 1) / 8]
[Stage 79:=====================>                                    (3 + 1) / 8]
[Stage 79:=============================>                            (4 + 1) / 8]
[Stage 79:====================================>                     (5 + 1) / 8]
[Stage 79:===========================================>              (6 + 1) / 8]
[Stage 79:==================================================>       (7 + 1) / 8]
[Stage 80:>                                                        (0 + 0) / 64]
[Stage 80:>                                                        (0 + 1) / 64]
[Stage 80:>                                                        (1 + 1) / 64]
[Stage 80:=>                                                       (2 + 1) / 64]
[Stage 80:==>                                                      (3 + 1) / 64]
[Stage 80:===>                                                     (4 + 1) / 64]
[Stage 80:====>                                                    (5 + 1) / 64]
[Stage 80:=====>                                                   (6 + 1) / 64]
[Stage 80:======>                                                  (7 + 1) / 64]
[Stage 80:=======>                                                 (8 + 1) / 64]
[Stage 80:========>                                                (9 + 1) / 64]
[Stage 80:========>                                               (10 + 1) / 64]
[Stage 80:=========>                                              (11 + 1) / 64]
[Stage 80:==========>                                             (12 + 1) / 64]
[Stage 80:===========>                                            (13 + 1) / 64]
<strong>Your submission token id is 831012-5b86c32c3098359a14220f689b083af7:2832ebfe6e36e337177812c5cf404851:ip-172-31-36-186
Please include this submission token id when you need support for your code submission.</strong> My question is about tokenCountPairTuple - here is where i am:   I believe i understand what IDF is and what the goal is but cannot figure out approach to the tokenCountPairTuple.  I understand from another post it should be in the form of:

[
   (token1, token2, token3),
   (token1, token3, token4),...
]

where each list in the RDD is a token list unique to that document.

Assuming this is about right, then what the heck does tokenCountPairTuple look like? I am getting 'Data cleaning (1c)' issues... My earlier submission this morning hung for a long time. Now, this particular submission has 'Data' Cleaning' issues. All tests pass locally.
Not sure whether it's autograder issue or it's my code.

My submission token id is 831374-80f93b9ab6316d3c2b9ac8db1e6186b7:f203f13fe1261e73075fd91a7601ce38:ip-172-31-45-13

Thanks.
 hi friends, 
i need some help in lab 3-3c as "how to apply computeSimilarity function on each element of crossSmall RDD to calculate the similarities" as i have tried it  but the execution is taking hell lot of time and i am not confident as if that can give correct output.

can you provide some hint

thanks I am having problems with the autograder for lab 2. The token is:
submission token id is 832475-8dd9b7061c2265200431813ec0c687e7:f620047331e32dd02f9b189057fa4c29:ip-172-31-36-183
The tests passed locally. Should I resubmit and try again? Hi,

I'm getting the following error when I try to upload my file:

Exception happened during processing of request from ('127.0.0.1', 44393)
Your submission token id is 832889-38f17d465264be6fa433a0a91de9b590:121918b31aea9f58885fb07fa4377625:ip-172-31-39-79

What should I do in this situation?

Thank you in advance for your help,

Regards.

 So what is the deal with this? Why would it pass the first one and not the second?

1 test passed.
1 test failed. incorrect result for tf test Hi 

Can someone please explain me the difference between collect() and items()? My understanding is collect() returns a list on RDD. Not very sure about items().

Also need some help on lambda functions on reduceByKey(). How does rdd.reduceByKey(lambda x,y : x+y) gives sum of the values? Is it same as rdd.reduceByKey(sum) ? What happens if values are not numeric? And also what other functions are supported like sum ?


Thanks in advance. Additional reading materials are appreciated.

Regards
Arijit Is it normal? Or it's my algorithm's problem.

Thank you. Just to be clear, when the instructions ask for: 

"Create an RDD that is a combination of the small Google and small Amazon datasets that has as elements all pairs of elements (a, b) where a is in self and b is in other. The result will be an RDD of the form: [ ((Google URL1, Google String1), (Amazon ID1, Amazon String1)), ((Google URL1, Google String1), (Amazon ID2, Amazon String2)), ((Google URL2, Google String2), (Amazon ID1, Amazon String1)), ... ]"

is that just asking for the cartesian product of the two sets? Can anyone explain (by hinting) how we can use badRecords to get for example hourly 404 codes. I have been  thinking a lot but  cannot find the solution. I had to use other means to provide correct answers to the exercise questions. I am thinking maybe the structure of my badRecords is not suitable for any join, union.. Hi, I tried the following count and it gave me count of 31381, I wonder anything went wrong. 


Update:  Thanks for helping. I got it fixed and removed the code. I tried to avoid using groupByKey and this is what I came up with:

dayToHostPairTuple.<REDACTED> 
POSTING SOLUTIONS IS A VIOLATION OF THE HONOR CODE

which, in the final result, yields counts of:


Unique hosts per day: [(1, 2582), (3, 3222), (4, 4190), (5, 2503), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4457)]

The idea is to daisy chain them together with a comma, then separate by the comma to perform a count of elements.  That yields an answer very similar to the correct answer of:


Unique hosts per day: [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)]

 
Does anyone see an obvious explanation for the variances?  I also tried stripping commas from a and b, in the event they were in the string, but that did not change the result. I am not using any extra collect than what is there in code . Also retyped full assignment so that there is not indentation error or white space. But still I am getting same error , please help TA !

I have already used my 3 submission , please help me ! .

Thanx again
 
The trace is below :
 
Tokenize a String (1a)
----------------------

[Stage 60:====================>                                    (6 + 1) / 17]
[Stage 60:==========================>                              (8 + 1) / 17]
[Stage 60:================================>                       (10 + 1) / 17]
[Stage 60:==========================================>             (13 + 1) / 17]
[Stage 60:=================================================>      (15 + 1) / 17]
[Stage 61:=======================================>                (12 + 1) / 17]
                                                                                

[Stage 63:==============================================>         (14 + 1) / 17]
                                                                                

[Stage 64:=======================>                                 (7 + 1) / 17]
[Stage 64:==============================>                          (9 + 1) / 17]
[Stage 64:====================================>                   (11 + 1) / 17]
[Stage 64:=======================================>                (12 + 1) / 17]
[Stage 64:==============================================>         (14 + 1) / 17]
[Stage 64:====================================================>   (16 + 1) / 17]
[Stage 65:====================================>                   (11 + 1) / 17]
[Stage 65:====================================================>   (16 + 1) / 17]
                                                                                

[Stage 67:====================================>                   (11 + 1) / 17]
[Stage 67:====================================================>   (16 + 1) / 17]
                                                                                

[Stage 72:=============================>                            (4 + 1) / 8]
[Stage 72:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 73:=====================>                                    (3 + 1) / 8]
[Stage 73:=============================>                            (4 + 1) / 8]
[Stage 73:====================================>                     (5 + 1) / 8]
[Stage 73:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 83:=============================>                            (2 + 1) / 4]
[Stage 83:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 84:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 87:==============>                                           (2 + 1) / 8]
[Stage 87:=====================>                                    (3 + 1) / 8]
[Stage 87:=============================>                            (4 + 1) / 8]
[Stage 87:====================================>                     (5 + 1) / 8]
[Stage 87:===========================================>              (6 + 1) / 8]
[Stage 87:==================================================>       (7 + 1) / 8]
[Stage 88:>                                                         (0 + 1) / 8]
[Stage 88:=======>                                                  (1 + 1) / 8]
[Stage 88:==============>                                           (2 + 1) / 8]
[Stage 88:=====================>                                    (3 + 1) / 8]
[Stage 88:=============================>                            (4 + 1) / 8]
[Stage 88:====================================>                     (5 + 1) / 8]
[Stage 88:===========================================>              (6 + 1) / 8]
[Stage 88:==================================================>       (7 + 1) / 8]
[Stage 89:>                                                         (0 + 1) / 8]
[Stage 89:=======>                                                  (1 + 1) / 8]
[Stage 89:==============>                                           (2 + 1) / 8]
[Stage 89:=====================>                                    (3 + 1) / 8]
[Stage 89:=============================>                            (4 + 1) / 8]
[Stage 89:====================================>                     (5 + 1) / 8]
[Stage 89:===========================================>              (6 + 1) / 8]
[Stage 89:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 92:>                                                         (0 + 1) / 8]
[Stage 92:=======>                                                  (1 + 1) / 8]
[Stage 92:==============>                                           (2 + 1) / 8]
[Stage 92:=====================>                                    (3 + 1) / 8]
[Stage 92:=============================>                            (4 + 1) / 8]
[Stage 92:====================================>                     (5 + 1) / 8]
[Stage 92:===========================================>              (6 + 1) / 8]
[Stage 92:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 97:>                                                         (0 + 1) / 8]
[Stage 97:=======>                                                  (1 + 1) / 8]
[Stage 97:==============>                                           (2 + 1) / 8]
[Stage 97:=====================>                                    (3 + 1) / 8]
[Stage 97:=============================>                            (4 + 1) / 8]
[Stage 97:====================================>                     (5 + 1) / 8]
[Stage 97:===========================================>              (6 + 1) / 8]
[Stage 97:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 100:>                                                        (0 + 1) / 8]
[Stage 100:=======>                                                 (1 + 1) / 8]
[Stage 100:==============>                                          (2 + 1) / 8]
[Stage 100:=====================>                                   (3 + 1) / 8]
[Stage 100:============================>                            (4 + 1) / 8]
[Stage 100:===================================>                     (5 + 1) / 8]
Your submission token id is 834910-9d13a2bc6876b61d387f4e23040c90ce:aba7d2e08c5b0aa46b993d28bc348a24:ip-172-31-36-188
Please include this submission token id when you need support for your code submission. Hi,

I have so far the tests passed until 2e in Lab 3.  The instruction says '...each token maps to the token's frequency times the token's IDF weight'  Can I assume the following:

tfidf = tf (how many times a word occurs in recb000hkgj8k) x idf weight(the value of weight for that word in idfsSmallWeights)

When I examine the tokens in recb000hkgj8k, there are 2 occurences of 'autocad' and I assume tf = 2.0.  When I examine the weight for 'autocad' in idfsSmallWeights, it has a value of 200.0.

So, if I multiply these 2, I get 400.0 instead of 33.33333333333333 as follow:

Amazon record "b000hkgj8k" has tokens and weights:
{'autocad': 400.0, 'autodesk': 400.0, 'courseware': 400.0, 'psg': 400.0, '2007': 400.0, 'customizing': 400.0, 'interface': 400.0}

What did I miss?

Some online resources showed the use of log(),

Thanks. In the line:

DATAFILE_PATTERN = '^(.+),"(.+)",(.*),(.*),(.*)'

We exclude quotes for the second of five fields only (the product name field), but leave them in the captured strings everywhere else they occur. That seems pretty random. Is there a reason for that? Hi guys, I submit 3 times, and wait for 5 hours without any result. I know I misuse collect(). How can I cancel previous submissions or must wait util feedback returned? Thank you Hello, 
I'm stcuk in 4c . I develop code like that :

badEndpointsCountPairTuple = badRecords.map(lambda x: (x.endpoint, 1))
#print badEndpointsCountPairTuple.collect()badEndpointsSum = badEndpointsCountPairTuple.map(lambda x:(x[0],len(set(x[0])))).groupByKey()#print badEndpointsSum.collect()badEndpointsTop20 = badEndpointsCountPairTuple.takeOrdered(20, lambda s: -s[1])

But the result is this. can anyone help me?
(u'/shuttle/resources/orbiters/discovery.gif', 1), (u'/pub/winvn/release.txt', 1) What the exercise is telling us to do? still don't get it

For each ID in a dataset, tokenize the values, and then count the total number of tokens.
How many tokens, total, are there in the two datasets? Hi,

I just made my third attemp to lab3 , all tests are passing on my VM . I makde sure that i am not using any extra collect() and there is no white space.
What should i use other than collectAsMap() ? 

I am using  in 2f (which is already in code) , 4b (i added it to get idfsFullWeights) , 4c (for amazonNormsBroadcast & googleNormsBroadcast ) , 4f amazonWeightsBroadcast & googleWeightsBroadcast). What should i do instead of it ?

Thanks. For those getting the wrong number on total words:
- pay attention that your function lab3 1b is removing all the occurrences of stopwords.  

My function was returning only the first occurrence. That makes it pass through the assertions, but you won't get the right result on 1c 

Silly mistake, but might be useful for someone else. Hi! I'm new to Python. For 2c, I tried

def idfs(corpus):
    <REDACTED>
But I get 
File "<ipython-input-177-8ac029b55292>", line 15
    idfsSmall = idfs(amazonRecToToken.union(googleRecToToken))
            ^
SyntaxError: invalid syntax
I am stumped. I don't know what I did wrong. Any clues? Thanks in advance! What is dailyHosts meant to contain? 1.A tupel of (day, uniques host names)? 2.Or a tupel of (day, unique count of host names in that day)?Any help would be highly appreciated have been on this for sometime now. I've been beating my head against the wall over this one.  Nothing in the API seems to do this. Hi ,

I have been trying lab 3 since last 2 days. I have all my test passing on VM but all of sudden it breaks in autograder. Also response from autograder is not something that we can look at and start debuggung. Out of Memory error is generally a nightmare for developers and even while looking at full trace , one do not get the error.

Hence I would request TA & professor to help us in getting clear output of autograder as well as please increase number of allowed submissions.


Thank you.
 I keep running into the below error. I have performed vagrant halt, Up, reload a number of times. I have 8 GB of RAM and Windows 7 Professional.
Traceback (most recent call last):
  File "/usr/lib/python2.7/SocketServer.py", line 295, in _handle_request_noblock
ERROR:py4j.java_gateway:Error while sending or receiving.
Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 479, in send_command
    raise Py4JError("Answer from Java side is empty")
Py4JError: Answer from Java side is empty
    self.process_request(request, client_address)
  File "/usr/lib/python2.7/SocketServer.py", line 321, in process_request
    self.finish_request(request, client_address)
  File "/usr/lib/python2.7/SocketServer.py", line 334, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File "/usr/lib/python2.7/SocketServer.py", line 649, in __init__
    self.handle()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/accumulators.py", line 231, in handle
    num_updates = read_int(self.rfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 528, in read_int
    raise EOFError
EOFError
ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server Hi all,

I am trying to understand things step by step. Example..

filteredLogs = access_logs.filter(lambda log:log.response_code>=300 or log.response_code<=199)
print #print type(filteredLogs)print
print filteredLogs.take(1)
print print '---------------'print

endpointCounts = (access_logs .map(lambda log: (log.endpoint, 1)) .reduceByKey(lambda a, b : a + b))
print print '------------------------'print
endPoints = filteredLogs.map(lambda log: (log.endpoint,1))
print endPoints.take(1)
uniqueEP = endPoints.reduceByKey(lambda x,y:x+y)
print uniqueEP.take(1)
ep_rc_Map = filteredLogs.map(lambda log: (log.endpoint,log.response_code))
#print type(ep_rc_Map)print ep_rc_Map.take(2)not200 = ep_rc_Mapecpt = not200.map(lambda x:(x,1))
print type(not200)print not200.take(2)print type(ecpt)print ecpt.take(2)
endpointCountPairTuple = ecpt
endPointSum = endpointCountPairTuple.reduceByKey(lambda x,y:x+y)
print endPointSum.count()


I am wrong at this point, but looking more for methodology or setting up IDE for spark-submit, etc.

That is the point of all this, to learn how to move from step to step eventually in real world.

All suggestions are greatly appreciated. All,

I'm stuck in lab 3a.

First I do 

not200 = access_logs.map(lambda log: (log.endpoint,log.response_code)).filter(lambda s: s[0] != 200)

which returns me the pair log.endpoint,log.response_code correctly.

Now how do I count the number of 404, 304, etc?

That is the idea right? Any hints on calculating N?  We are using the corpus we previously constructed and we didn't put indicator variables in each rdd, so I'm kinda stumped on this one? The answer is 2 right? I'm not exactly clear on the description. Each rdd in our union is a document? Or is each pair a document.

Can somebody tell me in English what a 'document' means in this context. Trying to optimize by avoiding the use of groupByKey(), which is taking 2-3 minutes on 4e. Is there a better way (e.g., aggregateByKey)? Is there a function for reduceByKey to group by partition? Thanks in advance. here <Fill IN> are only at two lines

<FILL IN> return <FILL IN>

how to populate dict in just 1 mine ?  EDIT: maybe I should wait until I finish lecture 8 and the lab before posting this...

I suspect the answer is "depends on the domain and data sets", but I'm wondering what the practicing data scientists do to resolve the problem of "different definitions" where an account id could reference an individual, household, family, or something else.  Address normalization is the only example I can think of, where there is a convenient source of truth: the regional post office.

Tangent: I really appreciate the theme of lecture 7.  I often worry how the trend towards analytics might be used to make ill-informed decisions.
 Hi!

All lab 3 tests passed locally with no problem, but when submitting to autograder I've got a problem -
Your submission token id is 837609-050980580d06a7a3d723dd3e1e7a0e35:7278c93068e1c33401dd1b288a5ef3c0:ip-172-31-36-186
Please include this submission token id when you need support for your code submission.
So, what is the problem? Can any one please give hint regarding what function to use ?  To any one who might help,

I tried 3 submission without any success on Lab3.
Here is my token id:

Your submission token id is 837434-0792ccbedb83a9cf179b01e8928f3d43:2219d75e89e4cb5febc2d01bd172e256:ip-172-31-36-186

I rerun every cell and get all tests passed, not sure what to do next.

Thank you.
ISARA S. 2a and 2c passes which means the code for tf and idf calculation is correct. But if I do the product of tf and idfs and reuse the methods created in 2a and 2c, the answer is incorect. Am I missing any step ? The autograder seems to be working on my sumbission for at least the past 4 hours.

Could you check, please? Here to calculate uniqueTokens in list list and set, is there any direct function in pyspark to do it ?  In this tow section are are not suppose to do anything right ? Hello, 
For host calculation my solution different from the test result. Is there any one help me find the bug in my code

errHostsCountPairTuple = <REDACTED>POSTING SOLUTIONS IS A VIOLATION OF THE HONOR CODE

I know it s same for  4c I used the same code but result is different from the test result
I also try this code in below:
errHostsCountPairTuple = <FILL IN>
 Here to fill up 3 section of code , are people just filling in one line ? Here is the result I got which is a bit different from the expected. I am kind of having no idea how to make correction.

Top hours for 404 requests: [(0, 174), (1, 171), (2, 409), (3, 266), (4, 102), (5, 95), (6, 93), (7, 119), (8, 199), (9, 184), (10, 326), (11, 261), (12, 421), (13, 392), (14, 311), (15, 335), (16, 366), (17, 325), (18, 262), (19, 269), (20, 266), (21, 239), (22, 233), (23, 267)] I am new to python. How to split with "/" and "." as delimiters? Thanks On a fourth attempt, the autograder returned some results, but the two errors it returns appear to have nothing to do with my code.  Everything else passes (including subsequent tests (e.g., 4f), and everything passes locally, so I suspect this is an autograder issue...

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

All tests passed


Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/broadcast.py", line 106, in value
    self._value = self.load(self._path)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/broadcast.py", line 87, in load
    with open(path, 'rb', 1 << 20) as f:
IOError: [Errno 2] No such file or directory: u'/tmp/spark-9ea7de2b-06ed-4546-936a-570a6208302d/pyspark-bfa36ecb-efbc-4ea4-89bb-1984e50b5c62/tmpq33dBy'

Your submission token id is 840190-1f6faceb7a9c9945317b329b6ec1881c:956d674b144cc77b7bf6caa68c508d5f:ip-172-31-36-186
Please include this submission token id when you need support for your code submission.
 My previous post led to some confusion so I'm reposting hopefully with correct jargon.

Disclosure:  I've checked the PySpark API documentation on RDDs.  I've printed it out and have read it.  It doesn't cover concatenating / extending RDD values.

One way to get the unique tokens is to combine all values in the corpus RDD.  The python extend function does this.  When I try to use

corpus.values().reduce(extend)

I get the error

NameError: global name 'extend' is not defined

We were cautioned against using other libraries, but I thought extend was part of basic Python.

I've also seen posts where others describe using flatMap, followed by map to get an RDD with the tokens as keys and then groupByKey, but this seems convoluted.  Shouldn't we just concatenate (extend) all the values in the corpus RDD to get unique tokens?
 Just submitted Lab 3 and autograder reported my solution as incorrect. There are no error messages, how do I interpret the autograder's comments and results ? How do I find what went wrong? Solutions are running fine in notebook.

Saw professor's comment about collect() in another thread, there are 7 collect() usage in original lab file and my submitted file has same usage count.

Your submission token id is 839938-9834e6770b53e12163d54ad120a2dc57:b31016c5d9cd2cfc8552a22f5fdc6a9b:ip-172-31-36-188

Tokenize a String (1a)
----------------------

[Stage 68:======>                                                  (2 + 1) / 17]
[Stage 68:==========>                                              (3 + 1) / 17]
[Stage 68:=============>                                           (4 + 1) / 17]
[Stage 68:================>                                        (5 + 1) / 17]
[Stage 68:====================>                                    (6 + 1) / 17]
[Stage 68:=======================>                                 (7 + 1) / 17]
[Stage 68:==========================>                              (8 + 1) / 17]
[Stage 68:==============================>                          (9 + 1) / 17]
[Stage 68:================================>                       (10 + 1) / 17]
[Stage 68:====================================>                   (11 + 1) / 17]
[Stage 68:=======================================>                (12 + 1) / 17]
[Stage 68:==========================================>             (13 + 1) / 17]
[Stage 68:==============================================>         (14 + 1) / 17]
[Stage 68:=================================================>      (15 + 1) / 17]
[Stage 68:====================================================>   (16 + 1) / 17]
[Stage 69:=============>                                           (4 + 1) / 17]
[Stage 69:====================>                                    (6 + 1) / 17]
[Stage 69:=======================>                                 (7 + 1) / 17]
[Stage 69:==========================>                              (8 + 1) / 17]
[Stage 69:==============================>                          (9 + 1) / 17]
[Stage 69:====================================>                   (11 + 1) / 17]
[Stage 69:==========================================>             (13 + 1) / 17]
[Stage 69:==============================================>         (14 + 1) / 17]
[Stage 69:====================================================>   (16 + 1) / 17]

...

 Hello everyone,

I'm trying to get the cosineSimilarity function to work, but I might be missing something.

As I understand, I have to use my tokenize function to get a list of tokens, and pass it to the tfidf function, along with the idfsDictionary. I did this for both strings, and stored my results in the variables w1 and w2.

My cossim passed the test in 3b, but when I call it with w1 and w2, I get a result about 10x the expected.

#print w1{'photoshop': 22.22222222222222, 'adobe': 8.333333333333334}#print w2
{'adobe': 8.333333333333334, 'illustrator': 50.0}#print cosineSimilarity('Adobe Photoshop', 'Adobe Illustrator', idfsSmallWeights)
0.500277597875
Any thoughts?
  ("AmazonID GoogleURL", cosineSimilarityScore). An example entry from sims is: ('b000bi7uqs http://www.google.com/base/feeds/snippets/18403148885652932189', 0.40202896125621296)

Here the is space between AmazonId & GoolgeURL righT ? This applies to Lab2 and all the following labs.
I've written clunky, functioning code for Lab2.  It works but I can't believe it's the best way to solve this.
I really want to show my code to a Spark expert and ask "what's a good/elegant way to do this?"
Is there any way to accomplish this since we can't post code here?

If not, I'm afraid all I'll be learning from the labs is how to write crappy Spark code. Hi Instructor, 

Cos's busy week, I missed the deadline to submit my work on Lab2. Just wonder where I can get the answer before I start Lab 3.

Thanks,

Best Regards,
Andrew here how to create idfsFullWeights wihout using collectAsMap() since I assume that might cause OOM .

All those who got succes on autograder please reply.

Thanx If you join two RDDs and you want to create a new one with one key and one value, how would you approach it. I so far tried these without success

(rdd1.join.rdd2).map(lambda key,x,y: (key, x/y))
(rdd1.join.rdd2).map(lambda key, xy: (key, xy[0]/xy[1])

I tried all the above with reduceBykey without success. All I am getting when I print(debug) is the original joined rdd => rdd = ((key1, (val1, val2)),(key2,(val1, val2)))

What am I doing wrong?? Here for commonTokens I am using combination of join , swap , map & groupByKey(). 

Is ther any other way that do not cause OOM ? error is  "Tokenize a String (1a)"All test is passed on my local machine Here is the submission token id.  842351-b4130d56d9cabc4969c00b7aabcd843d:0928156d990cdb648e91487846e65bd2:ip-172-31-36-180 Here I am doing broadcastvariable.value to get dictionary. Then using amazonRec key in that dictionary to get one more dictionary and for that dictionary i am using elements in token to calculate s.

Is that right ?  I am having trouble transforming the inverted list paired with the ID into multiple elements for the InvPairsRDD.
I mean, I have a result like ([(K1,V1),(K2,V2)...],ID). How can I get it transformed to ((K1,V1),ID) ((K2,V2),ID) and so on?
Any ideas? I'm getting 100% tests passed locally. Whenever I try running it on the autograder I get a response like this and I'm clueless whether it's a problem in my code or an autograder issue.Tokenize a String (1a)
----------------------

[Stage 61:======>                                                  (2 + 1) / 17]
[Stage 61:==========>                                              (3 + 1) / 17]
[Stage 61:=============>                                           (4 + 1) / 17]
[Stage 61:================>                                        (5 + 1) / 17]
[Stage 61:====================>                                    (6 + 1) / 17]
[Stage 61:=======================>                                 (7 + 1) / 17]
[Stage 61:==========================>                              (8 + 1) / 17]
[Stage 61:==============================>                          (9 + 1) / 17]
[Stage 61:================================>                       (10 + 1) / 17]
[Stage 61:====================================>                   (11 + 1) / 17]
[Stage 61:=======================================>                (12 + 1) / 17]
[Stage 61:==========================================>             (13 + 1) / 17]
[Stage 61:==============================================>         (14 + 1) / 17]
[Stage 61:=================================================>      (15 + 1) / 17]
[Stage 61:====================================================>   (16 + 1) / 17]
[Stage 62:================>                                        (5 + 1) / 17]
[Stage 62:=======================>                                 (7 + 1) / 17]
[Stage 62:==========================>                              (8 + 1) / 17]
[Stage 62:================================>                       (10 + 1) / 17]
[Stage 62:=======================================>                (12 + 1) / 17]
[Stage 62:==========================================>             (13 + 1) / 17]
[Stage 62:==============================================>         (14 + 2) / 17]
[Stage 62:====================================================>   (16 + 1) / 17]
                                                                                

[Stage 64:================>                                        (5 + 1) / 17]
[Stage 64:=======================>                                 (7 + 1) / 17]
[Stage 64:==============================>                          (9 + 1) / 17]
[Stage 64:================================>                       (10 + 1) / 17]
[Stage 64:=======================================>                (12 + 1) / 17]
[Stage 64:==============================================>         (14 + 1) / 17]
[Stage 64:=================================================>      (15 + 1) / 17]
                                                                                

[Stage 66:=>                                                       (1 + 1) / 33]
[Stage 66:===>                                                     (2 + 1) / 33]
[Stage 66:=====>                                                   (3 + 1) / 33]
[Stage 66:======>                                                  (4 + 1) / 33]
[Stage 66:========>                                                (5 + 1) / 33]
[Stage 66:==========>                                              (6 + 1) / 33]
[Stage 66:============>                                            (7 + 1) / 33]
[Stage 66:=============>                                           (8 + 1) / 33]
[Stage 66:=============>                                           (8 + 2) / 33]
[Stage 66:===============>                                         (9 + 1) / 33]
[Stage 66:================>                                       (10 + 1) / 33]
[Stage 66:==================>                                     (11 + 1) / 33]
[Stage 66:====================>                                   (12 + 1) / 33]
[Stage 66:======================>                                 (13 + 1) / 33]
[Stage 66:=======================>                                (14 + 1) / 33]
[Stage 66:=========================>                              (15 + 1) / 33]
[Stage 66:===========================>                            (16 + 1) / 33]
[Stage 66:===========================>                            (16 + 2) / 33]
[Stage 66:==============================>                         (18 + 1) / 33]
[Stage 66:================================>                       (19 + 1) / 33]
[Stage 66:=================================>                      (20 + 1) / 33]
[Stage 66:===================================>                    (21 + 1) / 33]
[Stage 66:=====================================>                  (22 + 1) / 33]
[Stage 66:=======================================>                (23 + 1) / 33]
[Stage 66:========================================>               (24 + 1) / 33]
[Stage 66:==========================================>             (25 + 1) / 33]
[Stage 66:============================================>           (26 + 1) / 33]
[Stage 66:=============================================>          (27 + 1) / 33]
[Stage 66:===============================================>        (28 + 1) / 33]
[Stage 66:=================================================>      (29 + 1) / 33]
[Stage 66:==================================================>     (30 + 1) / 33]
[Stage 66:====================================================>   (31 + 1) / 33]
[Stage 66:======================================================> (32 + 1) / 33]
[Stage 67:========>                                                (5 + 1) / 33]
[Stage 67:============>                                            (7 + 1) / 33]
[Stage 67:=============>                                           (8 + 1) / 33]
[Stage 67:================>                                       (10 + 1) / 33]
[Stage 67:==================>                                     (11 + 1) / 33]
[Stage 67:======================>                                 (13 + 1) / 33]
[Stage 67:=======================>                                (14 + 1) / 33]
[Stage 67:===========================>                            (16 + 1) / 33]
[Stage 67:==============================>                         (18 + 1) / 33]
[Stage 67:=================================>                      (20 + 1) / 33]
[Stage 67:===================================>                    (21 + 2) / 33]
[Stage 67:========================================>               (24 + 1) / 33]
[Stage 67:============================================>           (26 + 1) / 33]
[Stage 67:=============================================>          (27 + 1) / 33]
[Stage 67:=================================================>      (29 + 1) / 33]
[Stage 67:==================================================>     (30 + 1) / 33]
[Stage 67:======================================================> (32 + 1) / 33]
                                                                                

[Stage 70:========>                                                (5 + 1) / 33]
[Stage 70:============>                                            (7 + 1) / 33]
[Stage 70:===============>                                         (9 + 1) / 33]
[Stage 70:==================>                                     (11 + 1) / 33]
[Stage 70:====================>                                   (12 + 1) / 33]
[Stage 70:=======================>                                (14 + 1) / 33]
[Stage 70:=========================>                              (15 + 1) / 33]
[Stage 70:============================>                           (17 + 1) / 33]
[Stage 70:================================>                       (19 + 1) / 33]
[Stage 70:=================================>                      (20 + 1) / 33]
[Stage 70:=====================================>                  (22 + 1) / 33]
[Stage 70:=======================================>                (23 + 1) / 33]
[Stage 70:==========================================>             (25 + 1) / 33]
[Stage 70:============================================>           (26 + 1) / 33]
[Stage 70:===============================================>        (28 + 1) / 33]
[Stage 70:=================================================>      (29 + 1) / 33]
[Stage 70:====================================================>   (31 + 1) / 33]
                                                                                

[Stage 71:=============================>                            (2 + 1) / 4]
[Stage 71:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 73:=============================>                            (2 + 1) / 4]
[Stage 73:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 75:=======>                                                  (1 + 1) / 8]
[Stage 75:==============>                                           (2 + 1) / 8]
[Stage 75:=====================>                                    (3 + 1) / 8]
[Stage 75:=============================>                            (4 + 1) / 8]
[Stage 75:====================================>                     (5 + 1) / 8]
[Stage 75:===========================================>              (6 + 1) / 8]
[Stage 75:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 76:=======>                                                  (1 + 1) / 8]
[Stage 76:==============>                                           (2 + 1) / 8]
[Stage 76:=====================>                                    (3 + 1) / 8]
[Stage 76:=============================>                            (4 + 1) / 8]
[Stage 76:====================================>                     (5 + 1) / 8]
[Stage 76:===========================================>              (6 + 1) / 8]
[Stage 76:==================================================>       (7 + 1) / 8]
[Stage 77:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 79:====================================>                     (5 + 1) / 8]
                                                                                

[Stage 80:==============>                                           (1 + 1) / 4]
[Stage 80:=============================>                            (2 + 1) / 4]
[Stage 80:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 81:=============================>                            (2 + 1) / 4]
[Stage 81:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 82:=============================>                            (2 + 1) / 4]
[Stage 82:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 83:==============>                                           (1 + 1) / 4]
[Stage 83:=============================>                            (2 + 1) / 4]
[Stage 83:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 84:==============>                                           (1 + 1) / 4]
[Stage 84:=============================>                            (2 + 1) / 4]
[Stage 84:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 85:=============================>                            (2 + 1) / 4]
[Stage 85:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 86:==============>                                           (1 + 1) / 4]
[Stage 86:=============================>                            (2 + 1) / 4]
[Stage 86:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 87:==============>                                           (1 + 1) / 4]
[Stage 87:=============================>                            (2 + 1) / 4]
[Stage 87:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 90:>                                                         (0 + 1) / 8]
[Stage 90:=======>                                                  (1 + 1) / 8]
[Stage 90:==============>                                           (2 + 1) / 8]
[Stage 90:=====================>                                    (3 + 1) / 8]
[Stage 90:=============================>                            (4 + 1) / 8]
[Stage 90:====================================>                     (5 + 1) / 8]
[Stage 90:===========================================>              (6 + 1) / 8]
[Stage 90:==================================================>       (7 + 1) / 8]
[Stage 91:>                                                         (0 + 1) / 8]
[Stage 91:=======>                                                  (1 + 1) / 8]
[Stage 91:==============>                                           (2 + 1) / 8]
[Stage 91:=====================>                                    (3 + 1) / 8]
[Stage 91:=============================>                            (4 + 1) / 8]
[Stage 91:====================================>                     (5 + 1) / 8]
[Stage 91:===========================================>              (6 + 1) / 8]
[Stage 91:==================================================>       (7 + 1) / 8]
[Stage 92:>                                                         (0 + 1) / 8]
[Stage 92:=======>                                                  (1 + 1) / 8]
[Stage 92:==============>                                           (2 + 1) / 8]
[Stage 92:=====================>                                    (3 + 1) / 8]
[Stage 92:=============================>                            (4 + 1) / 8]
[Stage 92:====================================>                     (5 + 1) / 8]
[Stage 92:===========================================>              (6 + 1) / 8]
[Stage 92:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 95:>                                                         (0 + 1) / 8]
[Stage 95:=======>                                                  (1 + 1) / 8]
[Stage 95:==============>                                           (2 + 1) / 8]
[Stage 95:=====================>                                    (3 + 1) / 8]
[Stage 95:=============================>                            (4 + 1) / 8]
[Stage 95:====================================>                     (5 + 1) / 8]
[Stage 95:===========================================>              (6 + 1) / 8]
[Stage 95:==================================================>       (7 + 1) / 8]
                                                                                

[Stage 96:==============>                                           (1 + 1) / 4]
[Stage 96:=============================>                            (2 + 1) / 4]
[Stage 96:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 97:==============>                                           (1 + 2) / 4]
[Stage 97:=============================>                            (2 + 1) / 4]
[Stage 97:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 100:>                                                        (0 + 1) / 8]
Your submission token id is 843611-bdedb42cee3b0068bced3a83fa702f7e:fc61fd175fc28b5470c79b8e5993ff84:ip-172-31-39-80
Please include this submission token id when you need support for your code submission. It looks like we can avoid broadcasting the mappings from each record to its weight vector (i.e., amazonWeightsRDD and googleWeightsRDD) by including the Amazon and Google tf-idf weights for each token in the inverted index.

That is, we can create inverted indices:
amazonInvPairsRDD = [(token, (id, tf-idf of token in document id)]
googleInvPairsRDD = [(token, (url, tf-idf of token in document url)]

Then, in the three steps of (4e), rather than finding the list of common tokens for each pair (id, url), we can directly compute the dot product of the weight vectors for the two documents by:

Using the two inverted indices, create an RDD of pairs:
(token, ((id, tf-idf of token in id), (url, tf-idf of token in url)))
Creating a new "swapped" RDD, in which we multiply the tf-idfs together:
((id, url), (tf-idf of token in id)*(tf-idf of token in url))
And finally outputting an RDD mapping each pair of documents with overlapping tokens to the dot product of their weight vectors:
((id, url), dotprod(amazonWeight[id], googleWeight[url]))


Are there reasons to prefer the approach in Lab 3 -- i.e., that is, collecting and broadcasting amazonWeightsRDD and googleWeightsRDD? All test passed in my machine but got an exception during submission.  Please Help. 

Tokenize a String (1a)
----------------------
----------------------------------------
Exception happened during processing of request from ('127.0.0.1', 37109)
Your submission token id is 843703-d9a5270f49ae984899bba3f6e7f6f819:b9ad45e36e120ecd35d4934ddfafe2d5:ip-172-31-39-22
Please include this submission token id when you need support for your code submission. How to proceed from this point? First attempt upload was successful with 87% while attempting some change in 3f question, it failed completely.Data cleaning (1c)
------------------
----------------------------------------
Exception happened during processing of request from ('127.0.0.1', 34329)
----------------------------------------
Your submission token id is 844044-37b8a9c41bd3ea5fd796d8a0f9fc11b9:005262137395e4a838b9a93a59a8166d:ip-172-31-39-81
Please include this submission token id when you need support for your code submission.
 I think I'm a bit confused on how broadcast variables work.  In 4f, step 1 says to create a dictionary of tokens to weights for each of the two datasets, and make that a broadcast variable.  

I think that went fine, however, apparently you can't get values from broadcast dictionaries the same way you get them from ordinary dictionaries.  

In implementing the fastCosineSimilarity function, I tried to get the sum of the product of a token's weight for amazon or for google by simply accessing it through the dictionary key.  And I think that's the part that's throwing an error, because I get: 
TypeError: 'Broadcast' object has no attribute '__getitem__'
in the usual place in the long block of error messages where the real errors appear so far in our labs.  

Am I misunderstanding how to access elements of a broadcast dictionary?  Or have I messed something else up?  Aieee... thanks. Vagrant was working fine for me until the last time I used my laptop. Today, I started everything as usual, but when I fire up the ipython notebooks the VM cannot import any spark-related libraries.

This is on Ubuntu 14.04. 
To run the VM, I always cd to `/macro/myvagrant` and run `vagrant up`.
The message I got today was: 

==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Clearing any previously set forwarded ports...==> sparkvm: Clearing any previously set network interfaces...==> sparkvm: Preparing network interfaces based on configuration... sparkvm: Adapter 1: nat==> sparkvm: Forwarding ports... sparkvm: 8001 => 8001 (adapter 1) sparkvm: 4040 => 4040 (adapter 1) sparkvm: 22 => 2222 (adapter 1)==> sparkvm: Booting VM...==> sparkvm: Waiting for machine to boot. This may take a few minutes... sparkvm: SSH address: 127.0.0.1:2222 sparkvm: SSH username: vagrant sparkvm: SSH auth method: private key sparkvm: Warning: Connection timeout. Retrying... sparkvm: Warning: Connection timeout. Retrying... sparkvm: Warning: Remote connection disconnect. Retrying... sparkvm: Warning: Remote connection disconnect. Retrying...==> sparkvm: Machine booted and ready!==> sparkvm: Checking for guest additions in VM...==> sparkvm: Setting hostname...==> sparkvm: Mounting shared folders... sparkvm: /vagrant => /home/nikos/macro/myvagrant==> sparkvm: Machine already provisioned. Run `vagrant provision` or use the `--provision`==> sparkvm: to force provisioning. Provisioners marked to run always will still run.

The message I get from within the ipython notebook is: 

ImportError: No module named pyspark.sql

or whatever else it is I'm trying to import. I am thinking of removing vagrant and installing from scratch, unless someone comes up with a quick suggestion.

to anyone reading this and thinking of doing the same MAKE SURE YOU BACKUP THE NOTEBOOKS WITH YOUR WORK BEFORE REMOVING VAGRANT!

Thanks!

Nikos def parse_apache_time(s): """ Convert Apache time format into a Python datetime object Args: s (str): date and time in Apache time format Returns: datetime: datetime object (ignore timezone for now) """ return datetime.datetime(int(s[7:11]), month_map[s[3:6]], int(s[0:2]), int(s[12:14]), int(s[15:17]), int(s[18:20]))
please help me to understand the code in bold .   It seems I need to write a lambda function to create all possible combinations of 2 lists. Am I heading the wrong direction on this, or does this seem reasonable?

Thanks a lot,
Pat
 I am stuck in Lab 3 2c. My code returns Adobe as the smallest IDF token.Here is my implementation:
from operator import add
def idfs(corpus):
    """ Compute IDF
    Args:
        corpus (RDD): input corpus
    Returns:
        RDD: a RDD of (token, IDF value)
    """
    N = count of elements in corpus # 400

    uniqueTokens = flattening out the values of corpus # ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', ....]

    tokenCountPairTuple = RDD containing a token and 1 # [('clickart', 1), ('950', 1), ('000', 1), ('premier', 1), ('image', 1), ('pack', 1), ('dvd', 1), ('rom', 1) ...]

    tokenSumPairTuple = reducing by key to have total counts of a token # [('dance', 1), ('breath', 2), ('themes', 3), ('known', 7), ...]

    idf_values = compute IDF values using mapValues and (lambda x: N/x)
    return idf_values
 I have wasted more than a day in last lab and now it happened again, notebook stopped detecting the previous code in the file. Is there a easy way to fix this issue? I have already rebooted machine/VM few times, deleted and re added the notebook and still no luck.  def parseApacheLogLine(logline): """ Parse a line in the Apache Common Log format Args: logline (str): a line of text in the Apache Common Log format Returns: tuple: either a dictionary containing the parts of the Apache Access Log and 1, or the original invalid log line and 0 """ match = re.search(APACHE_ACCESS_LOG_PATTERN, logline) if match is None: return (logline, 0) size_field = match.group(9) if size_field == '-': size = long(0) else: size = long(match.group(9)) return (Row( host = match.group(1), client_identd = match.group(2), user_id = match.group(3), date_time = parse_apache_time(match.group(4)), method = match.group(5), endpoint = match.group(6), protocol = match.group(7), response_code = int(match.group(8)), content_size = size ), 1)
what is the above code (in bold) is actually doing ? confused . help me out of this  Does anyone have any bright ideas for debugging a wrong value in 4f? I've got all the code running right, everything passed every upstream test... and I keep getting the value 3.45526462763e-05 instead of the tested result 4.286548414e-06. I've double-checked to make sure every upstream calculation used floats, so there's no integer division bug.  I've produced a verbose version of fastCosineSimilarity that prints out all its intermediate values, and nothing looks weird/no None values or anything freakish like that.  

So I'm forced to suspect that one of the dozens of calculations over the last 12 hours straight was incorrect in some way that slipped by one of the tests.  But how to figure out which?  There has to be a way to debug this that doesn't involve just doing the lab over...  for this line:recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1]
should the indexes "[0][1]" be crossed out to get a meaningful run?
 Hello, Big Data World!

I've been enjoying this course so far, but I'm never sure of where an operation will run or where the results are sent to.

I don't know if I have missed something, or if TFMs (the fine manuals) are not so fine...For instance, collect() according to the pySpark package documentation, "Return a list that contains all of the elements in this RDD"... but it fails to mention that it will return it to the driver program.So, is there any way to know what operations run where? Or even better, is there any way to "trace" the execution and/or dispatching of code and data among driver program and its nodes?Thank you! Hi,

 I have got below output from Auto grader. I am not able to figure out which test cases failed. I see from the course progress page that Lab 2 has no score yet. Can someone please help ? To add, all test cases are working fine in local VM..

Data cleaning (1c)
------------------

[Stage 0:>                                                          (0 + 0) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:============================================>              (3 + 1) / 4]
                                                                                

[Stage 1:>                                                          (0 + 1) / 4]
[Stage 1:==============>                                            (1 + 1) / 4]
[Stage 1:=============================>                             (2 + 1) / 4]
[Stage 1:============================================>              (3 + 1) / 4]
                                                                                

[Stage 2:>                                                          (0 + 1) / 4]
[Stage 2:==============>                                            (1 + 1) / 4]
[Stage 2:=============================>                             (2 + 1) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
                                                                                

[Stage 3:>                                                          (0 + 1) / 4]
[Stage 3:==============>                                            (1 + 1) / 4]
[Stage 3:=============================>                             (2 + 1) / 4]
[Stage 3:============================================>              (3 + 1) / 4]
                                                                                

[Stage 4:>                                                          (0 + 1) / 4]
[Stage 4:==============>                                            (1 + 1) / 4]
[Stage 4:=============================>                             (2 + 1) / 4]
[Stage 4:============================================>              (3 + 1) / 4]
                                                                                

[Stage 5:>                                                          (0 + 1) / 4]
[Stage 5:==============>                                            (1 + 1) / 4]
[Stage 5:=============================>                             (2 + 1) / 4]
[Stage 5:============================================>              (3 + 1) / 4]
                                                                                

[Stage 6:>                                                          (0 + 1) / 4]
[Stage 6:==============>                                            (1 + 1) / 4]
[Stage 6:=============================>                             (2 + 1) / 4]
[Stage 6:============================================>              (3 + 1) / 4]
                                                                                

[Stage 7:>                                                          (0 + 1) / 4]
[Stage 7:==============>                                            (1 + 1) / 4]
[Stage 7:=============================>                             (2 + 1) / 4]
[Stage 7:============================================>              (3 + 1) / 4]
                                                                                

[Stage 8:>                                                          (0 + 1) / 4]
[Stage 8:==============>                                            (1 + 1) / 4]
[Stage 8:=============================>                             (2 + 1) / 4]
[Stage 8:============================================>              (3 + 1) / 4]
                                                                                

[Stage 9:============================================>              (3 + 1) / 4]
                                                                                

[Stage 10:==============>                                           (1 + 1) / 4]
[Stage 10:=============================>                            (2 + 1) / 4]
[Stage 10:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 11:==============>                                           (1 + 1) / 4]
[Stage 11:=============================>                            (2 + 1) / 4]
                                                                                

[Stage 12:>                                                         (0 + 1) / 4]
[Stage 12:==============>                                           (1 + 1) / 4]
Your submission token id is 837561-5a16ad15068fbd364ac27b30631c1001:c6f50e66d91f0d21017009e1f177997e:ip-172-31-39-81
Please include this submission token id when you need support for your code submission. I have completed and sucessfully passed both lab 1 and 2.

Some of the exercises in lab 3 are just too difficult despite spending hours and also reading through the postings in piazza. Can i throw in the towel for lab 3 by not completing some of the exercises? Can the autograder just mark those exercises with <fill in> as incomplete instead of complaining and refusing to grade the exercises?

 My individual test for all the code sections passed but still my code submission is failing.  What could I be missing ? 
All the variable it is saying not defined are well defined I made sure of it.

I am seeing the following output:-

Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 28, in 
NameError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
name 'dayAndHostTuple' is not defined

All tests passed
Top ten error endpoints (3a)
----------------------------
All tests passed
Number of unique hosts (3b)
---------------------------
All tests passed
Number of unique daily hosts (3c)
---------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dailyHosts' is not defined

Visualizing unique daily hosts (3d)
-----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithHosts' is not defined

Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'avgDailyReqPerHostList' is not defined

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithAvg' is not defined

Counting 404 (4a)
-----------------
All tests passed
Listing 404 records (4b)
------------------------
All tests passed
Top twenty 404 URLs (4c)
------------------------
All tests passed
Top twenty-five 404 response code hosts (4d)
--------------------------------------------
All tests passed
404 response codes per day (4e)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'errByDate' is not defined

Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithErrors404' is not defined

Five dates for 404 requests (4g)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'topErrDate' is not defined

Hourly 404 response codes (4h)
------------------------------
All tests passed
Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
All tests passed
-- 9 cases passed (56.0%) --


Your submission token id is 848348-fa55224fb1a06977f9bf13504b5ec850:746d1052ce40878a637ab15cc130cd37:ip-172-31-39-151
Please include this submission token id when you need support for your code submission.

 I completed Lab 3 last night with all tests passing correctly on my machine. Running all cells of Lab 3 does take a few minutes.

After uploading it to the autograder I never got an answer. Only the message "Your files have been submitted" appears. Unsure what happened, so I uploaded the file again this morning and the result is the same, except
that now it states "used 2 of 10 submissions".

Some status feedback on the auto grading process would be highly appreciated. Any thoughts???

Update:
Resubmitted by lab3 this morning. Now I get a timeout response with the following token id. Is there some way to get a detailed report using the token at what stage the timeout happened?

I would really like to know where my solution (that runs perfect on my local machine) timed out, but for now I don't have more time. I've got a full-time job to attend to.

Your submission token id is 850413-fb6fa509f0ac37ddee71bb58587a9c16:9c3799d4854cdb95d24759872814a13e:ip-172-31-39-156 I spent quite some time on lab 3, 4e which requires to produce the commonTokens-  a map of (ID, URL) to the list of tokens that those two documents have in common.

Part of the difficulty for me was I think a particular word in the instructions that I found confusing, and I am wondering if I am misunderstanding it, or is it indeed confusing. I am referring to the "iterable" part below, in the quote from the notebook:
"Using the two inverted indicies (RDDs where each element is a pair of a token and an ID or URL that contains that token), create a new RDD that contains only tokens that appear in both datasets. This will yield an RDD of pairs of (token, iterable(ID, URL))"

I think the proper way to describe it is to change the last sentence to something more generic such as "This will yeild an RDD of (token, (ID, URL)) pairs"


One I stopped worrying the "iterable" instruction in step 1 of this exercise, I was able to solve it, and my work was accepted by the grader.

Can the instructors clarify why the word "iterable" there?
Thank you! A little help here :
 
 Exactly what am I supposed to broadcast? The amazonWeightsRDD is (ID,dictionary_of_tokens_and_their_values) format so are we supposed to make it into an RDD of ({token1:value1}....so on) and then broadcast it? 

I have used collectAsMap() to broadcast the RDDs as dictionaries and when I run the cell I get the following error. What might be the cause of the error? 

---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-52-9558261ccd87> in <module>()
     19 
     20 similaritiesFullRDD = (commonTokens
---> 21                        .map(fastCosineSimilarity)
     22                        .cache())
     23 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in cache(self)
    189         """
    190         self.is_cached = True
--> 191         self.persist(StorageLevel.MEMORY_ONLY_SER)
    192         return self
    193 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in persist(self, storageLevel)
    205         self.is_cached = True
    206         javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)
--> 207         self._jrdd.persist(javaStorageLevel)
    208         return self
    209 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _jrdd(self)
   2286         command = (self.func, profiler, self._prev_jrdd_deserializer,
   2287                    self._jrdd_deserializer)
-> 2288         pickled_cmd, bvars, env, includes = _prepare_for_python_RDD(self.ctx, command, self)
   2289         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(),
   2290                                              bytearray(pickled_cmd),

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _prepare_for_python_RDD(sc, command, obj)
   2204     # the serialized command will be compressed by broadcast
   2205     ser = CloudPickleSerializer()
-> 2206     pickled_command = ser.dumps(command)
   2207     if len(pickled_command) > (1 << 20):  # 1M
   2208         broadcast = sc.broadcast(pickled_command)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py in dumps(self, obj)
    409 
    410     def dumps(self, obj):
--> 411         return cloudpickle.dumps(obj, 2)
    412 
    413 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dumps(obj, protocol)
    814 
    815     cp = CloudPickler(file,protocol)
--> 816     cp.dump(obj)
    817 
    818     #print 'cloud dumped', str(obj), str(cp.modules)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dump(self, obj)
    131         self.inject_addons()
    132         try:
--> 133             return pickle.Pickler.dump(self, obj)
    134         except RuntimeError, e:
    135             if 'recursion' in e.args[0]:

/usr/lib/python2.7/pickle.pyc in dump(self, obj)
    222         if self.proto >= 2:
    223             self.write(PROTO + chr(self.proto))
--> 224         self.save(obj)
    225         self.write(STOP)
    226 

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    560         write(MARK)
    561         for element in obj:
--> 562             save(element)
    563 
    564         if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    634                 write(APPENDS)
    635             elif n:
--> 636                 save(tmp[0])
    637                 write(APPEND)
    638             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    679                 for k, v in tmp:
    680                     save(k)
--> 681                     save(v)
    682                 write(SETITEMS)
    683             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    304             reduce = getattr(obj, "__reduce_ex__", None)
    305             if reduce:
--> 306                 rv = reduce(self.proto)
    307             else:
    308                 reduce = getattr(obj, "__reduce__", None)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in __getnewargs__(self)
    169         # This method is called when attempting to pickle an RDD, which is always an error:
    170         raise Exception(
--> 171             "It appears that you are attempting to broadcast an RDD or reference an RDD from an "
    172             "action or transformation. RDD transformations and actions can only be invoked by the "
    173             "driver, not inside of other transformations; for example, "

Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.








 I submit the code, receiving no feedback. I din't include any collect() except collectAsMap for dictionary. Also I refill code with new version to ensure no format issue. Since I only have few opportunity. Could instructor help me look at my submission status? My edx userId is dynumber1. Thank you! The progress bar is in progress from yesterday (((

I am trying to resubmit twice. It's a pity that a very enjoyable and interesting course is being ruined by the autograder's inconsistent behavior.

Here's my submission history for lab 3:

1 -> Got the "stage" error report that many others have copied/pasted. Submission token id is 789954-0673f410b495e3470ae49c32d7892bb0:96f23109927bb1f20ca4eb3c6919280b:ip-172-31-47-110. I assume this is my bad, as I had forgotten a print statement (one-line output).
2 -> Removed the print statement and resubmitted, got 94%.
3 -> Fixed the error that cost me 6% and resubmitted to get full marks, no response after 1+ hours
4 -> Verified that the fix above did not introduce any collect() invocation, resubmitted, no response after 6+ hours
5 -> Verified that I have no unrequested collect() invocations in general, resubmitted, got the "stage" error again, submission token id is 845324-108d3309a03c4a82ac80f48e00d774e9:96f23109927bb1f20ca4eb3c6919280b:ip-172-31-39-22
6 -> Decided to resubmit the _exact_ file I used in 2, as I was running out of submissions and 94% is better than 0%. No response from autograder.

Can you please have a look? I submit the code for an hour, receiving no feedback. I didn't include any collect() except collectAsMap for dictionary. Also I refill code with new version to ensure no format issue. Since I only have few opportunity. Could instructor help look at my submission status? My edx userId is dynumber1. Thank you! I tried this for a long time, but I always get 0 as similarity. I used below code to compute dot product 

POSTING SOLUTIONS IS A VIOLATION OF THE HONOR CODE YOU AGREED TO IN TAKING THIS COURSE:  https://www.edx.org/edx-terms-service

Does that look correct ? Can someone please point out where am I going wrong.

Thanks Hi 

So I am in 3e lab2 and since this morning, when I try to run , the code just looses its formatting and 
there is no syntax highlight as well. 

I tried copying the notebook using one of the option, shutdown the original copy. But still no benefit

Will I loose my code if shutdown vagrant and reboot ? I have saved it as checkpoint.

Any help please ? Where can I download the log data as I don't have the VM with 10G memory? Thanks. Hi,

 I submitted lab 2 and this is the autograder output I get. I am not able to see the number of failed test cases. Just to add, I had all the tests working successfully in local VM. Also, course progress page does not show any score for lab 2.

Data cleaning (1c)
------------------

[Stage 0:>                                                          (0 + 0) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:>                                                          (0 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:==============>                                            (1 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:=============================>                             (2 + 1) / 4]
[Stage 0:============================================>              (3 + 1) / 4]
                                                                                

[Stage 1:>                                                          (0 + 1) / 4]
[Stage 1:==============>                                            (1 + 1) / 4]
[Stage 1:=============================>                             (2 + 1) / 4]
[Stage 1:============================================>              (3 + 1) / 4]
                                                                                

[Stage 2:>                                                          (0 + 1) / 4]
[Stage 2:==============>                                            (1 + 1) / 4]
[Stage 2:=============================>                             (2 + 1) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
                                                                                

[Stage 3:>                                                          (0 + 1) / 4]
[Stage 3:==============>                                            (1 + 1) / 4]
[Stage 3:=============================>                             (2 + 1) / 4]
[Stage 3:============================================>              (3 + 1) / 4]
                                                                                

[Stage 4:>                                                          (0 + 1) / 4]
[Stage 4:==============>                                            (1 + 1) / 4]
[Stage 4:=============================>                             (2 + 1) / 4]
[Stage 4:============================================>              (3 + 1) / 4]
                                                                                

[Stage 5:>                                                          (0 + 1) / 4]
[Stage 5:==============>                                            (1 + 1) / 4]
[Stage 5:=============================>                             (2 + 1) / 4]
[Stage 5:============================================>              (3 + 1) / 4]
                                                                                

[Stage 6:>                                                          (0 + 1) / 4]
[Stage 6:==============>                                            (1 + 1) / 4]
[Stage 6:=============================>                             (2 + 1) / 4]
[Stage 6:============================================>              (3 + 1) / 4]
                                                                                

[Stage 7:>                                                          (0 + 1) / 4]
[Stage 7:==============>                                            (1 + 1) / 4]
[Stage 7:=============================>                             (2 + 1) / 4]
[Stage 7:============================================>              (3 + 1) / 4]
                                                                                

[Stage 8:>                                                          (0 + 1) / 4]
[Stage 8:==============>                                            (1 + 1) / 4]
[Stage 8:=============================>                             (2 + 1) / 4]
[Stage 8:============================================>              (3 + 1) / 4]
                                                                                

[Stage 9:============================================>              (3 + 1) / 4]
                                                                                

[Stage 10:==============>                                           (1 + 1) / 4]
[Stage 10:=============================>                            (2 + 1) / 4]
[Stage 10:===========================================>              (3 + 1) / 4]
                                                                                

[Stage 11:==============>                                           (1 + 1) / 4]
[Stage 11:=============================>                            (2 + 1) / 4]
                                                                                

[Stage 12:>                                                         (0 + 1) / 4]
[Stage 12:==============>                                           (1 + 1) / 4]
Your submission token id is 837561-5a16ad15068fbd364ac27b30631c1001:c6f50e66d91f0d21017009e1f177997e:ip-172-31-39-81
Please include this submission token id when you need support for your code submission. I am getting output like this. I am getting correct result but just formatting is changed.

[('brown', 0.16666666666666666), ('lazy', 0.16666666666666666), ('jumps', 0.16666666666666666), ('fox', 0.16666666666666666), ('dog', 0.16666666666666666), ('quick', 0.16666666666666666)]


 Hi everyone,
I am stuck on this exercise. I have been searching the posts related to this question on Piazza, but I still i do not figure it out.

When I print out the results of groupedByDay and sortedByDay, I get the following respectively:

[(8, 4406), (12, 2864), (4, 4190), (16, 4340), (20, 2560), (1, 2582), (5, 2502), (9, 4317), (13, 2650), (17, 4385), (21, 4134), (22, 4456), (10, 4523), (18, 4168), (14, 4454), (6, 2537), (11, 4346), (15, 4214), (3, 3222), (19, 2550), (7, 4106)]
[(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)]
With this result, i think it is not possible to apply the formula "map(lambda a, b: (a, b[0] / b[1])).cache()" on sortedByDay to get avgDailyReqPerHost because I do not have both b[0] and b[1]. My question is how to have them both. 








 I'm having trouble with this exercise, the number of unique tokens is right but not the smallest idf token and value, in my case it is (aided,1) , and on the other hand the value in my case is 1 not and float number as I specify in the code

any clue?

 the return of my function is the following:

[('aided', 1), ('duplex', 1), ('dance', 1), ('verses', 1), ('9999', 1), ('targeted', 1), ('xml', 1), ('paris', 1), ('german', 1), ('wants', 1)]
There are 4772 unique tokens in the small datasets. Hello,

I am having issues in 2c, there are errors that are really small:






Amazon record "b000hkgj8k" has tokens and weights:
{'autocad': 33.333333333333336, 'autodesk': 8.333333333333334, 'courseware': 66.66666666666667, 'psg': 33.333333333333336, '2007': 3.5087719298245617, 'customizing': 16.666666666666668, 'interface': 3.0303030303030307}








In [21]:
















# TEST Implement a TF-IDF function (2f)
Test.assertEquals(rec_b000hkgj8k_weights,
                   {'autocad': 33.33333333333333, 'autodesk': 8.333333333333332,
                    'courseware': 66.66666666666666, 'psg': 33.33333333333333,
                    '2007': 3.5087719298245617, 'customizing': 16.666666666666664,
                    'interface': 3.0303030303030303}, 'incorrect rec_b000hkgj8k_weights')














1 test failed. incorrect rec_b000hkgj8k_weights






I have tried already adding float conversions to all of the numbers, and move the calculations around.
The differences are suspiciously small, changing the counts even by 1 element will yield very different numbers.

I have already read and try the tips from other discussion about precision here. And updated the lab file. 

Any pointers will be appreciated.

Best regards,

Oscar
 Hello I am facing issues with the 4f question:























# TEST Identify common tokens from the full dataset (4f)
similarityTest = similaritiesFullRDD.filter(lambda ((aID, gURL), cs): aID == 'b00005lzly' and gURL == 'http://www.google.com/base/feeds/snippets/13823221823254120257').collect()
Test.assertEquals(len(similarityTest), 1, 'incorrect len(similarityTest)')
print similarityTest[0][1]
Test.assertTrue(abs(similarityTest[0][1] - 4.286548414e-06) < 0.000000000001, 'incorrect similarityTest fastCosineSimilarity')
Test.assertEquals(similaritiesFullRDD.count(), 2441100, 'incorrect similaritiesFullRDD.count()')














1 test passed.
2.77132234918e-06
1 test failed. incorrect similarityTest fastCosineSimilarity
1 test passed.





The difference seems to be quite big, so I must be having an issue calculating either the norms or the common elements.

Has anyone had similar issues? How did you pinpoint where the error was?

regards,

Oscar I'm performing lab3 exercise 1c. I'm unable to understand whats the format of vendorRDD. It says "vendorRDD (RDD of (recordId, tokenizedValue))". What tokenize value mean? Uptil now I have been mapping googlesmall and amazonsmall to function defined in 1b. Then after which I count the number of elements? What am I doing wrong? Hi,

In next week, this course will be over. I was wondering whether there's other option to practice Spark programming.
E.g. uploading data, running program. Also, whether there's any defined problem set to practice spark programming.

In this module we used python for Spark programming. Can we use SparkR to do the same?

Thanks! Hello everyone,
Even after following the discussions and considering flatmap and flatmapValues, I am stuck in ques 3 part 2c. Can anyone provide me with some hint so that I could get clear about the question.
I did the following:
I used flatMapValues and map to convert the documents to a list of documents into the set of all tokens :22520 and then used the disctict function to make a set of unique tokens (4722) .
For tokenCountPairTuple I mapped it to (token,1) but I am stuck with tokenSumPairTuple?
Any help will be appreciated.
Thank you

 - I used flatMap with set() in the lambda to get the unique entries for each...but first I dumped the id's from the tuples using a .values (so there are more than one ways to get to the answer).
 I am having trouble correctly defining the fastCosineSimilarities function. Here are the steps which I am taking in the function to get it right:

1. I am extracting amazonRec & googleRec from record by assigning these variable the values record[0][0] & record[0][1] respectively.

2. I am extracting the list of tokens in a variable tokens by equating it to record[1].

3. I am looping over each element in the list tokens and then finding the dot product by adding  googleWeightsBroadcast.value[googleRec][element]*amazonWeightsBroadCast.value[amazonRec][e] to the 'sum' variable over each iteration.

4. Then I am getting the cosine similarity value by : sum/(amazonNormBroadcast.value[amazonRec]*googleNormBroadcast.value[googleRec])

and I am getting the following error:
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-65-44633944fd6c> in <module>()
     24                        .cache())
     25 
---> 26 print similaritiesFullRDD.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 154.0 failed 1 times, most recent failure: Lost task 0.0 in stage 154.0 (TID 630, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-65-44633944fd6c>", line 17, in fastCosineSimilarity
TypeError: list indices must be integers, not str

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)<br /> 
<p><p>Where am I making mistake?</p></p> I have a problem my result is 
['brown', 'lazy', 'jumps', 'fox', 'dog', 'quick']
i don't know how solucted this order
 
My solution is: 
 
return list(set(simpleTokenize(string))-(stopwords))
 
What should change?
 
Thanks Hello everybody,

I would like to know how much material of "Learning Spark: Lightning Fast Data Analysis"[1] is covered in this course (and maybe the following course in the XSeries).

Is it recommended to still get the book or could one skip over it and dive right into "Advanced Analytics with Spark: Patterns for Learning from Data at Scale"[2] (assuming familarity with Scala) right away?


Best regards

[1] Learning Spark: Lightning Fast Data Analysis
by Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia
http://shop.oreilly.com/product/0636920028512.do

[2] Advanced Analytics with Spark: Patterns for Learning from Data at Scale
by Sandy Ryza, Uri Laserson, Sean Owen, Josh Wills
http://shop.oreilly.com/product/0636920035091.do I'm very new to Python and I can't figure out the syntax of the lambda function that is supposed to go in my filter to remove the stopwords. If I do:

lambda x: x not in stopwords

I get this:

These are the stopwords: set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'with', u'had', u'should', u'to', u'only', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'did', u'these', u't', u'each', u'where', u'because', u'doing', u'theirs', u'some', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'below', u'does', u'above', u'between', u'she', u'be', u'we', u'after', u'here', u'hers', u'by', u'on', u'about', u'of', u'against', u's', u'or', u'own', u'into', u'yourself', u'down', u'your', u'from', u'her', u'whom', u'there', u'been', u'few', u'too', u'themselves', u'was', u'until', u'more', u'himself', u'that', u'but', u'off', u'herself', u'than', u'those', u'he', u'me', u'myself', u'this', u'up', u'will', u'while', u'can', u'were', u'my', u'and', u'then', u'is', u'in', u'am', u'it', u'an', u'as', u'itself', u'at', u'have', u'further', u'their', u'if', u'again', u'no', u'when', u'same', u'any', u'how', u'other', u'which', u'you', u'who', u'most', u'such', u'why', u'a', u'don', u'i', u'having', u'so', u'the', u'yours', u'once'])
A quck brown fox jump over he lzy dog.
It looks like it removed the 'i' stopwords, but it didn't do them as one piece and I'm not sure why. Any explanation and help would be greatly appreciated! Hi,

I am joining 2 rdd's as follows:

1. day, total number of Hosts per day
2. day, total number of distinct Hosts per day
3. Then I am joining them using the following:
sortedByDay.join(dayAndHostDisctinctCount).sortByKey()

When I print the result for 1 or 2 I get a tuple with day and the list e.g.


[(1, 33996), (3, 41387), (4, 59554), (5, 31888), (6, 32416)]
[(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537)]

But when I print the join result I get the following day, (Iterable object, integer distinct count of host):

[(1, (<pyspark.resultiterable.ResultIterable object at 0xb0a1e2cc>, 2582)), (3, (<pyspark.resultiterable.ResultIterable object at 0xb0a1e7cc>, 3222)), (4, (<pyspark.resultiterable.ResultIterable object at 0xa977eeac>, 4190)), (5, (<pyspark.resultiterable.ResultIterable object at 0xb0a3e4cc>, 2502)), (6, (<pyspark.resultiterable.ResultIterable object at 0xb0a3ec2c>, 2537))]
What am I doing wrong, why am I not getting a number and why am I getting an object back?

Any help will be highly appreciated.

Thanks in advance I have been trying to find a solution for quite a long time but have not found any. I tried list comprehensions, dict comprehensions, flatMap, map, but to no avail.
So far, my most recent attempt is:
...flatMap(lambda (k,v):{x: (x[v], k) for x in v}.items())
Can I please have some hints?
P.S. I am new to Python I have the correct no of tokens in 1(c),  I have then found the no of non repeated tokens in each id and created a tuple of (id, tokens) and reduced by key to concatenate the tokens.  I end up with the correct record but with 501 words with a total length of 3367.    Ny ideas where I may be going wrong?  Thanks -  Sorted please ignore. Trying to figure out if I did something really inefficient with my approach.

I have 5 transformations to create my commonTokens RDD I am just finding links provided in the lectures notes are sort of overwhelming, there are so many links provided for each topic n lot of them have overlapping functionalities explained, it will be nice if the organizers take time to provide relevant links with some recommendation.
 
 
Thanks
  Hi guys,
  This is entirely unrelated to the course, but I wanted to practice on Time Series data, particularly financial. I found https://github.com/cloudera/spark-timeseries but am unsure where to go from here, the files are in Scala, I was expecting some sort of build system in the library but don't know how Spark libraries work. Any help would be appreciated! The solution I have submitted (which passes the tests), as is follows the instructions, I have 4 broadcasts which are obtained by collectAsMap from an RDD.

amazonWeightsBroadcast: id -> token -> weightgoogleWeightsBroadcast: url ->  token -> weightamazonNormsBroadcast: url -> normgoogleNormsBroadcast: url -> norm

As each broadcasts corresponds to a variable in memory, is it feasible to do so in a real problem? Cardinalities for the weights is number of documents times tokens and this can be huge?

So, the question is: is this way to solve the problem really scalable?

Thanks !!! I don't know what to tokenize. The data is structured as "id - tittle - description - manufacturer - price", so I'm uspposing that the description is the item I need to tokenize, but it is not clear with current instructions that is the correct way to proceed.

So, what do I have to tokenize? Looks like i may miss out on the deadline for Lab2 as i keep getting "Connection errors". Ultimately this could impact my ability to learn and keep up with the course.

Any help is hightly appreciated.

 my below format is working with f logfile expression on a fixed length  
^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+)(\W+)(\S*)(\s)(\S+)(\s)(\d+)(\s)(\d+)

but the problem is 
if there is increment in log length then it stops working . how to solve this kind of problem. 
see it is working on first two expression but not include last part of last expression . please help me 

127.0.0.1 - - [01/Aug/1995:00:00:01 -0400] "GET /images/launch-logo.gif HTTP/1.0" 200 1839
ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:43:39 -0400] "GET / HTTP/1.0 " 200 7131ix-sac6-20.ix.netcom.com - - [08/Aug/1995:14:45:46 -0400] "GET /cgi-bin/imagemap/countdown69?293,287 HTTP/1.0 " 302 85 Hi. I'm about to install Ubuntu onto my virtual machine, Virtual Box. Ubuntu asks if I want to "erase disk and install Ubuntu Warning: This will delete your programs, documents, photos, music and any other files in all operating systems." I understand Ubuntu will only delete what's inside my Virtual Box not my OS X. However, since our sparkvm also shares the same VirtualBox... will my sparkvm be erased too??

If sparkvm will be removed, can I install sparkvm back into my VirtualBox after I install Ubuntu?

I really like this course. My needs for Ubuntu is not immediate... So, will we use Virtual Box with sparkvm in the next course about machine learning??? when is the last day of the course? Hi
I had passed the tests till 3b. I took  out all the <Fill in> and the cells which I have not completed. I left the collect() in the examples as is. There are no errors until 3b.
However the autograder is taking a while.  Anything else I need to do?

Ram I have tried removing pure numbers as not being words, but that gives me to low a count. I have tried removing single characters as not being words, but that also gives me to low a count.

I passed all the tokenizer tests, but clearly those test are not complete. I have no idea what the definition of a word is, and unless I guess right I can not pass this test. I think we have to use a filter function to remove all the stopwords, but when I use filter() with "x not in stopwords" i get the error:

AttributeError: 'str' object has no attribute 'filter'

what do I do? thanks I love this course , although with my job not able to spend as much time as I want to.  I think it will be nice if the autograder was a faster and gave error results sooner.
I have taken other edx Python/computational science online courses and the autograder there appeared to be faster.

One of the reasons I could not submit any of the labwork for lab1.  Hate to fail the course just for not being able to get thru the autograder

Maybe this i because of the complexity of RDD.

The course is really fantastic!

Thanks

Ram I cannot get the meaning of this str in the key.
An example from the Spark documentation says 
>>> sc.parallelize([10,4,2,12,3]).top(3, key=str)
[4,3,2]

I cannot really get what func is this str ? where can you convert .ipynb to.py online? For those new to Python like me, you might find it useful to download an IDE like PyCharm that can help you with code completion and quick tests in 'scratch' files.

https://www.jetbrains.com/pycharm/download/

https://www.jetbrains.com/pycharm/help/scratches.html

 I am stuck here.  I keep running into memory errors, no matter what I do.
I join the RDDs, then apply swap.  groupByKey sometimes gives a memory error.  Using collect() afterwards always does.  

I have been trying to use reduceByKey(a,b:[a] + [b]) instead of groupByKey() but I get a list of lists and then flattening it is a problem.

What am I doing wrong?  

Thanks,
Jason For both 2a/2f I used python for loops..

So for instance in 2f there are two fills ins..

tfs  = <FILLIN>
tfIdfDict = <FILLIN>

My final code looks like this:

tfs=filledin
for each item in tfs dict
      do something, updating the value of each item
return tfs

Is there a better way to be doing this? I had submitted lab2 and received a grade, but now am getting timeouts when doing subsequent submissions.

Will I get to keep the grade I had on the first submission? when will the course be offered again? Hi all,

set() seems to alter the order of words in the result making the third test in 1b to fail, am I missing something too big? because I am assuming the solution uses set() substraction.

As usual any help is welcome.

cheers, Hi,

I passed the test for 3a...  dotprod(a, b) worked fine.

But I getting error in 3b
KeyError: 'photoshop'

I guess this due the fact Keys are not common in arg dictionaries a and b. 



---- solved it using For loops, I was looking for a better way to do it though (in one line of code)

How can I work with 2 dictionaries with common keys only... and ignore NOT common keys...

Any help, pointers please!!

 I performed the function computeSimilarity() but when I try to apply it, it reports an error:

I apply it in this way, I suppouse it's wrong:

similarities = (crossSmall                .map(lambda x: computeSimilarity)                .cache())

Any clue how to apply it correctly. Hi guys,I am getting Zero Division Error, and I am stuck in the above exercise. I cant seem to step into the computeSimilarity function by printing also Any Ideas? ZeroDivisionError                         Traceback (most recent call last)
<ipython-input-521-afe1bd55c7a7> in <module>()
     20     return (googleURL, amazonID, cs)
     21 
---> 22 record = computeSimilarity(crossSmall.collect()[1])
     23 record
     24 

<ipython-input-521-afe1bd55c7a7> in computeSimilarity(record)
     17     googleValue = googleRec[1]
     18     amazonValue = amazonRec[1]
---> 19     cs = cosineSimilarity(googleURL, amazonID, idfsSmallWeights)
     20     return (googleURL, amazonID, cs)
     21 

<ipython-input-517-d9c3d83350ed> in cosineSimilarity(string1, string2, idfsDictionary)
     11     w1 = tfidf(tokenize(string1), idfsDictionary)
     12     w2 = tfidf(tokenize(string2), idfsDictionary)
---> 13     return cossim(w1, w2)
     14 
     15 cossimAdobe = cosineSimilarity('Adobe Photoshop',

<ipython-input-515-9ddd8e6b09c7> in cossim(a, b)
     33                 then by the norm of the second dictionary
     34     """
---> 35     return ((dotprod(a, b) / norm(a)) / norm(b))
     36 
     37 testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }

ZeroDivisionError: float division by zero

  I submitted my answer to Lab 3 almost 4 hours ago and the autograder hasn't hasn't responded yet.

What shall I do?

My username is n-riesco I return an IDFS with elements like 
[('aided', 1.0), ('limited', 4.0), ('searchable', 1.0), ('magnetic', 2.0), ('saves', 3.0)]

The first test passes (uniqueTokenCount).  Then the next line 

idfsSmall.takeOrdered(1, lambda s: s[1])[0]

thows an Exception:

.....

TypeError: 'float' object has no attribute '__getitem__'

I suppose this is because the takeOrdered() method gets something like .takeOrdered(1, 3.0)[0].

What is it expecting?

 I do not know where a document begins and ends in a corpus once union amazon and good. corpus.count() will give 400 which is the number of tokens and not documents. Am I to guess the number for N=2. Thanks for the help I do not know where a document begins and ends in a corpus once union amazon and good. corpus.count() will give Hi -

How long should I wait for the autograder to give a result for Lab 3?

It has been spinning for about 5 minutes now.  I resubmitted once after waiting for a while.  Any intuition on how long to wait?

Jason I'm new to Python and this might be a stupid question, but I have absolutely no idea where to start with this question

Can someone help point me in the right direction? Here I' m posting my code of the problem 3c. I'm getting the right answer here: dayHostCount = dayGroupedHosts.countByKey(). When I print dayHostCount, I get all I need, but I can't cache dayHostCount. It says there's no such attribute as cache() Could you help me to understand what I'm doing wrong?


 Can you recommend an online self study resource to study statistics for the first time? Thanks Hello!
Im heading to a workshop this week with Women Who Code to begin exploring  machine learning web service. I'm going with Anaconda for installing pandas, numpy and scikit-learn. They recommend the Flask web framework, to deploy apps on the Heroku platform.
My question is how does this compare with the setup in the Scalable Machine Learning class and will there be any config conflicts that I need to manage?
Thanks in advance for any advice you can offer.
Best If you access Twitter streams, you get the whole lot out there.

But if you want to be specific about a topic or a biz entity, how do you filter? My spark is still rusty and I am already taking too long on this.

Can anyone give me some hints on forming crossSmall RDD? zip doesn't work according to the other post.

 I have used filter(None, list) and other methods also.....
I have checked each String in canopy....all are working perfectly
But in my notebook the results are :

1 test passed.
1 test failed. simpleTokenize should handle empty string
1 test failed. simpleTokenize should handle puntuations and lowercase result
1 test failed. simpleTokenize should not remove duplicates

Oh...........i have got the bug........ Hi I have submitted by Lab2 to the auto grader. I have been waiting for over 20 minutes. But the autograder has not yet given me a result. Does it take so long? Or is there something wrong I maybe doing?All the tests have passed on my local machine. I am worried that if it pass the mid night deadline it may only give 80% score for late submission (which is not the case).Look forward to your response. getting wrong similarity in lab 3 3c ...something like 0.000291... and my test is getting failed..anyone got and solved the same issue i am getting the following output




These are the stopwords: set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'with', u'had', u'should', u'to', u'only', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'did', u'these', u't', u'each', u'where', u'because', u'doing', u'theirs', u'some', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'below', u'does', u'above', u'between', u'she', u'be', u'we', u'after', u'here', u'hers', u'by', u'on', u'about', u'of', u'against', u's', u'or', u'own', u'into', u'yourself', u'down', u'your', u'from', u'her', u'whom', u'there', u'been', u'few', u'too', u'themselves', u'was', u'until', u'more', u'himself', u'that', u'but', u'off', u'herself', u'than', u'those', u'he', u'me', u'myself', u'this', u'up', u'will', u'while', u'can', u'were', u'my', u'and', u'then', u'is', u'in', u'am', u'it', u'an', u'as', u'itself', u'at', u'have', u'further', u'their', u'if', u'again', u'no', u'when', u'same', u'any', u'how', u'other', u'which', u'you', u'who', u'most', u'such', u'why', u'a', u'don', u'i', u'having', u'so', u'the', u'yours', u'once'])






---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-41-2e9d92b6d9e6> in <module>()
     13     return filter(list(stopwords),simpleTokenize(string))
     14 
---> 15 print tokenize(quickbrownfox) # Should give ['quick', 'brown', ... ]

<ipython-input-41-2e9d92b6d9e6> in tokenize(string)
     11         list: a list of tokens without stopwords
     12     """
---> 13     return filter(list(stopwords),simpleTokenize(string))
     14 
     15 print tokenize(quickbrownfox) # Should give ['quick', 'brown', ... ]

TypeError: 'list' object is not callable

 please extend the last day of course to july 13 for those who r on vacations. For questions 4(e) and 4(g), I am using sortBy() to sort by date or ErrorCounts as applicable. My output for 4(e) seems to be matching exactly with the one from the Test, but I am getting test failed for some reason. For 4(g), I am not sure why my output is showing first tuple as (12, 195 ), while the correct answer starts with (7,532),..

4(e)
errDateSorted = (errDateSum                 .sortBy(lambda x: x[0]))
errByDate = errDateSorted.cache()print errByDate.take(40)

4(e) output from my code:

[(1, 243), (3, 303), (4, 346), (5, 234), (6, 372), (7, 532), (8, 381), (9, 279), (10, 314), (11, 263), (12, 195), (13, 216), (14, 287), (15, 326), (16, 258), (17, 269), (18, 255), (19, 207), (20, 312), (21, 305), (22, 288)]
1 test failed. incorrect errByDate
output for Test:
Test.assertEquals(errByDate, [(1, 243), (3, 303), (4, 346), (5, 234), (6, 372), (7, 532), (8, 381), (9, 279), (10, 314), (11, 263), (12, 195), (13, 216), (14, 287), (15, 326), (16, 258), (17, 269), (18, 255), (19, 207), (20, 312), (21, 305), (22, 288)], 'incorrect errByDate')

4(g):==>

topErrDate = errDateSorted.sortBy(lambda (x,y): y).take(5)
also tried:
topErrDate = errDateSorted.sortBy(lambda x: x[1]).take(5)

my output for 4(g):
[(12, 195), (19, 207), (13, 216), (5, 234), (1, 243)]

Output from Test:
[(7, 532), (8, 381), (6, 372), (4, 346), (15, 326)], 'incorrect topErrDate') Everything was going ok with me while solving lab 1, but then i had an error at part 4(4D Load a text file):

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 98, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process() I think I may have it right up to:
tokenCountPairTuple:
 [('aided', 1.0), ('precise', 1.0), ('duplex', 1.0), ('dance', 1.0), ('breath', 1.0)]

But not for tokenSumPairTuple:
[('aided', 1.0), ('precise', 1.0), ('complements', 1.0), ('dance', 1.0), ('enjoyable', 1.0),...

Tried using sortByKey, and map for (x,y):x+y or (x,y):x,y.count(). but is not summing up for me. Any thoughts would be appreciated. Thanks I am getting the following. The total token count should be 22520 instead of 22858 which I am getting. Did anyone else get this result? Where could be my error? Thanks.

There are 16839 tokens in the amazonSmall datasets

There are 6019 tokens in the googleSmall datasets

There are 22858 tokens in the combined datasets
 In the tfidf function, after using tf(), I now have two dictionaries with the same keys and different values (one with the TFs and the other with the IDFs), 
So how do I create a new dictionary from them, where the value is the product?
I can do it easily using a for loop, but the code suggets it can be done without one.
I looked up StackOverflow, and they recommended using the Counter class, but we cannot use that.
 Is it ok? I thought this question was pretty straight forward. Join the two RDDs, map swap on each element, and reducebykey to get the shared tokens. Here is my output result. But I only get a count of 18988. Can anyone please tell me what I am doing wrong here?

print commonTokens.take(3)

[(('b000io79mu', 'http://www.google.com/base/feeds/snippets/4268441331041279311'), ('get', 0.34547918391929533, 'play', 0.5233207891435739, 'people', 1.459141494435612)), (('b000egidpe', 'http://www.google.com/base/feeds/snippets/17939626833908573527'), ('encore', 0.394752688172043)), (('b0007zf3ge', 'http://www.google.com/base/feeds/snippets/13342689240311947950'), ('version', 0.4305281921380993, 'software', 0.07492489550679206))]




 I run lab 2 (1b) but there is no output. Reading lab 2 (1c) it mentions that I should see the output from lab 2 (1b).

I restart the vagrant (cmd: 'vagrant halt' then 'vagrant up --provider=virtualbox') a number of times but it does not make a difference.

Do I need to re-run lab 1 first for lab 2 to run normal?

Please help.

Thanks. Hi,

I have submitted lab2 for 2 times and due to some small error I resubmitted fixing it.

From then I didn;t get output from the grader.

Please l me let know. I resubmitted it again a few minutes back. hi,
do not understand where the value b000o24l3q out, I have the maximum number with the function:
max (vendorRDD.map (lambda t. len (t)) collect ())
I do not understand what I do. Please help.
thanks I have two set of file, and want to remove the tuple from file1 which I have it in file2
 
file1:
[(111,apple),high]
[(222,Samsung),medium]
[(333,nokia),low]
 
file2:
[(333,nokia),low]
 
Result:
[(111,apple),high]
[(222,Samsung),medium]
 
Can you please help me ?
 I just noticed that I'm getting HotSpot error files -- basically the "crash dump" files produced by Java when it crashes.  For example:

## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (malloc) failed to allocate 30146560 bytes for committing reserved memory.
That error doesn't mean Java blew out its memory.  It means the collection of JVMs running on the VM blew out the VMs memory.  Either there were too many JVMs, or the JVMs are given more heap than the server has available, or an unexpectedly large number of threads were running, or something along those lines.

So I wonder if the autograder is running into similar issues?  I have six of these files and they're all the same ... the VM itself ran out of memory.
 I managed to get it working joining the two RDDs, swapping and grouping by key but I get a timeout when submitting the notebook. I've been trying to get 4e to work using reducebyKey instead of groupByKey: I've tried everything I can think of (appending, extending, adding, converting to list and appending/extending) inside the reduceByKey but I either get a list of lists which I don't think is the correct way to do it or 'None'. Could someone please give me just a little hint where to look?
Thanks a lot in advance! Hi,

I submitted the lab3, autograder has been in the stand still for hour(3hr max).
It still working.

What is the average response time for lab3 submission ?
 I have successfully completed Lab1 and Lab2 so find this perplexing. When I run ln [2] of Lab3, I get the following error:
No module named test_helper


Any thoughts on whats going on? vagrant is up and running.
 4a. Insert below code at the end of 4a
print amazonFullRecToToken.take(3)print googleFullRecToToken.take(3)

[('b000jz4hqo', ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund']), ('b0006zf55o', ['ca', 'international', 'arcserve', 'lap', 'desktop', 'oem', '30pk', 'oem', 'arcserve', 'backup', 'v11', '1', 'win', '30u', 'laptops', 'desktops', 'computer', 'associates']), ('b00004tkvy', ['noah', 'ark', 'activity', 'center', 'jewel', 'case', 'ages', '3', '8', 'victory', 'multimedia'])]
[('http://www.google.com/base/feeds/snippets/11125907881740407428', ['learning', 'quickbooks', '2007', 'learning', 'quickbooks', '2007', 'intuit']), ('http://www.google.com/base/feeds/snippets/11538923464407758599', ['superstart', 'fun', 'reading', 'writing', 'fun', 'reading', 'writing', 'designed', 'help', 'kids', 'learn', 'read', 'write', 'better', 'exercises', 'puzzle', 'solving', 'creative', 'writing', 'decoding']), ('http://www.google.com/base/feeds/snippets/11343515411965421256', ['qb', 'pos', '6', '0', 'basic', 'software', 'qb', 'pos', '6', '0', 'basic', 'retail', 'mngmt', 'software', 'retailers', 'need', 'basic', 'inventory', 'sales', 'customer', 'tracking', 'intuit'])]


<strong>4b. Insert below code at end of 4b. </strong>
i = 0
for k in idfsFullWeights.keys():
    print k, idfsFullWeights[k]
    i += 1
    if i > 3: 
        break
        
print amazonWeightsRDD.take(3)
print googleWeightsRDD.take(3)

[('b000jz4hqo', {'rom': 2.4051362683438153, 'clickart': 56.65432098765432, '950': 254.94444444444443, 'image': 3.6948470209339774, 'premier': 9.27070707070707, '000': 6.218157181571815, 'dvd': 1.287598204264871, 'broderbund': 22.169082125603865, 'pack': 2.98180636777128}), ('b0006zf55o', {'laptops': 11.588383838383837, 'desktops': 12.74722222222222, 'backup': 2.8015873015873014, 'win': 0.501859142607174, 'ca': 9.10515873015873, 'v11': 50.98888888888888, '30u': 84.98148148148148, '30pk': 254.94444444444443, 'desktop': 2.23635477582846, '1': 0.3231235037318687, 'arcserve': 24.28042328042328, 'computer': 0.6965695203400122, 'lap': 127.47222222222221, 'oem': 46.35353535353535, 'international': 9.44238683127572, 'associates': 7.284126984126985}), ('b00004tkvy', {'case': 5.28078250863061, 'center': 6.953030303030303, 'noah': 208.5909090909091, 'ages': 7.871355060034306, 'multimedia': 7.070878274268105, 'jewel': 7.192789968652038, '3': 0.6964638033085445, 'victory': 34.765151515151516, 'activity': 10.175166297117517, '8': 1.2641873278236915, 'ark': 208.5909090909091})]
[('http://www.google.com/base/feeds/snippets/11125907881740407428', {'quickbooks': 17.48190476190476, '2007': 4.985334057577403, 'learning': 5.932773109243698, 'intuit': 13.379008746355684}), ('http://www.google.com/base/feeds/snippets/11538923464407758599', {'better': 1.5195364238410596, 'kids': 2.6073863636363637, 'help': 0.71703125, 'read': 6.373611111111112, 'puzzle': 4.98804347826087, 'solving': 9.97608695652174, 'decoding': 114.72500000000001, 'write': 3.8241666666666667, 'exercises': 4.881914893617022, 'designed': 0.8466789667896679, 'writing': 9.695070422535212, 'learn': 1.1950520833333333, 'fun': 2.2495098039215686, 'superstart': 57.362500000000004, 'creative': 1.130295566502463, 'reading': 6.286301369863014}), ('http://www.google.com/base/feeds/snippets/11343515411965421256', {'customer': 4.011363636363637, 'tracking': 3.7248376623376624, '6': 1.4952753339850113, 'pos': 41.71818181818182, 'sales': 6.728739002932551, '0': 0.7094928880643167, 'qb': 59.5974025974026, 'inventory': 6.320936639118457, 'retailers': 41.71818181818182, 'basic': 5.959740259740259, 'need': 0.6245236799129015, 'mngmt': 104.29545454545455, 'retail': 5.214772727272727, 'intuit': 4.2569573283859, 'software': 0.2247746865203762})]


<strong>4c. Insert below code at end of 4c.</strong> 
i = 0
mydict = amazonNorms.collectAsMap()         ###Note: amazonNorms does not need .collectAsMap() since definition already includes this. Same for googleNorms and broadcast
for k in mydict.keys():
    print k, mydict[k]
    i += 1
    if i > 3: 
        break

i = 0
mydict = googleNorms.collectAsMap()
for k in mydict.keys():
    print k, mydict[k]
    i += 1
    if i > 3: 
        break


Results:

 b000ea9u2a 124.172212584
b0009mg80a 62.9799786096
b000hkgj8k 249.765979303
b0002wt64m 107.664192287
http://www.google.com/base/feeds/snippets/13159207338580799968 268.778514313
http://www.google.com/base/feeds/snippets/15932863893105585101 19.2647914305
http://www.google.com/base/feeds/snippets/18388451726220202579 577.670733155
http://www.google.com/base/feeds/snippets/14837029540644805566 431.716147405


<strong>4d. Insert below code at end of 4d.</strong> 
print amazonInvPairsRDD.take(3)
print googleInvPairsRDD.take(3)

Results:
[('rom', 'b000jz4hqo'), ('clickart', 'b000jz4hqo'), ('950', 'b000jz4hqo')]
[('quickbooks', 'http://www.google.com/base/feeds/snippets/11125907881740407428'), ('2007', 'http://www.google.com/base/feeds/snippets/11125907881740407428'), ('learning', 'http://www.google.com/base/feeds/snippets/11125907881740407428')]


<strong>4e. Insert below code at end of 4e.</strong> 
print commonTokens.take(3)

Results:
[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120']), (('b000bnb72g', 'http://www.google.com/base/feeds/snippets/6959766318020460134'), ['software']), (('b00009apna', 'http://www.google.com/base/feeds/snippets/13986409474793202514'), ['business'])]


<strong>4f. Insert below code at end of 4f.</strong> 
print commonTokens.first()
print fastCosineSimilarity(commonTokens.first())

Results:
(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120'])
(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), 0.004328984263097415)



Hope this helps!


Edit to replace dict with mydict. dict() is built in function and overwriting means the built-in is no longer callable.  So I did a silly thing and after submitting the correct file (93% score) for lab 1 I submitted an incorrect file. Used to "best score taken" policy of other courses I figured this would not do anything bad I left it but now the progress page has 0 for lab 1 instead of the correct grade - did anyone run into similar things? Is there a fix? Thanks! My unique token values are correct.

But I am getting the following for the smallest IDF. So my smallest IDF token is adobe instead of software.

[('adobe', 1.520912547528517), ('software', 2.1621621621621623), ('cs3', 2.9197080291970803), ('pro', 3.5714285714285716), ('design', 3.6036036036036037), ('new', 3.6363636363636362), ('0', 4.166666666666667), ('3', 4.25531914893617), ('tools', 4.3478260869565215), ('windows', 4.545454545454546), ('create', 4.597701149425287)]
Please help me to solve this issue. Thanks in advance Hey,

Hey.

For 3e I tackled it as follows:

1. accessed the log, created tuple day and host.
2. groupByKey
3. sortByKey
4. joined sortedByDay and daily hosts and then .map(lambda (a,(b,c)):(a,(b / c))).cache()

No syntax errors show up but the code fails on runtime, I am finding it very frustrating as I have played with the map function above and continually run into the same errors as here:

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-47-59e6002a38a5> in <module>()
      9 avgDailyReqPerHost = sortedByDay.join(dailyHosts).map(lambda (a,(b,c)):(a,(b / c))).cache()
     10 
---> 11 avgDailyReqPerHostList = avgDailyReqPerHost.take(30)
     12 print 'Average number of daily requests per Hosts is %s' % avgDailyReqPerHostList

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 227.0 failed 1 times, most recent failure: Lost task 0.0 in stage 227.0 (TID 560, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-47-59e6002a38a5>", line 9, in <lambda>
TypeError: unsupported operand type(s) for /: 'ResultIterable' and 'int'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
<p></p>
<p></p> I am a little confused here.  If I wrote the following wrong code
uniqueTokens = corpus.map(lambda l: l[1]).take(1)
I got the following output for uniqueTokens:  
[['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund']]
So I am supposed to break down the above list to emit the pair of word in the form of (word,1) and then eliminate the duplicates?  Or I just need to eliminate duplicates within each above list?
 in lab 3 3c - did anyone change the googlerec and amazonrec into string ..because i am getting the "tuple has no object split "error and if i am changing the googleRec = str(record[0]) and amazonRec = str(record[1]) , my function compute similarity is functioning but wrong similarity value...can some one help me out with thisjQuery17109326219635549933_1434995786490??? in lab 3 3c - did anyone change the googlerec and amazonrec into string ..because i am getting the "tuple has no object split "error and if i am changing the googleRec = str(record[0]) and amazonRec = str(record[1]) , my function compute similarity is functioning but wrong similarity value...can some one help me out with this????? I am confused about the question wording
Create two collections, one for each of the full Amazon and Google datasets, where IDs/URLs map to the norm of the associated TF-IDF weighted token vectors.
What is exactly required?

----------------

Sorry - solved!! dayAndHostTuple = access_logs.<FILL IN>groupedByDay = dayAndHostTuple.<FILL IN>sortedByDay = groupedByDay.<FILL IN>

### Code
dayAndHostTuple = access_logs.map(lambda log: (log.date_time.day, log.host) )
groupedByDay = dayAndHostTuple.groupByKey()
sortedByDay = groupedByDay.sortByKey()

### Error Message
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-29-1ad2aba2922b> in <module>()
     14 groupedByDay = dayAndHostTuple.groupByKey()
     15 
---> 16 sortedByDay = groupedByDay.sortByKey()
     17 
     18 # print groupedByDay.take(2)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sortByKey(self, ascending, numPartitions, keyfunc)
    584         # the key-space into bins such that the bins have roughly the same
    585         # number of (key, value) pairs falling into them
--> 586         rddSize = self.count()
    587         if not rddSize:
    588             return self  # empty RDD

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
 Hi to solve lab 3 question 1d, we have RDD input as id and list of tokens so I do the following

-I first do map to find length of tokens using (a,b) : (a, len(b))
-Then I use takeOrder to find max using  (k,v) : -v)

When I print using above code I get correct output


[('b000o24l3q', 1547)]

but when I put above code as part of return statement of findBiggestRecord() I get the following error:

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-113-8d46b64ec33a> in <module>()
     12 biggestRecordAmazon = findBiggestRecord(amazonRecToToken)
     13 print 'The Amazon record with ID "%s" has the most tokens (%s)' % (biggestRecordAmazon[0][0],
---> 14                                                                    len(biggestRecordAmazon[0][1]))

TypeError: object of type 'int' has no len() Hi All,

I would like to get in touch with fellow students from the Netherlands to form a community for Spark users in the Netherlands.Kind RegardsRudolf Schimmel
  I think i read it for about 500 times, and still i'm having difficulties of understanding the instructions.
so the N is cool.
 
but what comes afterwards i can't understand.
do i need something like 
[(doc1, uniqueTokens),(doc2, uniqueTokens),(doc3, uniqueTokens),(doc4, uniqueTokens),(doc4, uniqueTokens)...]?
and afterwards 
[(token, documentCount), ....]? Unfortunately, I was busy the past couple of weeks and didn't get much chance to keep up with the course after week 1. Can I still complete the course and get a certificate?

Also, for those of you who finished the first 2 labs, how long did it take you to do each of those? We have three announcements to share with you:

1) The Verified Track deadline for CS100.1x is June 26 23:59 UTC.  Sign up here.

2) CS190.1x - Scalable Machine Learning starts next week, and the course syllabus and FAQs are now available online. CS190 was developed in conjunction with CS100 and students are encouraged to take both courses. Notably, the software setup is identical for both courses, as is the intro Spark material. If you’ve already set up a VM for CS100, you can continue to use it in CS190. Additionally, you can submit lab0 and lab1 from CS100 to receive credit for lab0 and lab2 in CS190.

3) CS100.1x and CS190.1x comprise the BerkeleyX Big Data XSeries, and students who complete both courses on the Verified Track this summer will receive an XSeries Big Data Certificate.

#pin
 Hi class,

I just started the course last week and I a bit behind. I tried setting up the lab VM but never got to run it. I have Spark set up on my own machine, hence I can follow most of the assignments regardless. The issue is mostly with the data files that are all included in the VM, but there is not way to access them if i can not run the VM. 

So here is where I am: 
===========================
➜ $ vagrant up Bringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Clearing any previously set forwarded ports...==> sparkvm: Clearing any previously set network interfaces...==> sparkvm: Preparing network interfaces based on configuration... sparkvm: Adapter 1: nat==> sparkvm: Forwarding ports... sparkvm: 8001 => 8001 (adapter 1) sparkvm: 4040 => 4040 (adapter 1) sparkvm: 22 => 2222 (adapter 1)==> sparkvm: Booting VM...==> sparkvm: Waiting for machine to boot. This may take a few minutes...The guest machine entered an invalid state while waiting for itto boot. Valid states are 'starting, running'. The machine is in the'poweroff' state. Please verify everything is configuredproperly and try again.
If the provider you're using has a GUI that comes with it,it is often helpful to open that and watch the machine, since theGUI often has more helpful error messages than Vagrant can retrieve.For example, if you're using VirtualBox, run `vagrant up` while theVirtualBox GUI is open.
===========================

As suggested by the TAs, the instructor and lots of helpful on various forums, I tried starting the VM from the GUI but I also get the following the error there:

=[ Error ]=====================
RTR3InitEx failed with rc=-1912 (rc=-1912)The VirtualBox kernel modules do not match this version of VirtualBox. The installation of VirtualBox was apparently not successful. Executing'/etc/init.d/vboxdrv setup'may correct this. Make sure that you do not mix the OSE version and the PUEL version of VirtualBox.
===========================

I ran the suggested command and although it seems to go fine, the error does not go away.

===========================
➜ $ sudo /etc/init.d/vboxdrv setup Stopping VirtualBox kernel modules ...done.Uninstalling old VirtualBox DKMS kernel modules ...done.Trying to register the VirtualBox kernel modules using DKMS ...done.Starting VirtualBox kernel modules ...done.
===========================

I removed VB and Vagrant a few times (again as suggested by people here), but yet have not been successful in bringing up the VM. Long story short. I only need the initial data files. included in the VM and if there is a way to access those I would be fine. I would appreciate any help solving this problem.

Thank I know some edx courses release all the course content, notably videos/exams/labs upfront to give the students maximum time flexibility.

Considering the deadline for the all labs are due to the very last day of cs190.1x ( which means you don't aim to push the students to the strict weekly deadlines), wouldn't it be even greater , if you can also grant the flexibility of reaching the class materials any time student finds suitable ? For Instance, I would much prefer to take all the class in a weekend rather than in 5 weeks.

Thanks for these great courses. I am able to generate dayGroupedHosts as a list of tuples of day and a list of unique hosts.

So to get to dayHostCount, I should be able to count the number of unique hosts for each of those days. I am doing the following, but due to some type of errors, I am unable to print out desirable result.

dayHostCount = dayGroupedHosts.map(lambda a: (a[0],a[1].count()))
print dayHostCount

I expected to see something like "[(1, 2582), (3, 3222), (4, 4190), (5, 2502),.........]", instead I got "PythonRDD[658] at RDD at PythonRDD.scala:43" Hello,
I got a timeout response when submitting the lab 3, so I'd like to improve my code.
Now my script requires 400 seconds to run on my machine, but just the codes of 4f cost 137 seconds. Besides, 4e cost 45 seconds (getting rid of groupByKey saved a lot of time here) and part 5 cost 170 seconds.
It seemed that I wasted too much time in 4f and I could not figure out the reason. 
Basically, I looked for weights in amazon dataset by amazonWeightsBroadcast.value[amazonRec] and the same with google dataset. And then I calculated the sum by iterate the items of both dictionaries. At last the norm was extracted through amazonNormsBroadcast.value[amazonRec] (and google).

Thanks,
Wenting 
 I am auditing the course for free and during enroll it was written I will get a free Honor code certificate after sucessfull completion of the course !! 
So is it ? will I get a certificate I have a problem with questions 1.c and 1.d.:
I get a correct answer to question 1.c when I tokenize the datasets using this instruction:
amazonRecToToken = amazonSmall.flatMap(lambda x: tokenize(x[1]))

I then just count() the vendorRDD
and I get a count of 22520

But I can't find a solution to question 1.d. and I think the problem comes from the way I compute amazonRecToToken. I tried another way with

amazonRecToToken = amazonSmall.map(lambda x: (x[0], tokenize(x[1])))

But I can't find the solution to 1.c. Counting the length of maps:
vendorRDD.map(lambda x: len(x[1])).count()

produces a count of 400 and I can't manage to reduce these lengths

Any idea what is wrong with my program?
 Hi,

I do not understand why the grader does not accept most of my code, even though everything but the last test pass in my laptop. It shows bizarre errors related to connections with Java.
My token is

911231-af31dfbd59df001a3bd4ff42e62480b9:149820072abc6f3f3ae0f85ade8b5c7e:ip-172-31-46-196


Here we go:

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
IndexError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
list index out of range

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 923, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 712, in collect
    with SCCallSiteSync(self.context) as css:
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/traceback_utils.py", line 72, in __enter__
    self._context._jsc.setCallSite(self._call_site)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 536, in __call__
    answer = self.gateway_client.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 369, in send_command
    response = self.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 362, in send_command
    connection = self._get_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 318, in _get_connection
    connection = self._create_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 325, in _create_connection
    connection.start()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 432, in start
    raise Py4JNetworkError(msg)
Py4JNetworkError: An error occurred while trying to connect to the Java server

Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1174, in takeOrdered
    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 712, in collect
    with SCCallSiteSync(self.context) as css:
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/traceback_utils.py", line 72, in __enter__
    self._context._jsc.setCallSite(self._call_site)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 536, in __call__
    answer = self.gateway_client.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 362, in send_command
    connection = self._get_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 318, in _get_connection
    connection = self._create_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 325, in _create_connection
    connection.start()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 432, in start
    raise Py4JNetworkError(msg)
Py4JNetworkError: An error occurred while trying to connect to the Java server

Implement a TF-IDF function (2f)
--------------------------------
All tests passed
Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
ERROR      | pid 13 | java_gateway.py, line 431: An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
ERROR      | pid 13 | java_gateway.py, line 431: An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
ERROR      | pid 13 | java_gateway.py, line 431: An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
Implement a cosineSimilarity function (3b)
------------------------------------------
All tests passed
Perform Entity Resolution (3c)
------------------------------
All tests passed
Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
All tests passed
Perform a Gold Standard evaluation (3e)
---------------------------------------
All tests passed
Tokenize the full dataset (4a)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 923, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 712, in collect
    with SCCallSiteSync(self.context) as css:
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/traceback_utils.py", line 72, in __enter__
    self._context._jsc.setCallSite(self._call_site)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 536, in __call__
    answer = self.gateway_client.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 362, in send_command
    connection = self._get_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 318, in _get_connection
    connection = self._create_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 325, in _create_connection
    connection.start()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 432, in start
    raise Py4JNetworkError(msg)
Py4JNetworkError: An error occurred while trying to connect to the Java server

Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 923, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 712, in collect
    with SCCallSiteSync(self.context) as css:
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/traceback_utils.py", line 72, in __enter__
    self._context._jsc.setCallSite(self._call_site)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 536, in __call__
    answer = self.gateway_client.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 362, in send_command
    connection = self._get_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 318, in _get_connection
    connection = self._create_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 325, in _create_connection
    connection.start()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 432, in start
    raise Py4JNetworkError(msg)
Py4JNetworkError: An error occurred while trying to connect to the Java server

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
All tests passed
Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
ERROR      | pid 13 | java_gateway.py, line 431: An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
ERROR      | pid 13 | java_gateway.py, line 431: An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 923, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 712, in collect
    with SCCallSiteSync(self.context) as css:
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/traceback_utils.py", line 72, in __enter__
    self._context._jsc.setCallSite(self._call_site)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 536, in __call__
    answer = self.gateway_client.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 362, in send_command
    connection = self._get_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 318, in _get_connection
    connection = self._create_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 325, in _create_connection
    connection.start()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 432, in start
    raise Py4JNetworkError(msg)
Py4JNetworkError: An error occurred while trying to connect to the Java server

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 923, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 712, in collect
    with SCCallSiteSync(self.context) as css:
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/traceback_utils.py", line 72, in __enter__
    self._context._jsc.setCallSite(self._call_site)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 536, in __call__
    answer = self.gateway_client.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 362, in send_command
    connection = self._get_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 318, in _get_connection
    connection = self._create_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 325, in _create_connection
    connection.start()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 432, in start
    raise Py4JNetworkError(msg)
Py4JNetworkError: An error occurred while trying to connect to the Java server

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 712, in collect
    with SCCallSiteSync(self.context) as css:
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/traceback_utils.py", line 72, in __enter__
    self._context._jsc.setCallSite(self._call_site)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 536, in __call__
    answer = self.gateway_client.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 362, in send_command
    connection = self._get_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 318, in _get_connection
    connection = self._create_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 325, in _create_connection
    connection.start()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 432, in start
    raise Py4JNetworkError(msg)
Py4JNetworkError: An error occurred while trying to connect to the Java server

ERROR      | pid 13 | java_gateway.py, line 431: An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
ERROR      | pid 13 | java_gateway.py, line 431: An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
ERROR      | pid 13 | java_gateway.py, line 431: An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
-- 12 cases passed (63.0%) --


Your submission token id is 911231-af31dfbd59df001a3bd4ff42e62480b9:149820072abc6f3f3ae0f85ade8b5c7e:ip-172-31-46-196
Please include this submission token id when you need support for your code submission.

Thanks,
 Adrian
 I am trying to find the best way to learn this course. Concepts are easy, usage is harder which is the way most computer programming works out to be. Example, I want to understand how joins work (again not from concept) when writing scripts. .

I am using lab to run my examples. This is due to not having any other way to do these things than interactive shell. I do have spark standalone on my own laptop. The different explanations on how to run a script on spark using python are non-helpful and contradictory. Back to using class iPython notebook.

I post 
 data = [1, 2, 3, 4, 5]distData = sc.parallelize(data) which is direct from spark site and it errors, when I add sc.parallelize(data,1). There is a server error, looking for what looks to be an HDFS directory. This is the short version of issue to test a simple join.This class, and spark in general should document basic environment. SOMEONE is writing scripts, and has idea on best practice.Please advise! Hello all!



 
I have been banging my head against this figurative wall for some time now, and any help is appreciated.
I have no idea how to make my code work. It is below.



def idfs(corpus):
    """ Compute IDF
    Args:
        corpus (RDD): input corpus
    Returns:
        RDD: a RDD of (token, IDF value)
    """
    N = corpus.count()
    uniqueTokens = corpus.flatMapValues(lambda x: x)
    #tokenCountPairTuple = uniqueTokens.<FILL IN>
    #tokenSumPairTuple = tokenCountPairTuple.reduceByKey(lambda x,y : x+y)
    return None #(tokenSumPairTuple.map(lambda (token,count) : (token,N/count)))

idfsSmall = idfs(amazonRecToToken.union(googleRecToToken))
#uniqueTokenCount = idfsSmall.count()

#print 'There are %s unique tokens in the small datasets.' % uniqueTokenCount

My lines are just commented not to interfere with my gradual process.
 
My returning none is just for now as the last line needs to be commented.
 
So I am struggling to figure out how to initialize tokenCountPairTuple, and I am also a bit doubting about the accuracy of uniqueTokens.
 
Maybe I should do uniqueTokens = corpus.flatMapValue(lambda x:x).distinct() ?
 
How should I proceed with tokenCountPairTuple? lab 2 (1c): is something wrong with invoking <xyz>.count() below?
Thanks.
---------------------

# TODO: Replace <FILL IN> with appropriate code# HINT: Do you recall the tips from (3a)? Each of these <FILL IN> could be an transformation or action.hosts = (access_logs.map(lambda log: (log.hosts, 1))                    .cache())uniqueHosts = hosts.reduceByKey(lambda a, b: a+b).cache()uniqueHostCount = uniqueHosts.count()
print 'Unique hosts: %d' % uniqueHostCount


-----------------------
When I run, it gives this weird error:
Py4JJavaError                             Traceback (most recent call last)<ipython-input-58-623b3bd14424> in <module>()  4 .cache())  5 uniqueHosts = hosts.reduceByKey(lambda a, b: a+b).cache() ----> 6 uniqueHostCount = uniqueHosts.count()  7   8 print 'Unique hosts: %d' % uniqueHostCount /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)  930 3  931 """ --> 932 return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()  933   934 def stats(self): /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)  921 6.0  922 """ --> 923 return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)  924   In week 3, the lectures were very simplistic and underwhelming and lab 3 is turning out to be tougher than expected for me. 

Are there any recommended books which can accompany the lectures? Hi,

Followed the instructions to installed Vagrant . 

after installing Vagrant to default directory, the command "vagrant up " , looks for vagrant/bin/base  , but file not found . 
Vagrant tries to download remote /base file  .

Anyone had same issue please ? 
 In 1d, I get different answers depending on whether I use first() or take(1) to return the id with the most tokens. I seem to get the correct answer when I use take(1), but when I use first(1) it just returns a count of 1. Anybody know what is going on here? I am not able to run cells, I can run some of them but no majority. They take long time or I don't know and it make impossible to solve the lab2.

PD: I have upload the VM

It is solved, I did nothing, only try I stuck at the step 
For each of the unique tokens, count how many times it appears in the document and then compute the IDF for that token: N/n(t)
It was easy to create uniqueTokens RDD ("list") but I need a hint what should be at the place 
tokenCountPairTuple = uniqueTokens.<FILL IN>

How could we combine corpus and uniqueTokens?

Thanks.
 "For each of the unique tokens, count how many times it appears in the document and then compute the IDF for that token: N/n(t)"

I could of course take a brute force approach and do that all in Python, but my impression is that would all be done on the driver, is that correct?

It doesn't seem to be a scalable or very Sparklike approach, so I'm looking for a way to do this with RDDs.  Since the list of unique tokens is already an RDD, and since there is an intersection() method for RDDs, it seems like I need to get the lists for the tokens in each document into RDDs too.  I haven't decided on a good way to do that yet, but I'm wondering if this is the best approach.  Comments? I have a lab output that has multiple "NameError: name '<variable>' is not defined".

Potentially related: I have a "PicklingError: Can't pickle builtin"

Is it possible this PicklingError is causing the script to terminate prematurely? This occurs halfway through the output...

Everything ran fine locally. Full autograder output: http://pastebin.com/jmy2reN8 I got the set of uniqueTokens, using list and set operators in my lambda function so now each of the elements in uniqueTokens is a single list with the unique tokens for each document (original element).

Was this the desired outcome for uniqueTokens?

I am stuck now at how to turn each of these single-list elements into a sequence of multiple single-token-pair items in a new RDD.  This seems to expand the length of the new RDD where each item in the first RDD generates a variable number of new items in the second RDD.   

Does this require looping through the single-item list's indices in a lambda function?

 
1-I did a map map(lambda log:(log.data_time.day,log.host)).distinct()
2-again applied a map to get the count of the hosts per day map(lambda log: (log,1))
3-I applied reduceByKey to get the count reduceByKey(lambda a,b:a+b)
4-I sorted and applied cache sortByKey().cache()

But I am getting in the 4 step .

Could some one help with this and let me know if I am doing something wrong or missing some logic
any help would be appreciated. I have implemented a dict comprehension for creating a Python dictionary where each token maps to the token's frequency times the token's IDF weight....

but I get an error of the sort TypeError: 'dict' object is not callable

so what am I doing wrong? Hello, I just thought that this question should have multiple answers, Am I right? I am trying to print the hosts but they do not get printed in the expected order( e.g. 
[4406, 2864, ...)In 3c the dayHostCount has the day and the number of unique hosts (e.g. (1, 2582)), dailyHosts is just dayHostCount cached and the result is as expected since dailyHostsList sorts dailyHosts. 3c tests pass, the same for daysWithHosts (which i sort) but hosts are not in the correct order. Any suggestion? Got stuck at 3c. Was not able to solve the counting of the hosts part. Then gave up. Would love to come back and solve it once I figure out how to do the counting. Hi,

After completing lab3  (all the built-in tests passed), I uploaded the python file to the autograder. But after a long time I got a timeout error - the exact error 
and token id is in attached file since I was unable to copy and paste. I tried a second submission and it has been more than 2 hours without any success. I have followed the submission guidelines (no additional print statements, used cache() where ever required etc). Can I submit the python file for manual grading?

thanks






timeout_error.txt I was wondering the differences between  RDD vs PipelinedRDD, and how can I fix the errors produced when use actions after a  PipelinedRDD? In Lab 1 C it has reduceByKey, when I use "add" as a input to the function it comes back as undefined. Yet when I look in the class notes and in the documentation "add" is a valid input parameter. Hi,
After the autograder choked on my submission, the Instructor asked if I used collectAsMap..

My question is, how do we go about avoiding collectAsMap if we're required to use broadcast variables? The broadcast variables depend on a non-RDD variable, so the only way out is to collectAsMap() the RDD and pass that to the broadcast variable.

Could you please give me a hint as to how I can replace collectAsMap when I need to pass a dictionary to the broadcast variable? Hello,

After a lot of problems with using up too much memory - which forced me to rerun the lab3 after every failed attempt at 4e, I have succeded in getting the commonTokens RDD of lab3 4e by doing the following operations:
   a) join the both datasets;
   b) do the swap
   c) reduce the various tokens to a list
       I believe the problem might be here. I have used a function that takes arguments x and y, verifies if they are lists, converts them to lists if not, and appends y to x.
   d) convert single tokens to lists
       Needed because entries with a single token do not undergo a reduction and are therefore not changed to lists in step c)
Doing a map such as lambda x: [x,] in order to convert tokens to lists before the reduction would be much simpler, but in my case caused the lab to crash.
Is there less demanding and smarter way to go about the preparation of commonTokens?

Thanks in advance! This course is ridiculous.

Labs are poorly explained. There is little guidance of any value. 

Ticks me off that I wasted time on this crap. just wanted to let you know taht the autocoder is taking way too long and i cannot determins the progress. Hi
It will be great if the cloud is accessible to students a little while after the course is over - will help in playing around and learning a bit more. Is this a feature available ? atleast for verified profiles?

 Hello there!
In lab2 3a I was trying to use .reduceByKey(lambda a, b: a + b) and was getting some strange error. So I searched the docs for pyspark and discovered this heavenly artifact called add. Using .reduceByKey(add) magically solved all of my problems and I am happy as a baby right now.

My question is: is this going to be prejudicial to me in the autograder/future? Hello -

I'm a bit confused on why i can't a cache of the RDD i just finished creating. Can any interpret the error?

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-295-dfb9cf210ab6> in <module>()
      9                  .takeOrdered(21))
     10 
---> 11 errByDate = errDateSorted.cache()
     12 print '404 Errors by day: %s' % errByDate

AttributeError: 'list' object has no attribute 'cache'
 Can somebody explain to me what this means in english.

Create an RDD (not a pair RDD) containing the unique tokens from each document in the input corpus. For each document, you should only include a token once, even if it appears multiple times in that document.

What is a document? 

If our RDD has just tokens then how are we going to know what website they go with?

  Up until this point, all my tests have passed. I'm not sure what the error could be since I am obtaining a value for my similarity score.

One thing I did do right before this was put a try/except block in my tfidf function in order to handle an error of a key not being in the dictionary. 

Any suggestions?

 I go to this web link and get a 404 error.

http://www.google.com/base/feeds/snippets/17242822440574356561 I have tried to execute Lab2 Exercise 3e several times now. It just doesn't do anything for hours. I have never seen it completing.

Just as an exampleprint access_logs.map(lambda a: a).take(3) doesn't produce any output even after 30 minutes. Mind you, that is the only uncommented statement in the cell. I know that that statement should not be part of the solution; I only executed it to try to figure out what is happening.I have tried variations of that ipython statement, such as access_logs.take(3)print access_logs.take(3)and nada - no output.I know that the cell does complete executing when the only statement I have is print 'All done'As a reminder I have run 3e many times with the same non-result.The previous cells, all the way to 3d, do execute. The whole of Exercise 4 executes. I submitted my lab1 and lab2 homework before the deadline and get the grade. But yesterday I found I can still submit the lab1 and lab2 homework, so I resubmit them. But I can not get feedback until now, and my lab1 and lab2 grade now becomes 0. I need your help!  I got an error while running the "#Just run this code" part in lab 1, it says:
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-37-dab03e7ea20c> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
      2 shakespeareWordsRDD = shakespeareRDD.map(lambda a: a.split(" "))
----> 3 shakespeareWordCount = shakespeareWordsRDD.count()
      4 print shakespeareWordsRDD.top(5)
      5 print shakespeareWordCount

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 98, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <genexpr>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "<ipython-input-31-652ac72e06b7>", line 18, in removePunctuation
IndexError: string index out of range

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
 
  I have tweaked and gone through my notebook a few times to make sure I conform to the submission guidelines. 
My notebook runs cleanly locally, but when I submit to the autograder I get the following: 

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
All tests passed
Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect smallest IDF value

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 5, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect rec_b000hkgj8k_weights

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect cossimAdobe

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect similarityAmazonGoogle

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect similarityAmazonGoogle

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect avgSimDups

Tokenize the full dataset (4a)
------------------------------
All tests passed
Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
All tests passed
Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
All tests passed
Create inverted indicies from the full datasets (4d)
----------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4e)
-------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect similarityTest fastCosineSimilarity

-- 12 cases passed (63.0%) --


[Stage 138:=======>                                                 (1 + 1) / 8]
[Stage 138:==============>                                          (2 + 1) / 8]
[Stage 138:=====================>                                   (3 + 1) / 8]
[Stage 138:============================>                            (4 + 1) / 8]
[Stage 138:===================================>                     (5 + 1) / 8]
[Stage 138:==========================================>              (6 + 1) / 8]
[Stage 138:=================================================>       (7 + 1) / 8]
                                                                                

[Stage 141:>                                                        (0 + 1) / 8]
[Stage 141:=======>                                                 (1 + 1) / 8]
[Stage 141:==============>                                          (2 + 1) / 8]
[Stage 141:=====================>                                   (3 + 1) / 8]
[Stage 141:============================>                            (4 + 1) / 8]
[Stage 141:===================================>                     (5 + 1) / 8]
[Stage 141:==========================================>              (6 + 1) / 8]
[Stage 141:=================================================>       (7 + 1) / 8]
                                                                                
Header datafile line: "id","title","description","manufacturer","price"
Header datafile line: "id","name","description","manufacturer","price"

[Stage 146:===========>                                           (13 + 1) / 64]
[Stage 146:===============>                                       (18 + 1) / 64]
[Stage 146:===================>                                   (23 + 1) / 64]
[Stage 146:========================>                              (28 + 1) / 64]
[Stage 146:============================>                          (33 + 1) / 64]
[Stage 146:================================>                      (38 + 1) / 64]
[Stage 146:====================================>                  (43 + 1) / 64]
[Stage 146:=========================================>             (48 + 1) / 64]
[Stage 146:=============================================>         (53 + 1) / 64]
[Stage 146:=================================================>     (58 + 1) / 64]
[Stage 146:======================================================>(63 + 1) / 64]
                                                                                
Header datafile line: "id","title","description","manufacturer","price"
Header datafile line: "id","name","description","manufacturer","price"

Your submission token id is 933560-c929a1ddd6bd1d6f36167d5d3fac85c8:87ca5e56269fafe2d3e986561a9a7970:ip-172-31-37-109
Please include this submission token id when you need support for your code submission.
Did I miss something, or is there another issue? 

Thank you
 Can someone debug this?  The error response is to big to show.

uniqueTokens = corpusRDD.map(lambda s: tokenize( s[1] ) )print uniqueTokens.take(3)
 My job took well over 2 hours before it finished. Thankfully, I got it right. It may be my machine. But I'm glad I didn't abort the job thinking that I had spun out an infinite loop.

It would be very helpful to inform the students the ideal configuration of the machine they need to use to tackle some of this labs while they are enrolling for the course. I do appreciate the fact that in Lab 3, you did in fact substantially reduce the sample size for the dataset. Had it been larger than this, I'd have given up. My lab3 passed the tests when running on the python notebook platform without taking too much time.  But the autograder fails without any messages  (just says timeout error). Also it does not indicate at what point it failed. So it is hard to debug.  I have already lost 2 submissions out of 10 without knowing which question caused the failure. I would request the staff to  please consider removing the submission limit or provide some help on what questions may be causing the timeout.  Hi. In 2c (computing IDFS), I tried to make no assumptions on the size of the # of unique tokens (i.e. I assumed it was a "big data set").
Consequently, I tried to implement (pseudo code indicated below, hopefully this doesn't violate the rules for the class):

uniqueTokens.map(lambda tok: corpus.map(lambda (id, lis) ....

It seems that Spark doesn't like nesting one transform within another. Is this by design? If so, how does one cope with the case where you are mapping over a large set and in the map you want to map over another large set?

Do we assume that uniqueTokens is "small" here and so can be "passed" to the workers (perhaps by a broadcast?)
 My code passed all the test cases in the assignment and there were no extra print statements or empty cells. I followed the instruction threads, i had added only three characters in the regular exp, but it did not work. Can any one reset my submissions so i can work out.

Thanks  Hello,
Sometimes while experimenting with commands we screw up what was correct. There should be a way to download our own last submitted assignments(which is obviously correct as it passed the autograder).

Is this possible?

Regards,
Devendra I'm really stuck on 3C, can someone send me a private message with the solution so I can learn from it?

Thanks Do I understand properly that:

1. "each document" below actually means "all documents" ?
* Create an RDD (not a pair RDD) containing the unique tokens from each document in the input corpus ... 
 
2. The word "document" below actually means "corpus" ?
* For each of the unique tokens, count how many times it appears in the document and then compute the IDF for that token: N/n(t)
     
3. The fact that uniqueTokens are RDD (not pair RDD) implies that uniqueTokens.<FILL> would have to involve
either producing a cartesian product of uniqueTokens and documentsor search within each document for each token?
    If correct, why not producing pair RDD rathen than RDD right away? As per my understanding upgrade to verified is available till today but i do not see the link .. Can some one help me what is wrong Hello,

I have a error when in run 3c in the lab ..
I think error becomes from similarityAmazonGoogle = similar(...)", but no idea ..

I tried the technique of  Abhinav Goyal : https://piazza.com/class/i9esrcg0gpf8k?cid=2289 , 
and the cosineSimilarity is
0.000303171940451
like in the next "TEST".So I think the problem becomes from the function 'similar' , or maybe the definition of similarities .. ?

similarities = (crossSmall                .map(lambda x: (computeSimilarity(x)))                .cache())
 

Error :

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-68-91815ea1a2da> in <module>()
     46 print(aR[0][1])
     47 
---> 48 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     49 print 'Requested similarity is %s.' % similarityAmazonGoogle

<ipython-input-68-91815ea1a2da> in similar(amazonID, googleURL)
     35     """
     36     return (similarities
---> 37             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     38             .collect()[0][2])
     39 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 435, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 306, in load_stream
    " in pair: (%d, %d)" % (len(keys), len(vals)))
ValueError: Can not deserialize RDD with different number of items in pair: (53, 39)

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
<br /></span></span></span></span></span></span></span></span></span></span></span> I am stuck on fastCosineSimilarity...

what is amazonRec? what is s? how to compute...any hint?

Thanks! To whom it may concern, I read in a post on Piazza on 21Jun that the deadline for Lab 2 was extended until 23 June, 00:00 UTC.  I submitted the lab withing the last 30 minutes.  I received a 100% minus the late penalty of 20% for a grade of 80%.  Although I am auditing the course and the grade does not matter, I like to do my best and wanted to validate what was said on the discussion group about the extended deadline.  Please verify.  If it was extended, then how/when will the adjustment to the grade be made?

Thanks for your attention! I have been stuck in 3c for hours.
It always gets the same error. So I did cartesian to get crossSmall and the rest fill-ins seem straightforward such as string slicing and map(cs) but the error keeps showing:

Py4JJavaError                             Traceback (most recent call last)
 in ()
     36             .collect()[0][2])
     37 
---> 38 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     39 print 'Requested similarity is %s.' % similarityAmazonGoogle

 in similar(amazonID, googleURL)
     33     """
     34     return (similarities
---> 35             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     36             .collect()[0][2])
     37 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 86.0 failed 1 times, most recent failure: Lost task 0.0 in stage 86.0 (TID 378, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "", line 19, in computeSimilarity
  File "", line 11, in cosineSimilarity
  File "", line 11, in tfidf
  File "", line 11, in 
KeyError: 'and'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

Can anyone explain? Dear Admins

I am unable to get grades on my submission on lab 3. Tried 3 times. The second one got stuck and after 2 hours, I've decided to post again. The error is below. Rechecked my code and, seems to me, no bigger improvements can be done. It runs with no problem locally and takes about 3-4 mins to run all of it

Any ideas?

Thanks

Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 939481-0cafc2b1698c15fe6e997ea103847e19:6c6ed69865be64a632cb6b6072575c18:ip-172-31-47-207
Please include this submission token id when you need support for your code submission. The directions for 2(c) first define the IDF for a token t as the ratio of the total number of documents to the number of documents containing t, but then they talk about counting the number of times t occurs in a particular document. Isn't this just a mistake? If the IDF is determined by the number of documents that contain t, then how many times any document contains t is irrelevant as long as that number is at least 1.

The exact phrase the directions use calling for counting the number of times t occurs in a particular document is

 "For each of the unique tokens, count how many times it appears in the document and then compute the IDF for that token: N/n(t)".

This doesn't make any sense if the IDF is determined by the number of documents that contain t. Hello,

My notebook is taking far too long to execute the apache log processing program. Please assist. There is no way I can complete the exercise if each step takes 45 mins to complete


Executor IDAddressRDD BlocksMemory UsedDisk UsedActive TasksFailed TasksComplete TasksTotal TasksTask TimeInputShuffle ReadShuffle WriteThread Dump<driver>localhost:5611418476.3 MB / 801.8 MB0.0 B10313212.9 m836.4 MB0.0 B0.0 BThread Dump Shouldn't the answer to this question be the third one?

In Lab 2, Apache web server log analysis, why did we include a check for log lines that failed to be correctly parsed?

1. To count the number of lines that were correctly parsed
2. To dynamically measure data quality
3. To make the lab hard to complete
4. To teach you about regular expressions

Hahaha
 I think the last step in the problem definition should be edited this way:


 #### For each of the unique tokens, count how many times it appears in the document THE NEW RDD and then compute the IDF for that token: *N/n(t)*

or maybe

 #### For each of the unique tokens, count how many times it appears in the document U and then compute the IDF for that token: *N/n(t)* I'm falling test below basically due  the token order... Ordering shouldn't be relevant for passing referred test. IMHO
 
Please advice 

# TEST Removing stopwords (1b)Test.assertEquals(tokenize(quickbrownfox), ['quick','brown','fox','jumps','lazy','dog'], 'tokenize should handle sample text') 
Which Amazon record has the biggest number of tokens? In other words, you want to sort the records and get the one with the largest count of tokens.

# TODO: Replace <FILL IN> with appropriate codedef findBiggestRecord(vendorRDD): """ Find and return the record with the largest number of tokens Args: vendorRDD (RDD of (recordId, tokens)): input Pair Tuple of record ID and tokens Returns: list: a list of 1 Pair Tuple of record ID and tokens """ return vendorRDD.####biggestRecordAmazon = findBiggestRecord(amazonRecToToken)print 'The Amazon record with ID "%s" has the most tokens (%s)' % (biggestRecordAmazon[0][0], len(biggestRecordAmazon[0][1]))

Return value of the function can have at most 1 record(key,value pair)

But the test conditions are looking for all records in descending order. 

There is mismatch between what the function says as the return condition versus how the tests are set up.
Tests are assuming all records are being returned from the function. I think I figured out how to write down in 2C except computing uniqueTokens part. I have used flatmapvalues to convert corpus but I have no idea about removing duplicates in corpus before flatmapvalue-ing. I have tried to use set() but it is not working. Could you give me some ideas? After submitting lab3.py to the autograder,there is message displayed:
"Your files have been submitted. As soon as your submission is graded, this message will be replaced with the grader's feedback."

and a buffering symbol is there.

I submitted two solutions.The first one at 3.30pm(22th) UTC.After the submission I closed the window and when later opened the buffering symbol was there.
The second submission at 1.30am UTC(23rd) and still the buffering continues with no timeout error

The time taken to execute on my machine with default sparkvm settings was 13min 52sec.

What should I do?

Thanks 



Folks: How can I get the correct list in 3c, yet be unable to cache the result?
avgDailyReqPerHost.cache()avgDailyReqPerHostList = avgDailyReqPerHost.take(30)
print 'Average number of daily requests per Hosts is %s' % avgDailyReqPerHostLis

Average number of daily requests per Hosts is [(1, 13), (3, 12), (4, 14), (5, 12), (6, 12), (7, 13), (8, 13), (9, 14), (10, 13), (11, 14), (12, 13), (13, 13), (14, 13), (15, 13), (16, 13), (17, 13), (18, 13), (19, 12), (20, 12), (21, 13), (22, 12)]1 test passed.
1 test failed. incorrect avgDailyReqPerHost.is_cache



 I want to host the labs online so that I can work on them both at work and home What is the timeout set to?  How long should I wait?  I submitted (for the 8th time) 55 minutes ago, and still no response.  This is after:

Renaming my previous worksheet out of the way (after stopping it)Uploading the updated template to start from a clean slateCopying my work into the new templateOptimizing my work a bit -- it now runs locally about 20 seconds faster at just over 3.5 minutes (on this laptop)Saving to a *.py fileDeleting all of section 5 from the PY file

Do I just resubmit again and again until I am out of resubmissions?  I have yet to get any feedback from the auto-grader.  Given that it runs so quickly, locally, it's hard for me to believe my solution is blowing out memory or taking too long.  (And anyway, if it was doing that, wouldn't I get a "timeout" or a crash rather than no response whatsoever?)

One thing I noticed -- when I uploaded the updated template, I immediately saved it so I could compare to my previous work.  I noticed that the PY files I've been uploading don't only have the code blocks in them.  They also have the markdown blocks.  I assume this is because I click Cell -> Run All?  Is this likely to cause a problem or slow things down?  The autograder just ignores markdown as comments in the PY file, right?

Should I edit the PY file and delete all of the markdown blocks and try again?  If so, should I keep "# In[7]:" type blocks, or do they not matter to the autograder?  I would expect that they're just comments that the autograder ignores, but maybe not?
 I run many problems with autograder. My experience is that don not add extra code, (simple naive check: when you download .py file checking whether the number of codes is the same with initial state.) Second is to avoid using collect() (collectAsMap() can be used in broadcast dictionary). However, I learn a lot from this lab. Friends, I am back again with a similar thread I started on lab2. Incidentally, I found lab2 quite challenging, at least some of the problems. 

So let me kindly ask those who have started lab3. What is the degree of difficlty for this lab? About the same as the previous labs? I know each lab is different.

Question to the course staff, something I struggled with at times while doing lab2. When a problem has something like RDD.<Fill In>, is it indeed the case that the problem can be solved with <Fill In> replaced by just one transformation or action? And if in fact, a chained set of transformations/actions are needed then the problem will always be in this form:

RDD.(<Fill in>
         <Fill In>
         <Fill In> ...)

Kindly confirm. 

Cheers
Ram Hi,

Question: Having 2 RDDs:
x_RDD: [(a,n1], (b,n2), (c, n3), ...]
y_RDD: [(a, m1), (b, m2), (c, m3), ...]

Note: a, b, c, etc. are keys for each entry in each RDD and having the same values.
and n1, n2, n3, etc. and m1, m2, m3, etc. are just int numbers.

How do I get the new z_RDD that is like this:
z_RDD: [(a, n1/m1), (b, n2/m2), (c,n3/m3), ...]

Thanks. Hello,

I have submitted the lab3 to the auto-grader and i wanna know which part of the exercise needs some optmizing.


submission token id is 943077-05f7de4bd9781c2cacf0a9c90a25c127:86c6ce267b853d10c9490f92d5550aed:ip-172-31-37-110

Thanks "To look up the similarity scores for true duplicates, we perform a left outer join using the goldStandard RDD and simsFullRDD and extract the"

 my Lab 3 4e part has taken more than 40 minutes and it is still running. What might cause the issue, any hint? Hi Folks,

Yet to start with lab 3. But thought i would look into TF-IDF and cosine similarity. Sharing an article which was useful.

https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/
 Hello all!

I have been working on 3b and have run into a problem.

Here is my code for 3b:

# TODO: Replace <FILL IN> with appropriate code
def cosineSimilarity(string1, string2, idfsDictionary):
    """ Compute cosine similarity between two strings
    Args:
        string1 (str): first string
        string2 (str): second string
        idfsDictionary (dictionary): a dictionary of IDF values
    Returns:
        cossim: cosine similarity value
    """
    w1 = tfidf(string1.split(' '),idfsDictionary)
    w2 = tfidf(string2.split(' '),idfsDictionary)
    return cossim(w1, w2)

cossimAdobe = cosineSimilarity('Adobe Photoshop',
                               'Adobe Illustrator',
                               idfsSmallWeights)

print cossimAdobe

Here is my error message:

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-227-92d0d8a3a59e> in <module>()
     15 cossimAdobe = cosineSimilarity('Adobe Photoshop',
     16                                'Adobe Illustrator',
---> 17                                idfsSmallWeights)
     18 
     19 print cossimAdobe

<ipython-input-227-92d0d8a3a59e> in cosineSimilarity(string1, string2, idfsDictionary)
      9         cossim: cosine similarity value
     10     """
---> 11     w1 = tfidf(string1.split(' '),idfsDictionary)
     12     w2 = tfidf(string2.split(' '),idfsDictionary)
     13     return cossim(w1, w2)

<ipython-input-220-47f65163e8c4> in tfidf(tokens, idfs)
      9     """
     10     tfs = tf(tokens)
---> 11     tfIdfDict = dict((x,tfs.get(x) * idfs.get(x)) for x in tfs)
     12     return tfIdfDict
     13 

<ipython-input-220-47f65163e8c4> in <genexpr>((x,))
      9     """
     10     tfs = tf(tokens)
---> 11     tfIdfDict = dict((x,tfs.get(x) * idfs.get(x)) for x in tfs)
     12     return tfIdfDict
     13 

TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'

Apparently, tfdf is messing the whole thing up, but it passed the tests.

Please help as I do not know what to do.

the code for it is:

def tfidf(tokens, idfs):
    """ Compute TF-IDF
    Args:
        tokens (list of str): input list of tokens from tokenize
        idfs (dictionary): record to IDF value
    Returns:
        dictionary: a dictionary of records to TF-IDF values
    """
    tfs = tf(tokens)
    tfIdfDict = dict((x,tfs.get(x) * idfs.get(x)) for x in tfs)
    return tfIdfDict

Please do not post code answers, but rather direct me as to what is necessary to be done with words. THANKS SO MUCH!! I found the language of the question to be a little ambiguous:

"Create a new nonDupsRDD RDD that has the just the cosine similarity scores for those "AmazonID GoogleURL" pairs from the similaritiesBroadcast RDD that do not appear in both the sims RDD and gold standard RDD."
Initially, I thought this meant that the pair should not appear in sims RDD and should not appear in gold standard RDD.

However, after some thought, I realized that this is actually implying that the pair should appear in sims RDD and should not appear in gold standard RDD.(The sims RDD is coming from similaritiesBroadcast RDD which has all possible combinations. So, that has to have all the pairs by definition). We have to create weight RDDs that map IDs/URLs to TF-IDF weighted token vectors,

I am trying to map the values of  amazonFullRecToToken to a dictionary given by the function tfidf() using a lambda function as lambda tokens: tfidf(tokens, idfsFull) expecting that I will get something like
[('b000jz4hqo', {'clickart': 0.5, 'dvs': 2.4, , 'rom': 1.2, ...}), ('b00004tkvy', {'noah': 5.2, 'ark':8.3, ...}), ...]
This gives me errors. Can someone please tell me what am i doing wrong?
solved it!! After I submit the python file,the feedback is strange,but in the previous submition it worked well,what should I do?thinks
Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 589
SyntaxError: Non-ASCII character '\xe6' in file /ok/submission.py on line 590, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'simpleTokenize' is not defined

Removing stopwords (1b)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tokenize' is not defined

Tokenizing the small datasets (1c)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'totalTokens' is not defined

Amazon record with the most tokens (1d)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'biggestRecordAmazon' is not defined

Implement a TF function (2a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tf' is not defined

Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'corpusRDD' is not defined

Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueTokenCount' is not defined

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'rec_b000hkgj8k_weights' is not defined

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dp' is not defined

Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsSmallBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonFullRecToToken' is not defined

Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsFullCount' is not defined

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonNormsBroadcast' is not defined

Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'invert' is not defined

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'commonTokens' is not defined

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similaritiesFullRDD' is not defined

-- 0 cases passed (0.0%) --


Your submission token id is 949837-c4194d0600ff4b00dfc68b3080e7e80a:b08f5d989fd6e31774ffde5961885280:ip-172-31-34-202
Please include this submission token id when you need support for your code submission. Thinking that the slowness of using groupByKey vs. reduceByKey in 4e would be paid back by avoiding having to use split in 4f, I compared the 2, and found that using reduceByKey is 2.5 times faster than using groupByKey, but surprisingly using split in 4f is not discernibly slower, in fact it might even be faster, one possibility is that any advantage of not having to use split by starting with an iterable is balanced by the simplicity of serializing a string. Or possibly the extra CPU cost is overshadowed by the transmission cost or possibly it's both and in addition my comp setup (many cores, lots of memory) favors something like this.

Results

Approach                     4e time                    4f time
reduceByKey-split       28 seconds        7 seconds
groupByKey-nosplit     70 seconds        7 seconds Is it normal for the autograder to just not terminate?  Lab3 runs happily on my machine, but the autograder has taken a few hours (twice) and not timed out or finished. How one could develop Domain Expertise?
How do you approach a new dataset, without prior domain knowledge?

Any ideas?
 It looked like the autograder took a very long time / not resonded at all. It turned out that when I logged off and on in the EdX dashboards the results were there.So try this if you're not seeing any response. What are some the examples of commercial data-integration tools?
Any benchmarks or report about them?
 What are some examples of commercial or open-source data-exploration tools?
 Hi, I got this message. What to do for my solution? It worked on my macbook. Is there any approximate time limit on local computer to pass autograder?
Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 950223-c6483946c7ea218c3b13ba437a8dc4d1:9b18f96061041dc4fce5a19a5c1d387e:ip-172-31-32-200
Please include this submission token id when you need support for your code submission.
Thanks I tokenized the string element of (recordid,string) tuple. In count token function I'm mapping each element of vendorRDD to its x[1] element. Then with sum() function I'm calculating the total number of words. But still I'm getting value 27424. What's wrong with my code? so from a RDD with a lot of key value pairs i want the key "software" to give me the values [1,1,1...] (not the way to turn it into a python list /dict / set first) I am getting failed tests in areas that just don't make any sense (below). The 1c error doesn't even use sortedByDay yet. The others are all correct. I have rerun them all in my local python and all tests pass. Is it possible I have an out of date python notebook or something?

Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 28, in 
NameError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
name 'sortedByDay' is not defined
Average number of daily requests per hosts (3e)
-----------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'avgDailyReqPerHostList' is not defined

Average Daily Requests per Unique Host (3f)
-------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'daysWithAvg' is not defined
 Hi,

I was asking this question for lab2 (4h) but after re-reading it, part (4h) was simpler. which I then solved (4h) already.  I was thinking (4h) was asking to print out the top 20-25 hours in the entire log file per any day, any hour.

If you don't mind, please answer the question below for my own knowledge.
======================

Question: so far, I only see one element served as a key for each entry in an RDD. Can I form a key with (a, b) where together (a,b) forms a unique key for each entry.

X_RDD: [((a1,b1), val1), ((a2,b2), val2), etc.]
where (a1,b1), (a2,b2), etc. are keys of each entry.

If possible, please give an example.

Thanks. Hi, i'm from El Salvador, since the beginning of the course i was saving money to purchase my verified certificate, however i don't see the option to purchase it anymore.
Am I no longer able to purchase a verified certificate for this course? though I have the money now.

what do you think i should do?
Did I miss the XSeries Certificate? As a data scientist we must be able to set up the environment on our own can the instructors please provide a step by step procedure to set up the environment from scratch it will be a great help. Please help, Looks like SparkVM is corrupt.

 I have been facing lot of difficulty in 3c.

What do i do in similarityRDD ?

I would be more than happy if anyone could guide me through the problem step by step.
 I get the following error when submit Lab 3 to Autograder:

SyntaxError: Non-ASCII character '\xc2' in file /ok/submission.py on line 178, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

any ideas on  how to solve this problem?

my submission token id is:
submission token id is 953391-364dffcdfc8b206e3e6d7ba0c45c68b1:fad00dd18a12ef5f5e9e14874deefdfe:ip-172-31-46-112 Unable to solve the problem with the following approaches :



1) Found the total number of documents in corpus and created a unique set of  tokens using flatmap

2) trying to find the occurrence of each unique token in documents. For one occurrence in a document I emit (token,1.0)
To find the tuples I try the following approach :

i) take a cartesian product of uniquetokens and corpus : I get token1, doc1  ; token1,doc2 etc. Now I figure out if token1 is in doc 1 and if so emit (token1,1) else (token1,0) . Then using reduceByKey I get the sum and find idf. For software its coming .0452

ii) trying to take 1 token at a time from unique token rdd and lookup in corpus. But not able to call map from within a map

Please help

 ('adobe', 0.0076045627376425855), ('software', 0.010810810810810811)

Please Help.


 lab 2 is not fetching results . already restarted it 4 times but problem occurs again after some initialization  How does the example data in the labs get onto our machines?

Is my impression that Vagrant put it into our virtual machines correct? The lab notebooks seem to be getting the data from files, not Web pages.

How can I get this data into simple files on my real machine in places where I know where to find it?

I strongly suspect that the "correct" answer for Lab 3 part 2(c) is just wrong -- my code says that 'software' only occurs in 8 of the 400 documents in the small dataset, giving it an IDF of 50 like several other tokens including 'personal', 'skills', 'files', 'need', 'high', and 'even' -- but I want to check my code by looking at the small dataset in the Vi (or Vim) editor. The small dataset is plenty small enough for Vi to handle.

I'd like to examine the data with grep, too. All the old tools are still good for debugging.

(There's a good chance that "look at the data in an editor" is debugging/testing advice this course will give later.) I was doing lab3 1c and was not able to crack it so I looked back at 1b and came to know that there is no 'the' in the stopword list. Has anyone came to know about that before? This is the example that appears in the slides about closures:
signPrefixes = loadCallSignTable()

def processSignCount(sign_count, signPrefixes):
    country = lookupCountry(sign_count[0], signPrefixes)
    count = sign_count[1]
    return (country, count)

countryContactCounts = (contactCounts 
                        .map(processSignCount)
                        .reduceByKey((lambda x, y: x+ y)))

In this code signPrefixes is a parameter of the function, so why does signPrefixes is said to be included in a closure because it is a global variable accesed by the function?As far as I know, closures only include values for the free variables of a function not for its parameters.What I am understanding wrong?Thanks !!!! I am getting  0.500277597874  has my 3b solution, Its actually wrong, I cecked my dot product and norm function both are working fine , I revisited the my previous tfidf function too, I unable to figure out where I am messing , Please give me hint to check which functionality is making error here.

Can some one please validate this value from 3b cosineSimilarity

w1 = {'photoshop': 22.22222222222222, 'adobe': 8.333333333333334}
w2 = {'adobe': 8.333333333333334, 'illustrator': 50.0}

with this I am getting the value as 0.500277597874 , I belive some thin is wrong with my w1, w2  .

 Hi All,

In iPythonNotebook it is mentioned that data files can be found at metric-learning project at cs100/lab3 location. 
I am not able to find this location to download data files. Can anybody help me with this? (3c) Perform Entity Resolution 

I got stuck in creating crossSmall Rdd , Please let me know the steps or function that like to be used to create this rdd , I tried with join and union but still I got stuck in it.

 I am doing it by join->swap->reduceByKey->map
It passed the test cases but seems to be very slow. Can someone provide any hint to an alternate solution.
Instructor has been hinting at a solution using union but I don't get how it can be faster than join. you said that with cheaper memory we can keep data in memory instead of writing it to slow disks. Well, we write in disks only. What is meant by this 'memory'? 
Hi,

I resubmitted my lab01 file but the autograder did not respond after (about 14) hours. I tried to submit again but unfortunately i got the same issue.

Could anybody help me? My user id: radvanszkit

Thanks is advance!

Regards,
Tamás
 In exercice lab3 3e, i can't make the reduce because of some values written in exponential form... How could I force not to write in this format?

regards
xavier Just in case some of you, guys are still stuck here, below are the printouts from my solution:

similaritiesBroadcast: (just to see its contents
[('http://www.google.com/base/feeds/snippets/11448761432933644608', 'b000jz4hqo', 0.0)]

sims: (using simple mapping)
[('b000jz4hqo <a target="_blank">http://www.google.com/base/feeds/snippets/11448761432933644608',</a> 0.0)]

# see sims count
sims count: 40000

goldStandard: (just to see how the RDD looks)
[(u'b000jz4hqo <a target="_blank">http://www.google.com/base/feeds/snippets/18441480711193821750',</a> 'gold')]

# see its count
goldStandard count: 1300

join sim and goldStandard for trueDupsRDD:
trueDupsRDD: (using join and map)
[0.35731731230209896, 0.2815191447747824, 0.02611365713566299, 0.8509488529300135, 0.05913886212440182]


trueDupsCount: 146

avgSimDups: 0.264332573435

nonDupsRDD take 5: (using subtractByKey() is simpler)
[0.0, 0.0, 0.0, 0.0020822177137848814, 0.00015793383390710276]

avgSimNon: 0.00123476304656

There are 146 true duplicates.
The average similarity of true duplicates is 0.264332573435.
And for non duplicates, it is 0.00123476304656. I know a few have had the same problem so I was wondering if we could have some info on:

the max timeout value ?the virtual resources available to the autograder (clock speed, memory) ?

Would help better simulate and understand the timeouts.

For reference I have optimised my code to run in 8 minutes tops on a 2015 entry model macbook pro and it still gets a timeout.

Thanks for the help, I saw there is an updated version of lab 3. I downloaded it and I tried to upload it in Jupyter. I still have the old version in the notebook. Why? What is the procedure to upload a new version and replace the old one?

In addition, how can I merge the cells that I have already completed with the new version? Is there a smart way to do that? Or do I need to copy them in a file and paste them back in the new notebook?

Thank you In addition to Spark's default machine learning library MLlib, there is another library being developed by the amp-lab: KeystoneML.

Can the instructors comment on the differences and/or if KeystoneML will one day supersede MLlib or be integrated into it? Or is it simply an either-or decision we need to make regarding which library to use? Hello,

I implemented 3c but the test fails.

def computeSimilarity(record):
    ...
    googleValue = googleRec[1]
    ...
    cs = cosineSimilarity(googleValue, amazonValue, idfSmallWeights)
    return (googleURL, amazonID, cs)

The result I get is:

Requested similarity is 0.00080500

which is wrong, since the test in 3c expects something smaller.

All tests prior to 3c were passing and I think the implementation of 3c is correct. I suspect that I have an error prior to 3c and that it has not been caught by the tests.

What do you think, is it possible?

Or does my implementation look wrong?
 Hi,

To produce the list of unique tokens in each document, I used RDD's map function,
to convert the list of tokens into a set, and convert it back as a list, as in lambda (id, t): list(set(t))

How does this scale across nodes?
What if the size of each document is a few MBs?

Is there a Sparkish way to handle this?

Also, If you pass the python's len() function via lambda, will len() be executed on the driver, or on the nodes? i return filter() with 'x not in list(stopwords)', but i get error:

AttributeError: 'str' object has no attribute 'filter'

what is problem? i no understand because i call stopwords a list. please help! thank you Could anyone explain me the 2 last parameters in textfile: ok first is the name of the file, and then?

Thanks

sc.textFile(filename, 4, 0)
 Hi,

Most probably I am doing something wrong and getting:
```
Exception                                 Traceback (most recent call last)
 in ()
     23 
     24 similarities = (crossSmall
---> 25                 .map(lambda record: computeSimilarity(record))
     26                 .cache())
     27 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in cache(self)
    189         """
    190         self.is_cached = True
--> 191         self.persist(StorageLevel.MEMORY_ONLY_SER)
    192         return self
    193 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in persist(self, storageLevel)
    205         self.is_cached = True
    206         javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)
--> 207         self._jrdd.persist(javaStorageLevel)
    208         return self
    209 
```

Some ideas on making it work. RDD should be serialisable and cashed() but not bradcasted()
The RDD looks ok: 
print crossSmall.take(1)
[(('http://www.google.com/base/feeds/snippets/11448761432933644608', 'spanish vocabulary builder "expand your vocabulary! contains fun lessons that both teach and entertain you\'ll quickly find yourself mastering new terms. includes games and more!" '), ('b000jz4hqo', 'clickart 950 000 - premier image pack (dvd-rom)  "broderbund"'))]

Thanks for the ideas. Assuming the VendorRDD

This seem to work:

takeOrdered(5, lambda x:-1*len(x[1]))

but NOT:

takeOrdered(5, lambda (x,y):(x,-1*len(y)))

Can someone explain why the second one doesn't work? In the syllabus there's no listing for the class meetings. Only the lab there.
But you can still see lecture 9 and 10 in the tag line. 
Any confirmation from the instructors the wording for exercise (1b) lab3 is very misleading. from the description it seems that we want to remove stop words from a string. but later it states:
"
implement tokenize, an improved tokenizer that does not emit stopwords.
"

so now it looks like we want to leave the string unaltered, i.e. leave stop words in a string. is this correct? that means tokenizer does nothing but count the number of words. this is very confusing, because in the tests below we clearly have to remove the stopwords. what do we do? I have been wondering about this as I often use csv and usually it's just monotone text separated by commas. Actually, I have always been wondering how does the filesystem (parser) know which are rows and which are not? Hi,

I do not use here the 'tokens' variable from the input parameters and I still get the right answer (pass the 4f test) - is this the right way?

Thanks. I clicked on those links and only to find 404 errors 
what's up Hi,
In Lab 3, 2f, I have what seems to be a rounding error, even though all previous tests pass (including 2c).
My results are (I highlighted the slight differences) :

Amazon record "b000hkgj8k" has tokens and weights: {'autocad': 33.333333333333336, 'autodesk': 8.333333333333334, 'courseware': 66.66666666666667, 'psg': 33.333333333333336, '2007': 3.5087719298245617, 'customizing': 16.666666666666668, 'interface': 3.0303030303030307}

I really don't know how to solve this, any ideas ?
 My tests are passing, but for some words, e.g., "aided" when they only appear in one document out of 400, then I get 400.0 as the IDF value according to N/n(t).

So, in 2f, I get the following, and I compute it like this idfs[x]/tfs[x]

Hints on how to proceed are highly welcomed. I've been stuck with some steps (fortunately a few parts can be solved even without having completed the previous steps).

Amazon record "b000hkgj8k" has tokens and weights:
{'autocad': 1200.0, 'autodesk': 1200.0, 'courseware': 2400.0, 'psg': 4800.0, '2007': 126.31578947368422, 'customizing': 600.0, 'interface': 109.09090909090911  Isn't there a elegant way to merge 2 rdd's ? Of course I could collect both and create a new one using parralelize but I do not thinke that thats the way to go. Any suggestions please. Hi,

when is going to be released lab4?
Because the Scalable Machine Learning course has already started (week 0), and next week there will be some overlap. 
Is there any chance to anticipate it?

Thank you very much I have created my broadcast variable amazonWeightsBroadcast from amazonWeightsRDD
I would like to search in this broadcast variable and get the dictionary for a value say 'b000jz4hqo'. A single value on amazonWeightsRDD looks like this

[('b000jz4hqo', {'rom': 2.4051362683438153, 'clickart': 56.65432098765432, '950': 254.94444444444446, 'image': 3.6948470209339774, 'premier': 9.27070707070707, '000': 6.218157181571815, 'dvd': 1.287598204264871, 'broderbund': 22.169082125603865, 'pack': 2.98180636777128})] first of all i dont know how many submissions i have on lab quizzes.
second, for lab 3 quiz part 2, ie. the hard one, should we read anything into
"much worse" as opposed to just "worse".
I definately find this question could use some clarification.
are we to imagine a new formula of F-measure that almost completely ignores the "weighting" of false negatives and how formula-wise might that look like?
cheers Update - while all the parts in this lab are now running, my queries on the RDDs and space are still open
--------------------------------------------------------------------------------------------------------------------------------------------------------
While working on this solution, using join,map,groupByKey, I once got this to run displaying the current count but the subsequent verification test gave series of errors followed by this error:

Py4JNetworkError: An error occurred while trying to connect to the Java server

Subsequent re-runs (of all earlier cells also since notebook was re-opened) gave the following error in 4e

MemoryError: out of memory.

The driver memory currently is 1.5 Gb (seen under http://localhost:4040/environment/)
while Oracle VB shows Base memory as 2Gb and processors:2.
My m/c has 4Gb RAM.

Is it the side-effect of caching so many RDDs or are they auto-discarded after notebook closure?
I also noticed that the physical file "box-disk1.vmdk" is currently over 6Gb in size and growing. Can space be reclaimed after shutting down the Oracle VB?

Any ideas?

 Hi

Are we allowed to ask for an example of how the expected RDD's for each step in a lab question should look like?

I'm having a terribly hard time to figure out what I should be doing :(

 If I have [("a", 1), ("b", 1), ("a", 1)] I can use groupByKey to get [('a', [1, 1]), ('b', [1])].

But what do I do if I have [('a', [1, 1]), ('b', [1])] and I want to get [("a", 1), ("b", 1), ("a", 1)]? I tried to submit twice this morning. 
The first time it did nothing but increment my submission count. 
The second time it has run for nearly an hour with no response. 

Is there a URL we could use to see the saturation rate of the autograder so we could know when it would be a good time to submit or not? 
If we had visibility into whether the autograder is active, available, or too busy would allow us to manage when we submit for grading. If this is something that could be made available, that is. 

Thank you for a great course!


 I'm trying to get an RDD for amazonWeightsRDD but for the moment I have a dictionary with the values and there is necessary to turn it into an RDD.

But I don't know how to turn the dictionary into a RDD.

I tried the formula sc.parallelize(dictionary) but i didn't work.

Any clue? import sysimport osfrom test_helper import Test
baseDir = os.path.join('data')inputPath = os.path.join('cs100', 'lab2', 'apache.access.log.PROJECT')logFile = os.path.join(baseDir, inputPath)
def parseLogs(): """ Read and parse log file """ parsed_logs = (sc .textFile(logFile) .map(parseApacheLogLine) .cache())
access_logs = (parsed_logs .filter(lambda s: s[1] == 1) .map(lambda s: s[0]) .cache())
failed_logs = (parsed_logs .filter(lambda s: s[1] == 0) .map(lambda s: s[0])) failed_logs_count = failed_logs.count() if failed_logs_count > 0: print 'Number of invalid logline: %d' % failed_logs.count() for line in failed_logs.take(20): print 'Invalid logline: %s' % line
print 'Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (parsed_logs.count(), access_logs.count(), failed_logs.count()) return parsed_logs, access_logs, failed_logs
parsed_logs, access_logs, failed_logs = parseLogs()

i am trying to run exactly the same script provided in the lab 2 but it is giving me an error saying Global Name sc not defined  (Main reason for new post is I am not able to see 'submit/post' button when I try to add comments for discussion 2621. Adding this to same folder as post 2621)

Thank you for the general email for all the students regarding this new course.

It is so tempting to sign up.

But I have already signed up 3 courses(honor code) including CS100.1x without giving too much thought about almost back and forth completion time for lessons/assignments.

I barely have time to catch up on each weeks lessons/discussions and rushing up to finish homework.

After going through both live and archived courses, I feel live courses let the students to have good sense of in-class experience with organized Schedule/Assignment completion time.

I may try to sign up. But not sure how much I will be able to catch with lessons/assignment completion time as all the courses I have signed up have gone well into more than 2 weeks which I feel I should try my best to stick with those already signed up courses.

My sincere thanks to CS100. 1x Professor/Team.

Also my sincere thanks to CS190. 1x Professor/Team print 'Content Size Avg: %i, Min: %i, Max: %s' % ( content_sizes.reduce(lambda a, b : a + b) / content_sizes.count(), content_sizes.min(), content_sizes.max()

what the code in Bold is actually doing ? why we use %i for Avg and min and %s for max and how the code stored value in them ? Hi All,


What is your avg. execution time for lab 3? Mine is ~ 8 to11 mins. I'm trying to submit it since last 2 days but got stuck with no response most of the time, got Timeout error once. I've used 5/10 submissions. I'm worried that it will stuck again so want to get sense of others' exec. time before re-submitting it.

In my case 4(e) seems to be the costly one, takes ~3:30 mins on individual cell run but I think it takes more while running with all cells. Anybody using reduceByKey instead of groupByKey? I tried using reduceByKey(lambda a, b : (a,b)) but it causes 4(f) tests fail. Any hints?
>> Oh it just got this at 4(e):
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 2062.0 failed 1 times, most recent failure: Lost task 7.0 in stage 2062.0 (TID 5863, localhost): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)

And for all those who succeeded, what was your autograder's response time. Last one I submitted was yesterday morning and no response so far, in-progress spinner is busy since then.

Any help will be appreciated.

NOTE: I've read other threads and there are no extra prints, collects, collectAsMaps, debugs, formatting issues .... NEVER MIND!!!!    
                                    ] Hi, I've been working on Ex 2a of Lab 3 and I'm having a problem with the second test. The thing is my dictionary contains the correct pairs of Key, Values for both tests, but just for the second test the order is not the same as the order in the assert sentence. 

I have: {'two': 0.3333333333333333, 'one_': 0.6666666666666666}

Instead of: {'one_': 0.6666666666666666, 'two': 0.3333333333333333}

I think it's not entirely incorrect since dictionaries shouldn't necessarily be ordered, but the fact is that I won't pass the second test unless I solve this problem. I guess I could sort the dictionary inside the TF function but it doesn't seems like a good (al least not elegant) option.

I would be grateful if someone could help me with this problem.
Thanks in advance. 
In Lab3 (4e) I 've identified too much common tokens from the full dataset :

I found 5966373 common tokens in place of  2441100 as test should assert.

Any idea why I got all of these ? May be is it related to partial or empty tokens or "dummy tokens" due to bad/buggy parsing of text ? 

Any idea where and how I should investigate in the previous steps of Lab3 ?

(Have tried with distinct() in the pipeline. Not a good idea because  the process is never ending now !

I was obliged to stop by force the process. Is there a better way to stop smoothly ?

My SparkVM virtual machine is now in a "guru meditation" state  :-)

Time to take a cup of tea ;-)

$ vagrant statusCurrent machine states:
sparkvm gurumeditation (virtualbox)
The VM is in the "guru meditation" state. This is a rare case which meansthat an internal error in VitualBox caused the VM to fail. This is alwaysthe sign of a bug in VirtualBox. You can try to bring your VM back onlinewith a `vagrant up`.
C:\Users\Eugene\myvagrant>

 hi all, how do i get started on (1c) of lab3? i try to use tokenize(amazonSmall) but i get the error:

AttributeError: 'PipelinedRDD' object has no attribute 'lower'

the lower comes from my definition of 'simpleTokenize' but I don't see why this is happening. any help is appreciated, thanks! Are these the parameters to computeSimilarity()?

Why it is returning 0.0?

Thanks


[(('http://www.google.com/base/feeds/snippets/17242822440574356561', 'diana ross the supremes yamaha the best of diana ross & the supremes - smart pianosoft "this innovative software series enables your disklavier mark iii piano to perform with the world\'s most popular cds! using yamaha\'s pianosmart technology this companion diskette will magically empower your disklavier mark iii to accompany the ..." '), ('b000o24l3q', 'adobe premiere pro cs3 upgrade "note: this is the upgrade version of adobe premiere pro cs3. tell your story with maximum impa (...)ased computer." "adobe"'))]
Requested similarity is 0.0.
 I run the cell for lab3 5a, go the following error
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-118-7fb21d3ac1f5> in <module>()
     23                   .map(gs_value)
     24                   .cache())
---> 25 print 'There are %s true duplicates.' % trueDupSimsRDD.count()
     26 assert(trueDupSimsRDD.count() == 1300)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 269.0 failed 1 times, most recent failure: Lost task 1.0 in stage 269.0 (TID 892, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 125, in dump_stream
    for obj in iterator:
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1636, in add_shuffle_key
    d = outputSerializer.dumps(buckets[split])
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 402, in dumps
    return cPickle.dumps(obj, 2)
MemoryError

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:914)
	at scala.collection.Iterator$GroupedIterator.go(Iterator.scala:929)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:969)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:972)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalSorter.spillToPartitionFiles(ExternalSorter.scala:370)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:211)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
<p></p>
<p>Please advise</p> Why is amazonWeightsRDD and googleWeightsRDD puking error ?

Steps:
1) mapped whole amazon dataset using a function that maps to (x,y)
Here x == id/URL , y == tfidf(amazonFullRecToToken,idfsFullBroadcast.value)

 Let's talk practical data analysis for a second.  I feel like a domain-based approach could add a lot to the product listing matching.  

On on product listing pages, a lot of text is probably manufacturer-supplied.  Many computer manufacturers, book publishers, etc., for example, have their own lengthy advertising blurb for each product that they seem to manage to place in everywhere that it's sold.    

Thus, in addition to using than use TF-IDF weights, why not create a statistic capturing the longest shared continuous string length?  A sensible implementation would probably divide that by overall document length, leave stopwords and punctuation (but not linebreaks) in, etc. 

Does anyone know if that kind of feature shows up in the real-world analysis of such datasets?  Kinda tempted to try to implement it in our lab (now that it got graded... And throws a random indentation error, snarl!) and see if I can't nudge the accuracy up a bit.  Depending on time and motivation... Hi all,

May somebody tell me which is the format of the variables in the function?

uniqueTokens
It should be and RDD like ['rom', 'clickart', '950', 'image', 'premier', '000', 'dvd', 'broderbund', 'pack'], right?
 tokenCountPairTuple
3 options:
(id, [a,b,c,.... etc]) ?
(id, count of tokens)?
other?

tokenSumPairTuple
I'm lost here

EDIT:
More detailed instructions as I read in other topic (hope that helps)

Compute the number of items in the corpus, NCreate an RDD that has, for each item in the corpus, a list of the unique tokens in that corpusUse that RDD to create a ('token', 1) tuple corresponding to each time a token appears in an item in the corpusUse reduceByKey or similar to get something like ('token', <# of corpus items token appears in>)Apply a map function to turn the values in the RDD from counts to IDF values (N/ ...)

Thx in advance ! I am not getting correct result of Lam3 2F
 
I am following below approach.
1. prepared dictionary from tokens.

Result :
{'autocad': 2, 'autodesk': 1, 'courseware': 2, 'psg': 1, '2007': 2, 'customizing': 2, 'interface': 2}
tfidDict : {'autocad': 100.0, 'autodesk': 100.0, 'courseware': 200.0, 'psg': 400.0, '2007': 10.526315789473685, 'customizing': 50.0, 'interface': 9.090909090909092}2. calculate "tfIdfDict" by dividing values as "idfs[k] / tfs[k]".3. return "tfIdfDict" from function.I am getting below output.Amazon record "b000hkgj8k" has tokens and weights:
{'autocad': 100.0, 'autodesk': 100.0, 'courseware': 200.0, 'psg': 400.0, '2007': 10.526315789473685, 'customizing': 50.0, 'interface': 9.090909090909092}Not sure where my Mistake is. However, 2C passed with all 3 test.


 Hi,
I am facing problem by crating amazonRecToToken. I am using below code to RDD , but it is not working.

Could please help me on how can i create amazonRecToToken from amazonSmall
lambda x:(x.id,tokenize(x.product)))


Thanks
Veerendra Hi All

This is more of a suggestion that a question.

I just noticed that the first line of googleSmall contains the phrase:
both teach and entertain you\'ll quickly find yourself mastering new terms.

Mapping this line through tokenize leaves the token 'll' in the set.  So I went back up to stopwords and used a union method on the set in order to include ['ll'] in stopwords

Defensive programming

Regards
Jim

 I have quite literally no idea what I can do. For starters I want to verify whether all my precode is all right:

1. amazonWeightsBroadcast: Broadcasted dictionary of tokens and tfidf weights (is this tf weights? Or idf weights? Or tfidf weights?
2. googleWeightsBroadcast: Same as above for Google dataset.

3. Does fastCosineSImilarity receive ((ID, URL), list of tokens) or ((ID, URL), str of tokens)?

4. The amazonWeightsRDD, when turned into a dictionary gives me a keyError? am I supposed to use the idfsFullWeights/idfsFullBroadcast.value dictionary?

5. What does "computes the sum for each of the tokens in the token list of the products of the Amazon weight for the token times the Google weight for the token" even mean? Someone break this sentence down for me?

Many thanks in advance to whoever responds. Can anyone explain me what does this mean????
Create a fastCosinesSimilarity function that takes in a record consisting of the pair ((Amazon ID, Google URL), tokens list) and computes the sum for each of the tokens in the token list of the products of the Amazon weight for the token times the Google weight for the token. The sum should then be divided by the norm for the Google URL and then divided by the norm for the Amazon ID For those who missed the deadline to sign and pay for a verified certificate, will our grades be reserved till new launch of the course so we could pay and get it? I tried to load one of Spark's datasets, but I'm getting Connection reset by peer in PySpark. I'm using Spark 1.4.0.

I'd like to know if I'm doing something wrong. Or maybe I need to configure Spark somehow to reserve more memory or something.
PySpark code:     from pyspark.mllib.util import MLUtils     # Load and parse the data file.     data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")     print "Count:", data.count()     print data.take(1) A similar code in scala works fine on the same machine. Scala code:     import org.apache.spark.mllib.util.MLUtils     // Load and parse the data file.     val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")     data.take(1) The same PySpark code works on a different, more powerful machine. Here's a the output for PySpark (you can see that count action worked, but take action failed): Count: 100 --------------------------------------------------------------------------- Py4JJavaError                             Traceback (most recent call last) <ipython-input-12-fa876bc8c4fc> in <module>()       4 data = MLUtils.loadLibSVMFile(sc, spark_home+"data\\mllib\\sample_libsvm_data.txt")       5 print "Count:", data.count() ----> 6 print data.take(1)       7       8 ''' C:\Users\denis.perevalov\Downloads\spark-1.4.0-bin-hadoop2.6\python\pyspark\rdd.pyc in take(self, num)    1263    1264             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts)) -> 1265             res = self.context.runJob(self, takeUpToNumLeft, p, True)    1266    1267             items += res C:\Users\denis.perevalov\Downloads\spark-1.4.0-bin-hadoop2.6\python\pyspark\context.pyc in runJob(self, rdd, partitionFunc, partitions, allowLocal)     879         mappedRDD = rdd.mapPartitions(partitionFunc)     880         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions, --> 881                                           allowLocal)     882         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))     883 C:\Users\denis.perevalov\Downloads\spark-1.4.0-bin-hadoop2.6\python\lib\py4j-0.8.2.1-src.zip\py4j\java_gateway.py in __call__(self, *args)     536         answer = self.gateway_client.send_command(command)     537         return_value = get_return_value(answer, self.gateway_client, --> 538                 self.target_id, self.name)     539     540         for temp_arg in temp_args: C:\Users\denis.perevalov\Downloads\spark-1.4.0-bin-hadoop2.6\python\lib\py4j-0.8.2.1-src.zip\py4j\protocol.py in get_return_value(answer, gateway_client, target_id, name)     298                 raise Py4JJavaError(     299                     'An error occurred while calling {0}{1}{2}.\n'. --> 300                     format(target_id, '.', name), value)     301             else:     302                 raise Py4JError( Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 53, localhost): java.net.SocketException: Connection reset by peer: socket write error         at java.net.SocketOutputStream.socketWrite0(Native Method)         at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109)         at java.net.SocketOutputStream.write(SocketOutputStream.java:153)         at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)         at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)         at java.io.DataOutputStream.flush(DataOutputStream.java:123)         at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:251) at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772) at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:208) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)         at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)         at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)         at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)         at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)         at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)         at scala.Option.foreach(Option.scala:236)         at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)         at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)         at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)         at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></p> I'm total confused on the format needed for crossSmall. I'm stuck about 4hrs and could not understand what should be the format of the crossSmall and how to get googleURL, googleValue from googleRec and for amazon as well. 

I'm able to get:
[(google url, google string), (google url, google string), ...... ,(amazon id, amazon string), (amazon id, amazon string)], but i think the required format needs me to separate all the google data in to single array such that it can be accessed record[0] for google and record[1] for amazon. So I'm missing something. In lab 3 3e I get the following output 

There are 146 true duplicates.
The average similarity of true duplicates is 0.249552385844.
And for non duplicates, it is 0.00118732947106.
All the previous tests pass so the cosine similarity calculation would seem to be correct but after joining sims with the gold standard (correct count accoring to test) and using the mean method (or using reduce with a lambda to add the values then dividing by the trueDupsCount) - the average is wrong - also converting to a python list and running reduce gets the same wrong average - any ideas where I am going wrong - thanks After Lab3 (4e)  Issue : sparkvm gurumeditation (virtualbox)

C:\Users\Eugene\myvagrant>vagrant statusCurrent machine states:
sparkvm gurumeditation (virtualbox)
The VM is in the "guru meditation" state. This is a rare case which meansthat an internal error in VitualBox caused the VM to fail. This is alwaysthe sign of a bug in VirtualBox. You can try to bring your VM back onlinewith a `vagrant up`.
C:\Users\Eugene\myvagrant>vagrant upBringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Checking if box 'sparkmooc/base' is up to date...==> sparkvm: Clearing any previously set forwarded ports...There was an error while executing `VBoxManage`, a CLI used by Vagrantfor controlling VirtualBox. The command and stderr is shown below.
Command: ["modifyvm", "31e8ddab-ef0c-440b-93ec-6a841d2ae3bc", "--natpf1", "delete", "ssh", "--natpf1", "delete", "tcp4040", "--natpf1", "delete", "tcp8001"]
Stderr: VBoxManage.exe: error: The machine 'sparkvm' is already locked for a session (or being unlocked)VBoxManage.exe: error: Details: code VBOX_E_INVALID_OBJECT_STATE (0x80bb0007), component Machine, interface IMachine, callee IUnknownVBoxManage.exe: error: Context: "LockMachine(a->session, LockType_Write)" at line 471 of file VBoxManageModifyVM.cpp
C:\Users\Eugene\myvagrant>vagrant halt==> sparkvm: Forcing shutdown of VM...

C:\Users\Eugene\myvagrant> 

After "vagrant up", everything seems OK !


 I am having problems calling user defined functions in Lab parts 3 and 4.  I can create the needed datasets but cannot use map to iterate through the sets.  I keep getting:
  raise Exception(
--> 244             "It appears that you are attempting to reference SparkContext from a broadcast "
    245             "variable, action, or transforamtion. SparkContext can only be used on the driver, "
    246             "not in code that it run on workers. For more information, see SPARK-5063."

I know this is an issue of sending the collection back to the driver, but I have idea how to call user defined function on a dataset.

ie: for this:


idfsSmallBroadcast = sc.broadcast(idfsSmallWeights)similaritiesBroadcast = (crossSmall .<FILL IN> .cache()) Hi all,

In Lab 3 3C i get 
Requested similarity is 1.29738296716e-05
Did anyone else face this issue?
Thanks in advance! This code is not working as it is always running ...therefore may i treat this as non violation for forum usage ...not sure what is wrong need hint ...all test upto 3b is passing, completely stuck with 3c

I am removing the code, it is working but took 10 mins to complete. the code was not wrong but the assumption everything runs in seconds time for this exercise is wrong. I am removing it as it may be against honor code.

Thanks Kevin - your rely helped I'm very frustrated with the video classes: basically the instructor is just reading the slide content without any further explanations.

I don't see any value in doing this! You can just as well post the slides without any videos!

I'd wish the videos would contain additional explanations and insights on top of the slides!
 Hi, I dont understand this part: " For each ID in a dataset, tokenize the values"
wich suppose to be the ID? I have to create that ID?
 Does keyerror 'A' has a specific meaning? Where can we find what these errors mean? Thanks Question 3c refers to idfs_small_weight.
Should this be instead idfsSmallWeights ?

I do not think we have defined  idfs_small_weight. earlier ? Provided I am on the right track, if I have an RDD of the following form[ (tok1, tok2), (tok1, tok3, tok4), (tok3, tok4) ]I am struggling to get to the following[(tok1 ,2), (tok2, 1), (tok3, 2), (tok4, 2)]Any pointers would be helpful. Py4JJavaError                             Traceback (most recent call last)
<ipython-input-155-7fb21d3ac1f5> in <module>()
      7                      .map(lambda x: x[1])
      8                      .cache())
----> 9 assert (simsFullValuesRDD.count() == 2441100)
     10 
     11 # Look up all similarity scores for true duplicates

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 412.0 failed 1 times, most recent failure: Lost task 0.0 in stage 412.0 (TID 1564, localhost): java.net.SocketException: Broken pipe
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:159)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.io.FilterOutputStream.write(FilterOutputStream.java:97)
	at org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:409)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:421)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:421)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:421)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Any ideas? For exercise (1c), how do I sum all values in a list of lists? so i have:

[('b000jz4hqo', 10), ('b0006zf55o', 19), ('b00004tkvy', 12), ('b000g80lqo', 128), ('b0006se5bq', 17), ('b000ehpzv8', 15), ('b00021xhzw', 53), ('b000gzwjgc', 137), ('b0000dbykm', 18), ('b00029bqa2', 47), ('b0007prnjo', 104), ('b000aazr5i', 177), ('b000bhl1r8', 85), ('b000i82j80', 6), ('b00006hmwc', 37), ('b000in6u62', 6), ('b000083k56', 15), ('b00006hvvo', 167), ('b0000ycfcw', 20), ('b00066dd5m', 14), ('b00002sac9', 64), ('b000bcz8ng', 68), ('b000fm18vi', 19), ('b00009apna', 28), ('b0009rgzgm', 65), ('b000ap2wyw', 7), ('b000o24l3q', 1548), ('b0009jlux8', 41), ('b000cszg2m', 47), ('b00023azve', 44), ('b000ndicuy', 439), ('b0000ycfdq', 4), ('b0007yepy6', 22), ('b000cpmtwk', 39), ('b0002qnd2y', 43), ('b000h22pg8', 431), ('b0009h9q1i', 161), ('b000cqyclu', 29), ('b0001db6i6', 62), ('b000bl40dg', 189), ('b000h22rbg', 426), ('b000bezsyi', 33), ('b0007g9760', 96), ('b0001bg5gq', 9), ('b000ivhozk', 33), ('b000buqo9a', 32), ('b00032heya', 31), ('b000butd6q', 7), ('b00032hexq', 35), ('b000b6vlh4', 60), ('b0009i9tqy', 33), ('b00024yohy', 47), ('b00020633g', 17), ('b000cbqbby', 68), ('b0002x1omi', 34), ('b0009stm6g', 9), ('b000ap0h94', 29), ('b0000ycfe0', 66), ('b00006j02n', 90), ('b0000aka82', 32), ('b00002s8if', 9), ('b0009ren8e', 47), ('b000a3x6te', 35), ('b000bnb72g', 58), ('b000ap422c', 30), ('b000098xjo', 9), ('b000314vvu', 139), ('b000100eg4', 229), ('b000fa5ens', 22), ('b0002e3g6o', 29), ('b000ju9ndq', 17), ('b000nqq29y', 10), ('b0007iqg2q', 15), ('b000099sin', 62), ('b000hj9r3u', 133), ('b0007yll20', 184), ('b0000afwwl', 80), ('b0000ak7c5', 25), ('b000162zp6', 87), ('b000i84dsy', 90), ('b000gaoo7y', 228), ('b00008ajjc', 66), ('b000jj4fbw', 53), ('b000kmcf0g', 57), ('b000ozibhk', 62), ('b000h774k0', 22), ('b000162zq0', 25), ('b0006g2wke', 22), ('b000qs8n64', 12), ('b000cpshfs', 94), ('b000ehq008', 16), ('b000f7ixx4', 419), ('b000brgbpa', 47), ('b0000c6fjm', 49), ('b00005yx8i', 8), ('b000hkgj8k', 13), ('b000040p1z', 8), ('b0002719lk', 32), ('b00006h38u', 29), ('b000lu8a10', 8), ('b00006akwh', 108), ('b0006oei8u', 29), ('b000hysu2y', 48), ('b000f613x2', 18), ('b000p9cr34', 42), ('b000j4k804', 15), ('b00008mopv', 193), ('b000nwsclo', 189), ('b000ek6ekq', 80), ('b0008ews9o', 38), ('b0002agbtc', 33), ('b0009y6f0g', 34), ('b000g017kg', 10), ('b0006g31i6', 18), ('b000i2qubi', 774), ('b000qfqa1w', 325), ('b000056b62', 97), ('b000fbk6gc', 47), ('b0007d8es0', 47), ('b0006z16qq', 39), ('b0002w37x8', 46), ('b0006g31ao', 18), ('b00005atxo', 68), ('b000eavbdq', 9), ('b000ozhfsq', 196), ('b000083k83', 13), ('b0002bqqxw', 23), ('b000e6q3em', 87), ('b0009hmx36', 15), ('b00007bgty', 20), ('b0006g2zce', 27), ('b000jx3qxq', 5), ('b000ehpzyu', 18), ('b000al7fu2', 183), ('b00005lzly', 134), ('b000bta4k6', 189), ('b000m06st0', 15), ('b000300hhi', 14), ('b0000swyw4', 167), ('b000ndibtg', 1466), ('b0001g6u98', 22), ('b000hcmtly', 14), ('b000fbgate', 23), ('b0000y7wc0', 189), ('b000e6g7x4', 26), ('b0002nt34o', 41), ('b000cc09uw', 43), ('b0001gu7g0', 38), ('b00073fl7w', 26), ('b000192rq0', 158), ('b00004s4xy', 39), ('b000b86a9q', 18), ('b000jbxxtk', 16), ('b000hvxix8', 13), ('b0000cc778', 96), ('b000jywfq4', 21), ('b000042olp', 112), ('b000bvbmci', 78), ('b000j588ge', 84), ('b000mtdwa4', 45), ('b00006lhf6', 56), ('b000s8jxpc', 206), ('b0009zhg8k', 8), ('b000e5n0mg', 23), ('b00079pmwa', 118), ('b0000dzfvd', 21), ('b000i2jd0s', 63), ('b0009xndso', 10), ('b00099qrok', 55), ('b000068xl9', 38), ('b000bi7uqs', 157), ('b0006g2wr2', 155), ('b000fbgaqc', 27), ('b00008ava7', 107), ('b000hvtk9e', 20), ('b000jjrwas', 60), ('b000jx29wa', 15), ('b00063blg8', 30), ('b0002bqrq8', 33), ('b000ecoami', 99), ('b000067vpa', 101), ('b0006b4zr2', 78), ('b000aly75i', 40), ('b000a7mfn8', 123), ('b0007d8eq2', 30), ('b000hlt57q', 42), ('b000fepvcs', 19), ('b0000y7wck', 166), ('b000gv8u32', 67), ('b000btas3y', 117), ('b0001o04tw', 65), ('b00004swll', 33), ('b000qv5xqy', 68), ('b000bw7kxw', 177), ('b000b5ss76', 11), ('b000067fdw', 48), ('b000h5ex7e', 8), ('b00008pbus', 18), ('b000fqvxge', 171), ('b0007q7mg2', 21)]

where the first entry is the id and the second entry is the number of tokens associated with that id. i want to sum all the second entries. how do i do this? do i need to join all the data or is it  just some simple count() or sum() function? thanks! Does anyone have a sense for how far towards the O'Reilly Databricks Spark Certified Developer these two courses will get you? The page at Databricks says to work through O'Reilly "Learning Spark" book which is in my queue, but it would be nice to know if we're 20% through or 80% through what's required for that... I am getting 22,523 tokens instead of 22,520 tokens for the combined data sets: 16,710 for amazon and 5,813 for google.

Any suggestions?

Regards, Is it possible to include the task admin screen discussion. How is our code written in Python actually getting executed in background i.e. how many tasks, worker, data it has processed.
I am still missing the real feel of Spark till now.. Hey guys,

1) I joined two RDDs
2) I used map to apply swap function and results look fine
3) I tried to use reduceByKey(lambda x,y: x + '' + y).map(lambda x: (x[0], x[1].split()) but it gives an error, even when I don't use the second map function

Does anyone know what is causing the problem?

Thanks! Hello Anthony,

I am not sure but I feel that either there is bug in 1d :) or it could have second possibility:


First lets take the bug scenario:

Consider the line::

print 'The Amazon record with ID "%s" has the most tokens (%s)' % (biggestRecordAmazon[0][0],
 len(biggestRecordAmazon[0][1]))

Now,in this line we have :: len(biggestRecordAmazon[0][1])) -- see the "len"

As per the test case I am getting correct answer :: 
[('b000o24l3q', 1547)]
but the for the 2nd value we are trying to find out len -- we cannot have len for 1547-
so basically we cannot have len(biggestRecordAmazon[0][1])) = len(1547)

If this is a bug then we will have to remove "len" in original code and it should only be  biggestRecordAmazon[0][1] instead of len(biggestRecordAmazon[0][1]))

Now, lets consider the second possibility

Or what I can do is that I can extract list as shown below::

('b000o24l3q', [['adobe', 'premiere', 'pro', 'cs3', 'upgrade', 'note', 'upgrade', 'version', 'adobe', 'premiere', 'pro', 'cs3', 'tell', 'story', 'maximum', 'impact', 'using', 'adobe', 'premiere', 'pro', 'cs3', 'upgrade', 'software', 'start', 'finish', 'solution', 'efficient', 'video', 'production', 'includes', 'adobe', 'encore', 'cs3', 'adobe', 'onlocation', 'cs3', 'windows', 'formerly', 'award', 'winning', 'dv', 'rack', 'hd', 'save', 'time', 'set', 'capture', 'using', 'adobe', 'onlocation', 'output', 'expand', 'creative', 'options', 'via', 'integration', 'adobe', 'effects',........etc

Then on this list I can try finding len?

Please let me know which approach do you feel is better?

Thanks,
vibhor I applied flatmap on all before calling idfs and got the message:
There are 17078 unique tokens in the full datasets. There are 12443 Amazon weights and 11236 Google weights
Wheras amazon weights should be 1363.

The RDDs - I started from - have the same structure:
print fullCorpusRDD.take(10)print amazonFullRecToToken.take(10)

If I understand it right, there are 1363 ids (items, sentences) in the amazon dataset:
print amazonFullRecToToken.count()
But the weights are figures related to the distinct tokens , which are many more than sentence-ids.

I'm puzzled.
Thanks for any help.

 I'm really unclear as to what "associated value" means here:

"Each element of the corpusRDD should be a pair consisting of a key from one of the small datasets (ID or URL) and the value is the associated value for that key from the small datasets"

Now, I happen to know that there are 200 distinct key values in amazonRecToToken and 200 distinct key values in googleRecToToken, but after that I'm stuck.

 @Instructor - Hello,

I enrolled for this course today and i want to go for a verified certificate.But, i can't find any option wherein i could make the payment for the verified certificate.Please help. @Instructor - According to the screenshot attached below. One of your instructor replied to a query saying that the last date for opting for a verified certificate is 23rd July. But, i cannot see any option to go for verified certificate. Please check and increase the deadline for the verified certificate so that we can opt for it.

Attaching the screenshot.

 Can someone comment on the background of why we sometimes have to use spark API methods to perform operations on pure python objects. Example:
in 3(b) of lab1 we apply reduce(add) to a list to find the sum of a list. 
I was expecting to parallelize before applying this method to convert the result of map to make it an RDD before I could apply the reduce method which is an RDD method. Python list does not have a reduce method.
Am I right in assuming that Spark methods apply to RDDs? How to remove multiple exclamatory marks.I tried using string.punctuation but it removes only one exclamatory mark.I could pass the test but had a manipualtive method of saying :
filter(x not in[....]).The [...]contained various exclamatory marks and space and...

How could it be more elegant?  I started working on Lab3.
Little curious, how is my computer getting all these .csv files. I did not download them.
We are concatenating 'data', 'cs100',  'lab3', and name of the csv file. But, we are not adding any URL. I searched in my computer, I do not have these data files. From where these files coming to my computer? Hi all,

In the tfidf function I'm confused about calculate token frequencies. I guess it's calculated as:

tf = (number of times token appears) / (number of tokens)

Questions:
1. number of times token appear.... where???? in idfs dict?
2 .number of tokens.. is it the length of idsfs dict?
3. tfs should be a dict?

Thx in advance !!
Regards, Howdy -

I was one of the lucky folks who got Databricks Cloud Access. I am student 0297.

Small problem...my permissions are not sufficient to view or clone the homework notebooks in the EdX MOOC folder. Likewise, my permissions are not adequate to upload the notebooks to Databricks cloud, once I retrieve them from Github.

I thought the point was that Cloud access was supposed to make doing the homework easier...but it seems I have no choice but to do it on the local VM like everybody else.

Also, I signed up for the 2nd course in the series. Will my Databricks Cloud student subscription be continued to cover the second course?

I was only informed I even had access AFTER I had more than halfway completed HW 2 locally. As such, I have yet to have any opportunity whatsoever to compelte even 1 homework on Databricks Cloud. Just got through 4f and wanted to post my thoughts on how I debugged.  I spent considerable amount of time with this.  I looked at each variable I defined in the function and how I was extracting the variables from "record".  I noticed that token was only pulling the first token, which completely makes sense.

if record =  (('b00005lzly','http://www.google.com/base/feeds/snippets/13823221823254120257'),'data', 'complete', 'includes', 'software' )

then record[1] is the the second item on the list.

Because of this I implemented a step to iterate over the the length of "record" and collect all items excluding item[0].  This also did not work in the end.  So then I thought, why don't I make the tokens their own list. Therefore, I adjusted my swap function to return (key, [token]). 

now for record =  (('b00005lzly','http://www.google.com/base/feeds/snippets/13823221823254120257'),['data', 'complete', 'includes', 'software'] ),

all of the tokens are the second item or record[1]. This fixed everything and passed all tests and really made my day.

I don't know if I am the only one who encountered this problem, but hopefully this will help someone. HI I have finished the lab2, though it is one day late when I submitted it. I should get 80 pts
according to the policy--getting a 100 raw score. But I didnot see it from the progress panel.
any TA could help me in this?
My submission id is  
Your submission token id is 880828-b8eb4f7a2ee0135303c5ebdce0d25053:53ad9c51eb5d5fd0c194719e5c982ba6:ip-172-31-32-91 Everything was going well, I was just finishing lab 3 4f, saved everything and shutdown the notebook and the vm.

Today, I booted the vm to finish the exercises, went to 4f and clicked on 'Cell' -> 'Run all above'. And it executes until the test in 3b. After a really long time, it passes the test and stops at the next one.

Anyone have any idea of what's going on? I'm reading how to start a cluster

https://spark.apache.org/docs/1.4.0/spark-standalone.html

What's the appropriate way to start a cluster?

If I have N machines with m cores each. Which of the following should I launch:
1. One master on one of the machines. N-1 workers on each of the other machine. Each using m cores
2. One master on one of the machines. N workers on each machine including master. Each using m cores.
3. One master on one of the machines. (N-1)*m workers with 1 core for each worker. Each machine other than master gets m workers.

Anything else?

 Hello,
I am new to python and kind struggling to find out the correct one
here is what I tried but I get wired answers in the answer set

return [t for t in string if t not in stopwords]

If I am doing could some guide me in the correct way of doing. I followed the advice the instructor and others gave, limit use of `collect()`, use `collectAsMap()` only when indicated.
I submitted my lab3 yesterday at around 5pm, 18 hours later (now) autograder is still running, will update here once it's finished.
Locally, lab3 took about 8~9mins to finish from start to end, with 2 processors and 4gb memory in Virtual box.

Just wanted to see if others are experiencing the same long wait. I am trying to compute the similarities but gettting error : "
AttributeError: 'tuple' object has no attribute 'map'
"

I have written below code for similarities.

similarities = (crossSmall<REDACTED>
                .cache())


 Can somebody please elaborate on the structure of the result CorpusRDD to be formed.
I am assuming it to be the following one,correct me if i'm wrong.
((amazon.id,amazon.value),(google.id,google.value)) I understand what a 'black box' is in this context – a process that is hidden such that one does not know the algorithm that is transforming the inputs into the outputs. I don't know, though, what a 'dart board' is in the Data mining and analysis context.  
Hi,


Finally I get the results back from the Autograder this morning.  But it says I have lost 20 points because of late submission.
 
Any chance I can get credit of 20 points since I was trying to submit for a couple fo days.
 
Thanks
 
Ram

 Hi folks,

I am passing all tests for the tf, and idf calculations. However, my numbers with the tf-idf calculation seem to be wrong. I'd greatly appreciate if someone who has solved 2f correctly could share with me their IDF and TF values for the tokens 'autocad' and '2007' so that I can compare with my results to determine whether my error is in my IDF, TF or TF-IDF methods.

Thanks a lot!

I had an error in my tf method that I just noticed. Problem is sorted. Hello,

I get the following counts after the invert - 

There are 90427 Amazon inverted pairs and 62987 Google inverted pairs.

Any hints on what could be wrong ? Appreciate any help Thanks !

print amazonWeightsRDD.flatMap(lambda record: record[1].keys()).count() --> gives 
90427 Hello guys!

I succeeded in implementing a tf function, but I know it won't pass the second stage, since it doesn't reduce by key.

My problem is: I don't know how to reduce it.

Either I got a dictionary, and I can't put the reduceByKey function, or I have a RDD (that's not what's asked) and obviously it doesn't pass the test.

I tried to find a way to reduce the dictionary but failed so far.

Any help? badEndpointsCountPairTuple = badUniqueEndpoints.reduceByKey(lambda a, b: a + b)

badEndpointsSum = badEndpointsCountPairTuple.map(lambda x:(x[0],len(set(x[0])))).groupByKey()

print 'Top Twenty 404 URLs: %s' % badEndpointsTop20

badEndpointsTop20 = badEndpointsSum.takeOrdered(10, lambda s: -1 * s[1])
Following is the trace:

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-103-4b07ac0a54f3> in <module>()
     15 pprint(badEndpointsSum.takeSample(False,5))
     16 
---> 17 badEndpointsTop20 = badEndpointsSum.takeOrdered(10, lambda s: -1 * s[1])
     18 pprint(badEndpointsTop20.takeSample(False,5))

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in takeOrdered(self, num, key)
   1172             return heapq.nsmallest(num, a + b, key)
   1173 
-> 1174         return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
   1175 
   1176     def take(self, num):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 552.0 failed 1 times, most recent failure: Lost task 0.0 in stage 552.0 (TID 1072, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1174, in <lambda>
    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
  File "/usr/lib/python2.7/heapq.py", line 432, in nsmallest
    result = _nsmallest(n, it)
  File "<ipython-input-103-4b07ac0a54f3>", line 17, in <lambda>
TypeError: unsupported operand type(s) for *: 'int' and 'ResultIterable'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

 I've got this email:

Instructor Ameet Talwalkar posted a new Note. Your instructor selected to notify everyone in real time of this post, bypassing user email preferences.Verified ID Deadline, Scalable Machine Learning (CS190.1x), XSeries Certificate
We have three announcements to share with you:

1) The Verified Track deadline for this course is June 23 23:59 UTC.  Sign up here.

...

However, when I go to sign up there, I get:

The verification deadline for Introduction to Big Data with Apache Spark was Jun 22, 2015 at 23:59 UTC. Verification is no longer available.

What happens?

Thanks
 Dear instructor,

It suggests to use a sort and take 1st but there is another way. Is other method acceptable?

Naci Hello,

I am kind of stuck as I am new to python .

I am not sure how do i count the words from the tuple that is generated 
I used the following function to tokenize
.map(lambda a:(a.Id,tokenize(a.product)))
return list.count(vendorRDD)

Could you suggest me the right way to do it
error:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-60-547947bf48df> in <module>()
     12     return list.count(vendorRDD)
     13 
---> 14 totalTokens = countTokens(amazonRecToToken) + countTokens(googleRecToToken)
     15 print 'There are %s tokens in the combined datasets' % totalTokens

<ipython-input-60-547947bf48df> in countTokens(vendorRDD)
     10         count: count of all tokens
     11     """
---> 12     return list.count(vendorRDD)
     13 
     14 totalTokens = countTokens(amazonRecToToken) + countTokens(googleRecToToken)

TypeError: descriptor 'count' requires a 'list' object but received a 'PipelinedRDD' Hi, I tried to submit my lab three times. The last version run in 18 minutes on my notebook with no errors. 
Can any TA help me?

My submition token
975467-429a0b104bfaa1a4c6cc512b1670a497:b3c893560ddaa80829b0085304317bf1:ip-172-31-42-165

Also I have a question about this statement in lab:  "Do not use collect() actions to collect all data from an RDD". How can I put an RDD into broadcast variable without collecting the items? Can you please give me some ideas about convertinga  list in an RDD inside a function that I have to use in a lambda later on? 

someFunction : sc.parallelize(list)

amazonRecToToken = (amazonSmall.map(lambda a : (a[0], someFunction)))

It seems like if someFunction uses Spark Context, it cant be used in a lambda. Please help.
It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063. 1d says:  Which Amazon record has the biggest number of tokens?
In other words, you want to sort the records and get the one with the largest count of tokens


It seems like if we're looking for the largest entry, the most efficient way to do this is to use a reduce operation, where the reduce simply compares the token count of the two records and returns the larger of the two.  Sorting the entire data set, as suggested, seems like it would take a lot longer.  I haven't been able to get the reduce to work yet, and wanted to check to see if it's simply never going to work and I should give up and do the sort method instead.
 Hi,

  I don't get what I am doing wrong in lab 3, 4b. My tfidf function (which passed all the tests before) just doesn't work here.  I did:

amazonWeightsRDD = tfidf(amazonFullRecToToken.flatMap(lambda (k,v): v),idfsFullBroadcast.value)

  But I get a funny error about how 'PipelinedRDD' object is not iterable.  I don't have a PipelinedRDD object, I have a list of strings (tokens), so what gives?  Any hints would be appreciated. As usual I can't understand the instructions: Is this the idea ?

tf_dict = { 'cat' : 2, 'dog', 3}idf_dict = {'cat' : 3, 'dog' : 5}answer = {'cat' : 6, 'dog' : 15} Problem is with the instanceof check:

Test.assertTrue(isinstance(amazonNormsBroadcast, Broadcast), 'incorrect amazonNormsBroadcast')

Results in an error
NameError: name 'Broadcast' is not defined Hi,
This is the returned of my function for lab3 (1d):

def findBiggestRecord(vendorRDD):

    print vendorCountSortedList[0]    return vendorCountSortedList[0]

biggestRecordAmazon = findBiggestRecord(amazonRecToToken)print 'The Amazon record with ID "%s" has the most tokens (%s)' % (biggestRecordAmazon[0][0],                                                                   len(biggestRecordAmazon[0][1]))

Here is the run output:
('b000o24l3q', 1547)The Amazon record with ID "b" has the most tokens (1)

Notice the ID value is "b" but not "'b000o24l3q'. This causes my lab 3 (1d) to failed in the next asserted test. But looking at the record I printed out the line above it, the record ID looks right.

# TEST Amazon record with the most tokens (1d)Test.assertEquals(biggestRecordAmazon[0][0], 'b000o24l3q', 'incorrect biggestRecordAmazon')Test.assertEquals(len(biggestRecordAmazon[0][1]), 1547, 'incorrect len for biggestRecordAmazon')

Running output of the asserted Test above:
1 test failed. incorrect biggestRecordAmazon1 test failed. incorrect len for biggestRecordAmazon

Note: I downloaded the lab3 latest from GitHub for this work.

Please help. Thanks!
 Can a partially complete notebook be submitted? The description is:
Create an RDD (not a pair RDD) containing the unique tokens from each document in the input corpus. For each document, you should only include a token once, even if it appears multiple times in that document.¶
Should should we even keep the IDs? Or should we discard them and then use flatmap to get all of the tokens into one list?

Thoughts welcome! The test-block shows that the correct value for idfsFullCount should be 17,078, but when I run that first code segment in section of 4b replacing "Small" with "Full"  in (xxxFullRecToToken)
....
I get the old Small number for unique Tokens.  I'm not sure how this could happen.  The record counts from 4a are correct ....

Amazon full dataset is 1363 products, Google full dataset is 3226 products
Does anyone have any suggestions?
 I have been stuck on this for a long, long time 

similarities = (crossSmall.map(computeSimilarity).cache())

gives me the following:

242         # This method is called when attempting to pickle SparkContext, which is always an error:
    243         raise Exception(
--> 244             "It appears that you are attempting to reference SparkContext from a broadcast "
    245             "variable, action, or transforamtion. SparkContext can only be used on the driver, "
    246             "not in code that it run on workers. For more information, see SPARK-5063."

I've tried moving this out to standard python and the code runs FOREVER.

Any idea what I am doing wrong here? Hi,

All previous tests are successful but on this section I am getting: 

Requested similarity is 0.000720187336971

Anyone having the same result? When the autograder throws back a time out error, it does not give a hint which part of the code takes too long to execute. To find the time-eating part of the code, one has to submit incrementally longer partial assignments. This, in turn, eats the time of the student and submissions. Please make the time out error message more helpful.


(I did not have collect() or groupByKey() expressions and my code could be executed cell by cell without any errors on my machine.)  I am confused about what we are supposed to return in 4d.  It says " list of pairs of token to ID", but does this mean a list of lists?  So for example if the input record is:

('colors', {'red':1},{'blue':2}), 

then the output would be

( ('red', 'colors'), ('blue','colors') )? this is what i have done for getting s. I dont know if this is right?

 tfidf the tokens and the broadcastvalue[record[ID]] for both amazon and google broadcast values.

I then dotprod the output of the above.
I got s=  0.00365581524652 I dont know if this is right ?

for computing value  = divide s by norming amazonRec and then the value divide by norming googleRec

and i get this error

TypeError: string indices must be integers, not str

you help is very much appreciated, I trying to fix this for the past 5 hours. 
If I take a break away from my laptop; my previously-running IPython notebook often shows a "Connection Failed" error.

I typically have to go through the protocol of doing a 'vagrant reload' and then "Ctrl+Enter'; RE-processing each of the IPython notebook cells from the beginning to get back to the place I had left just prior to seeing the "Connection Failed" error.

This is not optimal as the current Lab2 takes awhile to execute each step on the large sample data file.

I usually do a Ctrl+S to save/checkpoint my notebook after successful unit test points.   

Is there an easy way to reconnect to the VM and then pickup processing at the last-saved checkpoint?  
This would be so much more time-efficient; and user-friendly for me!





 Would you please tell me what is wrong in this. 
idfsSmallBroadcast = sc.broadcast(idfsSmallWeights)
I got this error
---> 20 idfsSmallBroadcast = sc.broadcast(idfsSmallWeights)
     21 
     22 similaritiesBroadcast = (crossSmall

AttributeError: 'list' object has no attribute 'broadcast'I

  unfortunately I stick in 1d with print amazonRecToToken I got it sorted in the right way

[('b000o24l3q', 1547),..

but within the function return vendorRDD. I stuck and I do not have a clue to solve it :(

please help me Hi what do we mean by the following statement

We will treat each document as a vector in some high dimensional space. 

Do we mean document here is one line of RDD or it is entire RDD? Hello, my 4f test failed so I am debuging it.

First display values:

print similarityTest
amazonRec = 'b00005lzly'googleRec = 'http://www.google.com/base/feeds/snippets/13823221823254120257'D = commonTokens.filter(lambda ((aID,gURL), tokens): aID == amazonRec and gURL == googleRec).take(1)
for t in D[0][1]:     print t     print amazonWeightsBroadcast.value[amazonRec][t]     print idfsFullBroadcast.value[t]     print googleWeightsBroadcast.value[googleRec][t]

na = amazonNormsBroadcast.value[amazonRec]ng = googleNormsBroadcast.value[googleRec]
print naprint ng

and see:


[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), 7.249921151292871e-06)]
data
18.5040322581
18.5040322581
18.5040322581
complete
7.72558922559
7.72558922559
7.72558922559
includes
13.6577380952
13.6577380952
13.6577380952
software
2.47252155172
2.47252155172
2.47252155172
11490.7640508
7139.01784366
The first weird thing I see is that both weigth for amazon and google for the same token is the same. Maybe it makes some sense as we calculate it for full document. Any pointer where could be the root cause? Of course all other tests passed. But I believe there is something wrong with my code :(

Thanks for any pointers.

Regards,
Ryszard Hello there. I'm stuck in lab2 3c because I have no idea which attributes I can call from log
I tried dir(access_logs) and access_logs.___dict___ but no success.

Example:
access_logs.map(lambda log: (log.host))
I'm mapping access_logs passing this lambda function. But I have no idea which command I can use to tell my that I can use log.host

Thanks Hello everybody,

I would like to ask a question about RDD. I have the following RDD :

[('b00002s8if', 8), ('b00002sac9', 63)]

I tried to sum the values 8 and 63. But I don't know that transform is necessary. I used some transforms but nothing works.

Could somebody help me?
Thanks in advance





 I'm stuggling with 4b I think partially because I'm not sure I understand exactly what I'm supposed to compute.

I know what a TF is and I know what an IDF is.  I don't know what is meant by the word "weight".

I don't understand the difference between idfsFull and idfsFullWeights. My mac has the following specs:
          OSX: Yosemite 10.10
          Processor: 2 GHz Intel Core i7
          Memory: 8 GB 1600 MHz DDR3

The lab3 file sizes are:
             1.8MB Amazon.csv
             1.1MB Google.csv
             100K Amazon_Google_PerfectMapping.csv

1. All files are super small and great for working on them in class. However, the time that Spark takes processing them, parsing them, converting to RDDs, and so on takes really long time. For example, code in (3d), (3e), (4b), (4e), (4f) takes quite some time. Although I haven't time these individual cels, I would expect their code to run in less than a second rather than a few seconds (maybe 30 seconds in some cases?). Why is Spark so slow? What timing can we expect when we process files of 1TB or larger?

2. The lab3 analysis at the end where precision, recall, and f scores are compared is great. My question is why do we need a Spark accumulator class?

3. Can we create broadcast functions? Or broadcast is useful only for RDDs and data variables?

4. What is the difference in process timing of the two approaches for entity resolution(ER)? Do we really save time with the 'scalable' ER?

5. What coding strategies can we use to speed things up?  Hi, 

I have submitted my solution for lab 3, and it has been running for more than one hour, without failure nor timeout, and it's still running. Tests pass locally, it is normal such a long running time?

Thanks in advance, 

Greetings, 

Juan Rodriguez

--- 

I have visited the progress page and the lab appears completed at 100%, it looks like there was some error refreshing the page but the submission was successful, I'm using Google Chrome Version 43.0.2357.124 (64-bit) on MacOSX 10.9.5. Also I get the following error

Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

although I haven't modified that class, but all the tests appear as passed anyway



All tests passed
-- 19 cases passed (100.0%) --


Your submission token id is 979318-3f6c0b2d8482d9e506cc9aed778df66f:6ee100f234f5f50c336b1a1787f98de9:ip-172-31-34-148
Please include this submission token id when you need support for your code submission.

 


Could someone please confirm that my submission has passed ok?

Thanks a lot for your help

Best Regards, 

Juan
 Anyone can think of a reason for getting 4773 words instead of 4772 in this exercise?

It's driving me crazy, I really don't get it...

- For uniqueTokens I'm getting unique tokens by combining map function on a set, and then casting the set to a list.
- For tokenCountPairTuple I'm using flatMap and maping to tuples > (K, 1.0)
- For tokenSumPairTuple simply using reduceByKey

Assert tests 2 and 3 are working correctly, so I think the resulting RDD is correct, the only problem is the number of tuples, I'm getting one extra tuple for the expected result. Really weird...
 On question 4b, I get the correct counts and I pass the tests, but I not sure of my output. Here is what I get from the AmazonWeightRDD. Is this right?
In addition what is the role of idfsFullBroadcast in this exercise? I am not sure I understand it.

Thank you!
[{'rom': 2.4051362683438153, 'clickart': 56.65432098765432, '950': 254.94444444444443, 'image': 3.6948470209339774, 'premier': 9.27070707070707, '000': 6.218157181571815, 'dvd': 1.287598204264871, 'broderbund': 22.169082125603865, 'pack': 2.98180636777128}, {'laptops': 11.588383838383837, 'desktops': 12.74722222222222, 'backup': 2.8015873015873014, 'win': 0.501859142607174, 'ca': 9.10515873015873, 'v11': 50.98888888888888, '30u': 84.98148148148148, '30pk': 254.94444444444443, 'desktop': 2.23635477582846, '1': 0.3231235037318687, 'arcserve': 24.28042328042328, 'computer': 0.6965695203400122, 'lap': 127.47222222222221, 'oem': 46.35353535353535, 'international': 9.44238683127572, 'associates': 7.284126984126985}, {'case': 5.28078250863061, 'center': 6.953030303030303, 'noah': 208.5909090909091, 'ages': 7.871355060034306, 'multimedia': 7.070878274268105, 'jewel': 7.192789968652038, '3': 0.6964638033085445, 'victory': 34.765151515151516, 'activity': 10.175166297117517, '8': 1.2641873278236915, 'ark': 208.5909090909091}] I'm stuck in 2c.
Could somebody tell me the intermediate output results so I know I'm on the right path?

Based on what I've read/got the output of:

N = 4772
uniqueTokens = 400
tokenCountPairTuple = ???
tokenSumPairTuple = ???

I'm lost at what I need to do in the last two steps.
 the code which we have written must be able to pass through the grader if the code is running on my computer and it cant be run on the ec2 cloud then it should no be the students problem my code passed each and every test atleast partial tests must be executed and graded if not all tests please look into the matter because some solutions really are long with no work arounds. Mine took forever to finish, I had to kill it since it has been running for 2 hrs and not done yet.  I did use broadcast for all 4 weights & norms.  Not sure what else I can do ... anyone has the same issue? I am stuck in the section  : Pre-compute TF-IDF weights.  Build mappings from record ID weight vector. If you have an account as part of the mooc on mooc01 or mooc02.cloud.databricks.com, note that you do not have to install the test_helper library or do any of the steps in the  dbc-mooc-setup.dbc archive (mounting the spark-mooc bucket or attaching test_helper module).  That setup has already been done for you.  Those instructions are meant for Databricks customers who are running their own clusters.

If you do try adding test_helper, you'll just cause the clusters to show a "Failed to attach" error message like this:

Error: Installation failed with message: Requirement already satisfied (use --upgrade to upgrade): test-helper in /home/ubuntu/databricks/python/lib/python2.7/site-packages Cleaning up... Requirement already satisfied (use --upgrade to upgrade): test-helper in /home/ubuntu/databricks/python/lib/python2.7/site-packages Cleaning up...

That is simply noting that the library is already available in the default Python site-packages on those machines.  But it confuses other students who think it means that the library is not available....

I've removed all the existing student copies of test_helper, in order to clean up the "Failed to attach" errors.
#pin I passed the tests for 1b.

I am dong exactly what others are saying for 1c.

Yet, I am getting a huge bunch of unintelligible error feedback that nobody else is mentioning...ending with this line:

Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
Which I have interpreted to mean that I need to collect. But that hasn't helped, because when I collect, I created a list, which I can not then map.
 Can we apply a lambda function to sortByKey?

For example, in Lab 3 question 1d:
If I would like to use the length of the lists (the second element of the <key,value> pair) as a sorting criteria? 
Can would I go about doing it?
my original attempt was something like vendorRDD.sortByKey(lambda (k,v):-1*len(v)), but it is not doing what I wanted?

Many thanks guys So say I have this:

[['cat','dog','tennis'],['cat','burrito','dog','tennis'],['burrito','tennis','dog']]

I want to count how many time  The solution of Scalable ER is NOT scalable as proposed by the teacher in lab3.
You need to broadcast amazonWeightsBroadcast and googleWeightsBroadcast.
These two python dictionaries could be very large and do not necessarily fit in one machine.

The scalable  solution would need a 'join' to be  performed to pick the weights for the specific URL and ID
 Can somebody tell me what is wrong with this? 

   w1 = tfidf( tokenize(string1), idfs)    w2 = tfidf( tokenize(string2), idfs) Hi all,

Just to share. Do not try this you might loose your work.

I accidentally deleted my notebook file from the VM Homepage (http://localhost:8001/tree#notebooks)
but the notebook's page was still open on my browser with all the information/code

I restarted the kernel and confirmed about the variables... manualy restart...
the notebook showed again running on (http://localhost:8001/tree#running) and then I was able to save a checkpoint and get it back again in the homepage and now it seems to be working fine.

Luky me! What is a document?

I was reading about the definition of N, and I found different views.

A document is the tuple string line from the data set:

Document 1: [('b000jz4hqo', ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund'])]...
The number of documents is N1: 400
If n(t) = 3, then IDF = N1/n(t) = 133.333333333

A document is one tuple from the dataset with repeated words :

Document 2: [('b000jz4hqo', 'clickart')]...
Number of documents is N2: 22520
If n(t) = 3, then IDF = N2/n(t) = 7506.66666667

A document is one tuple from the dataset without repeated words :

Document 3: [('b000jz4hqo', 'clickart')]...
Number of documents is N3: 15144
If n(t) = 3, then IDF = N3/n(t) = 5048.0

A document is one element from token list with unique words :  

Document 4: ['aided']...
Number of documents is N4: 4772
If n(t) = 3, then IDF = N4/n(t) = 1590.66666667

I hope by showing these computations I'm not breaking the honor code, but I think it is critical to understand what to do.

 Does our dotproduct function have to be able to handle dictionary's that have keys that don't always match, even though that is not tested for?  Thanks.

 This lab is a god dam nightmare! In other post I noticed the line in amazonSmall was a k,v pair where mine is just
     b000jz4hqo: clickart 950 00 - premier image pack ............

When I try to convert it to k,v using map with split() I receive an error "tuple" object has no attribute 'split'


Am I approaching this correctly? In other words does it need to be converted to k,v so the value can be tokenized?

Or is there a way to "loop" through all the elements (except the key) to tokenize?




  Hello all!

I am getting a strange error message when I am running the following (incomplete code):

# TODO: Replace <FILL IN> with appropriate code
sims = similaritiesBroadcast

trueDupsRDD = <FILL IN>PLEASE POST ONLY RESULTS NOT SOLUTIONS CODE

#print 'There are %s true duplicates.' % trueDupsCount
#print 'The average similarity of true duplicates is %s.' % avgSimDups
#print 'And for non duplicates, it is %s.' % avgSimNon
My error message is this:

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-79-bea540d6c86b> in <module>()
      8 trueDupsCount = dict(trueDupsRDD.map(lambda (idStr,(cosScore,goldString)) : (idStr,1)).reduceByKey(lambda x,y : x+y).collect())
      9 avgSimDups = trueDupsRDD.map(lambda (idStr,cosScore,count):(idStr,cosScore)).reduceByKey(lambda x,y : x+y).map(lambda (idStr,sumCosScore):(idStr,float(sumCosScore)/trueDupsCount.get(idStr)))
---> 10 print avgSimDups.take(10) #toDebug. toDelete
     11 #nonDupsRDD = (sims
     12 #              .<FILL IN>)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 238.0 failed 1 times, most recent failure: Lost task 0.0 in stage 238.0 (TID 1537, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
  File "<ipython-input-79-bea540d6c86b>", line 9, in <lambda>
ValueError: need more than 2 values to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

So whenever I comment out the line with my print (debug) statement, I do not have any error, but when I print take(n=10) or collect(), I get this error.

Makes me think there is a logical error in my code.

Please help out if you can! Any help is appreciated. Hi,

in lab 3 (2c), the answer for the total unique tokens between two small data sets amazon and google is 400.

But in lab 3 (2d) the expected total unique tokens is 4772.
My output for lab 3 (2d) is still 400 as expected.

See below the output for my lab3 (2d):
uniqueTokens: 15144tokenCountPairTuple: 15144tokenSumPairTuple: 400There are 400 unique tokens in the small datasets.
In lab 3 (2d), amazon and google small data set is "unioned" before being processed. In theory, the combined total number of unique tokens should still be 400. Why does lab 3 (2d) expects 4772 unique tokens?

Am I missing something?

Thanks for your help. Before I have a stroke can you tell me if the tests for Lab 3 3A are incomplete because they don't handle cases where the keys in both dictionaries don't match. And in Lab 3 3B our dotproduct function needs to handle that?

Some of the posts seem to imply people are on a fools errand for the remainder of the lab if their dotproduct function can not handle dictionary's with keys that all don't match.

Can you please confirm. I'm 40 god dam hours in this god dam lab and my patience is running out!!! I was thinking about doing some small project at home. I have been thinking about setting up Spark on Raspberry Pi and try to see how it works. Maybe this is not the right way of learning it? Has anyone done something with Spark at home and would love to share what/how you did it? I would appreciate the insights. Hi,

I'm looking forward to the second part of this xSeries - Scalable Machine Learning which I enrolled. I wanted to complete both but saw this, first, part too late. Will this course be offered again and when will that be in that case?

With regards,
Luka Here is an example of a two dictionarys that don't have matching keys.

testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }testVec2 = {'cat': 1, 'bar': 0, 'baz': 20 }

My dotprod function give 100. 

Is that right? Since food and cat don't have matches we just ignore them? Would it be possible to add more tests or some sort of intermediate test for Lab3 1a and 1b in preparation for 1c?

My situation: I am able to pass 1a and 1b with a few a simple functions; however, I am unable to complete 1c. Even after modifying my tokenize and simpleTokenize functions (and still passing their respective tests), I either come up just a touch too low or too high from the expected 22520 combined tokens.

For example, you can modify the simpleTokenize to also exclude "{}()" and still pass 1a and 1b, but I come up short in 1c and only obtain 22509 tokens.

Could someone provide an intermediate results of 1c? That is, a listing of (id, sum(tokens)) for each id in amazonRecToToken and googleRecToToken so I can figure out which id is giving me issues in my tokenizer function(s)?
 How could i know the fields i can extract from the "access_logs" RDD? can i extract the day or shoud i write my own function to do so? I am getting a value of the following for Lab 3 3B Any ideas?



148.319480139



 Is there a post summary of the all the conceptual/logical erros as well as the typos in Lab 3.  I am pretty much done going on fools errands trying to complete this lab.

 This is more of a "theoretical" or "best-practice" question -- what are peoples thoughts on splitting this solution up into two map jobs vs. one map job. That is, it's possible to do this with (a) a map of ids and tokens followed by a map of ids and token lengths [and then some more -- to not totally give away the solution] vs. (b) a single map of ids and token lengths.

What are your thoughts on approach (a) vs. approach (b)? Are there advantages to keeping the map jobs as simple as possible (in the real world) and on a larger dataset?  Is this proof my dotproduct is working correctly? I get 120 for the dot product of these two.

testVec1 = {'foo': 2, 'bar': 3, 'baz': 5, 'larry' : 4, 'spark' : 5}testVec2 = {'cat': 1, 'bar': 0, 'baz': 20, 'john' : 2,  'spark' : 4} Take a look at this line.  It returns a string, representing exactly one (1) token.

recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1]

Now look at the assertion test.  It validates eight (8) tokens along with their weights.

Test.assertEquals(rec_b000hkgj8k_weights,                   {'autocad': 33.33333333333333, 'autodesk': 8.333333333333332,                    'courseware': 66.66666666666666, 'psg': 33.33333333333333,                    '2007': 3.5087719298245617, 'customizing': 16.666666666666664,                    'interface': 3.0303030303030303}, 'incorrect rec_b000hkgj8k_weights')

Should recb000hkgj8k  be defined like this...

recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()

...instead of this?

recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1] Anybody else getting 
There are 22530 tokens in the combined datasets in 1c? The result seems so close to the expected result, how would I debug this? '123a', '456_b', '789c123a'] 1 test failed.
simpleTokenize should handle puntuations and lowercase result

I did lower, strip, filter and removed !, dot, ! using join. What am I missing? This is causing problems further down in the assignment.
Thanks for help. 
flattened is an RDD. What is the operation to transform/ act on it so that flattened can be returned as one single list? or, all lists inside flattened can be merged into one single list? And I can assume after flattened is returned as a single list, all None will disappear?
 Although I passed all tests and got:
All tests passed
-- 19 cases passed (100.%) --

there is an exception reported by autograder:

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

It doesn't affect the score but what's wrong with it?? This class has nothing to do with 1a and only appears in 5a which I didn't change anything. I must be getting tired...

4f, second Test.assert fails because similarityTest[0][1]  =
(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), 4.286548413995202e-06)

Yet,
similarityTest = 
[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), (('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), 4.286548413995202e-06))]
similarityTest[0][0]
('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257')
which look good to me.

Any ideas of what's happening? Hi All,
 I got google token count 7640 and amazon token count 22895.

Are we supposed to remove stopwords also?

The problem definition does not mention this.

Thanks in advance
 I have tried 3 -4 times to submit lab 3, I don't see any feedback/response.  I completed lab3 and has passed all test. I then export as .py and submit to auto grader. The auto grader pin forever and never return. It does not even tell me whether it pass of fail.
 
I have tried submitting 2 times, it always spin forever.
 
How can I submit my work?  Completed the Lab 3 assignment. On my comp (i3-4360 stock) the whole thing runs in about 5 minutes.

Autograder timed out once, and second submission is still running.

I saw some other people also had issues with this. Is this a random error or is there any reason to this?

I am fairly confident of my assignment.

The only issue could be that in my initial submissions I iterated over .items() in a dictionary. In Python 3 this shouldn't be an issue, but in Python 2 I think this may increase memory requirements.

As the second submission was taking too long, I replaced all dictionary iterations to [.. for k over dict] and resubmitted. I will check the submission tomorrow.

----------

Edit: I just got graded. Changing all [... for k,v in dict.items()] to [... for k in dict] may have worked! I don't understand how to find the token and how many times it appears in each document. Especially since we are supposed to <fill in> after uniqueTokens (uniqueTokens.<fill in>). If uniqueTokens is the RDD having the transformation performed on it, then how do we access the documents (the tokens) for each id in the corpus? I don't understand. I am really lost, and I appreciate any help.  While using
rdd.reduceByKey(lambda (a, b) : a + b)
I am getting
TypeError: <lambda>() takes exactly 1 argument (2 given)

Why is that? Isn't (a, b) considered as one argument?

Further when I try
rdd.reduceByKey(lambda a, b : a + b)
It works fine without any error.

Although I expect it to be at least as bad as the previous code because lambda is taking one argument here which sort of doesn't make sense.

What's the difference among these two ways of writing lambda function precisely and why do one works while another one fails? Can I have a hint on how to combine two RDD pairs? When I do:

corpusRDD =amazonRecToToken.fullOuterjoin(googleRecToToken)

I am getting the following error:


---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-140-c8ae3df012ca> in <module>()
      2 from pyspark import SparkContext, SparkConf
      3 
----> 4 corpusRDD =amazonRecToToken.fullOuterjoin(googleRecToToken)

AttributeError: 'PipelinedRDD' object has no attribute 'fullOuterjoin'
 Learning python + spark together  => this assignment is a multi-day slog.  

My mind is fried after getting this far, and now i'm stuck on 4f.  Any thoughts on how to piece together the fastCosineSimilarity function would be helpful.  

My attempts have focused on getting some form of list comprehension on the sum variable (s?) to sum up all tokens from the xxxWeightsBroadcast maps, but maybe i'm not even accessing it correctly.  E.g. [sum(amazonWeightsBroadcast.value[tok]*googleWeightsBroadcast.value[tok]) for tok in tokens]
That apparently doesn't work, but Py4JJavaError is not that intuitive and i've been working in Python for well a few weeks now.  Hints would be appreciated.  

Thanks This might actually just be a pure Python question, but it is relevant to this course.

Is there an efficient way to deal with nested tuples, for example an RDD "RDDnested" that has this form: ((a,b),c)

To extract RDDsingle of just a, we can do this:

RDDtuple = RDDnested.map(lambda x: x[0]) which returns an RDD in the form (a,b), then
RDDsingle = RDDtuple.map(lambda x: x[0]) which gives us the desired output in the form a

For this particular example, we map twice using the same lambda function (we could be working with a more complicated RDD or looking for a different output, but the methods would be just the same) and create an intermediate RDD that serves no other function.

What I'd like to know is if these 2 to n number of map steps can be consolidated into fewer operations or fewer lines of code.

Are we allowed to execute multiple map transformations in a single line?: RDDsingle = RDDnested.map(lambda x: x[0]).map(lambda y: y[0]) to get the first element of the first element

Perhaps, using flatMap is a better idea: RDDsingle = RDDnested.flatMap(lambda x: x[0])

In any case, I thought it would be beneficial to start a thread. It seems that there are at least a couple of different ways to approach this problem, but I'm interested in finding the best (most efficient) way. Thanks everyone.

EDIT:

Maybe this: RDDsingle = RDDnested.map(lambda ((x,y),z): x)
 NEVERMIND! I figured it out! 

I can't figure out why this isn't working. I find the wording of this question impossible to understand, so maybe that's what it is. Anyway I am getting the  correct unique token count, but my idf numbers are slightly off:
[('adobe', 1.520912547528517), ('software', 2.1621621621621623), ('cs3', 2.9197080291970803), ('pro', 3.5714285714285716), ('design', 3.6036036036036037), ('new', 3.6363636363636362), ('0', 4.166666666666667), ('3', 4.25531914893617), ('tools', 4.3478260869565215), ('windows', 4.545454545454546)]
There are 4772 unique tokens in the small datasets.
Here's how I got this:
- counted the corpus to get N - flatMapped to only have tokens (no ids)- mapped to make tuples of (token, 1)
- reduced by key x+y to get (token, total_appearances)
- mapped to (token, float(N)/total_appearances)

Where did I go wrong??? For lab 3, I submitted once and waited 24 hours and the auto-grader is still grading. I tried submitting again and waited a few hours and nothing still. I tried once more and it's still grading. I ran the notebook on my vagrant and it runs in a reasonable amount of time, I don't use any groupByKey() statements, I only added collect() statements before broadcasting a variable, and I don't have any extra print statements. ~Thanks. My solution times out when I submit it to the auto grader. I think the problem is in ex 4, and 5. For example 4f took about 15 min. on my local machine. I am using caching and broadcast vars. I am new to python. Any tips for how to figure out were my implementation is spending all of its time?

thanks

Andy Hi,
I meet a problem in Lab3 3b


The result is different from the criteria 0.0577243382163, I double checked all the previous steps, all previous tests already passed
however the cossimAdobe is not what I expected, could anyone offer some help?
Thanks!

Here is the output:
w1:{'photoshop': 22.22222222222222, 'adobe': 8.333333333333334}
w2:{'adobe': 8.333333333333334, 'illustrator': 50.0}
cossimAdobe: 0.500277597875 recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1] I am trying to understand what [0] and[1] do in this expression.  There is nothing in the documentation.  Where this syntax is coming from?  Thanks! I'm getting:
Requested similarity is 2.20421857492e-23

All previous tests pass and I've checked dotprod and some others and I believe that they are correct (as well as 3c). Has anyone run into this symptom and corrected it? Thanks!

-Kevin Task: Compute Norms for the weights ..

Please suggest what

amazonNormsBroadcast
 
suppose to be.

is it related to norm() function previously used?

(
norm(testVec1) ?
)

Do I have to apply it to every line of RDD?

I have a problem understanding what it is and why we need it here.

I see that 'amazonWeightsRDD' has a form of:

[('b000jz4hqo',
  {'000': 6.218157181571815,
   '950': 254.94444444444443,
   'broderbund': 22.169082125603865,
   'clickart': 56.65432098765432,
   'dvd': 1.287598204264871,
   'image': 3.6948470209339774,
   'pack': 2.98180636777128,
   'premier': 9.27070707070707,
   'rom': 2.4051362683438153}),

Do I access values and take sqrt of them ?

Thank you ! "It is important to design a good data gathering architecture that includes integrity checks, as it is better to avoid importing bad data than to have to deal with it in the dataset" 

I really agree with this. Real life example: in the survey in CS 1901x, it asks us to enter the access code for piazza discussion forum and the username for edX to prevent people who are not enrolled in the course from submitting the survey. 

 I really enjoyed the class so far, but I would like to see more examples in real life just like the one I listed above along with theoretical lectures. Thanks!

Best,  I've got a RDD of [(word, 1)...] tuples (tokenCountPairTuple). But for some reason calling reduceByKey on it is erroring, what's wrong with this code?:

tokenSumPairTuple = tokenCountPairTuple.reduceByKey(lambda (x,y): x + y) return tokenSumPairTuple

idfsSmall = idfs(amazonRecToToken.union(googleRecToToken))uniqueTokenCount = idfsSmall.count()
print 'There are %s unique tokens in the small datasets.' % idfsSmall.collect() As we come to the last week of this course, I want to thank the teaching staff for a wonderful foundation that has been laid.
I this we students should now reinforce the learnings of this course by applying them to real-world problems.
To this end, I suggest that we come up with usecases (if the teaching staff can give inputs that would be great) and create solutions/products built on top of spark. I am sure someof you who are co-located are already considering this.

We could create a github repo for this and start contributing towards it and thus also build on the foundations of this course.

Any ideas welcome.

Thanks I like to know what is the expected output for lab 3 4e is it 

(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), '120')

or 

(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120','comp',['other']])

I am getting the first one as my output and it passes the test , I like to know this because I got struck in the next excersize .

Thanks,
Santhanam.E For this crossSmall = (googleSmall              .cache())

Can somebody explain what is goin on here? I'm completely fried and sick of this lab.

This lab just keeps going on and on an on. I think the staff needs to understand that when you can't show your code to anybody to find out what you are doing wrong, the class just becomes a complete waste of time, screwing around in these forums trying to get help but nobody can see your code so they really can't help you. I'm completely sick of the lab and course.

     - just got to 3c...the course designers do leave clues about which useful   here it is the form
= (googleSmall
    .cache() )
which implies some kind of join because how else are you going to get amazon data into the RDD? 
so that cuts the options down no end.

                                                                               Hello everybody,

I would like to ask a question about exercise  2a. My question is: 

Is it right to use 

 map(lambda item: (item,1),item  )

And how can I use this values after. I want to count the ocurrences of item. But I don't know how to get this values after. Is it necessary to keep this values somewhere

Thanks in advance

Carlota Vina Hi,
I can't get my solution through the autograder.
On my local machine it takes about 5 minutes to run.
When I break it down I see that 4e and 4f are just about the only ones that take any time to speak of.
4e is about 1.3 minutes and 4f is about 1.7 minutes.

My questions:
1) In 4e I create the broadcast variables as python dicts from the RDD using:
dict(nameOfRDD.collect())
It works, but I see in other threads that collect increases memory requirements.
Is there any other way to make the broadcast variables?
2) Also in 4e I use groupByKey to gather up all the results. This is indicated in the documentation as a slow operation.
Any tricks here? In other threads I have seen a comment that it should be OK in this case.
3) The count() operation takes almost a minute. And this is also the case for 4f.
This was added in the template so I guess it should be there. But is it really reasonable that a count operation takes so long? Or is this related to 1) above?

I have spent three submissions now, and I'm at a loss. Any help would be greatly appreciated.

Thanks! Guys, I did solve Lab3 1c (Tokenizing the small datasets), but here is the issue I spent a lot of time on.

In def countTokens(vendorRDD), we are supposed to return the count of all tokens as follows

return vendorRDD.<FILL IN>

I sill cannot see how <FILL IN> can be replaced with just one action. I am always working under the assumption that each <FILL IN> is to be replaced by just one transformation or one action (Anthony, kindly confirm once again). In this case I needed on transformation and one action as follows

return vendorRDD.Transformation().Action()

Was anybody able to crack this with <FILL IN> replaced by just one action?

Cheers
Ram
 Can someone please give me the clue how to get the googleURL and AmazonId, I am stuck with this for more than 4 hours.

I am getting error when I try to use indexes and also getting error when I tried map for the record.

Please provide your inputs. Thanks in advance

My code snippet :

# TODO: Replace <FILL IN> with appropriate codecrossSmall = googleSmall.cartesian(amazonSmall).cache()
def computeSimilarity(record): """ Compute similarity on a combination record Args: record: a pair, (google record, amazon record) Returns: pair: a pair, (google URL, amazon ID, cosine similarity value) """ googleRec = record[0] amazonRec = record[1] googleURL = googleRec[0] amazonID = amazonRec[0] googleValue = googleRec[1] amazonValue = amazonRec[1] print googleValue cs = cosineSimilarity(googleValue, amazonValue, idfsSmallWeights) return (googleURL, amazonID, cs)
#crossSmall = googleSmall.cartesian(amazonSmall).cache()
similarities = crossSmall.map(computeSimilarity).cache()
#similarities = (crossSmall# .<FILL IN># .cache())
def similar(amazonID, googleURL): """ Return similarity value Args: amazonID: amazon ID googleURL: google URL Returns: similar: cosine similarity value """ return (similarities .filter(lambda record: (record[0] == googleURL and record[1] == amazonID)) .collect()[0][2])
similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')print 'Requested similarity is %s.' % similarityAmazonGoogle
The error message is :

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-64-8afe6ead84bf> in <module>()
     48             .collect()[0][2])
     49 
---> 50 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     51 print 'Requested similarity is %s.' % similarityAmazonGoogle

<ipython-input-64-8afe6ead84bf> in similar(amazonID, googleURL)
     45     """
     46     return (similarities
---> 47             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     48             .collect()[0][2])
     49 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 115.0 failed 1 times, most recent failure: Lost task 0.0 in stage 115.0 (TID 497, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-64-8afe6ead84bf>", line 27, in computeSimilarity
  File "<ipython-input-34-b448b7884f20>", line 11, in cosineSimilarity
  File "<ipython-input-30-cd119070e7af>", line 13, in tfidf
KeyError: 't'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span> 


print amazonWeightsRDD.take(2)


[{'rom': 2.4051362683438153, 'clickart': 56.65432098765432, '950': 254.94444444444443, 'image': 3.6948470209339774, 'premier': 9.27070707070707, '000': 6.218157181571815, 'dvd': 1.287598204264871, 'broderbund': 22.169082125603865, 'pack': 2.98180636777128}, {'laptops': 11.588383838383837, 'desktops': 12.74722222222222, 'backup': 2.8015873015873014, 'win': 0.501859142607174, 'ca': 9.10515873015873, 'v11': 50.98888888888888, '30u': 84.98148148148148, '30pk': 254.94444444444443, 'desktop': 2.23635477582846, '1': 0.3231235037318687, 'arcserve': 24.28042328042328, 'computer': 0.6965695203400122, 'lap': 127.47222222222221, 'oem': 46.35353535353535, 'international': 9.44238683127572, 'associates': 7.284126984126985}]




Can some check if the above input data is correct? Because i have been stuck with this invert function for over 2hours.


If you can explain me how the output of amazonInvPairsRDD.take(2)  it would be great. 

invertedPair = invert((1, {'foo': 2,'bar':3,'apple':6}))

gives me output ->  
('foo', 1, 'bar', 1, 'apple', 1)

is the implementation of invert correct?
 Ok this must be something silly that I'm not seeing... I got total document count (N) to be 400, the biggest token (software) count is 94 so I DO get 400/94 = 4.255319149 BUT when I did my simple transform calculation:

xxxRDD.map(lambda (a,b):(a, (float(N/b))))

then I take a look at the idf for software it's only 4.0????      What am I missing? can anyone help me to find the error

lst=re.split(split_regex,quickbrownfox)
return filter(None,lst)
print simpleTokenize(quickbrownfox) # Should give ['a', 'quick', 'brown', ... ]

output is

['A', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']but all the tests fail i need a hint for completing lab3 1 a










 return re.split('["."," "]',quickbrownfox)
​
print simpleTokenize(quickbrownfox) # Should give ['a', 'quick', 'brown', ... ]

















['A', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '']



 I am getting confused by the description for idfs function.
Recall that the IDF weight for a token, t, in a set of documents, U, is computed as follows:
Let N be the total number of documents in U.Find n(t), the number of documents in U that contain t.Then IDF(t) = N/n(t). The steps your function should perform are:Calculate N. Think about how you can calculate N from the input RDD.Create an RDD (not a pair RDD) containing the unique tokens from each document in the input corpus. For each document, you should only include a token once, even if it appears multiple times in that document.For each of the unique tokens, count how many times it appears in the document and then compute the IDF for that token: N/n(t) 

Why count the number of times a unique token appears in the document? shouldn't it be "count the number of documents that unique token appears in"? Can some one please help clarify what is the logic to calculate the norm:
Create two collections, one for each of the full Amazon and Google datasets, where IDs/URLs map to the norm of the associated TF-IDF weighted token vectors.
Thanks! I am not getting the correct answer for the fastCosineSimilarity function.
1 test passed.
1 test failed. incorrect similarityTest fastCosineSimilarity
1 test passed.
The other 2 tests pass successfully. For the 2nd test, I am getting the value
2.7713223491844514593726105766016e-6
instead of
4.286548414e-06
which is not in the acceptable range.
The list of values for the amazonWeight and googleWeight for each token are respectively:
amazonWeight = [0.06147519022612796, 0.09904601571268237, 0.047422701719576715, 0.031698994252873564]googleWeight = [1.1938085327783559, 0.29713804713804715, 0.4138708513708514, 0.07726629849137931]amazonNorm = 115.56618022160956googleNorm = 389.96983923038397
Could someone confirm these values for me? I don't think I have done any wrong computation in the function. If these values are wrong, then what might be the issue? I am stuck in this part for well over 2 hours, all other tests are passing!
Any help would be appreciated :)
Thanks. I am trying to get the 'product' part of the text into an RDD as follows:

amazonProductRDD = amazonSmall.map(lambda x:x.product)

but when I give the following command I get an error:


print amazonProductRDD.first()

Am I doing something wrong ? I don't understand what I should do on this exercise.

If I have 400 documents and I have a token on several of these docs, if I divide 400 / number of repetitions of the token on the documents software should appear on 94 docs to get (4.25....).

This is what I'm doing
In data[('b000jz4hqo', ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund']), ('b0006zf55o', ['ca', 'international', 'arcserve', 'lap', 'desktop', 'oem', '30pk', 'oem', 'arcserve', 'backup', 'v11', '1', 'win', '30u', 'laptops', 'desktops', 'computer', 'associates']), ('b00004tkvy', ['noah', 'ark', 'activity', 'center', 'jewel', 'case', 'ages', '3', '8', 'victory', 'multimedia'])]
Number of documents
400
Per each document recover the unique elements
[('b000jz4hqo', ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund'])]Count one per each repetition on each document (maybe here is the problem)
[('clickart', 1), ('950', 1)]
As we have 400 elements, if we divide N/n(t).  
[('aided', 400), ('precise', 133), ('duplex', 400), ('dance', 400), ('breath', 200), ('themes', 133), ('known', 133), ('verses', 400), ('battle', 100), ('9999', 400), ('targeted', 400), ('layers', 200), ('content', 50), ('xml', 400), ('paris', 400), ('volume', 133), ('environment', 80), ('german', 400), ('wants', 400), ('diskcromwindows', 400)]Recover the min element
('content', 50) to get count of tokens in amazonRecToToken RDD (which I created by tokenizing x[1]) I did the following:

amazonRecToToken vendorRDD.flatMap(lambda x:x[1]).count()  similarly for google,

but total tokens across both datasets I am getting is 22509.

Any idea what I am doing wrong? Out of curiosity what will be the official Certification name that we receive when the course is done? Hi,
I believe that in the definition of what the norm() function has to return is wrong. Should not it just return a value (i.e. the norm of a vector) rather than a value and a dictionary of tokens ?

def norm(a): """ Compute square root of the dot product Args: a (dictionary): a dictionary of record to value Returns: norm: a dictionary of tokens to its TF values """ Could someone tell me the approximate time for lab3 3c?
And I hope in the later labs, instructor could point out the time for our checking. I think it should save us a lot of time. I am getting an error in test 2 in section 4f, If I print the similarity that I am  getting from b00005lzly and http://www.google.com/base/feeds/snippets/13823221823254120257 it is:

[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), 0.073307995152871)] 

Please help! Hi,

RDD1 = rdd1.union(rdd2)
RDD2 = rdd1+rdd2print type(RDD1)
print type(RDD2)<class 'pyspark.rdd.RDD'>
<class 'pyspark.rdd.RDD'>
print type(RDD1.take(1))
print type(RDD2.take(1))<type 'list'>
<type 'list'>
It seems to me that in term of result, I have the same result.Is it the same thing?
 I am trying to use map function to address this question in many ways, but it seems that something wrong with my function.

amazonRecToToken = amazonSmall.map(lambda s :(s[0],tokenize(s[1])))
Guys,please give me some practical advice... If you're thinking that, then try this blog post, which also links to a good explanation of TF-IDF (although the lab does a decent job of that anyway). Hi there,
Is there an inconsistency in the definitions?
I read : 
   Find n(t), the number of documents in U that contain t.
   Then IDF(t) = N/n(t).
then
   For each of the unique tokens, count how many times it appears in the document and then compute the IDF for that token: N/n(t)

why should I count the times t appears on a document if that value is not used for n(t)?
n(t) as stated in the lab is number of documents that contain t.

In the literature, the weight is the balance between times t appears in document versus documents t appears in.
it does not look clear to me.

Any clarification welcome.

Cheers,

 I have attempted to submit lab 3 several times now. Even though the lab runs within 5 mins on my local machine, the autograder doesn't display any results. I have attempted to optimize my code further and am now on my fourth submission attempt (still no joy). 

I'd appreciate instructor comments on what my options are.

Thanks for any help!

Edit: Hurrah! It worked on the fifth attempt!  I am getting a closely different value, i.e. instead of 0.000303171940451 I am getting 0.000404873234237. All my previous test cases have passed sucessfully. Wonder what the problem might be Before I start wasting hours because I did not get the question correctly what is meant by "tokenizing the values" ?

Does that mean that for each product ID I have to tokenize the title + description + manufacturer + price values for amazon and  title + description + manufacturer + price values for google ?

Thanks.


 Hi All

I need assistance please with understanding what the RDD's should look like for each step of this question.

I think I managed to create the dayToHostPairTuple correctly.  The result of my efforts are as follow:


[(8, 60142), (12, 38070), (4, 59554), (16, 56651), (20, 32963), (1, 33996), (5, 31888), (9, 60457), (13, 36480), (17, 58980), (21, 55539), (22, 57758), (10, 61245), (18, 56244), (14, 59873), (6, 32416), (11, 61242), (15, 58845), (3, 41387), (19, 32092), (7, 57355)]This looks correct in terms of 21 days, with the amount of hosts for each day.I'm not sure though what dayGroupedHosts should look like, and subsequently, the rest of the RDD's?
 I got following error message by just joining amazonInvPairsRDD and googleInvPairsRDD.
I increased VM's memory and processor allocation to 4GB and 4.
It didn't help.

I tried 
amazonInvPairsRDD.sample(False, 0.1, 1).join(googleInvPairsRDD)
, and it worked.

any advice would be appreciated.

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-42-dfc14e76dc72> in <module>()
     15                 .cache())
     16 #print amazonInvPairsRDD.sample(False, 0.4, 1).join(googleInvPairsRDD).map(swap).takeSample(False, 20)
---> 17 print 'Found %d common tokens' % commonTokens.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 90.0 failed 1 times, most recent failure: Lost task 2.0 in stage 90.0 (TID 452, localhost): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:172)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:197)
	at java.io.DataInputStream.readFully(DataInputStream.java:169)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:111)
	... 10 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 
In Lab2, Section 2c; I don't understand how the numbers were determined for offsetting slices displayed in the pie chart:

explode = (0.05, 0.05, 0.1, 0, 0, 0, 0)





 Hi,

I had to "break" my Python code on purpose before submitting to the autograder so I can get a partial grade. Otherwise it would not respond (I tried waiting for almost 24 hrs).

Specifically I got rid of the join in 4e... then I get 89... but I'm sure my solution is correct as it runs fine on my laptop, albeit it took a long time to complete.

Hope that helps  Hi all,

I am getting 22950 for my total token count. the problem is probably in how i defined my simpleTokenize and tokenize functions, but they passed all the tests. i use:

string.isspace() to deal with empty spaces, and .strip('.!?\#~') in my simpleTokenize.

then i use simpleTokenize to define my tokenize function, and i use: [z for z in x if z not in stopwords]

again this passed all the tests, but i get the incorrect token count for (1c). any suggestions? thanks

 I got message like this after i implented the code below and have no clue about it.

    similarities = (crossSmall                .map(lambda x : computeSimilarity(x))                .cache())


/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in __getnewargs__(self)
    242         # This method is called when attempting to pickle SparkContext, which is always an error:
    243         raise Exception(
--> 244             "It appears that you are attempting to reference SparkContext from a broadcast "
    245             "variable, action, or transforamtion. SparkContext can only be used on the driver, "
    246             "not in code that it run on workers. For more information, see SPARK-5063."

Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.


Where did i mess up?? Needs some instruction :((. Hi all,

Im getting trouble creating simsRDD, calculating cosineSimilarityScore
This is similaritiesBroadcast format:

[('http://www.google.com/base/feeds/snippets/11448761432933644608', 'b000jz4hqo', 0.0)]

So I try to map:

(lambda (googleUrl, amazonId, x): (amazonId + " " + googleUrl, similar(amazonId, googleUrl)))

if I try similar(url, id) as in 3c i get the correct value for tuple 0.40202896125621296
However i get following error (end of log)

/home/ubuntu/databricks/spark/python/pyspark/rdd.pyc in __getnewargs__(self)  169 # This method is called when attempting to pickle an RDD, which is always an error:  170 raise Exception( --> 171 "It appears that you are attempting to broadcast an RDD or reference an RDD from an "  172 "action or transformation. RDD transformations and actions can only be invoked by the "  173 "driver, not inside of other transformations; for example, " Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.

What I'm doing wrong?? Hi Guys, 

I need some help please much appreciated.
 
when i ran print fastCosineSimilarity(commonTokens.first())

i get the following result
(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), 0.0043289842630974155)

how ever when i run similaritiesFullRDD = (commonTokens.map(fastCosineSimilarity).cache()) i get following error , how do we fix this ?

--------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-54-d26574481015> in <module>()
     35 
     36 similaritiesFullRDD = (commonTokens.map(lambda x: fastCosineSimilarity(x)).cache())
---> 37 print similaritiesFullRDD.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 589, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-54-d26574481015>", line 36, in <lambda>
  File "<ipython-input-54-d26574481015>", line 18, in fastCosineSimilarity
  File "<ipython-input-20-302d0cdf54ac>", line 10, in tfidf
  File "<ipython-input-12-e7c40d960f69>", line 15, in tf
TypeError: unhashable type: 'list'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


 My code

1a && 1b pass

amazonRecToToken = amazonSmall.map(lambda s :(s[0],tokenize(s[1])))
googleRecToToken = googleSmall.map(lambda s : (s[0],stokenize(s[1])))


def countTokens(vendorRDD):
""" Count and return the number of tokens
Args:
vendorRDD (RDD of (recordId, tokenizedValue)): Pair tuple of record ID to tokenized output
Returns:
count: count of all tokens
"""
return vendorRDD.values().map(lambda s : len(s)).sum()
totalTokens = countTokens(amazonRecToToken) + countTokens(googleRecToToken)
print 'There are %s tokens in the combined datasets' % totalTokens

errors:

NameError: global name 'stokenize' is not defined

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745) given:

# TODO: Replace <FILL IN> with appropriate code
from pprint import pprint
#***
#*** 
#*** 
errByDate = errDateSorted.<SNIP Honor code violation>

print '404 Errors by day: %s' % errByDate.collect()

We get:

404 Errors by day: [(1, 243), (3, 303), (4, 346), (5, 234), (6, 372), (7, 532), (8, 381), (9, 279), (10, 314), (11, 263), (12, 195), (13, 216), (14, 287), (15, 326), (16, 258), (17, 269), (18, 255), (19, 207), (20, 312), (21, 305), (22, 288)]

than running:

# TEST 404 response codes per day (4e)
Test.assertEquals(errByDate, [(1, 243), (3, 303), (4, 346), (5, 234), (6, 372), (7, 532), (8, 381), (9, 279), (10, 314), (11, 263), (12, 195), (13, 216), (14, 287), (15, 326), (16, 258), (17, 269), (18, 255), (19, 207), (20, 312), (21, 305), (22, 288)], 'incorrect errByDate')
Test.assertTrue(errDateSorted.is_cached, 'incorrect errDateSorted.is_cached')

we get:

1 test failed. incorrect errByDate
1 test passed.

Why?? <SNIP honor code violation>

It's been 20 mins and this piece of code is still up and running. What is making is run for such a long time ?
Help me out please.

Thanks When trying to broadcast the variable I receive following exception from following code:
# Recompute IDFs for full dataset
idfsFullWeights = //compute idfs of fullCorpusRDD
idfsFullBroadcast = sc.broadcast(idfsFullWeights)


Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.

 
Can anyone assist with the error? just finished Lab3 and be graded but still thinking a piece of optimization:

pairRDD = sc.parallelize([('a', 1), ('a', 2), ('b', 1)])

print pairRDD.groupByKey().mapValues(lambda x: list(x)).collect()=>[('a', [1, 2]), ('b', [1])]print pairRDD.reduceByKey(lambda a,b: [a,b]).collect()=>[('a', [1, 2]), ('b', 1)]

my .reduceByKey() version doesn't put value into list when there is only one value, how to modify the lambda in order to give exactly the same result as .groupBykey()? I dont understand why my count in 1d is failing.I am also not getting individual counts right.so in a fix and want to know where exactly have I gone wrong.
so my summary of code from 1a is
1a->I defined 2 sets
           i)punc_set-set of string.punctuation
           ii)ponk-set of ' '
           then I used filter(lambda x: not in punc_set|ponk,re.split(split_regex,string.lower()))
I got all tests passed

1b->stepp=stopwords|punc_set|ponk
       then in return of tokenize function i wrote:
      filter(lambda x:x not in stepp,simpleTokenize(string))
I passed all tests

1c->Now here i used flatMap(lambda x:tokenize(x[1])) for converting to token
and in the function I returned using just .count() 
I get for googletokens  count=6814
and for amazontokens count=18780
and this is off..
 the tests fail and I dont understand where exactly have I gone wrong.I have included the explanation of 1a,b,c for obvious reasons as all are dependent on each other.

Please help! hi friends,

I need your help , i am not able to make the correct amazonweightsrdd or googleweightsrdd , as i am getting dictionary from tfidf and i need an rdd of tokens' weights mapped with ids , so basically problem is in my map function.

So can any one , who have done the same  ,  give me some hints to map the ids/urls to the dictionary returned by tfidf()? hi,
In lab3 2c I got the first two tests right but the smallest value is coming out to be 239.574468085 which is obviously wrong.
I calculated the idf as N/b where b is the 2nd element of tuple.
What is the mistake

I had got my mistake. Thanx anyways :) Create a new `nonDupsRDD` RDD that has the just the cosine similarity scores for those "AmazonID GoogleURL" pairs from the `similaritiesBroadcast` RDD that **do not** appear in both the *sims* RDD and gold standard RDD.

Any hints to check values in both rdds?

Thx ! Hi All
 My code for 1a and 1b passed. 
But I am getting lower numbers for 1c
For google small I get 5618 and for amazon I get 16233

In 1a, I am using 
chars_to_remove = ['!@#$%^&*()-+=<,>?"']

Am I doing something wrong here which is causing lower counts?

Can I post my full solution for 1A (  it is not correct anyways )?

Thanks in advance
 for Lab 3, 3a the function definition of norm, I noticed that we were given 'import math' by default but somehow this does not work for me.

I actually have to use 'from math import sqrt' in order to have it work

Does this signal that my codes are not working perfectly? Hello,

I am trying to submit the py file.
I select the .py file by browsing, and click "Check" button. When I do this, it says no file selected. It is actually resetting the selection I guess.

So far, I have tried these two approaches:
1. Select the .py file and click "Check" button.
2. Select the .py file, click "Save" and click "Check" button.

Whenever I click "Check" button, it says - "no file chosen".

EDIT: So far, made 4 attempts with no success.

Kindly please help.

 So, while I have successfully removed stopwords in 1b using a (very) simple set operation (since stopwords variable is already a set), the 3rd test fails, since my solution

['brown', 'lazy', 'jumps', 'fox', 'dog', 'quick']

as a list, is not equal to the test list

['quick','brown','fox','jumps','lazy','dog']

Of course, they are equal as sets, and the order of elements is certainly of no importance here...

Anyway, I remedied this with a (much less elegant) solution using list comprehensions, but I think the 3rd test should be modified so as to perform a set comparison (instead of a list one)...

 Hi, 

in local all passed, but when submit there is errors 
Your submission token id is 1012662-a3caef4e1b0416647b96f53e9c577777:9f997399b11370201a352c9ac61cc402:ip-172-31-11-20

Response from edx:

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 99.0 failed 1 times, most recent failure: Lost task 0.0 in stage 99.0 (TID 512, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "", line 23, in fastCosineSimilarity
ZeroDivisionError: float division by zero

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
All tests passed
Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect smallest IDF value

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 5, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals

[Stage 117:=======>                                                 (1 + 1) / 8]
[Stage 117:==============>                                          (2 + 1) / 8]
[Stage 117:=====================>                                   (3 + 1) / 8]
[Stage 117:============================>                            (4 + 1) / 8]
[Stage 117:===================================>                     (5 + 1) / 8]
[Stage 117:==========================================>              (6 + 1) / 8]
[Stage 117:=================================================>       (7 + 1) / 8]
                                                                                
15/06/24 11:19:29 ERROR PythonRDD: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 88, in main
    command = pickleSer._read_with_length(infile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 148, in _read_with_length
    length = read_int(stream)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 528, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "", line 23, in fastCosineSimilarity
ZeroDivisionError: float division by zero

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)
15/06/24 11:19:29 ERROR PythonRDD: This may have been caused by a prior exception:
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "", line 23, in fastCosineSimilarity
ZeroDivisionError: float division by zero

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)
15/06/24 11:19:29 ERROR Executor: Exception in task 0.0 in stage 120.0 (TID 566)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "", line 23, in fastCosineSimilarity
ZeroDivisionError: float division by zero

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)
15/06/24 11:19:29 ERROR TaskSetManager: Task 0 in stage 120.0 failed 1 times; aborting job
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect rec_b000hkgj8k_weights

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect cossimAdobe

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect similarityAmazonGoogle

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect similarityAmazonGoogle

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect avgSimDups

Tokenize the full dataset (4a)
------------------------------
All tests passed
Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
All tests passed
Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
All tests passed
Create inverted indicies from the full datasets (4d)
----------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4e)
-------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 713, in collect
    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 538, in __call__
    self.target_id, self.name)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/protocol.py", line 300, in get_return_value
    format(target_id, '.', name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 120.0 failed 1 times, most recent failure: Lost task 0.0 in stage 120.0 (TID 566, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "", line 23, in fastCosineSimilarity
ZeroDivisionError: float division by zero

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


-- 12 cases passed (63.0%) --

I first think there is problem with broadcast value, but in 2c there is no broadcast, tried several times to restart vagrant and kernel save result, all passed, twice submit to edx both time same errors.

 i am not sure how to proceed , i am following this steps and getting error
amazonSmall.lambda(x: simpleTokenize(x))

Please guide what is the correct way..give some hint step

  File "<ipython-input-59-aad07e910185>", line 2
    amazonRecToToken = amazonSmall.lambda(x: simpleTokenize(x))
                                        ^
SyntaxError: invalid syntax

 
  Anthony, thanks for the clarification to my earlier post cid=2826

While I finally got my score after more than 4 hours of submission yesterday, I re-submitted 3 times today, after fixing some errors, but I am yet to get any response from the server, after more than 4 hours.

The earlier Lab 3 score has been reset. I feel that this should be classified as a bug in the autograder as the score reset should be done after the server processes and scores a fresh submission, rather than upon submit. Hi everyone,

I got stucked yesterday the whole day with autograder timeouts. So I got an insight to change my broadcast variables to use collectAsMap() instead of collect(). This changed the overall time of part 4 from 16min to 3min.
This is quite obvious now, since in a (hash)map  I have O(1) access operations and in lists (collect() produces lists) I have O(n).

Hope this help other collegues.
Vitor I'm stuck in Lab 3 4d:

I keep getting

There are 23277314 Amazon inverted pairs and 55093628 Google inverted pairs.
Anybody seen this before?

I suspect it has to do with this part:
"where each element is a pair of a token and an ID/URL that contain that token"
How did you implement this? Any hints?Thanks I am confused about 

"For each of the Amazon and Google full datasets, create weight RDDs that map IDs/URLs to TF-IDF weighted token vectors"

I have amazonFullRecToToken and googleFullRecToToken as [(K, V)] where K is IDs/URLs and V is a token list
How should amazonWeightsRDD, googleWeightsRDD look like?

Thank you ! The last week of CS100 will overlap the first of CS190? Why? In the TestAssert for 4a the code is: 
Test.assertEquals(amazonFullRecToToken.count(), 1363, 'incorrect amazonFullRecToToken.count()')Test.assertEquals(googleFullRecToToken.count(), 3226, 'incorrect googleFullRecToToken.count()')
As I was watching each step this morning, I noted that this took 4.4 minutes according to the Spark management console. 

Are there any suggestions for improving the performance of an RDD.count() ? 
It appears that the majority of the time usage for my notebooks is in the "test cells" versus the "processing cells." 

I don't recall anywhere in the lectures where due attention was given count() performance, maybe I missed it? 
As I did the run this morning, I monitored it in the Spark console. 

Just over %50 of the time for my local run this morning was in the count() functions from the Test portions. 

How can we improve this? 

Thank you.







 Got a good nights sleep and back at it again! Called into work and told them I'm not coming! Currently on 3  C. I've worked on this lab close to 50 hours. Here we go!
 
I don't know what is the problem in my approach.
Steps:
1) pairs is the mapping(x,y) of record to y.keys and x  (Here x will be ID and y.keys would be key of the dictionary)
 
My amazonInvPairsRDD  maps invert function to amazonWeightsRDD. 
Since invert function takes a pair, (ID, token vector), so is my amazonWeightsRDD. 
 
When i run the test cell it tells 
AttributeError: 'tuple' object has no attribute 'map' I just noticed it in one of exercises, it caused to make me spent a lot of time since I was assuming the reduceByKey in a list would concatenate elements (each string) and append it to accumulator (as string element), but this isn't happened, I got the chars of each string appended one by one into accumulator, very wierd.

The instructors knows why? say RDD1 = [ (a, b),  (c, d)  ]

RDD2 = [ ( 1,2),  (3,4) ]

crossSmall should equal [ ( (a,b), (1,2) ),  ( (c,d), (3,4) ) ]  is that what they want for this?

If this is right, is there some type of join that does this every other element thing or do I have to write a python function to do this? Hi guys,
I am having the following problem. I suppose it comes from my implementation of tokenize (even if I don't really know how to fix it).
Here is the message i get from the execution:

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-64-8314e83b3703> in <module>()
     10 
     11 # Pre-compute TF-IDF weights.  Build mappings from record ID weight vector.
---> 12 amazonWeightsRDD = tfidf(amazonFullRecToToken, idfsFullBroadcast.value)
     13 googleWeightsRDD = tfidf(googleFullRecToToken, idfsFullBroadcast.value)
     14 print 'There are %s Amazon weights and %s Google weights.' % (amazonWeightsRDD.count(),

<ipython-input-20-569dab89c513> in tfidf(tokens, idfs)
      8         dictionary: a dictionary of records to TF-IDF values
      9     """
---> 10     tfs = tf(tokens)
     11     tfIdfDict = {d : tfs[d]*idfs[d] for d in tfs.keys()}
     12     return tfIdfDict

<ipython-input-59-c320f89587b2> in tf(tokens)
      8         dictionary: a dictionary of tokens to its TF values
      9     """
---> 10     nb=len(tokens)
     11     d=defaultdict(lambda:0)
     12     for t in tokens:

TypeError: object of type 'PipelinedRDD' has no len() As evident for lecture 5 video on File System Performance which lists various factors on which performance can depend, it is clear that scala and java platforms give better performance for text file IO. These languages also provide API for binary files and compressed file IO where as pySpark does not or it has to depend on an external library to provide this functionality. Then why pySpark has been chosen for this course and not scala/java? I assume that simplicity and scripting abilities of python and availability of lambda functions could be some of the reasons. Are there any other reasons? Is it better to learn spark with sclala/java than python, to get better performance? If I give
googleNormsBroadcast = sc.broadcast(googleNorms)

It gives type error. What is the mistake?


TypeError: 'PipelinedRDD' object is not iterable 4d-AttributeError: 'tuple' object has no attribute 'map'
Explanation: 4d code is running correctly and grader has passed all the test cases for 4d. But locally 4d test cases are not passing, first test case is failing and displaying error: 4d-AttributeError: 'tuple' object has no attribute 'map'.
The test case is -
invertedPair = invert((1, {'foo': 2}))Test.assertEquals(invertedPair[0][1], 1, 'incorrect invert result')

I have used only map and flatmap operations to get the result.
 I submitted lab3 .py to autograder 2 times and it always hang. Last time I submitted is 12 hours ago. It still hangs and saying:

Your files have been submitted. As soon as your submission is graded, this message will be replaced with the grader's feedback.

and it is still spinning.
 
Please help. I am desperate. I read forums for lab3 2f & I am new to python.

I was able to able to do the following
   For each token in the token list:
     Look up the weight of that token in the amazon corpus
       Look up the weight of that token in the google corpus
       Multiply those weights
   You now have a bunch of numbers. Add them together.

the sum comes out to be 164.872408293
I am also getting { key : multiplication } as
{'autocad': 33.33333333333333, 'autodesk': 8.333333333333332, 'courseware': 66.66666666666666, 'psg': 33.33333333333333, '2007': 3.5087719298245617, 'customizing': 16.666666666666664, 'interface': 3.0303030303030303}
 
What I don't understand is
   Once you have that sum, divide it by the norm for the Google URL
   Divide that number by the norm for the Amazon ID
   What to return so as assert works
 
Any idea or help is appreciated.
 
 
 
 How do you actually get unique tokens inside a document, but not across all the documents?

I use this flatMap(lambda (a,b):b).distinct() which lists all the tokens, an counts 4772 of them. In lab3 2c I initially used ....distinct() when trying to write uniqueTokens but didnt run good, so I checked on piazza posts, changed it to set(), and passed the tests.

I am wondering whats the difference between these two. Any advice would be appreciable! I mean no disrespect to the course staff who wrote lab3, but I find the second step instruction a bit wordy that I'm not sure if I fully understand it. In fact I've read it half a dozen of times already. The way I understand it, and please kindly correct me on this, is that:

For each token in tokens list, add all values for Amazon weight tokens. I understand that Amazon weight shall be taken from variable amazonWeightsBroadcast using token as the key. Do the same for Google weight tokens.Multiple the two sums.Divide the product in (2) by the norm of Google URL. Now, function norm takes a dictionary and I understand it as the value of googleWeightBroadcast.Divide again by the norm of Amazon ID.Steps 1 and 2 above seems pretty much like taking the dot product of amazonWeightsBroadcast and googleWeightBroadcast.In short, above steps 1 to 4 is taking the cosine similarity between amazonWeightsBroadcast and googleWeightBroadcast.Thank you in advance.
#pin For the pythonistas out there, is it possible to implement the solution without iterating multiple times?

I find myself (1) iterating over the items to (first) count the values, then I calculate the total sum, and then (2) I iterate [again] to create the percent of total.

This is one of those times where I really wish there was some way of sharing best-practices/solutions.  How can I figure what the two missing are?

Also I tried to save a list to a text file on my local directory but it does not work. Code runs but no file.
Can this be done with this vagrant?
 Hi,just wonder if the standard answer/coding will be posted after the course is over to help us review our programming practice and learn alternatives to the ones we fill in...Thank you for the great learning opportunity! hey,

i am trying to follow the labs instructions but i am stuck. i have:

for s in tokenDict.keys():  if s in tokens.keys():    tokenDict[s] += 1.0  else:    tokenDict[s] = 1.0 return tokenDict#<FILL IN>

but it returns the empty dictionary. should i loop over something else?
i just want to print the dictionary: {quick: 1.0, ....} to see that i have the correct code, then ill divide by the total number of tokens later.
i'm unfamiliar with python and i'd appreciate some practical help, thanks. I am using the following function 
flatMap(lambda (a,b):(a,len(b))).takeOrdered(1,lambda s: -1*s)

I get the following error but when I print it get the correct count that is required but not sure

[1547]
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-108-b7b70d0a1126> in <module>()
     10 
     11 biggestRecordAmazon = findBiggestRecord(amazonRecToToken)
---> 12 print 'The Amazon record with ID "%s" has the most tokens (%s)' % (biggestRecordAmazon[0][0],
     13                                                                    len(biggestRecordAmazon[0][1]))

TypeError: 'int' object has no attribute '__getitem__'

Could some please help me out. I just assume that for some people this example might help understand the idea of lazy evaluation. Thoughts on this idiom are welcome.

> import time
> rdd = sc.parallelize(range(0,10))
> def fun(x):>   time.sleep(1)>   return x**2
> t0 = time.time()> result = rdd.map(fun).collect()> t1 = time.time()> print t1 - t0

10.0575900078

> t0 = time.time()> result = rdd.map(fun).take(5)> t1 = time.time()> print t1 - t0

5.07436203957
 
I am new to python and I am trying to solve 1a using python, not spark. I figured how to create a dictionary from the list of tokens. I cant seem to be able to loop through the dictionary to count the unique keys:

d =dict((el,1) for el in tokens) for s in tokenDict.keys():    -- this must be wrong?     if the key is present, add 1
       otherwise ....

Thank you!


  File "<ipython-input-358-87d13500828a>", line 10
    for s in tokenDict.keys():
                              ^
IndentationError: unindent does not match any outer indentation level

  I'm entirely lost in this lab, I was pretty good so far.
I had a hard time go through the first parts, and now, I'm stuck in 2c for hours.

I understand what's expected, but I have no ideas how to write it.

Let's start by confirming steps :

1.  N = <FILL IN> : ok, easy. 400 is expected, we're working on a count() function. Right?
2. uniqueTokens = corpus.map<FILL IN>
Here starts the issue. Can you confirm my trial? This gives me a list of tuples
[('b000jz4hqo', 'rom'), ('b000jz4hqo', 'clickart'), ('b000jz4hqo', '950')]
uniqueTokens = corpusRDD.flatMapValues(lambda x:set(x)).collect()
But you can use list(x) which also gets you clean token collections
Not sure why you use collect...leaving it off gives you a rdd of lists to work with, no? And you can dump the id to simplify the output lists.
Haven't got much further than this but test1 is working.



3. tokenCountPairTuple = uniqueTokens.<FILL IN> 
Ok so Maybe I have to replace my tuple 
('b000jz4hqo', 'rom')
in 

('rom', 1) 
4. tokenSumPairTuple = tokenCountPairTuple.<FILL IN>
If I'm correct so far, I don't know how to make a foreach loop to count all the same words together with a total incremented. In the example above, if I have 5 'rom' in all documents, it gives me:

('rom', 5) 

and my tokenSumPairTuple becomes, for example

[('rom', 1), ('cat', 6), ('dog', 8)]

But I have a hard time figuring out how to achieve this.

5. return (tokenSumPairTuple.<FILL IN>)

a simple map function with a lambda (a, b) : (a, b /N) right?

If I'm fully wrong, please correct me, any advice will be more than welcome, I'm lost! Did I miss something? Where do I find the syntax for filtering words that are "not in" stopwords set?  As far as I have understood the problem we need to:
- join both RDD
- swap the keys with values (which can be done easily with lambda (x,y):(y,x))
- group by key
So is there any reason I'm missing for creating the custom swap function for this mapping instead of a normal lambda function as above? I'm a bit confused when reading description of question 4e. Here's my understanding and steps:

1. Using the two inverted indicies (RDDs where each element is a pair of a token and an ID or URL that contains that token), create a new RDD that contains only tokens that appear in both datasets. This will yield an RDD of pairs of (token, iterable(ID, URL)).

Steps:
 - Concatenated both RDDs rdds (amazon and google)
 - Filtered the only items that appear in both RDDs

My output looks like this:
[('aided', <pyspark.resultiterable.ResultIterable at 0xb0d917ec>),
 ('shop', <pyspark.resultiterable.ResultIterable at 0xb0d9192c>),
 ('dance', <pyspark.resultiterable.ResultIterable at 0xb0d9234c>), ...]
2. We need a mapping from (ID, URL) to token, so create a function that will swap the elements of the RDD you just created to create this new RDD consisting of ((ID, URL), token) pairs.
Steps:
 - flatMapped the ResultIterable
 - swapped key and value

My output looks like this:
[('b0007yepy6', 'aided'),
 ('b000hlt5j4', 'aided'),
 ('b000dz9yoa', 'aided'),
 ('b00004ochi', 'aided'),
 ('http://www.google.com/base/feeds/snippets/17293379770636740081', 'aided'),
 ('b00009apna', 'shop'),
 ('b000h22pg8', 'shop'), ...]
3. Finally, create an RDD consisting of pairs mapping (ID, URL) to all the tokens the pair shares in common
I'm stuck understanding this one. What is the type signature of the final RDD?

 Can also someone who solved this question verify my first two assumptions?

Thanks.  May be I am wrong. But, I think all the issues of confusion is because the flatmap(f(.)) takes a function f(.) instead of a list L like this flatmap(L).

What do you think? 
Huh?   What am I doing wrong here?  5 / 3  isn't 1!  :/

Simple answer, I'm sure...   Thanks!

 What should the format of amazonRecToToken be?

I think recb000hkgj8k is failing to get the values in my case as I get the error on that row and I can't print it as well.


---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-98-893bc3521405> in <module>()
     13     return tfIdfDict
     14 
---> 15 recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1]
     16 idfsSmallWeights = idfsSmall.collectAsMap()
     17 rec_b000hkgj8k_weights = tfidf(recb000hkgj8k, idfsSmallWeights)

IndexError: list index out of range

 

My amazonRecToToken in 1c for now is this: 


[(('b000jz4hqo', 'clickart 950 000 - premier image pack (dvd-rom)  "broderbund"'), ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund'])]


Seems like I should change something in 1c

Thanks.
 Hi,

I reviewed my code it takes 6 min to run on my machine, 
But in 26 mins it timedout on the autograder this is my 4th submission

I have no collects, extra cells or extra cells. It is really frustrating. Please help

Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 1021355-6bfa3530d97e83acae542376cee48278:4a79ae7f1fe71c82ebc7bc26a9ea2b4e:ip-172-31-15-250
Please include this submission token id when you need support for your code submission.
 

 Just to be sure:

You can not use a spark RDD in a function that you call from a RDD.map(functionWithRDDinIt) ?!?That would be a missed opportunity ?
 I'm almost positive that if you used Spark to make your tf function instead of pure python, you will not be able to pass the tests in 3C.

 Hi,

Can somebody tell me the weight of token 'autocad'? I have 200.0

Is token frequency for 'autocad' 1/12?
based on the fact that it appears only once in the document  recb000hkgj8k:
['autocad', '2007', 'courseware', 'customizing', 'interface', 'autocad', '2007', 'courseware', 'customizing', 'interface', 'autodesk', 'psg']

tf-idf should be: 0.08333333 * 200 = 16.66 but it is not, it is 'autocad': 33.33333333333333

Thanks, cheers Hi guys,

need some help with the similarity I get 0.0003931763...
I managed to recover the example to test on 3C on an excel sheet and found the same result "manually", what makes me think there is something wrong in my code before 3C, although I validated all the tests before.

Below are some tfidf I extracted from the example and I used to do the calculation on excel. They might be wrong but I can't guess where it comes from. Could someone please give me a hint on what is going wrong?

Thanks for your help!

{'pointing': 0.25856496444731736, 'demand': 0.12928248222365868, 'results': 0.03693785206390249, 'edl': 0.5171299288946347, 'recapturing': 0.25856496444731736, 'disc': 0.08618832148243913, 'comments': 0.5171299288946347, 'disk': 0.03693785206390249, 'layers': 0.06464124111182934, 'content': 0.01520970379101867, 'promote': 0.08618832148243913, 'flash': 0.10342598577892695, 'onlocation': 0.17237664296487826, 'exporting': 0.12928248222365868, 'focus': 0.08618832148243913, 'every': 0.016160310277957335, 'updates': 0.06464124111182934, 'telling': 0.12928248222365868, 'entire': 0.028729440494146375, 'customizable': 0.043094160741219564, 'notes': 0.17237664296487826, 'list': 0.12928248222365868, 'solution': 0.009576480164715458, 'adjust': 0.12928248222365868, 'small': 0.023505905858847036, 'enhance': 0.051712992889463474, 'rack': 0.25856496444731736, 'enjoy': 0.05745888098829275, 'clients': 0.10342598577892695, 'educators': 0.12928248222365868, 'phones': 0.5171299288946347, 'sync': 0.25856496444731736, 'budget': 0.08618832148243913, 'titles': 0.17237664296487826, 'video': 0.016681610609504346, 'download': 0.12928248222365868, 'click': 0.03232062055591467, 'even': 0.009234463015975622, 'boutiques': 0.25856496444731736, 'current': 0.08618832148243913, 'version': 0.010773540185304891, 'new': 0.008916033256804047, 'metadata': 0.5171299288946347
{'magically': 3.9215686274509807, 'diana': 23.52941176470588, 'series': 1.4705882352941175, 'innovative': 1.680672268907563, 'diskette': 3.9215686274509807, 'cds': 1.3071895424836601, 'technology': 0.5882352941176471, 'best': 0.6535947712418301, 'pianosoft': 3.9215686274509807, 'yamaha': 7.843137254901961, 'perform': 1.680672268907563, 'mark': 7.843137254901961, 'supremes': 23.52941176470588, 'piano': 1.9607843137254903, 'smart': 1.4705882352941175, 'ross': 23.52941176470588, 'disklavier': 7.843137254901961, 'accompany': 3.9215686274509807, 'empower': 3.9215686274509807, 'companion': 2.3529411764705883, 'world': 0.35650623885918004, 'iii': 5.88235294117647, 'using': 0.39215686274509803, 'pianosmart': 3.9215686274509807, 'popular': 1.3071895424836601, 'enables': 0.6191950464396285, 'software': 0.1251564455569462}


EDIT : now 3C is ok after recoding tf in 2A in a simplified way! Tests in 2A didn't catch my previous error, so maybe instructors should add some robustness...
 Cannot complete the course due to hectic schedule. Can someone mail me solved Labs from Lab2 onward (after the course ends here) as i still intend to complete the course in future and  I need some reference as there'll be no discussion group    As part of one of the labs, I was playing around with comparing the relative performance of different approaches to combining the values from a pairRDD on Keys. This is a simple aggregation by key where the resulting values of the same key are combined together in an iterable.

i.e. we start with an RDD of the form [(K1, V1), (K2, V2), ...] and after combination, it has the form: [(K1, (V1, V20, ...)), (K2, (V2, V11, ...)), ...]

I tested out 3 approaches
groupByKey() - Simple way to achieve this, but is not the most scalable as it pulls all data onto one workercombineByKey() using a list as the underlying iterable that collects the values for a common keycombineByKey() using a tuple as the underlying iterable that collects the values for a common key

Implementations were as follows for each approach:
myRDD.groupByKey()

myRDD.combineByKey(lambda value: [value], lambda x, value: x + [value], lambda x, y: x + y)

myRDD.combineByKey(lambda value: (value,), lambda x, value: x + (value,), lambda x, y: x + y)
I looked at the performance of these approaches as reported in the PySpark Shell application UI:
groupByKey - 59 seconds, 4.2MB shuffle Read, 191MB shuffle WritecombineByKey with lists - 59 seconds, 4.2MB shuffle Read, 191MB shuffle WritecombineByKey with tuples - 30 seconds, 4.2MB shuffle Read, 183.7MB shuffle Write

So as you can see, using combineByKey with a tuple based data representation results in 2X speed up over a list based data representation.

Question 1 - Is this simply because tuples are immutable data types in Python and more efficiently handled in the PySpark -> JVM translation? If not, why are tuples as underlying data structure so much more efficient?

Question 2 - groupByKey and combineByKey with lists seem to have exactly identical performance metrics. Is this merely a coincidence or are they essentially somehow doing the same thing? Don't see why the latter would be true as combineByKey is supposed to be doing computations in a distributed manner as opposed to groupByKey

Question 3a - I noticed that groupByKey results in a PySpark iterable object as the values in the resulting RDD. On the other hand combineByKey results in whatever Python iterable datatype is used in the functions provided to combineByKey (list, tuple, etc). Does this choice have a broader implication in performance for future computations using this RDD? That is, is the PySpark iterable datatype more efficiently handled than the Python list/tuple datatypes that show up with combineByKey, or are they all handled, stored and manipulated in the same manner in the back end with no implication on future performance characteristics of the RDD?

Question 3b - If it does matter whether the RDD data for iterables are stored in the native Pyspark iterable format, how do we write custom combineByKey methods that aggregate the values in that datastructure type as opposed to a native Python data structure.

Question 4 - Why do the combineByKey methods show pretty much the same amount of data shuffling numbers as groupByKey? Shouldn't the latter have higher numbers as all data needs to be shuffled to one worker, which should not be the case for combineByKey?

Question 5 - I noticed that PySpark threw an error when I tried to write the combineByKey method using list methods like append/extend. This is what I tried:
myRDD.combineByKey(lambda value: [value], lambda x, value: x.append(value), lambda x, y: x.extend(y))
This gave me errors on the call to append stating that no such method existed for NoneType. I don't see why that error should be popping up. Is it because mutation methods aren't allowed? I thought that this would probably be more efficient than the list approach that worked since it avoids creating new list objects every time a combination is performed. Never mind, rookie mistake on my behalf. x.append() mutates x but has a NoneType return. I guess you can't write a lambda function with a single expression that returns the appended list so this would require writing separate functions for the combiners.


I am trying to understand the broader performance implications of choices and decisions we make on the Python/PySpark side of things and to figure out what best practices one should be trying to follow to maximize performance. I'd really appreciate if the instructors could provide some answers to these questions. Apologies for the long post!
 The longest parts of my code for Lab 3 that take to run locally are in the test steps for 3b and 3c, which is strange since all that should be happening is the calculation of the absolute value of the difference between two floats. Each of these two tests takes about 4 minutes, which is frustrating since the whole workbook takes around 18 minutes, so the test cells seem to be the prime culprits in pushing the runtime towards the 20 minute timeout limit. Can anyone shed any light on what is going on with the tests?

For reference:
Your submission token id is 1022031-dcb13b515877f8b0740058ed690e42e1:953596decca50741d8443505812d5b13:ip-172-31-11-14

Edit: Everything is fine now. In trying to save one line of code, I ended up recalculating tf for each token when computing tfidf in 2f, which embedded an extra for loop for each run through and took things from linear to quadratic. It runs in ~8 min now. Everything is ok. Hi all,
Can you please explain fastCosineSimilarity(record)? How to compute sum ("computes the sum for each of the tokens in the token list of the products of the Amazon weight for the token times the Google weight for the token")?

Here token list = each token list of commonTokens?
Please give me some example about how to compute sum.

Thanks Can I get a hint on this? I did it in spark but if you do I will not allow you to pass the 3C.

This is what I have.

tokens = ['cat', 'cat', 'dog','tie','tie','bat']d =dict((x,1) for x in tokens)for key in d.keys():        d[key] = d[key] + 1     print d





{'tie': 2, 'bat': 2, 'dog': 2, 'cat': 2}





   
 Using windows 7 
after going through Vagrant installation, VAGRANT_HOME variable isn't created. 

What's the correct value for the  VAGRANT_HOME  ? 

Right now, I have c:\users\piter\vagrant directory created .

After vagrant is installed , the default ~/.vagrant.d. in windows is NOT created .  

What would be the correct value for vagrant_home please ? "c:\users\piter\vagrant "  ?  I've been struggling with the creation of the correct commonTokens RDD for the next 4f problem.

I'm following the "join -> map(swap) -> groupByKey()" procedure as I could not make reduceByKey produce the merged list I expect. The "Found 2441100 common tokens" feedback is always correect, but when I printed out the results of groupByKey(), I got:

>>>> print commonTokens.take(3)

[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), <pyspark.resultiterable.ResultIterable object at 0xb0e0c7ac>), (('b000bnb72g', 'http://www.google.com/base/feeds/snippets/6959766318020460134'), <pyspark.resultiterable.ResultIterable object at 0xb0e0c3cc>), (('b00009apna', 'http://www.google.com/base/feeds/snippets/13986409474793202514'), <pyspark.resultiterable.ResultIterable object at 0xb0e0c44c>)]
Where I expected a list of tokens, I instead got weird objects such as <pyspark.resultiterable.ResultIterable object at 0xb0e0c7ac>.

It seems that I might miss another map() step after the groupByKey(), could someone point me to the right direction?  The strange thing is that the Test for 4e always went through, the damage was done to the outcome of 4f.  Thanks a lot! Is there any IDE I can use to debug Spark programs?

I setup spyder and Eclipse/pydev to compile/autocomplete Spark libraries, but I don't know how to run from IDE into a remote cluster neither how to debug? Any ideas? Your submission token id is 1024675-433d91cf37902da3fe389731d97793ea:214a4b2d950c62e44733e71aeb49b7ed:ip-172-31-15-250
 What kind of performance gain, in terms of computational time, should I expect going from 3c to 3d, i.e. broadcasting the idfs weight table to the drivers? I see the time shortened by about 13%. Is that about right? I want to convert the list data to csv data format as below, i am facing difficulty, can anyone help me ?

Input: 

[[(3, 'c'), ('Python', 'CWI')], [(1, 'a'), ('Spark', 'Berkeley')], [(4, 'd'), ('Hive', 'Apache')]]
Output Should be:

3,c,Python,CWI
1,a,Spark,Berkeley
4,d,Hive,Apache
 Can someone plz explain me the question step by step?

I had applied the following solution for idfs function:
   
    N = corpus.count()
    uniqueTokens = corpus.flatMap(lambda (x,y): y.distinct())
    tokenCountPairTuple = uniqueTokens.map(lambda x: (x,1))
    tokenSumPairTuple = tokenCountPairTuple.reduceByKey(lambda a,b: a+b)
    return (tokenSumPairTuple.collect()) I am now 100% if you made your TF function (Lab 3 2A) with Spark and not pure Python you will not be able to complete Lab 3 3C.

Solution: Use only Python to do (Lab 3 2A)

 How to use map and reduce functions for dictionary?
I would like to write the dotprod(a,b) in one line instead of a for loop. Note, my for loop is working.
Help will be appreciated.
 #### Create a new `fullCorpusRDD` that contains the tokens from the full Amazon and Google datasets.

I am using the following code for the start of 4B:  Should I be using union to perform this because I do not understand the error that is arising.

fullCorpusRDD = amazonFullRecToToken.union(googleFullRecToToken)idfsFull = idfs(fullCorpusRDD)idfsFullCount = idfsFull.count()print 'There are %s unique tokens in the full datasets.' % idfsFullCount


---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-286-a18b06d2b246> in <module>()
      1 fullCorpusRDD = amazonFullRecToToken.join(googleFullRecToToken)
      2 idfsFull = idfs(fullCorpusRDD)
----> 3 idfsFullCount = idfsFull.count()
      4 print 'There are %s unique tokens in the full datasets.' % idfsFullCount

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 480.0 failed 1 times, most recent failure: Lost task 0.0 in stage 480.0 (TID 2912, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
  File "<ipython-input-162-66b3d606eba4>", line 11, in <lambda>
TypeError: 'float' object is not iterable

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

 I would like to formally commend the entire Staff for making this course possible. I would also like to apologize for any non civil statements. The Staff really needs to be commended for making this course happen. I can not imagine how difficult it was for them to create and run the course. It is really wonderful idea to make this type of instruction generally available to us.  Thank you and I would like to also apologize for any future non-civlil statments. I have put about 50 hours into this lab and I am now just started 3D!

 I have multitude of approaches, I am sure it is probably simple but getting worned out I am using
idfsFullBroadcast = map(fullCorpusRDD.broadcast(idfsFullWeights).value)

and I get RDD has no attribute broadcast.

All helps appreciated. 
The iPython notebooks have some nice qualities:
Allow you to try code snipetsComputations are retained to use in subsequent cellsAre self documentingetc.

But:
The homework notebooks are too long and pieces of code are too far apart because the instructions are interleaved.What to do in a cell may depend of something on the computations done far aboveThe text formatting capabilities are less than you can achieve for instance in a pdf.Because they are so long, you probably would not solve all of them in one sitting. When you come back, you may have forgotten the names of RDDs and other variables created in the previous sessionIn some cases, we are supposed to infer what is the structure ofthe elements of an RDD by its name

As a consequence I have:
Spent an inordinate amount of time scrolling up and down the notebooksWasted a couple of hours in lab3 part 3e because the Amazon id and Google URL were on different lines in the explanation. I read it as ('AmazonID', 'GoogleURL', cosineSimilarity) instead of ('AmazonID GoogleURL', cosineSimilarity)

I would have much preferred that the skeleton had been provided as a simple .py file that could be run in iPython or idle and a well formatted .pdf with the instructions. Short comments (# Part 3e) in the Python file would link the two
 I am just learning python and very new to it .

I tried to write the logic for 2a somehow but I am getting the error and not sure what exactly it is ..I have been working from long time to fix this ...

Any help would be greatly appreciated.

dict = {} for w in tokens: dict[w] = dict.get(w, 0.) + 1.0 length = float(len(tokens)) for k in dict: dict[k] = dict[k] / length  return dict

  File "<ipython-input-138-ac2950bd1c0c>", line 11
    dict[w] = doc_dict.get(w, 0.) + 1.0
       ^
IndentationError: expected an indented block
  Does anybody else find that they successfully get 4e to work (passes the test) but if they run it again, it fails? These are the error messages being generated:

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-46-02818d78d702> in <module>()
     17                 .cache())
     18 
---> 19 print 'Found %d common tokens' % commonTokens.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 108.0 failed 1 times, most recent failure: Lost task 0.0 in stage 108.0 (TID 545, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 253, in mergeValues
    d[k] = comb(d[k], v) if k in d else creator(v)
MemoryError

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
 I am not sure what I am doing wrong but 4e takes forever in lab 3.   I joined invPairRDD's and reduced the new collection by key.  I tested that the the swap function is doing its thing right, as far as I can tell.  Here's an example:

A sample joined RDD entry before swap:

[('aided', ('b0007yepy6', 'http://www.google.com/base/feeds/snippets/17293379770636740081', 'b000hlt5j4', 'http://www.google.com/base/feeds/snippets/17293379770636740081', 'b000dz9yoa', 'http://www.google.com/base/feeds/snippets/17293379770636740081', 'b00004ochi', 'http://www.google.com/base/feeds/snippets/17293379770636740081'))] 

after swap:

[(('b0007yepy6', 'http://www.google.com/base/feeds/snippets/17293379770636740081', 'b000hlt5j4', 'http://www.google.com/base/feeds/snippets/17293379770636740081', 'b000dz9yoa', 'http://www.google.com/base/feeds/snippets/17293379770636740081', 'b00004ochi', 'http://www.google.com/base/feeds/snippets/17293379770636740081'), 'aided')]

But once it gets to this step:
commonTokens = (fullpairRDD .map(swap) .cache())

It takes forever and then finally spits out only 6601 tokens.  Any ideas why?  Thanks much! I have a RDD key/value pair where value is a list like this: 
[("a1",["w1","w2","w3","w1"]), ("a2",["w3","w3","w3","w1"]), ....
how I can apply the function set (or other one), only to the list element to remove the duplicates?
I have this so far but not getting the correct results:
uniqueTokens = RDD.map(lambda x: (x[0],set(x[1])))
thanks for the help! I'm looking forward to the machine learning lab. The support vector machine and logistic regression examples at https://spark.apache.org/docs/latest/mllib-linear-methods.html appear easier than lab 3. Hi, instructor, 

I've submitted lab3 several times, but all I get is the timeout error message.

I've tried to optimize my code, but it does not work. Could you help me to look at my code?

Thanks!

Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 1006536-bc0f2871a407c1c9daa3ef7ea7161c2d:0ff43865ae60b4ae6e7f1cfc702fbdd0:ip-172-31-14-13
Please include this submission token id when you need support for your code submission. The autograder gave this error  after  its done with my submission:

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

All tests passed
Removing stopwords (1b)
-----------------------

But then it also displayed that beautiful and lovely green check, and an additional 20 points on my total score:)

I reviewed my code again on the browser and also on IDLE, and I find no
indication of any error. Running it on the Chromium browser is OK.

I presume it must be a parsing error or something that might be worth checking. Here is my submission token ID:

Your submission token id is 1027610-b845e093f283a47bede4fdef34cc984b:2aba9e95d15d936a8e9e330896e5aaa2:ip-172-31-3-109
Please include this submission token id when you need support for your code submission.

Thank you. Hello all!

With the following code, I run and pass two of the three tests, and I do not understand why.

# TODO: Replace <FILL IN> with appropriate code
def invert(record):
    """ Invert (ID, tokens) to a list of (token, ID)
    Args:
        record: a pair, (ID, token vector)
    Returns:
        pairs: a list of pairs of token to ID
    """
    pairs = []
    tokens = record[1].keys()
    for token in tokens:
        pairs.append( (record[0],token))
    return tuple(pairs)

amazonInvPairsRDD = (amazonWeightsRDD
                    .flatMap(invert)
                    .cache())

googleInvPairsRDD = (googleWeightsRDD
                    .flatMap(invert)
                    .cache())

print 'There are %s Amazon inverted pairs and %s Google inverted pairs.' % (amazonInvPairsRDD.count(),
                                                                            googleInvPairsRDD.count())
                                                                            
Output for my code:

There are 111387 Amazon inverted pairs and 77678 Google inverted pairs.
And test results:

1 test failed. incorrect invert result
1 test passed.
1 test passed.

Could anyone please help me figure out why the first test with the code below fails?
invertedPair = invert((1, {'foo': 2}))
Test.assertEquals(invertedPair[0][1], 1, 'incorrect invert result')


I do not know what to do with this, and have been banging my head against the wall for quite some time now.

Many many thanks! Does lab3 1a and 1c have any relation.This I am asking bcoz if there is then 1a is very difficult.In 1a in the instructions it is simply stated that just convert to lowercase and remove empty tokens.If this instruction was sufficient then never would have the tests failed.The test fail because they ask for more like remove punctuations.It is not only removing punctuations but also  '/' ,'.'..........So what is the point it will surely fail in 1d.

Give a hint at least.In the whole discussion I could not find a worthy hint or answer except for people saying use filter which everyone might be knowing but in what way can we use it to remove infinite
possibilities ?? I am in a dilemma where I am progressing in this course but I don't think I have understood what these errors are (probably I did not spend much time on Spark architecture).

When I go through these exercises I get messages which mention Java, Scala and Python (ofcourse).
How so when all that I have been doing is pyspark? Can anyone explain?

Thanks!
 So while doing lab 2c Im kinda lost and need some hints:

these is what i have:
N = count RDD
uniqueTokens = 
print uniqueTokens.take(3)
tokenCountPairTuple = 
print tokenCountPairTuple.take(3)
tokenSumPairTuple = 
print tokenSumPairTuple.take(3)
return (tokenSumPairTuple.collect())



output:

['http://www.google.com/base/feeds/snippets/18378775128309848631', 'b000kmcf0g', 'b0000c6fjm']


[('http://www.google.com/base/feeds/snippets/18378775128309848631', 1), ('b000kmcf0g', 1), ('b0000c6fjm', 1)]

[('http://www.google.com/base/feeds/snippets/18378775128309848631', 1), ('http://www.google.com/base/feeds/snippets/18404187604346548446', 1), ('http://www.google.com/base/feeds/snippets/6990565128537638388', 1)]


--------------------------------------------------------------------------- TypeError Traceback (most recent call last)  in () 20 21 idfsSmall = idfs(amazonRecToToken.union(googleRecToToken)) ---> 22 uniqueTokenCount = idfsSmall.count() 23 24 print 'There are %s unique tokens in the small datasets.' % uniqueTokenCount TypeError: count() takes exactly one argument (0 given)

can someone point me in the right direction?

Thanks
RF Hi All,

i've been playing with this for some time, and i'm getting this:

Requested similarity is 0.000720187336971.

All previous test cases are passed, but i think there is something extra missing..

tf, idfs, tfidf, dotprod, norm, cossim, cosineSimilarity, any of these could it be the problem ?? 

any hits ?? Pre-compute TF-IDF weights. Build mappings from record ID weight vector.

I am stuck here most probably due to incorrect syntax. So my map from amazonFullRecToToken to tfidf(amazonFullRecToToken,idfsFullBroadcast) requires a lambda function? I have tried everything but nothing has worked so far. If I spend any more time in the state of confusion I am going to have to start paying state income tax here!

OK, so after several days I slogged through 2c. Now I am stuck in 2d. The code is below, any clues?

# TODO: Replace <FILL IN> with appropriate codewordCountsCollected = (wordsRDD                       < Don't Post Code >print wordCountsCollected
 I want to know what exactly is a token.I mean '?'  '/'  '.'  '""'  ') '....tokens or only alphanumeric characters are token.Because if the former is a token then why in 1a do we need to remove punctuations and if we remove punctuations then we also need to remove '-'.So what exactly? My test for 2f TF-IDF function passed successfully. But when I use the tfidf function in 4b it returns me empty string. Can anyone suggest what I am doing wrong.

tfIdfDict = dict((k, float(tfs.get(k,0))*idfs.get(k,0)) for k in set(tfs.keys()))

Here is also my dotProd logic
return sum(a.get(k,0) * b.get(k,0) for k in a)

Any help is appreciated.

Got It Finally!!!! Long Sigh!!!!! 
The issue was not with my tfids nor dotProd. But my issue was with DrumRolllllllll "Tokenize" function. In tokenize function I did not convert my set into a lowercase hence the issues.
This is another pointer for the test cases which can be added in this excercise.

Thanks for all the help. Hi I am not able to proceed further with question 3c

similarities = (crossSmall <FILL IN> .cache())

what should be part of the FILL IN

I am using map and lambda rec : computeSimilarity(record) am I on correct path? I am getting the following exception please guide

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-69-bf88c05f1dbd> in <module>()
     42             .collect()[0][2])
     43 
---> 44 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     45 print 'Requested similarity is %s.' % similarityAmazonGoogle

<ipython-input-69-bf88c05f1dbd> in similar(amazonID, googleURL)
     39     """
     40     return (similarities
---> 41             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     42             .collect()[0][2])
     43 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 231, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-69-bf88c05f1dbd>", line 29, in <lambda>
  File "<ipython-input-69-bf88c05f1dbd>", line 25, in computeSimilarity
  File "<ipython-input-64-94a0fcd5edd1>", line 12, in cosineSimilarity
  File "<ipython-input-20-5d68e45bc9fa>", line 13, in tfidf
KeyError: 'o' I would like to know when is the next run of this course going to take place. I started the course but due to job commitments couldn't be regular and am left too much behind. So, I would like to drop it and join the next run of the course.

Thank you!! I already got uniqueTokens like a one dimensional python list, with every token only appearing once per document, but in my list several times.

i dont know how to access every one of it in the next object tokenCountPairTuple to asign a value to it to calculate with. can someone give me a hint to access it i think this is the solution to it, hopefully i´m right.

thanks in advance I'm trouble with tokens variable in 4f.
I can extract amazonRec and googleRec from record argument, but tokens stuck me!
Here same examples for take(3) elements of commonTokens
take(3)= [(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), <pyspark.resultiterable.ResultIterable object at 0xb09cd70c>), (('b000bnb72g', 'http://www.google.com/base/feeds/snippets/6959766318020460134'), <pyspark.resultiterable.ResultIterable object at 0xb09cdacc>), (('b00009apna', 'http://www.google.com/base/feeds/snippets/13986409474793202514'), <pyspark.resultiterable.ResultIterable object at 0xb09cddec>)]
amazonRec =  b00005lzly
googleRec =  http://www.google.com/base/feeds/snippets/18376072611700638452

Hints?  When downloading the script from the iPython notebook, it is save as a .txt file which I then have to convert to a .py file. However, when I submit the .py file the autograder returns 0% even though I passed all tests. Here are the first few lines of output from the autograder
Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 671
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'simpleTokenize' is not defined

 4e test case is executing successfully. And then 4f code is also running successfully but when I run 4f test cases, it throws keyerror:120. And this error is due to putting wrong key in "amazonWeightsBroadcast.value[key]" and here it is showing the key to be 120.

I could not understand, how this is possible. Since we have taken only those tokens which were common to both amazonWeightsBroadcast and googleWeightsBroadcast. So all the keys which we are passing to these broadcast dictionaries should be present in both dictionaries. Hi All,

I'm able to print the correct tuple with correct record and count. But I couldn't return it.

I get the following error.

TypeError                                 Traceback (most recent call last)
<ipython-input-313-5b3305ffe9e6> in <module>()
     13 
     14 print 'The Amazon record with ID "%s" has the most tokens (%s)' % (biggestRecordAmazon[0][0],
---> 15                                                                   len(biggestRecordAmazon[0][1]))

TypeError: object of type 'int' has no len()

 Before doing anything I just used a map and lambda to find the tuple with highest list count 

Got print result as ('b000o24l3q',1547). But I'm not sure how to sort without finding the count of list, so I'm facing roadblock to return it

Please help. Hi,

I am enjoying doing this course and learning about Spark. I have no past experience with Machine Learning, so is this (CS190.1x - Scalable Machine Learning) class still good for me to take? or does it require some kind of preparation/past experience with ML? Hello,

I am totally confuse , I don't know ,what s wrong on my code here is trace back:

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-50-c3160fda25e4> in <module>()
      9     return vendorRDD.takeOrdered(lambda s: -s[1])
     10 
---> 11 biggestRecordAmazon = findBiggestRecord(amazonRecToToken)
     12 print 'The Amazon record with ID "%s" has the most tokens (%s)' % (biggestRecordAmazon[0][0],
     13                                                                    len(biggestRecordAmazon[0][1]))

<ipython-input-50-c3160fda25e4> in findBiggestRecord(vendorRDD)
      7         list: a list of 1 Pair Tuple of record ID and tokens
      8     """
----> 9     return vendorRDD.takeOrdered(lambda s: -s[1])
     10 
     11 biggestRecordAmazon = findBiggestRecord(amazonRecToToken)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in takeOrdered(self, num, key)
   1172             return heapq.nsmallest(num, a + b, key)
   1173 
-> 1174         return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
   1175 
   1176     def take(self, num): I am getting the following exception while using the following code please guide thanks

idfsFullWeights = idfs(fullCorpusRDD)idfsFullBroadcast = sc.broadcast(fullCorpusRDD)

Exception                                 Traceback (most recent call last)
<ipython-input-80-4d9e4fca1e14> in <module>()
      7 # Recompute IDFs for full dataset
      8 idfsFullWeights = idfs(fullCorpusRDD)
----> 9 idfsFullBroadcast = sc.broadcast(fullCorpusRDD)
     10 
     11 # Pre-compute TF-IDF weights.  Build mappings from record ID weight vector.

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in broadcast(self, value)
    642         be sent to each cluster only once.
    643         """
--> 644         return Broadcast(self, value, self._pickled_broadcast_vars)
    645 
    646     def accumulator(self, value, accum_param=None):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/broadcast.py in __init__(self, sc, value, pickle_registry, path)
     63         if sc is not None:
     64             f = NamedTemporaryFile(delete=False, dir=sc._temp_dir)
---> 65             self._path = self.dump(value, f)
     66             self._jbroadcast = sc._jvm.PythonRDD.readBroadcastFromFile(sc._jsc, self._path)
     67             self._pickle_registry = pickle_registry

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/broadcast.py in dump(self, value, f)
     80         else:
     81             f.write('P')
---> 82             cPickle.dump(value, f, 2)
     83         f.close()
     84         return f.name

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in __getnewargs__(self)
    169         # This method is called when attempting to pickle an RDD, which is always an error:
    170         raise Exception(
--> 171             "It appears that you are attempting to broadcast an RDD or reference an RDD from an "
    172             "action or transformation. RDD transformations and actions can only be invoked by the "
    173             "driver, not inside of other transformations; for example, "

Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.

 

 TypeError                                 Traceback (most recent call last)
<ipython-input-46-298887ad6407> in <module>()
      1 # TEST Implement the components of a cosineSimilarity function (3a)
      2 Test.assertEquals(dp, 102, 'incorrect dp')
----> 3 Test.assertTrue(abs(nm - 6.16441400297) < 0.0000001, 'incorrrect nm')

TypeError: unsupported operand type(s) for -: 'dict' and 'float' Is it more than a few minutes? I'm getting the following errors and I think it's related to not having enough memory or something?

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-43-831fe52e81aa> in <module>()
     23                        .cache())
     24 
---> 25 print similaritiesFullRDD.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 101.0 failed 1 times, most recent failure: Lost task 0.0 in stage 101.0 (TID 504, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-43-831fe52e81aa>", line 22, in <lambda>
  File "<ipython-input-43-831fe52e81aa>", line 17, in fastCosineSimilarity
TypeError: list indices must be integers, not str

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
<p></p> Hi,

I was wondering why this does not work:

return vendorRDD.reduce(lambda a,b: len(a[1])+len(b[1]) )
It throws:
TypeError: 'int' object has no attribute '__getitem__' 
 
In this very simple example it does what I'd expect
s = (('one', ['a', 'b', 'c']), ('two', ['a', 'b']))rdd = sc.parallelize(s).reduce(lambda a,b: len(a[1])+len(b[1]))print rdd
Where is the difference to the lab exercise?

Thanks!
Thomas 
Same problem.

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block Hi everyone!

Does someone know a way to make time window operations like moving average or rollapply with PySpark? I am studying time series data and I would like to make roll apply operations as we can do in R (rollapply()).  

Imagine I have a set of dates and integers:
(2015/01/01, 2), (2015/01/02,7), (2015/01/03,10), ...

And I would like to calculate the average of each 5 days like:
Iteration 1: avg(day 01, day 02, day 03, day 04, day 05)
Iteration 2: avg(day 02, day 03, day 04, day 05, day 06)
Iteration 3: avg(day 04, day 05, day 06, day 07, day 08)
...

Is there a good way to calculate this? I believe I have everything in lab3 (3c) ready except this line:

similarities = (crossSmall                .map(lambda x: computeSimilarity(x))                .map(lambda (x, y, z): ((x,y), z))                .cache())

I got the following compile error:

Exception                                 Traceback (most recent call last)<ipython-input-91-342c45ec38f2> in <module>()  41 similarities = (crossSmall  42 .map(lambda x: computeSimilarity(x)) ---> 43 .map(lambda (x, y, z): ((x,y), z))  44 .cache())  45  /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in cache(self)  189 """  190 self.is_cached = True --> 191 self.persist(StorageLevel.MEMORY_ONLY_SER)  192 return self 

====================

without ".map(lambda (x, y, z): ((x,y), z)" I still get error. Seem to be quite straight forward. What is wrong with this line of code?

Thanks for your help.
 Would you please help me?
Would you please let me know whether the format of my amazonWeightsRDD is correct?

print amazonWeightsRDD.take(3)



[{'rom': 2.4051362683438153, 'clickart': 56.65432098765432, '950': 254.94444444444443, 'image': 3.6948470209339774, 'premier': 9.27070707070707, '000': 6.218157181571815, 'dvd': 1.287598204264871, 'broderbund': 22.169082125603865, 'pack': 2.98180636777128}, {'1': 0.3231235037318687, 'desktops': 12.74722222222222, 'computer': 0.6965695203400122, 'win': 0.501859142607174, 'ca': 9.10515873015873, 'v11': 50.98888888888888, '30u': 84.98148148148148, '30pk': 254.94444444444443, 'desktop': 2.23635477582846, 'laptops': 11.588383838383837, 'arcserve': 24.28042328042328, 'associates': 7.284126984126985, 'lap': 127.47222222222221, 'oem': 46.35353535353535, 'international': 9.44238683127572, 'backup': 2.8015873015873014}, {'case': 5.28078250863061, 'center': 6.953030303030303, 'noah': 208.5909090909091, 'ages': 7.871355060034306, 'multimedia': 7.070878274268105, 'jewel': 7.192789968652038, '3': 0.6964638033085445, 'victory': 34.765151515151516, 'activity': 10.175166297117517, '8': 1.2641873278236915, 'ark': 208.5909090909091}]
 I am failing the first test, but I am passing the second and third.
 
Since all the words in the test phrase ("Why a the?") are in the stopwords and the r'\W+' handles the quotes and question mark, I am assuming that I should get the asserted answer of [ ].
 
I have tried setting the string in that if statement branch equal to [ ], [' '], [" "], None, null ....
 
My attempt to learn Python on the fly is probably the root problem.  Where should I look in the Python documentation for a specific clue? print amazonSmall.getNumPartitions() # = 4print googleSmall.getNumPartitions() # = 4print googleSmall.count() # = 200print amazonSmall.count() # = 200
So, why can't I zip these two together without receiving loads of exceptions from pyspark and py4j? New to python and trying to figure out how to rearrange list of

((x, (1,2,3)),
(y, (1,2,3))) to

((1, x),
(2, x),
(3, x)),
((1, y),
(2, y),
(3, y))

I know it is either a loop or list comprehension.

Have any python experts seen good tutorial on net that I can look at?  I haven't seen any with exactly this structure yet. Also what 'pythonic' words would one use to describe this transformation that can help search for what it is doing?  'Nested' structure comes to mind.. anything else?

Thanks Step 3 specifies:"For each of the unique tokens, count how many times it appears in the document and then compute the IDF for that token: N/n(t)"

This is false.
n(t) specifies how many documents (of the corpus) contain that token, as it is already properly stated in the instructions twice before that line.

Having corrected that, it might make the exercise a bit easier to complete.

Cheers,
Damir
 Hello everybody

I'm running into an issue on Lab3 - 3c part. my code doesn't show results and continues compiling, making my laptop heat and the RAM decrease...

Some information about my code:
- I created the crossSmall RDD by using the cartesian transformation between two RDDs
- Inside the function computeSimilarity, thing are quite simple I guess...
googleRec = record[0] amazonRec = record[1] googleURL = googleRec[0] amazonID = amazonRec[0] googleValue = googleRec[1] amazonValue = amazonRec[1] cs = cosineSimilarity(googleValue,amazonValue, idfsSmallWeights)

- Again quite as simple 
similarities = (crossSmall .map(lambda r : computeSimilarity(r)) .cache())

Could you please help me figure out what I missed here ... or maybe have I made something wrong in my code above in the notebook... 

PS : to compute tf I used pure python code

Cheers Can Some one please help/guide  me to understand 4e- I am stuck
errDateCountPairTuple = badRecords.map(lambda :(day,response_code)) - to pair tuple with day and response code
errDateSum = This should give some thing like (1,23),(2,34)...by using groupByKey and Map
errDateSorted = (errDateSum.<fill in>)-This should give sorted value of errDateSum
errByDate = errDateSorted.<<fill in>- I am not understanding what is this <fill in> should be .Need some clarification abount errByDate

Thanks When I try to use tfidf(amazon_token, idfsFullBroadcast.value) I get a type error (see below). Apparently I cannot use len() to determine the length of the tokenized string and calculate tf(amazon_token). I am a new Python user and I am literally banging my head on the table...Can anyone explain/suggest what to do, please?
Thank you very much!

TypeError: object of type 'PipelinedRDD' has no len()  this is my result but it is not correct:

record : 
(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), <pyspark.resultiterable.ResultIterable object at 0xb06dc96c>)
amazonRec :
b00005lzly
googleRec :
http://www.google.com/base/feeds/snippets/18376072611700638452
tokens :
['120']
amazon vector : 
{'120': 5.75062656641604}
google vector : 
{'120': 30.593333333333334}
The sum : 
175.930835422
The Value : 
1.0

 Hi, 
my submission token is 
1038405-b7e5dea87a75cf911da4afc88f664ff3:ea9259156f1acaf035b41540e8d144e3:ip-172-31-12-127
I completed lab3 and I used python notebook to run all of them without any problem. 
I am not sure why it has IndentationError when I submitted my py file 


Thanks 
 I have a strange error with my notebook. Some of the cells have inverted the positions of the code cell and the test one. This is the case for tokenize: the test cell is before the code cell. This has happened while I was editing the notebook, but I don't know how.

To have the tests passed I need to run the function in the cell below the test and then the test. Do you know where it can come from?

In addition, when I submitted the test to the autograder, I had this message:
Tokenize a String (1a)
----------------------
Traceback (most recent call last):
File "", line 28, in NameError:
MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
name 'tokenize' is not defined
Of course tokenize is defined and all the other tests run properly, except 4f that I did not have the time to complete...

Do you know where this error comes from?
To the instructors, I have this message
Your submission token id is 1040165-72aeee396a3da353cc96a71bc11f35c7:af1a8ac16624092d52b3ce3f14b697f4:ip-172-31-10-193

 According to part 2 TF-IDF is computed from two numbers. However in part 4, at least in the first sections TF-IDF is aparently used to refer only to the idf values.

Is it so or I am miss reading the instructions?

Froget it: Solved Will someone please clarify the meaning of
For each of the unique tokens, count how many times [i.e., n(t)] it appears in the document and then compute the IDF for that token: N/n(t).
It appears to conflict with the earlier suggestion
Find n(t)</em>, the number of documents in <em>U</em> that contain t [i.e., a token].

 Hi,
   I am not able to understand this question. Can some one please give an example of an element in the corpus? Is this the union of 2 datasets? A little confused

Thanks in advance
Keshav With the following code:
__________________________________________________________idfsSmall = idfs(amazonRecToToken.union(googleRecToToken))uniqueTokenCount = idfsSmall.count()idfsSmall = idfs(corpusRDD)
tokenSmallestIdf = idfsSmall.takeOrdered(1, lambda s: s[1])[0]
print 'There are %s unique tokens in the small datasets.' % uniqueTokenCountprintprint tokenSmallestIdf
__________________________________________________________

I am getting the following output:


There are 4772 unique tokens in the small datasets.

('software', 0.14545454545454545)

and the following output for the test cases:


1 test passed.
1 test passed.
1 test failed. incorrect smallest IDF value

So, we can see the IDF value is incorrect. Where could be my error? Only place I calculate the IDF is
the following statement!

    corpusRDDTokenIDF = corpusRDDFlatListFlatTokenLength.map(lambda (x,y): (x, (N+0.0)/y))





 Update #2:

I think I just realized that my "extra-nesting" of the 'b000jz4hgo' record below was just an artifact of my retrieving it with take(1) - it listified it....

Update:  I put in an if-statement to convert the id to a str if it wasn't one to begin with.  Hope that's right!!

I also found that my mapping with invert was un-nesting the record one-level, so I was using one-too-many indices when testing with the 'b000jz4hgo' record below.  

Anyway, I'm passing the internal tests now.  Hopefull AUTOGRADER, the Great and Powerful, will agree  :-)

**************  original post  ******************
The first test argument for invert is:
(1, {'foo': 2})

But I think the argument we are passing to invert is of the following format [PLEASE CORRECT ME IF I'M WRONG!]:

[('b000jz4hqo', {'rom': 2.4051362683438153, 'clickart': 56.65432098765432, '950': 254.94444444444443, 'image': 3.6948470209339774, 'premier': 9.27070707070707, '000': 6.218157181571815, 'dvd': 1.287598204264871, 'broderbund': 22.169082125603865, 'pack': 2.98180636777128})] 3c is taking a very long time solve for me. I was able to map the similarities RDD within seconds, however, the collect command in the def similar function is taking a really long time to solve. The 'return' statement is fast without the collect. Only when it starts populating it is taking long. Isn't this practically "one record" after filtering? Not sure why this keeps happening. Any idea to speed it up?

Edit: Never mind, it worked. But, took a while to process! As I understand, lab 3 about entity resolution
can be viewed as a kind of decision stump
machine learning algorithm with training
examples as pairs of (ID, URL) from the
gold standard file and decision feature as
a cosine similarity between tf-idf vectors
corresponding to the ID and URL. 
Learning can happen by optimizing
the threshold value with respect to the
error on training set.

Am I right with this interpretation?

Thanks in advance for any comments! Possibly losing my mind but in question 4b, for idfsFullCount I get 17079 rather that 17078 and question 4d forgoogleInvPairsRDD.count() is 77679 rather that 77678.

All other unit tests up to 4d are passed. Is anyone else running in to this problem or has any suggestions.

thanks Here is the output of autograder. 1a actually passes when I run it. What could be wrong?
Thanks

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
All tests passed
Implement an IDFs function (2c)
-------------------------------
All tests passed
Implement a TF-IDF function (2f)
--------------------------------
All tests passed
Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
All tests passed
Perform Entity Resolution (3c)
------------------------------
All tests passed
Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
All tests passed
Perform a Gold Standard evaluation (3e)
---------------------------------------
All tests passed
Tokenize the full dataset (4a)
------------------------------
All tests passed
Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
All tests passed
Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
All tests passed
Create inverted indicies from the full datasets (4d)
----------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4e)
-------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4f)
-------------------------------------------------
All tests passed
-- 19 cases passed (100.0%) --


Your submission token id is 1042582-3b63c1fbda3f01dd2b14ee269313a197:3b5b5c35e4e14ed178cd3df0a5dbb7c2:ip-172-31-2-248
Please include this submission token id when you need support for your code submission.
 
Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

After 25 minutes the Autograder returned the above message. Already I have used 6 of 10 submissions.

Locally my code passes all tests from part 1 up to 4f and does not include "collect()" (I use "collectAsMap()"), "keys()", "groupByKey", FILL IN or print statements. I'm not experienced enough to tell if my code is optimized.Some info when running locally:With "Cell->Run All" my implementation runs in around 24 minutes (Part 1a up to part 4f).
OS: Win 7 64-bit, Processor: Intel Core 2 Quad CPU @ 2.40GHzAssigned to VM: 3 Cores and 1,7 GB memoryPart 3c runs in 5.5 minutes    Part 3d runs in 5.6 minutesAt part 5 I get:
ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server
...
error: [Errno 111] Connection refused"

------

EDIT: Every it's OK with Autograder now. It seems it was my fault (I had a not necessary piece of Python code that slowed down everything).

The Errno 111 error (probably memory error) at Part 5 continues to exist but does not affect the Autograder process.
 I had graded with 50% ok, now i grade again with more code completed and i get a total error:

Your submission token id is 1043674-91269cc246a8fbb02571449a8c7addcc:858f89d6688f4094a5ae5a2517ef3252:ip-172-31-3-110
Please include this submission token id when you need support for your code submission.curious first lines:Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 643
SyntaxError: Non-ASCII character '\xc2' in file /ok/submission.py on line 644, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details I don't understand why 

amazonWeightsRDD = amazonFullRecordToToken.map(lambda (k,v):(k,tfidf(v,idfsFullWeights)))
doesn't work All my tests passed until 3C.

I used a cartesian here.
crossSmall = (googleSmall
              .XXXXX
              .cache())

similarities = (crossSmall                .map(computeSimilarity)                .cache())
I am getting the following error.  What I am doing wrong?

---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-95-fb04981f51aa> in <module>()
     25 
     26 similarities = (crossSmall
---> 27                 .map(computeSimilarity)
     28                 .cache())
     29 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in cache(self)
    189         """
    190         self.is_cached = True
--> 191         self.persist(StorageLevel.MEMORY_ONLY_SER)
    192         return self
    193 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in persist(self, storageLevel)
    205         self.is_cached = True
    206         javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)
--> 207         self._jrdd.persist(javaStorageLevel)
    208         return self
    209 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _jrdd(self)
   2286         command = (self.func, profiler, self._prev_jrdd_deserializer,
   2287                    self._jrdd_deserializer)
-> 2288         pickled_cmd, bvars, env, includes = _prepare_for_python_RDD(self.ctx, command, self)
   2289         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(),
   2290                                              bytearray(pickled_cmd),

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _prepare_for_python_RDD(sc, command, obj)
   2204     # the serialized command will be compressed by broadcast
   2205     ser = CloudPickleSerializer()
-> 2206     pickled_command = ser.dumps(command)
   2207     if len(pickled_command) > (1 << 20):  # 1M
   2208         broadcast = sc.broadcast(pickled_command)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py in dumps(self, obj)
    409 
    410     def dumps(self, obj):
--> 411         return cloudpickle.dumps(obj, 2)
    412 
    413 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dumps(obj, protocol)
    814 
    815     cp = CloudPickler(file,protocol)
--> 816     cp.dump(obj)
    817 
    818     #print 'cloud dumped', str(obj), str(cp.modules)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dump(self, obj)
    131         self.inject_addons()
    132         try:
--> 133             return pickle.Pickler.dump(self, obj)
    134         except RuntimeError, e:
    135             if 'recursion' in e.args[0]:

/usr/lib/python2.7/pickle.pyc in dump(self, obj)
    222         if self.proto >= 2:
    223             self.write(PROTO + chr(self.proto))
--> 224         self.save(obj)
    225         self.write(STOP)
    226 

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    560         write(MARK)
    561         for element in obj:
--> 562             save(element)
    563 
    564         if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    634                 write(APPENDS)
    635             elif n:
--> 636                 save(tmp[0])
    637                 write(APPEND)
    638             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    679                 for k, v in tmp:
    680                     save(k)
--> 681                     save(v)
    682                 write(SETITEMS)
    683             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    304             reduce = getattr(obj, "__reduce_ex__", None)
    305             if reduce:
--> 306                 rv = reduce(self.proto)
    307             else:
    308                 reduce = getattr(obj, "__reduce__", None)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in __getnewargs__(self)
    169         # This method is called when attempting to pickle an RDD, which is always an error:
    170         raise Exception(
--> 171             "It appears that you are attempting to broadcast an RDD or reference an RDD from an "
    172             "action or transformation. RDD transformations and actions can only be invoked by the "
    173             "driver, not inside of other transformations; for example, "

Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.
 Dear TAs, 

I have some problems in the Lab2 questions 1c which has not been modified.
Can you please have a look?

Data cleaning (1c)
------------------
Traceback (most recent call last):
  File "", line 28, in 
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 1 times, most recent failure: Lost task 0.0 in stage 57.0 (TID 165, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "", line 13, in 
TypeError: unsupported operand type(s) for /: 'list' and 'int'.....Submission token id is 986665-ac33db3ce1b4fc9c7a492a6e6195945c:142235a07b1b4cc1c37c591bd7e31e75:ip-172-31-47-123 I read a lot of post about lab 2, i did it and I submitted the lab 2 on July 22 one hour before the deadline (00:00UTC).
Previusly i checked my lab in my VM and it passed all the test but until today the autograder dont give me the grade.
Is my submitted file still procesing? What do i have to do?
The file was submitted on time and all the exercises were ok in my VM, did i lost the points?
 May someone help me with this sentence, the more I read it the more I can't understand the meaning (its my problem, I am not proficiency in english)

computes the sum for each of the tokens in the token list of the products of the Amazon weight for the token times the Google weight for the token Hi, I passed the test for 3a, the functions dotprod, norm and cossim worked fine, but now in 3b when the only thing I have to do is call my functions I receive the error:
<ipython-input-487-052f6b7c8e39> in dotprod(a, b)
     13 
     14     for key in a:
---> 15         total = total + a[key] * b[key]
     16         print a[key]
     17         print b[key]

KeyError: 'photoshop'

this occur in the cossim call , I made some test, and everything looks ok, these are output from some variables:
w1 = tfidf(tokenize('Adobe Photoshop'), idfsSmallWeights)w2 = tfidf(tokenize('Adobe Illustrator'), idfsSmallWeights)​print w1print w2{'photoshop': 22.22222222222222, 'adobe': 8.333333333333334}{'illustrator': 50.0, 'adobe': 8.333333333333334}

then when I call the cosim(w1,w2) I get the error, cosim function is very simple: cossim = (dotprod(a,b) / norm(a) ) / norm(b)

any clues on this?

thanks !

 something is not working for me

when i pull the invert code into separate cell

record = amazonWeightsRDD.take(1)
#record = [(1, {'foo': 2})]

pairs = []
for row in record:
    for token in row[1].keys():
        pairs.append((token, row[0]))
print pairs

and run it i get following correct results:

a) with record = [(1, {'foo': 2})] i get




[('foo', 1)]





b)with record = amazonWeightsRDD.take(1)  i get

[('rom', 'b000jz4hqo'), ('clickart', 'b000jz4hqo'), ('950', 'b000jz4hqo'), ('image', 'b000jz4hqo'), ('premier', 'b000jz4hqo'), ('000', 'b000jz4hqo'), ('dvd', 'b000jz4hqo'), ('broderbund', 'b000jz4hqo'), ('pack', 'b000jz4hqo')]

amazonWeightsRDD.take(1) is as follows

[('b000jz4hqo',
  {'000': 6.218157181571815,
   '950': 254.94444444444443,
   'broderbund': 22.169082125603865,
   'clickart': 56.65432098765432,
   'dvd': 1.287598204264871,
   'image': 3.6948470209339774,
   'pack': 2.98180636777128,
   'premier': 9.27070707070707,
   'rom': 2.4051362683438153})]

But when i use my invert code in the official cell and instead of print pairs i use return (pairs)it doesn't work.

But when  i cut out all code below

amazonInvPairsRDD = amazonWeightsRDD.flatMap(invert).cache()
 it works or at least no errors


It seems that when it gets to a  count or take of the RDD  amazonInvPairsRDD i get error below


i've stopped and started notebook, searched through here, tried some other ways of getting pairs results. everything else in notebook above this passes.

what else should i look at to troubleshoot?

thanks

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-201-a80a249eb3b4> in <module>()
     22                     .cache())
     23 
---> 24 print 'There are %s Amazon inverted pairs and %s Google inverted pairs.' % (amazonInvPairsRDD.count(),
     25                                                                             googleInvPairsRDD.count())

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 192.0 failed 1 times, most recent failure: Lost task 0.0 in stage 192.0 (TID 584, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-201-a80a249eb3b4>", line 14, in invert
TypeError: 'int' object is not callable

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)<br /><br /><br />In [199]:<br /><br /># TEST Create inverted indicies from the full datasets (4d)<br /><br />invertedPair = invert((1, {'foo': 2}))<br /><br />Test.assertEquals(invertedPair[0][1], 1, 'incorrect invert result')<br /><br />Test.assertEquals(amazonInvPairsRDD.count(), 111387, 'incorrect amazonInvPairsRDD.count()')<br /><br />Test.assertEquals(googleInvPairsRDD.count(), 77678, 'incorrect googleInvPairsRDD.count()')<br /><br />---------------------------------------------------------------------------<br />TypeError                                 Traceback (most recent call last)<br /><ipython-input-199-495f36cc9914> in <module>()<br />      1 # TEST Create inverted indicies from the full datasets (4d)<br />----> 2 invertedPair = invert((1, {'foo': 2}))<br />      3 Test.assertEquals(invertedPair[0][1], 1, 'incorrect invert result')<br />      4 Test.assertEquals(amazonInvPairsRDD.count(), 111387, 'incorrect amazonInvPairsRDD.count()')<br />      5 Test.assertEquals(googleInvPairsRDD.count(), 77678, 'incorrect googleInvPairsRDD.count()')<br /><br /><ipython-input-197-1d5fdc5186af> in invert(record)<br />     10 <br />     11     for row in record:<br />---> 12         for token in row[1].keys():<br />     13             pairs.append((token, row[0]))<br />     14     return (pairs)<br /><br />TypeError: 'int' object has no attribute '__getitem__'<br /><br />

<p><p></p> <p></p></p> I passed all the test when working on lab3... when submitted it is giving so many errors (only 7 cases passed)... it is failing even in 1a... can somebody help please... it seems my whole effort will be wasted here... :(

I have already used 2 of my 10 submissions.... 

Your submission token id is 1045275-a2e98451c1d8ea2b09e157b652b9e539:5fd34e2c62b2468831beb3c04d56cdf3:ip-172-31-8-186Please include this submission token id when you need support for your code submission.
Your submission token id is 1045594-341bd6ea6a7ac2a972124b77bfe85df2:5fd34e2c62b2468831beb3c04d56cdf3:ip-172-31-8-109Please include this submission token id when you need support for your code submission.

---------------------------------------------
update-----------
---------------------------------------------
Instructor, please see the errors below

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 5
SyntaxError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
from __future__ imports must occur at the beginning of the file

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tf' is not defined

Create a corpus (2b)
--------------------
All tests passed
Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueTokenCount' is not defined

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'rec_b000hkgj8k_weights' is not defined

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsSmallBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
All tests passed
Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsFullCount' is not defined

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonNormsBroadcast' is not defined

Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonInvPairsRDD' is not defined

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'commonTokens' is not defined

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similaritiesFullRDD' is not defined

-- 7 cases passed (36.0%) --


Your submission token id is 1045594-341bd6ea6a7ac2a972124b77bfe85df2:5fd34e2c62b2468831beb3c04d56cdf3:ip-172-31-8-109
Please include this submission token id when you need support for your code submission.

 Sorry to post code, but in my last step on this problem I am trying to do a map with lambda (k,v): (k, float(N)/v) and for some reason it appears as if I am getting everything set to N (400,0) rather than the actual N/v.

the actual statement is: .map(lambda (k,v): (k, (float(N)/v)))

I think this should work but it is not clear why this map is not working.  I do know the values before the map are correct (or at least I think so) as I get 94 for software and if I divide the 400 by 94 I get the answer it is looking for: 4.25531914894

Man this is so frustrating.  Any help as always.

Thanks! In (4b) Compute IDFs and TF-IDFs for the full datasets, it is failing with the full data set. However the same action succeed each time with smaller data set. Not able to understand the reason behind the failure. 












# Pre-compute TF-IDF weights.  Build mappings from record ID weight vector.
amazonWeightsRDD = amazonFullRecToToken.map(lambda x: (x[0], tfidf(x[1], idfsFullBroadcast.value) ))
googleWeightsRDD = googleFullRecToToken.map(lambda x: (x[0], tfidf(x[1], idfsFullBroadcast.value) ))
​
print 'There are %s Amazon weights and %s Google weights.' % (amazonWeightsRDD.count(),
                                                              googleWeightsRDD.count())

















There are 4772 unique tokens in the full datasets.






---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-37-e5fb292da537> in <module>()
     13 googleWeightsRDD = googleFullRecToToken.map(lambda x: (x[0], tfidf(x[1], idfsFullBroadcast.value) ))
     14 
---> 15 print 'There are %s Amazon weights and %s Google weights.' % (amazonWeightsRDD.count(),
     16                                                               googleWeightsRDD.count())

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 97.0 failed 1 times, most recent failure: Lost task 0.0 in stage 97.0 (TID 571, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <genexpr>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "<ipython-input-37-e5fb292da537>", line 12, in <lambda>
  File "<ipython-input-20-428dbec40a5d>", line 13, in tfidf
KeyError: 'heavily'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)







 I've been looking at this screen for the past 2 days. The last suggestion I received from the instructor was to resubmit since "the queue was empty", but after nearly 24 hours, nothing has changed. No token, no timeout, just the spinning icon. The notebook takes around 10 minutes to complete on my vagrant, I tried clearing my cache disabling all plugins. Is there any resolution on this issue? I've noticed a few others have raised the same issue - will you provide an alternate submission process?

 Hi, I submitted answers to lab 3 about a day ago and the autograder still seems stuck. I'm currently re-submitting my code.

Would it be possible to get a reset on the # of submissions of autograder in case I need more? I used this method to get (key, value) to (key, [values]):

map(lambda (k,v):(k,[v])).reduceByKey(lambda a,b:list(set(a).union(set(b))))

But I wonder there is more easy way to do it or fast way?

thank you!
 Can you help explain why I get this error during submission of lab 3 result?
Does it mean my lab3 1a and 1d or 2a failed? In my local runs, they pass alright.

Thanks.

================================
Tokenize a String (1a)----------------------Traceback (most recent call last):  File "", line 28, in  File "", line 4 amazonFullRecToToken = amazon. ^ SyntaxError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION invalid syntax 
All tests passedRemoving stopwords (1b)-----------------------All tests passedTokenizing the small datasets (1c)----------------------------------All tests passedAmazon record with the most tokens (1d)---------------------------------------Traceback (most recent call last):  File "", line 1, in  TypeError: object of type 'int' has no len() 

Implement a TF function (2a) 
---------------------------- 
All tests passed 
Create a corpus (2b) 
 input RDD is [(id,[tokens]), (id,[tokens])
output RDD should be sorted in descending order by number of tokens

return inputRDD.sortBy(lambda x: <fill the logic>).collect()

hope that helps. I need clarification regarding 2c. for uniqueTokens I applied flatmap on values and then distinct to get something like [token1, token2, token3, ] and count() amounts to 4772. And then using lab 2 wordcount method for tokenCountPairTuple  to get [ (token1, 1), (token2, 1), (token3, 1) ]. Applying reduceByKey on this RDD to get tokenSumPairTuple will still be the same result since the tokens are unique, or what exactly am I missing?   Hi all,

I am stuck on this lab for 2 days now and the main reason (apart from being new to Python) is that I can't test anything that I do in that box. "print" simply doesn't work. It just goes to the next box without showing anything so there is no way of testing or doing try and error.
Anybody having the same issue?
Also, is there a way that I can post my code privately to the instructor? I need to get some help and feedback but I don't want to break the honor code.

Thanks
S
 I don't know how well the rest of you see important posts on piazza, but I tend to look for important notices on the edx platform first. So, I was surprised that there was nothing posted there about lab3 having a lot of typos and a missing broadcast variable.  I submitted my lab and get an error, here is submission token id:
1049511-623917a4800bc997a3fa9095d5e7eda6:e3348ed676171e4b78ec035684943cd9:ip-172-31-8-108Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
All tests passed
Implement an IDFs function (2c)
-------------------------------
All tests passed
Implement a TF-IDF function (2f)
--------------------------------
All tests passed
Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
All tests passed
Perform Entity Resolution (3c)
------------------------------
All tests passed
Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
All tests passed
Perform a Gold Standard evaluation (3e)
---------------------------------------
All tests passed
Tokenize the full dataset (4a)
------------------------------
All tests passed
Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
All tests passed
Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
All tests passed
Create inverted indicies from the full datasets (4d)
----------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4e)
-------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4f)
-------------------------------------------------
All tests passed
-- 19 cases passed (100.0%) --


Your submission token id is 1049511-623917a4800bc997a3fa9095d5e7eda6:e3348ed676171e4b78ec035684943cd9:ip-172-31-8-108
Please include this submission token id when you need support for your code submission. Hi,

I have problem in getting crossSmall,

I believe that the crossSmall should be,
crossSmall = (googleSmall              .zip(amazonSmall)              .cache())

when I test it with:
print crossSmall.take(1)
it throws me error:
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-26-a11fd59993e3> in <module>()
      4               .cache())
      5 
----> 6 print crossSmall.take(1)
      7 # def computeSimilarity(record):
      8 #     """ Compute similarity on a combination record

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 54.0 failed 1 times, most recent failure: Lost task 0.0 in stage 54.0 (TID 204, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1220, in takeUpToNumLeft
    yield next(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 306, in load_stream
    " in pair: (%d, %d)" % (len(keys), len(vals)))
ValueError: Can not deserialize RDD with different number of items in pair: (53, 39)

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48

But I can do this just fine.
print googleSmall.zip(googleSmall).take(1) So I guess googleRecToToken is vendorRDD?
So 

amazonRecToToken = amazonSmall.map(lambda x: (x.id, [all string(???)] ))
                                                           .map(tokenize(all string))
print amazonRecToToken.take(1)

def countTokens(vendorRDD):....

so the first map function makes a tuple something like
(id, ['A quick brown fox jumps over the lazy dog.'])
and the second map function makes a tuple like
(id, ['quick',  'brown', ... , 'dog'])
and countTokens(amazonRecToToken) counts the number of words inside of
['quick',  'brown', ... , 'dog']

right?

How do I get the ids? I remember in Lab2 we had endpoint by log.endpoint so shouldn't it be
lambda x: (x.id, ....)?

How do I get the string in one line like 
['A quick brown fox jumps over the lazy dog.']?
unlike id, string is made of many variables such as "title","description","manufacturer","price"

What is correct the format of  .map(tokenize(all string))?

I am sorry because they are kinda what we already did last labs  but unlike staffs here I don't remember all transformations and actions. I couldn't finish Lab2 completely because they were difficult to me. Please help me out with some examples..... thank you very much I am confused about how to join these two RDDs. I keep getting an error. I don't know what to do because all the join examples I see don't take lambda functions or anything like that. 

I think this is because the goldStandardRDD is in unicode and the url's are not strings. Is this supposed to be the case????? I'm trouble with message KeyError 'b' . See below a sample of commonTokens(3)
[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120']), (('b000bnb72g', 'http://www.google.com/base/feeds/snippets/6959766318020460134'), ['software']), (('b00009apna', 'http://www.google.com/base/feeds/snippets/13986409474793202514'), ['business'])]
Teste 14 ['120']
Maybe s and value variables must be revisited to change something, or my RDD are
in wrong way. Looks like tokens=['b','u','s','s','i','n','e','s','s'], for example

s = sum([ aWeightsBroadcast.value[aRec][token] * gWeightsBroadcast.value[gRec][token] for t in tokens]
value = s/(aNormsBroadcast.value[aRec] * gNormsBroadcast.value[gRec])

Any ideas? Hello,
I am doing part 2c and am a little confused. 
Read through the posts but didn't come across a clear answer. 

I just completed uniqueTokens. 
I did it by extracting the tokens list and applying the set() function (to get the distinct tokens) on the corpusRDD. 
To make it a list again, I used the list(). 
i.e. I did list(set(list of the tokens in the documents)). 

I now have an RDD consisting the list of unique tokens for each document.

How do I go from here to tokenCountPairTuple? 
It looks like in the notebook that I need to do something to the uniqueTokens.
I need to have an RDD composed of lists of tuples of pairs of token and count for each document.

Any hint on how to get there?

Thanks in advance, Guys here is my problem..

My commonTokens RDD has a small issue..

when i do
commonTokens.take(1)
my output is

[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), '120'))]
i feel the output should be
[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120']))]

how to do i make the token into a list of tokens..



that is convert '120' into ['120']


I will just give a verbose of what i did to get common tokens

merged the amazonInvPairsRDD and googleInvPairsRDD by joinused map to implement the swap functionreduce by key -> x,y : (x+y)

help please!

Because of this not being in a  list of tokens -> my 4F is getting erros for when a record has more than 1 token If we use big O analysis to do the analysis, for the first approach, cartesian( ) operation will be O(n^2), which will cause the quadratic time complexity.

For the second approach, when we get amazonInvPairsRDD, and googleInvPairsRDD which contains
[ (token1, id1) , (token2, id2) , ... ] and [ (token1, url1), (token3, url3) , ... ] , I use a join( ) operation, does the join( ) operation is a linear time operation? if so, then the time complexity for approach two is a linear one, otherwise, this approach is still a quadratic one, why this is more scalable? 
I spent almost the whole day finishing lab3. Fortunately it passed on the first try.

It takes about 4 minutes to run the whole notebook in my system: six cores 4GB

The hard part is trying to understand what is required in each <FILL IN>. Sometimes we are supposed to guess what is the structure of the content of each RDD and variable from its name. It would be nice in future iterations of the course to provide an rdd.take(1) sample of the required RDDs.
The other point is that tests are sometime quite simplistic: just check the rdd.count() but not that the content is correct. Several times I had to go backwards because I had an RDD of the correct length but the wrong content. I hope that the tests in the auto grader are more stringent. Hi All,
 
I set N = corpus.count and my tokenSumPairTuple.take(2) gives me :
[('aided', 1.0), ('precise', 4.0)]
 
My last test fails. Other Test conditions pass except for this one.
1 test failed. incorrect smallest IDF value
And when I print tokenSmallestIdf[0], tokenSmallestIdf[1], i get:
software 3.80952380952
My last idfs statement looks like:
 tokenSumPairTuple.map(lambda tup: (tup[0], float(N)/tup[1]))
 
 Please help with hints if any - stuck up for too long with this!
 
 Hi:

Browsed through the forums and still confused over 4f

first: Are we supposed to do sc.broadcast on amazonweightrdd/google?

second: what is "s=" is that the sum of the dot product? if so, how are we supposed to use the tokens? Is "s" supposed to be just google rec weight TFIDF multiplied by amazon rec TFIDF multiplied by number of tokens?

third: is "value" supposed to be "s" divided by norm(googleNormsBroadcast.value) and then by amazon?

finally I assume we are supposed to do .map(fastCosineSimilarity)?

I have done everything up to 4f by myself but 4f just seems to be so confusing Hi, instructor,

I passed all the cases in local machine, but failed in 1a when submitting. Could you help me to check what is wrong with my code? Thanks!

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 101.0 failed 1 times, most recent failure: Lost task 2.0 in stage 101.0 (TID 506, localhost): java.io.FileNotFoundException: /tmp/spark-32734ba2-d1a4-42f5-a84c-4dfde462e9a1/pyspark-cdace9a1-0d38-4775-81f5-aeea718ca261/tmpm8dQSr (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.(FileInputStream.java:146)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply$mcJ$sp(PythonRDD.scala:848)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply(PythonRDD.scala:847)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply(PythonRDD.scala:847)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1153)
	at org.apache.spark.api.python.PythonBroadcast.writeObject(PythonRDD.scala:847)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:110)
	at org.apache.spark.storage.BlockManager.dataSerializeStream(BlockManager.scala:1176)
	at org.apache.spark.storage.DiskStore.putIterator(DiskStore.scala:79)
	at org.apache.spark.storage.DiskStore.putArray(DiskStore.scala:64)
	at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1028)
	at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:419)
	at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:408)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.storage.MemoryStore.ensureFreeSpace(MemoryStore.scala:408)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:263)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)

                                                                                

[Stage 113:=====================>                                   (3 + 1) / 8]
[Stage 113:============================>                            (4 + 1) / 8]
[Stage 113:===================================>                     (5 + 1) / 8]
[Stage 113:==========================================>              (6 + 1) / 8]
[Stage 113:=================================================>       (7 + 1) / 8]
                                                                                

[Stage 122:>                                                        (0 + 1) / 8]
[Stage 122:=======>                                                 (1 + 1) / 8]
[Stage 122:==============>                                          (2 + 1) / 8]
[Stage 122:=====================>                                   (3 + 1) / 8]
[Stage 122:============================>                            (4 + 1) / 8]
[Stage 122:===================================>                     (5 + 1) / 8]
[Stage 122:==========================================>              (6 + 1) / 8]
[Stage 122:=================================================>       (7 + 1) / 8]
                                                                                

[Stage 125:>                                                        (0 + 1) / 8]
[Stage 125:=======>                                                 (1 + 1) / 8]
[Stage 125:==============>                                          (2 + 1) / 8]
[Stage 125:=====================>                                   (3 + 1) / 8]
[Stage 125:============================>                            (4 + 1) / 8]
[Stage 125:===================================>                     (5 + 1) / 8]15/06/25 04:23:23 ERROR PythonRDD: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 88, in main
    command = pickleSer._read_with_length(infile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 148, in _read_with_length
    length = read_int(stream)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 528, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)
Caused by: java.io.FileNotFoundException: /tmp/spark-32734ba2-d1a4-42f5-a84c-4dfde462e9a1/pyspark-cdace9a1-0d38-4775-81f5-aeea718ca261/tmpm8dQSr (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.api.python.PythonBroadcast</init>$$anonfun$writeObject$1.apply$mcJ$sp(PythonRDD.scala:848)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply(PythonRDD.scala:847)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply(PythonRDD.scala:847)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1153)
	at org.apache.spark.api.python.PythonBroadcast.writeObject(PythonRDD.scala:847)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:110)
	at org.apache.spark.storage.BlockManager.dataSerializeStream(BlockManager.scala:1176)
	at org.apache.spark.storage.DiskStore.putIterator(DiskStore.scala:79)
	at org.apache.spark.storage.DiskStore.putArray(DiskStore.scala:64)
	at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1028)
	at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:419)
	at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:408)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.storage.MemoryStore.ensureFreeSpace(MemoryStore.scala:408)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:263)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	... 4 more
15/06/25 04:23:23 ERROR PythonRDD: This may have been caused by a prior exception:
java.io.FileNotFoundException: /tmp/spark-32734ba2-d1a4-42f5-a84c-4dfde462e9a1/pyspark-cdace9a1-0d38-4775-81f5-aeea718ca261/tmpm8dQSr (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.(FileInputStream.java:146)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply$mcJ$sp(PythonRDD.scala:848)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply(PythonRDD.scala:847)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply(PythonRDD.scala:847)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1153)
	at org.apache.spark.api.python.PythonBroadcast.writeObject(PythonRDD.scala:847)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:110)
	at org.apache.spark.storage.BlockManager.dataSerializeStream(BlockManager.scala:1176)
	at org.apache.spark.storage.DiskStore.putIterator(DiskStore.scala:79)
	at org.apache.spark.storage.DiskStore.putArray(DiskStore.scala:64)
	at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1028)
	at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:419)
	at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:408)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.storage.MemoryStore.ensureFreeSpace(MemoryStore.scala:408)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:263)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)
15/06/25 04:23:23 ERROR PythonRDD: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 88, in main
    command = pickleSer._read_with_length(infile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 148, in _read_with_length
    length = read_int(stream)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 528, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-32734ba2-d1a4-42f5-a84c-4dfde462e9a1/pyspark-cdace9a1-0d38-4775-81f5-aeea718ca261/tmpm8dQSr (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.(FileInputStream.java:146)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply$mcJ$sp(PythonRDD.scala:848)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply(PythonRDD.scala:847)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply(PythonRDD.scala:847)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1153)
	at org.apache.spark.api.python.PythonBroadcast.writeObject(PythonRDD.scala:847)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:110)
	at org.apache.spark.storage.BlockManager.dataSerializeStream(BlockManager.scala:1176)
	at org.apache.spark.storage.DiskStore.putIterator(DiskStore.scala:79)
	at org.apache.spark.storage.DiskStore.putArray(DiskStore.scala:64)
	at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1028)
	at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:419)
	at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:408)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.storage.MemoryStore.ensureFreeSpace(MemoryStore.scala:408)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:263)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)
15/06/25 04:23:23 ERROR PythonRDD: This may have been caused by a prior exception:
java.io.FileNotFoundException: /tmp/spark-32734ba2-d1a4-42f5-a84c-4dfde462e9a1/pyspark-cdace9a1-0d38-4775-81f5-aeea718ca261/tmpm8dQSr (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.(FileInputStream.java:146)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply$mcJ$sp(PythonRDD.scala:848)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply(PythonRDD.scala:847)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply(PythonRDD.scala:847)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1153)
	at org.apache.spark.api.python.PythonBroadcast.writeObject(PythonRDD.scala:847)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:110)
	at org.apache.spark.storage.BlockManager.dataSerializeStream(BlockManager.scala:1176)
	at org.apache.spark.storage.DiskStore.putIterator(DiskStore.scala:79)
	at org.apache.spark.storage.DiskStore.putArray(DiskStore.scala:64)
	at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1028)
	at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:419)
	at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:408)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.storage.MemoryStore.ensureFreeSpace(MemoryStore.scala:408)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:263)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)
15/06/25 04:23:23 ERROR Executor: Exception in task 1.0 in stage 125.0 (TID 582)
java.io.FileNotFoundException: /tmp/spark-32734ba2-d1a4-42f5-a84c-4dfde462e9a1/pyspark-cdace9a1-0d38-4775-81f5-aeea718ca261/tmpm8dQSr (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.(FileInputStream.java:146)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply$mcJ$sp(PythonRDD.scala:848)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply(PythonRDD.scala:847)
	at org.apache.spark.api.python.PythonBroadcast$$anonfun$writeObject$1.apply(PythonRDD.scala:847)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1153)
	at org.apache.spark.api.python.PythonBroadcast.writeObject(PythonRDD.scala:847)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:110)
	at org.apache.spark.storage.BlockManager.dataSerializeStream(BlockManager.scala:1176)
	at org.apache.spark.storage.DiskStore.putIterator(DiskStore.scala:79)
	at org.apache.spark.storage.DiskStore.putArray(DiskStore.scala:64)
	at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1028)
	at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:419)
	at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:408)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.storage.MemoryStore.ensureFreeSpace(MemoryStore.scala:408)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:263)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)
15/06/25 04:23:23 ERROR TaskSetManager: Task 1 in stage 125.0 failed 1 times; aborting job
15/06/25 04:23:23 ERROR Executor: Exception in task 3.0 in stage 125.0 (TID 583)
org.apache.spark.TaskKilledException
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

Your submission token id is 1053538-8815a02de0542bf55f78e78b70b20b10:0ff43865ae60b4ae6e7f1cfc702fbdd0:ip-172-31-4-252
Please include this submission token id when you need support for your code submission.

 Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.

  Hi,
I found all the places that take very long in
(1) everything that has a call to 'filter' in it and
(2) all the assert boxes in part 4.
The second thing is a bit strange, because if I save the result of the first
call to count() in a variable and use that in the assert, the assert still takes
minutes to finish.
Any insight in that?
Heike
 Please ignore this question. I re-ran the Test Assert section and it works fine. Turning this into a note.


lab 3 (4c): here is my output
1363amazonNorms first entry: ('b000jz4hqo', 262.3974984324429)
----------------------
Then my line of code for the amazon broadcast variable:
amazonNormsDict = amazonNorms.collectAsMap()amazonNormsBroadcast = sc.broadcast(amazonNormsDict)
-----------------------
Test line: failed
Test.assertTrue(isinstance(amazonNormsBroadcast, Broadcast), 'incorrect amazonNormsBroadcast')

Here is the failed message: What do I miss?

NameError                                 Traceback (most recent call last)<ipython-input-221-b3a02719cf56> in <module>() 
 1 # TEST Compute Norms for the weights from the full datasets (4c) ---->
 2 Test.assertTrue(isinstance(amazonNormsBroadcast, Broadcast), 'incorrect amazonNormsBroadcast') 
 3 Test.assertEquals(len(amazonNormsBroadcast.value), 1363, 'incorrect amazonNormsBroadcast.value') 
 4 #Test.assertTrue(isinstance(googleNormsBroadcast, Broadcast), 'incorrect googleNormsBroadcast') 
 5 #Test.assertEquals(len(googleNormsBroadcast.value), 3226, 'incorrect googleNormsBroadcast.value') 

NameError: name 'amazonNormsBroadcast' is not defined 

------------------------------------------------------------------
I add more print out to show more info. Still don't know why the Test Assert rejects it.

amazonNorms: 1363amazonNorms first entry: ('b000jz4hqo', 262.3974984324429)len(amazonNormsDict): 1363len(amazonNormsBroadcast.value): 1363 I have a problem with 3b.

I got the following result:

{'photoshop': 22.22222222222222, 'adobe': 8.333333333333334}
{'illustrator': 50.0, 'adobe': 8.333333333333334}
0.0585852406541

My dotprod function seems to be good (keys for both dicts), 3a test passes, but got the wrong number.
Could you please help?
Anybody had this same bad result first?
Thanks.






  Sometimes I only see a blank page after logining to mooc01.cloud.databricks.comNo matter how many times I reload the page and relogin.It makes me unable to go further for the completion of Lab 3.

It is happening now and I cannot continue my Lab 3.
Can anyone please help me with this critical issue? Hi,

To solve countToken Problem first I create RDD with map function. Every element in RDD contain an ID and the number of tokens related to that ID as follow for amazon:
[('b000jz4hqo', 9), ('b0006zf55o', 18), ('b00004tkvy', 11)]
The question is how to I sum up all values in above RDD with reduce function ? All my tests passed except 'incorrect similarityTest fastCosineSimilarity'. I've compared my weighted RDD with corresponding from this forum and find that mine are wrong. For example,  googleWeightsRDD.first() is:

('http://www.google.com/base/feeds/snippets/11125907881740407428', {'quickbooks': 61.18666666666667, '2007': 17.44866920152091, 'learning': 20.764705882352942, 'intuit': 93.65306122448979})


Edit:
Found my mistake, I didn't calculate tdidf for each token(


 Hello everybody,

I would like to ask a question about 2c, I have the unique tokens in each document:

[['00320289', 'hampton', 'features', 'classic', 'video', 'artists', 'follow', 'legends', 'hudson', 'battles', '2', 'music', 'rich', 'thunderous', 'various', 'lionel', 'never', 'papa', 'volume', 'released', 'jo', '15', 'buddy', 'jones', 'drum', 'like', 'home', 'dvd', 'vhs', 'many', 'krupa', 'solos', '00320249', 'gene'], ['1', '20', 'cal', 'ts', 'softgrid', 'mlp', '4', 'english', 'microsoft', 'user']]

My question is : What do I have to do now?. Do have I to count in amazonRecToToken.union(googleRecToToken)
the number of this tokens?

I have the unique tokens in each document. Do I have to get the unique tokens in all documents and the to apply a amazonRecToToken.union(googleRecToToken)?

Thanks in advance

Carlota Vina








 Using leftOuterJoin, trueDupSimsRDD.count() is 1300. Using join, trueDupSimsRDD.count() is 1299. I think that means that the gold standard file contains an item that does not exits in our data set.

Shouldn't we use join if we want to find the true duplicates in our data set?

Edit: fixed a typo Repeatedly I failed to construct amazonNorms. It says too many values to unpack for the following map

(lambda (x, y):(x, norm(y)))

Please help me to complete this. Hi,

When I am using sortByKey with takeOrdered I am getting the following:
The Amazon record with ID "b000jz4hqo" has the most tokens (9)

Could you please tell me what is wrong with my approach and how to solve the problem.

After some modification I have:
The Amazon record with ID "b000s8jxpc" has the most tokens (205)


Thanks in advance.

Regards,
 Wahi Where do I download the lab 3 data. I do not find the link in the ipynb or in the lab3 description on edx In lab 3 3c problem I'm getting requested similarity as 0.0003628231441 where as it must be 0.000303171940451. Any idea where I might be wrong? 
Why there's no any lecture to teach us about mllib before answering Lab4?
 I am receiving the below error during submission. To debug, I tried the following:
1) Performed vagrant halt and up
2) Restarted kernel
3) Re-ran all cells

Everything seems to work in the iPython notebook, and all tests pass. However, while uploading to the auto-grader, this is happening. I've tried this 3 times already.
(The code does not seem to have indentation issues - I'm still looking into it more to check if I can find the cause)

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 2
    simpleTokensNoStopWords=[]
    ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
unexpected indent
:
:
Your submission token id is 1056196-078c6bb54158562322fba6699a9d07c0:6861aef0c96e02bfb27f04688353e6f2:ip-172-31-15-207
Please include this submission token id when you need support for your code submission.
===
Update:
Same issue on 5th attempt at submission! Request help in sorting this out. Everything is running and passing locally.
Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 2
    simpleTokensNoStopWords = []
    ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
unexpected indent

 
Your submission token id is 1057621-4ce17fd604f69c45c16f4d9b59455670:6861aef0c96e02bfb27f04688353e6f2:ip-172-31-8-219
Please include this submission token id when you need support for your code submission. I am trying to 
amazonRecToToken = amazonSmall.map(lambda x: (x[0], tokenize(x[1])))
googleRecToToken = googleSmall.map(lambda x: (x[0], tokenize(x[1])))

(not sure if i'm missing something)

but I get the following error msg.


Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.

I'm not sure what's wrong.
 I can't figure out following in 1c "
count the total number of tokens
".

fold and reduce doesn't work for me. Any ideas? Your course staff has been very busy behind the scenes today rolling out autograder fixes to better deal with errors in student submissions!

Now, the autograder should always provide you with feedback, although sometimes that feedback will still only be that your submission has timed out.

From looking at a lot of your failed submissions, we've identified three common problems that cause timeouts:
print statements in helper functions that are used in map()  or filter(). Although we've mentioned it many times, make sure you remove any print statements that you put in your code, especially helper functions. print statements can be helpful for debugging tokenizing or parsing functions, but when you apply them with a map() to Shakespeare's plays, it generates a huge amount of log data at workers and crashes them. With help from Databricks engineering, the course staff deployed an autograder fix that lets us ignore print statements in your submissions.Overly complex regular expressions that cause long running times or timeouts. Please make sure that in Lab 2, your change to the regular expression is no more than three characters and in Lab 3, in your tokenize function, you do not use any string or regular expression functions other than re.split, string.tolower and string.len, and do not to change the regular expression string. This code fragment from tokenize in Lab3 is an example of what you should never do: 
string = string.lower().replace(r'([!"#$%&()*+,\'\-./:;<=>?@[\\\]^_`{|}~])+'," ")  

After running for more than 50 minutes, this code fragment caused a student's submission to timeout.
collect() applied to large RDDs. We observed many cases of students using collect() to convert an RDD into a Python list and then using Python functions to manipulate the list. This is not distributed, highly inefficient, very time consuming, and often causes out of memory errors. In Lab 3, a typical solution has seven collect() actions, but we've seen student submissions with more than twice as many. Examples of common errors that cause timeouts or out of memory errors:
Using: 
   for i in vendorRDD.collect():
     measure len() to find biggest X

Instead, use: vendorRDD.takeOrdered(helper function)
Using:
   unionRDD = sc.parallelize(A.collect()+B.collect())

Instead, use: A.union(B)
Using:
   sum = sum(RDD.collect())

Instead, use: RDD.sum() 
We hope the autograder changes and these hints will help everyone write better code and have a smooth autograder experience!

[UPDATE] More technical details about the earlier autograder issues

To manage costs for the course, for setup and Labs 1 and 2, we used Amazon's t2.medium instances running two autograder containers. This configuration worked very well for tens of thousands of submissions. However, the problematic submissions that we started receiving early on with Lab 3 and then with later Lab 2 submissions, caused the autograder to become very compute and memory bound and this impacted submissions from other students with effects ranging from timeouts to "lost" (24 hours and no feedback) submissions. 

We switched to a single autograder container per instance (effectively doubling compute and memory resources for each submission), which helped reduce but not eliminate the  effects of problematic submissions (occurrences of timeouts and "lost" submissions for other students).  

We then switched to larger, more powerful, and twice as expensive Amazon c3.large instances. Since the switch, we have not seen any impact on other students from problematic submissions.

Rolling out each change has taken several hours as we perform changes in a rolling manner to avoid any impact on in-progress submissions.

#pin

 Given two RDDs A and B, I am using A + B instead of A.union(B), is this approach right? Is it distributed? For Lab 3 1c, are we supposed to tokenize with the function defined in 1b, or the re.split() with the defined regex from 1a? Either way, I'm not getting the expected sum of 22520 tokens. Is there any additional processing I'm supposed to be doing? Hi guys,

I am really struggling with getting my head around constructing the swap function..
Here is what I am thinking but is not working 


token = record[0].keys() keys = (record[1].keys(),record[1][1]) return tuple(keys, token)

can you please help When I used the following

map(lambda x: (x...,len(x.....))).takeOrdered(1, lambda s: -1 * s[1])

I passed the first test but yes the second test shouldn't be  in the len format that's why I cannot pass the second one.

First test = 'b000o24l3q'

However, when I've grouped everything in takeOrdered , using the following logic


takeOrdered(1,lambda x:(x[....],-1*len(x[....])))

it resulted the wrong answer...as below


[('b00002s8if', ['trudy', 'time', 'place', 'house', 'ages', '3', '6', 'ibm'])]
The Amazon record with ID "b00002s8if" has the most tokens (8)

it should have been 'b000o24l3q', [ ' ' , ' ' ]

Please kindly suggest ....

 i am confused in lab3 2c ... can u tell me step by step process to complete the task I get an error

'list' object has no attribute 'map'

I mapped the tokens to an unique list then I reduced to combine those lists with unique words to one list.
uniqueTokens: ['rom', 'clickart', '950', 'image', 'premier', '000', 'dvd', 'broderbund', 'pack', 'laptops', 'desktops', 'backup', 'ca', 'v11', 'associates', '30u', '30pk',......]
Now I try to count the words but I receive this error msg and I have no clue to continue :(

Who can help me? First, I would like to acknowledge the hard work the course instructors are putting into this to make this all happen. Thank you!

However, I have got one big criticism to raise: Given the name of the course, I was hoping to find an introduction to Spark and its workings. However, the topic seems to be mostly about basic principles of ETL and data munging. Part of that we indeed are doing in Spark, but after lab 3, I must say this is more about how to work with data in Python than about teaching us how Spark actually works. And the lectures are even worse, most videos discuss issues in data munging and/or statistics, which is not terribly relevant towards understanding Spark. Why do the videos not discuss issues relevant to Spark? How to tune your Spark setup, what to think about in distributed environments, etc.?

And if I check the Spark API, it is full of stuff we have not even touched or, as in lab 3, was only introduced very superficially (referring to Accumulators). What about tricky bits of Spark configuration settings, RDD storage settings, job tracking, and profiling? What about Spark SQL and Streaming, interfacing with Apache Hive, windowing functions? And so forth...

The labs ask us to figure out how to write cosine similarity functions in Python and fix regular expressions. Instead, I would really have liked to get some hands-on knowledge with Spark - so far this course has been mostly "Text processing with Python (coincidentally using some Spark)". I'm stuck on lab3 2c.

When i run the test cases, i get the below results:

1 test passed.
1 test failed. incorrect smallest IDF token
1 test failed. incorrect smallest IDF value
Unique Token Count is correct, but the other 2.
I'm not able to identify the problem here. Any Help? Hey , I was solving lab 3 3b. I tokenized each string in function tfidf and then set parameter to cossim and I have already passed the cossim function test. As I calculate cosineSimilarity for cossimAdobe answer I'm getting is 0.05772433821630337 instead of 0.0577243382163 as there is difference of 4 least significant digits and rest of answer is correct i.e, my answer is having 4 digits extra thus I'm not able to pass the test. Please help in letting me know what is going wrong?Here's the error that I'm getting:-
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-29-2fa498854f27> in <module>()
      1 # TEST Implement a cosineSimilarity function (3b)
----> 2 Test.assertTrue(abs(cossimAdobe - 0.0577243382163) < 0.0000001, 'incorrect cossimAdobe')

TypeError: unsupported operand type(s) for -: 'list' and 'float' 

Hello,

I would like to ask a question about RDD uniqueTokens. My RDD uniqueTokens is

[['00320289', 'hampton', 'features', 'classic', 'video', 'artists', 'follow', 'legends', 'hudson', 'battles', '2', 'music', 'rich', 'thunderous', 'various', 'lionel', 'never', 'papa', 'volume', 'released', 'jo', '15', 'buddy', 'jones', 'drum', 'like', 'home', 'dvd', 'vhs', 'many', 'krupa', 'solos', '00320249', 'gene'], ['1', '20', 'cal', 'ts', 'softgrid', 'mlp', '4', 'english', 'microsoft', 'user']]
 
But , I think that it isn't right because I have a list for each document. Do I have to get a single list with every token in every document.

Could everyone confirm this?

Thanks in advance

Carlota Vina I passed the tests for 4d with success. However, when I join the two RDDs I don't get the correct count for commonTokens (Found 5966373 common tokens).

When I use groupByKey() on the join, I get: Found 6601 common tokens

What am I doing wrong??

Thanks. I have no idea what is wrong with 4e ...all the previous test have passed upto 4d 


print commonTokens.takeSample(False,10)print list(commonTokens.take(1)[0][1])
print 'Found %d common tokens' % commonTokens.count()

result 
[(('b0002ce0v6', 'b0007wv3ei'), <pyspark.resultiterable.ResultIterable object at 0xb0a1792c>), (('b000088nqp', 'b0001zjrus'), <pyspark.resultiterable.ResultIterable object at 0xb056b24c>), (('b000e8jlbc', 'b000h22pg8'), <pyspark.resultiterable.ResultIterable object at 0xb056b46c>), (('b000ep8oju', 'b000hc0m36'), <pyspark.resultiterable.ResultIterable object at 0xb09ddf4c>), (('b000fp0k0u', 'b000gpsz42'), <pyspark.resultiterable.ResultIterable object at 0xb09dd28c>), (('b0001g6q2e', 'b000ldqs9s'), <pyspark.resultiterable.ResultIterable object at 0xb09dde2c>), (('b00002cf7w', 'b0007yepyg'), <pyspark.resultiterable.ResultIterable object at 0xb09dd90c>), (('b000aazr5i', 'b000ofnri8'), <pyspark.resultiterable.ResultIterable object at 0xb056b1cc>), (('b000bgpqr0', 'b0001h5w9g'), <pyspark.resultiterable.ResultIterable object at 0xb056b62c>), (('b000bdezfw', 'b000qxd2tm'), <pyspark.resultiterable.ResultIterable object at 0xb0bddd0c>)]
['sound', 'tracks', 'll', 'like']
Found 1230379 common tokens

 Hi guys,

I'm having some issues while submitting the lab3:

1060819-bcc028b6caf5e0ce04633f85d23f2fee:b94e4fa7035299a12e53ef9bf38aa269:ip-172-31-8-110

It Turns out that I'm getting a TestFailed error (section 2b) in the autograder but not in the notebook. 

Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect corpusRDD.count()



Thanks in advance!
 lab 3_4e took too long to show result..........however i passed the test.
Am I doing anything bad? Hi,

I think I am calculating the idfsFullWeights in a wrong way... this is my code

idfsFullWeights = tfidf(fullCorpusRDD.flatMap(lambda (row):(row[1])).collect(),idfsFull.collectAsMap())idfsFullBroadcast = sc.broadcast(idfsFullWeights)

is that ok? Hello guys,
In order to acquire the verified certificate is sufficient having the total greater than 45%?
For instance my total is 66 % so i cannot perform the week 4 or there are other constraints?
when the verified certificate is provided? when the course is finished?


Thanks Where i can learn spark in deeper details (basic and not)? Lessons don't provide support to the lab so it is very difficult to complete it.


Thanks,

Lica

 Hi,

I have an issue getting amazonWeightsRDD
Here are my piece of codes from (4a) and (4b)
(4a) This one works well
amazonFullRecToToken = amazon.map(lambda record: (record[0], tokenize(record[1])))
googleFullRecToToken = google.map(lambda record: (record[0], tokenize(record[1])))

(4b)
# TODO: Replace <FILL IN> with appropriate code
fullCorpusRDD = amazonFullRecToToken.union(googleFullRecToToken)
idfsFull = idfs(fullCorpusRDD)
idfsFullCount = idfsFull.count()
print 'There are %s unique tokens in the full datasets.' % idfsFullCount

# Recompute IDFs for full dataset
idfsFullWeights = idfsFull.collectAsMap()
idfsFullBroadcast = sc.broadcast(idfsFullWeights)

# Pre-compute TF-IDF weights.  Build mappings from record ID weight vector.
amazonWeightsRDD = amazonFullRecToToken.map(lambda record: (record[0], tfidf(record[1], idfsFullBroadcast)))
googleWeightsRDD = googleFullRecToToken.map(lambda record: (record[0], tfidf(record[1], idfsFullBroadcast)))
print 'There are %s Amazon weights and %s Google weights.' % (amazonWeightsRDD.count(),
                            	                                  googleWeightsRDD.count())
The first print is good:
There are 17078 unique tokens in the full datasets.
But the second print failed.

Can someone point me why is it wrong?
My codes for idfsSmallWeights work fine.

 Hi fellow students,

my solution to 4e is running for an hour already and won't finish. I have no idea why, because I don't see where my solution could be more efficient.
What I'm doing to solve the first part is:
1. Join the two inverted pair RDDs from 4d
2. Group this result by key. 

Isn't this the way to go?

I'd be glad for any advice.

All the best!

  how do we make a single list from mulitiple lists? Hello dear all,

I did submit my lab on the autograder yesterday and it is still running. It still shows "Your files have been submitted. As soon as your submission is graded, this message will be replaced with the grader's feedback.".

I was expecting it to fail on a timeout or to success. What should I do, should I try to resubmit?

Thanks in advance So this problem is really weird. Suddenly, the "Download as -> Python (.py)" option disappeared from Jupyter. The others, IPython Notebook, HTML, Markdown etc. are still available, but Python is not. Instead, I'm only offered "Script", which downloads a .txt file. Has anybody experienced the same issue?

Thanks!  Hello Teachers,

Today a submit lab 1 again, because in the progress tab said that I had not sent it, but this is not true. 

I had sent in date.

It has taken 20 points unfairly, I am very disappointed with the theme of laboratories and his malfunction.


Thanks very much. Please help me out in coding my invert function.

I am new to python. It's been more than 8 hrs in this part of the lab. I did saw many discussions saying that it is just a  line of code.
Please help me with that one line please. Hello,

I am using the following regex to parse the apache logs,
'([\S])+ (\-) (\-) \[([\d]{2}\/[A-Za-z]{3,}\/\d{4}\:\d{2}\:\d{2}\:\d{2} \-\d{3,})\] \"([A-Z]{3,}) (\/[\S\-\/]*[\S])* ([A-Z]{4}\/[0-9]\.[0-9])\" ([0-9]{3}) ([0-9]*)'
and the error is:

https://bpaste.net/show/f0d0580507e0

Please tell us where the problem is. Thanks. corpusRDD contains  (id, list of tokens)
how do we make a single list of tokens  for the entire corpusRDD
i did as
corpus.map(lambda x : list(set(list(x[1])))).reduce(lambda x,y: x.union(y)) def norm(a):
""" Compute square root of the dot product
    Args: a (dictionary): a dictionary of record to value
   Returns: norm: a dictionary of tokens to its TF values """

this seems odd to me...the solution involves producing a vector norm (square root of sum of squares)
why does it say it returns a dictionary? error? Hi all,

I have never coded in Python prior to this course, so forgive me if this is really stupid.

I'm facing a compilation issue when I tried the following snippet :

return vendorRDD.values().reduce(lambda x, y : len(x) + len(y))

Since the RDD is a pair tupple of (string, [stringTokens]), shouldn't the variables x and y be list objects? I'm getting the following error messages:

TypeError: object of type 'int' has no len()

I've around found a way around the problem, but I'm just curious why this previous approach had failed. 

Could anyone help explain this to me?

Thanks I remember accessing a page that allows one to see a table of the time taken for each task (code block) to run in the ipython notebook, along with 1 or 2 other statistics, but I can't remember how or where to access it now that I have closed it. I have tried looking through the past labs and lecture notes, but I can't seem to find the link.

Would appreciate if anyone could direct me!

#pin Hi,

I think I already write the correct function for invert.
    pairs = []    for idurl, tfidf_dict in record:        for token in tfidf_dict.iterkeys():            pairs.append((token, idurl))    return pairs
Then I define
amazonInvPairsRDD = (amazonWeightsRDD                    .flatMap(invert)                    .cache())
however I got this error:
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 158.0 failed 1 times, most recent failure: Lost task 0.0 in stage 158.0 (TID 764, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):

May I know why i get the error?

 Hi all,

I'm completely stuck and getting crazy... So far I`ve defined:

1. broadcasts
like sc.broadcast and collectAsMap

2.similaritiesFullRDD
mapping fastCosineSimilarity function

3. fastCosineSimilarity function:
Im lost here:
I suppose:
amazonRec = record[0][0] googleRec = record[0][1] tokens = record[1]

is it right?

And how do I calculate s and value? I've read some topics but everyone is different..

Thx in advance: I am able to get N  and uniqueTokens 

but how to compute the followings ?

tokenCountPairTuple = uniqueTokens.
tokenSumPairTuple = tokenCountPairTuple.

why uniqueTokens in tokenCountPairTuple ? 

uniqueTokens count will be always 1, right ?  Hello,
Can anyone help in telling the substitute of cartesian function? How to use the substitute? I have used cartesian function twice til now and was wondering if there are better substitutes for 2c or 3c.


Thanks. My Spark app crashed and so I deleted it and started afresh.
Unfortunately, all my old .pynb assignments have reverted to their original state...brand new notebooks, with no code in them.

How do I convert copies of my .py assignments to .pynb?

 I am trying to perform join on 2 rdd

print amazonInvPairsRDD.take(1)print googleInvPairsRDD.take(1)commonTokens = (amazonInvPairsRDD.join(googleInvPairsRDD)) 

Well it is not complete yet, I know that but the natural join should give me the (token (ir, url)) without the swap.

Why python is throwing error? Any idea


[[('rom', 'b000jz4hqo')]]
[[('quickbooks', 'http://www.google.com/base/feeds/snippets/11125907881740407428')]]




---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-57-51a3821fe9c5> in <module>()
     14 commonTokens = (amazonInvPairsRDD.join(googleInvPairsRDD).cache())
     15 
---> 16 print 'Found %d common tokens' % commonTokens.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 108.0 failed 1 times, most recent failure: Lost task 0.0 in stage 108.0 (TID 507, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1806, in <lambda>
    map_values_fn = lambda (k, v): (k, f(v))
ValueError: need more than 1 value to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
</div>
</div> 
Hello, I think there is a mistake in the comment, norm should return the square root value of the dot product not a dictionnary of tokens to its TF values. It's probably abvious but could confuse some people if there is indeed a mistake. Thank you. In lab 3 we have created many broadcast variables in order to do more efficient computation by reusing idfs of tokens, for example. In case of idfs because of the size of a dictionary
typically is not very very large, I agree that it is possible and efficient to send and STORE
at every worker a copy of this information.

But when in the 4f exersice we need to create an amazonWeightsBroadcast
and googleWeightsBroadcast variables, I somewhat confused. It seems to me
that effectively this two variables represent full dataset we need to analyze
in this lab (mapping between IDs and vectors of tokens with tf-idf weights). So,
it seems to me inefficient to STORE a copy of the full dataset at each worker
by broadcasting this RDDs. What then the point of distributed computing?

May be I understand something wrong. Please, explain me where my
logic broken! Thanks in advance for any help!  I pass all tests on VM but autograder returned some weird errors!
I appreciate any help in debugging.
Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
.
.
. I am facing this error all of a sudden, while working on LAB 2. Can anyone kindly help me this ?

 Hi Friends,

I've managed to complete the lab3 till 4e. The execution time for all of these normal and response time is acceptable. However, for 4f, I've been waiting. Please note that it works fine with limited number of records (or say one record given in the unit test just below the 4f). Results are also fine.

Inputs are appreciated.

Also, if you can suggest the execution average time for you.

Regards,
Neeraj  Hi guys.

I'm trying to verify my idfs values computed at step 4(b). Here's the print of top 100 from idfsFull, i.e idfsFull.take(100) . Can you verify that you have the same?

[('aided', 917.8),
 ('shop', 81.94642857142857),
 ('skating', 1147.25),
 ('poor', 1529.6666666666667),
 ('paperless', 4589.0),
 ('bevy', 2294.5),
 ('productspowerful', 4589.0),
 ('multinational', 4589.0),
 ('fledged', 2294.5),
 ('bin', 917.8),
 ('17100', 1147.25),
 ('teen', 1529.6666666666667),
 ('fp', 4589.0),
 ('tomboy', 4589.0),
 ('operations', 152.96666666666667),
 ('evergreen', 2294.5),
 ('oper', 4589.0),
 ('questionable', 4589.0),
 ('consolidate', 917.8),
 ('dunk', 4589.0),
 ('covenant', 4589.0),
 ('continuity', 1147.25),
 ('gill', 4589.0),
 ('applicationcalled', 4589.0),
 ('brushing', 4589.0),
 ('soln', 2294.5),
 ('bending', 4589.0),
 ('047875752832', 4589.0),
 ('slips', 458.9),
 ('1094', 4589.0),
 ('rhythmic', 1529.6666666666667),
 ('antique', 2294.5),
 ('corrugated', 4589.0),
 ('hinder', 4589.0),
 ('exchanged', 917.8),
 ('trim', 917.8),
 ('swallowedrainb', 4589.0),
 ('unrelenting', 4589.0),
 ('opacity', 4589.0),
 ('holmes', 4589.0),
 ('378517z', 4589.0),
 ('givens', 4589.0),
 ('436753', 4589.0),
 ('605', 4589.0),
 ('joshua', 4589.0),
 ('dad', 1529.6666666666667),
 ('emotion', 1529.6666666666667),
 ('em', 417.1818181818182),
 ('embroiled', 4589.0),
 ('achievements', 2294.5),
 ('exotic', 218.52380952380952),
 ('pervasive', 2294.5),
 ('pre', 86.58490566037736),
 ('swim', 2294.5),
 ('limitations', 1529.6666666666667),
 ('precise', 97.63829787234043),
 ('colleague', 2294.5),
 ('regularly', 382.4166666666667),
 ('afterlight', 2294.5),
 ('fruitless', 2294.5),
 ('riddle', 4589.0),
 ('bb0d4na', 4589.0),
 ('13791', 4589.0),
 ('630', 1529.6666666666667),
 ('mickey', 2294.5),
 ('innocents', 4589.0),
 ('shiba', 4589.0),
 ('function', 143.40625),
 ('locate', 254.94444444444446),
 ('lich', 4589.0),
 ('101st', 4589.0),
 ('enjoy', 30.593333333333334),
 ('loggers', 1147.25),
 ('chaplin', 4589.0),
 ('istopmotion', 2294.5),
 ('summarizes', 2294.5),
 ('hotspots', 2294.5),
 ('teh', 2294.5),
 ('pioneering', 4589.0),
 ('authorize', 917.8),
 ('mystical', 2294.5),
 ('ints3', 4589.0),
 ('upgra', 2294.5),
 ('greats', 1529.6666666666667),
 ('versus', 1147.25),
 ('120mhz', 4589.0),
 ('heavy', 764.8333333333334),
 ('omniweb5', 4589.0),
 ('composed', 1147.25),
 ('pronunciations', 4589.0),
 ('julio', 4589.0),
 ('arthate', 4589.0),
 ('invasiveness', 4589.0),
 ('knowing', 241.52631578947367),
 ('shipments', 4589.0),
 ('blueprint', 4589.0),
 ('photographs', 417.1818181818182),
 ('lowest', 1529.6666666666667),
 ('preserve', 218.52380952380952),
 ('upgraded', 4589.0)] Below is my invert function:pairs = []
    tokens = record[1].keys()
    for token in tokens:
        pairs.append((token,record[0]))
    return pairs

amazonInvPairsRDD = amazonWeightsRDD.flatMap(invert).cache()
googleInvPairsRDD = googleWeightsRDD.flatMap(invert).cache()
print amazonInvPairsRDD.take(3)

This outputs:


[('paperless', 'b000jz4hqo'), ('cromprofessional', 'b000jz4hqo'), ('yellow', 'b000jz4hqo')]
There are 16959809 Amazon inverted pairs and 36247336 Google inverted pairs1 test passed.
1 test failed. incorrect amazonInvPairsRDD.count()
1 test failed. incorrect googleInvPairsRDD.count()

Where is the problem ?
 Hi please help in figuring out the reason why there is a mismatch in the count of tokens

this is what i am doing in 1c

amazonRecToToken = amazonSmall.flatMap(lambda x: (tokenize(''.join(x[1])))).collect()googleRecToToken = googleSmall.flatMap(lambda x: (tokenize(''.join(x[1])))).collect()def countTokens(vendorRDD):    return len(vendorRDD)

the ''.join(x[1]) is convert the list into string

my output is showing "There are 27424 tokens in the combined datasets"

my simple tokenize
def simpleTokenize(string):    if string == ' ':        return []    else:        textlower = string.lower()        final=re.split(split_regex,textlower)        if '' in final:            final.remove('')        return final

my tokenize is

def tokenize(string):
    if type(string) != list:        wordlist= simpleTokenize(string)        else:        wordlist=string    for i in stopwords:        if i in wordlist:            wordlist.remove(i)    return wordlist


though all my test passed for simple tokenize and tokenize... i am unable to get 1C right please help me... I have a problem with 4d. Code seems to work fine, but I cannot pass the Tests.

This is my invert and operations code:
<CODE REDACTED> - Please do not post code as it violates the Honor Code (provide before and after snapshots of the data in the RDDs using rdd.take(2) instead to help understand the issue)

print 'There are %s Amazon inverted pairs and %s Google inverted pairs.' % (amazonInvPairsRDD.count(),
                                                                            googleInvPairsRDD.count())
These are the results I get:
There are 29792 Amazon inverted pairs and 68891 Google inverted pairs.

But 2 of 3 test do no works. I have this message:



1 test passed.
1 test failed. incorrect amazonInvPairsRDD.count()
1 test failed. incorrect googleInvPairsRDD.count()


I would appreciate any help, please!

 From quickly examining the code on the official PySpark page, I noticed the difference between the two:
foreach forces evaluation of the RDD by .collect()ing, while map doesn't.

Is this correct and are there any other practical differences?

Thank you,
Damir This lab is getting challenging :) I could not find any example in the course slides...how do I compute the dot product?
the common values are  5,2,3 and 20,1, 0....if I add those, we dont get 102? Thank you! 


{'baz': 5, 'foo': 2, 'bar': 3}
{'baz': 20, 'foo': 1, 'bar': 0}
 MemoryError occured when I run 5a. 
Each test before 5a successfully have been passed.

I'm confused.
Does anybody know how to fix it?

Thanks in advance.

Traceback:
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-112-7fb21d3ac1f5> in <module>()
     23                   .map(gs_value)
     24                   .cache())
---> 25 print 'There are %s true duplicates.' % trueDupSimsRDD.count()
     26 assert(trueDupSimsRDD.count() == 1300)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 312.0 failed 1 times, most recent failure: Lost task 1.0 in stage 312.0 (TID 827, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 253, in mergeValues
    d[k] = comb(d[k], v) if k in d else creator(v)
MemoryError

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
 Hello TAs and fellow friends.
I used 4 submissions already to submit lab 3 and every time I get the following message: that non of my functions are defined and that something is raising an exception.
I tried the copy paste trick for cell 1a to see if that at least passes, but no luck. Could you please please advise me what to do before I run out of submission rounds? The note book runs 100% and passes all tests and I have no extra print commands.

thanks a lot.
Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 1
    
    ^
SyntaxError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'simpleTokenize' is not defined

Removing stopwords (1b)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tokenize' is not defined

Tokenizing the small datasets (1c)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'totalTokens' is not defined

Amazon record with the most tokens (1d)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'biggestRecordAmazon' is not defined

Implement a TF function (2a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tf' is not defined

Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'corpusRDD' is not defined

Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueTokenCount' is not defined

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'rec_b000hkgj8k_weights' is not defined

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dp' is not defined

Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsSmallBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonFullRecToToken' is not defined

Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsFullCount' is not defined

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonNormsBroadcast' is not defined

Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'invert' is not defined

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'commonTokens' is not defined

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similaritiesFullRDD' is not defined

-- 0 cases passed (0.0%) --


Your submission token id is 1074783-33ae024b0b703f80b48702b1eccc0bd4:f76d3d4150266a546f5fc8da3b1c5b54:ip-172-31-0-241
Please include this submission token id when you need support for your code submission. Though the autograder has evaluated lab3 sucessfully, i have a question.

In section 4b and 4e for the variables idfsFull.count() and commonTokens.count() each took around 4 minutes.The rest of the code including the groupByKey(),collectAsMap(),collect() took around 4 minutes.How is it possible?

I used datetime module

Also removing the comments decreased the run time.Is it possible? All of my code and tests appear to work up until lab 3 4d, including the unit tests @2608.

I test my invert() function by adding the code
print invert(('foo',{'a':2,'b':3,'c':5,'d':7}))
print invert(('bar',{'e':11,'f':13,'g':17}))
immediately after the function definition is complete, which gives me
[('a', 'foo'), ('c', 'foo'), ('b', 'foo'), ('d', 'foo')]
[('e', 'bar'), ('g', 'bar'), ('f', 'bar')]
as output.

But after I define amazonInvPairsRDD and googleInvPairsRDD, when I try
print amazonInvPairsRDD.take(3)
print googleInvPairsRDD.take(3)
I get the very long error at the bottom of this post. (I put it at the bottom instead of here because it seems to screw up formatting for any text I put after it.)
The gist of the error is that in tf(), there's a TypeError: 'dict' object is not callable.

So, my tf() function, which worked fine every time until this case, is now somehow trying to call a dict as a function...?From this point onward, tf() gives the same error every time. If I go back to the cell in 2a where tf() is defined and tested for the first time, it now gives me the following error - even though everything worked perfectly the first time it was run:

---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)<ipython-input-47-8718d1cc4245> in <module>()     10     return dict((i,tokens.count(i)/j) for i in tokens)     11 ---> 12 print tf(tokenize(quickbrownfox)) # Should give { 'quick': 0.1666 ... }<ipython-input-47-8718d1cc4245> in tf(tokens)      8     """      9     j = float(len(tokens))---> 10     return dict((i,tokens.count(i)/j) for i in tokens)     11      12 print tf(tokenize(quickbrownfox)) # Should give { 'quick': 0.1666 ... }TypeError: 'dict' object is not callable

This condition persists until I restart my vm (e.g. "vagrant reload"). After restarting, everything works fine again... until I reach 4d again.Any pointers?Thanks!


---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-46-f782af8e13fd> in <module>()
     22                     .cache())
     23 
---> 24 print amazonInvPairsRDD.take(3)
     25 print googleInvPairsRDD.take(3)
     26 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 111.0 failed 1 times, most recent failure: Lost task 0.0 in stage 111.0 (TID 517, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-40-bcfc62671ec8>", line 12, in <lambda>
  File "<ipython-input-24-b07617cf2b68>", line 10, in tfidf
  File "<ipython-input-12-8718d1cc4245>", line 10, in tf
TypeError: 'dict' object is not callable

    at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
    at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
    at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
    at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
    at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
    at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
    at scala.Option.foreach(Option.scala:236)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) I passed all the cases in jupyter, but failed when submitting.Could someone help me to understand the reason of the failure?

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
NameError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
global name 'parseDatafileLine' is not defined

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'totalTokens' is not defined

Amazon record with the most tokens (1d)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'biggestRecordAmazon' is not defined

Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'corpusRDD' is not defined

Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueTokenCount' is not defined

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'rec_b000hkgj8k_weights' is not defined

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsSmallBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonFullRecToToken' is not defined

Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsFullCount' is not defined

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonNormsBroadcast' is not defined

Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonInvPairsRDD' is not defined

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'commonTokens' is not defined

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similaritiesFullRDD' is not defined

-- 4 cases passed (21.0%) --


Your submission token id is 1075025-4f27045a5935b9cf15f7a201c91a5c0d:fbbd5413750159142ebbe9c5e86c5b6e:ip-172-31-8-187
Please include this submission token id when you need support for your code submission.
 Hi,

sim = similaritiesBroadcast.....
gives this kind of output:
[('b000jz4hqo <a target="_blank">http://www.google.com/base/feeds/snippets/11448761432933644608',</a> 0.0)]
I am getting 0.0 at the end always, is it right ?

My line is: sims = similaritiesBroadcast.map(lambda x: (x[1]+" "+x[0],x[2]))

I appreciate any help I am thinking about it for a long period of time.

Thank you. Lab 3 - 1a , I can split the string into the tokens and remove punctuations using the given regex and convert to lower case. But can I get some hint on how to remove the empty string? I thought that the norm is : "
"norm that returns the square root of the dot product of a dictionary and itself"
So is the norm : nm = norm(testVec1) => 2*2 + 3*3 + 5*5 = 38

How can Test.assertTrue(abs(nm - 6.16441400297) < 0.0000001, 'incorrrect nm') pass?

Please help. I take this

s = dotprod(amazonWeightsBroadcast, googleWeightsBroadcast) value = s/(norm(amazonWeightsBroadcast)*norm(googleWeightsBroadcast))

but it seems wrong. Anyone suggests? I believe my TF is correct since by TF function passed.
{'autocad': 0.16666666666666666, 'autodesk': 0.08333333333333333, 'courseware': 0.16666666666666666, 'psg': 0.08333333333333333, '2007': 0.16666666666666666, 'customizing': 0.16666666666666666, 'interface': 0.16666666666666666}
My tdidf fails since for the amzon record I get tdiff asAmazon record "b000hkgj8k" has tokens and weights:set([('customizing', 0.8333333333333333), ('autodesk', 0.5833333333333333), ('interface', 4.666666666666666), ('courseware', 0.3333333333333333), ('2007', 6.666666666666666), ('autocad', 0.6666666666666666), ('psg', 0.08333333333333333)])
I calculated tdidf as v*idfs[key] where v is the value iterating tdf dictionary items

Please let me know where I errored.
Thanks. Hello. I pretty much understand (or I thought so till now) lambda functions, but I read through the link pasted in the description of the lab2 and have double thoughts about it now.

What is this 'pair' in the second line? And what does the expression pair[1] do to the pairs list? These code are given so hopefully not in violation of honor code:

recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1]

amazonRecToToken is original an RDD w/ [(amazonID), [tokenized description]]

but recb000hkgj8k turns out to be a list of ONLY the tokenized description.    Can someone explain how exactly the amazonID was stripped in the process?

I would guess it's in .collect()[0][1] but I'm not quite getting this so would appreciate a better explanation.   Tx! Hi Guys,I am stuck with regex. I have been able to:1. Remove Puncuations2. Make all the case to lower case3. Split4. I use replace remove the '.' This allows me to pass the 1st and 4th test.However the '.' also appears within a word and at the end of a sentence. Which causing test 3 to fail. Any hints will be highly appreciated. I am using map on function tokenize. When I use count method on amazonRecToToken, I get an error.
Obviously I am doing something wrong.
The map function using tokenize shoudl simply return the list of tokens? Can someone post the 5c plot for those of us who didn't manage to complete all the previous exercises? Hi - having all kinds of problems with 4f . In trying to debug I am finding that the commonTokens I have created does not generate the tokens in list format . I have had no luck putting it into that format
[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), '120')] My Lab 3 notebook executes fine with no extra print statements, etc.  Submitted to autograder and all tests seemed to have passed and the progress page shows me as earning 100%. However, autograder returns this following error which seems like it would be fatal. Anyway, just thought I'd bring it up.

Tokenize a String (1a)----------------------Traceback (most recent call last): File "", line 28, in  File "", line 4 class VectorAccumulatorParam(AccumulatorParam): ^IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTIONexpected an indented block
All tests passedRemoving stopwords (1b)-----------------------All tests passedTokenizing the small datasets (1c)----------------------------------All tests passedAmazon record with the most tokens (1d)---------------------------------------All tests passedImplement a TF function (2a)----------------------------All tests passedCreate a corpus (2b)--------------------All tests passedImplement an IDFs function (2c)-------------------------------All tests passedImplement a TF-IDF function (2f)--------------------------------All tests passedImplement the components of a cosineSimilarity function (3a)------------------------------------------------------------All tests passedImplement a cosineSimilarity function (3b)------------------------------------------All tests passedPerform Entity Resolution (3c)------------------------------All tests passedPerform Entity Resolution with Broadcast Variables (3d)-------------------------------------------------------All tests passedPerform a Gold Standard evaluation (3e)---------------------------------------All tests passedTokenize the full dataset (4a)------------------------------All tests passedCompute IDFs and TF-IDFs for the full datasets (4b)---------------------------------------------------All tests passedCompute Norms for the weights from the full datasets (4c)---------------------------------------------------------All tests passedCreate inverted indicies from the full datasets (4d)----------------------------------------------------All tests passedIdentify common tokens from the full dataset (4e)-------------------------------------------------All tests passedIdentify common tokens from the full dataset (4f)-------------------------------------------------All tests passed-- 19 cases passed (100.0%) --
Your submission token id is 1077461-d28e473e9f600b7528d932c0eefb9307:de6ef9a9d4316acac71d3b79fbfd2854:ip-172-31-8-112Please include this submission token id when you need support for your code submission. I have clicked on the link for the data and get sent to

https://code.google.com/p/metric-learning/
which, with my humble computer skills, has nothing!!  I have subverted from the link:


Use this command to anonymously check out the latest project source code:


# Non-members may check out a read-only working copy anonymously over HTTP. svn checkout http://metric-learning.googlecode.com/svn/trunk/ metric-learning-read-only

and gotten nothing looking like the data files for the lab.  Is there a more fruitful way to get the files:

The directory contains the following files:
Google.csv, the Google Products datasetAmazon.csv, the Amazon datasetGoogle_small.csv, 200 records sampled from the Google dataAmazon_small.csv, 200 records sampled from the Amazon dataAmazon_Google_perfectMapping.csv, the "gold standard" mappingstopwords.txt, a list of common English words
??




 Finally completed Lab3. Thank you for creating an interesting lab.
While the concepts were clear enough, most of my time was spent trying to understand what is the output expected at each stage.

I suggest input and output samples as in the pySpark documentation to clarify what is expected. Hi,

Can you give further explanation of the lab3's part 5? I really want to understand this. 
I am not able to understand what is going on here ... what is S and Value can anyone explain

s = <FILL IN> value = <FILL IN> key = (amazonRec, googleRec) return (key, value) amazonNorms = amazonWeightsRDD.map(lambda (x,y): (x,norm(y)))

I tried norm(y.value) as well. Any help is appreciated. Thanks Hi, my knowledge of english is limited, I dont know what is a "
Gold Standard evaluation
I see it on the 3e exercise , but dont understand wich is the main objective of this function.

thanks What are the axis labels for the plot? Hi please guide me how do I find line numbers in every test case. My solution for 1a is through and test case is passing in notebook but when I try to submit it to final grader 1a fails saying syntax error its very small code I dont see any syntax error why autograder is saying syntax error. Please guide. Thanks.

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 7
    .)
     ^
SyntaxError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
invalid syntax

All tests passed
Removing stopwords (1b) It doesn't seem to be interfering with the asserted correctness of my program but sure takes a long time. I've nothing to the template except to fill-in. I made a three character change to the regex. Is anyone else seeing this output?

 Below is a snapshot from the slide. Does the signPrefixes variable needs to be passed as an argument to the processSignCount function?

 I think it might be a bit misleading to many people that most of the commonTokens entries contain a single element, e.g:
 
(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), '120')
Documentation for the fastCosineSimilarity function only makes it even more confusing by claiming
 
Args:
   record: ((ID, URL), token)
Instead of 
Args:
   record: ((ID, URL), list of tokens)
So I think for many people (including myself) the inconsistency of the task description, commonTokens samples and the above-mentioned documentation makes it a bit confusing of how to approach the sum calculation.
 
The bottom-line: s has to be calculated by iterating over the list of tokens and multiplying accordingly, not taking a single token.
 
PS: while overall I really like the course, I think that lab3 was been created by cutting some corners during the preparation phase. I have experienced a lot of frustration not because I was lacking understanding or essential skills, but because it was not clear what was exactly expected. It is one thing if you have a master plan of how to implement something and an absolutely different thing if you follow someone else's plan, scattered over many pages of code. I am sure giving the lab to someone who is not a part of the team and who is seeing the material for the first time beforehand would have helped a lot.
 
I am really looking forward to the lab 4, but feel a bit scared of what's coming if terms of time investments because it seems to be quadratic so far in relation to the lab # , i.e O(lab # ^ 2) =)  
Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 650
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'simpleTokenize' is not defined

Removing stopwords (1b)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tokenize' is not defined

Tokenizing the small datasets (1c)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'totalTokens' is not defined

Amazon record with the most tokens (1d)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'biggestRecordAmazon' is not defined

Implement a TF function (2a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tf' is not defined

Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'corpusRDD' is not defined

Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueTokenCount' is not defined

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'rec_b000hkgj8k_weights' is not defined

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dp' is not defined

Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsSmallBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonFullRecToToken' is not defined

Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsFullCount' is not defined

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonNormsBroadcast' is not defined

Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'invert' is not defined

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'commonTokens' is not defined

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similaritiesFullRDD' is not defined

-- 0 cases passed (0.0%) --


Your submission token id is 1080804-23ec2a022c854ae2a82ea40d03235d9e:a654acf160fc78dd0502f899189034f1:ip-172-31-8-187
Please include this submission token id when you need support for your code submission.

I have passed all the tests but on ipynb but when i download i get txt file which on conversion to .py using save as shows above error.. please help..  LAB3 4f takes around 20 minutes on my "very small"  laptop
( Intel i3 - 2012 - 
  VM Virtual Box 2Gb memory ( more than 50% of physical memory)  - 1 processor   - host = Windows 81
)
 
Lab 3 4f is "the main reason why"  I 'm rejected by the autograder ( configured with a timeout I guess around 20 minutes  ). 
 

 
Questions to our instructor :
 
What is the configuration of the Cloud supporting the Mooc  ( number of nodes, memory available , etc ..., number of notebooks running simultaneously )I just  got a timeout error from the grader  after double of the elapsed time used on my little laptop. On my laptop total elapsed time is around 30 minutes. What should be the elapsed time on my laptop to have a chance to avoid timeout when grading Lab 3.Is the Mooc cluster currenly  overloaded ?What is the time frame best indicated to avoid timeout.

Is it a typical "overloaded cloud" story or a "success" story ;-)

Why are we not graded on the steps successfully processed ?  To get graded A on Lab3 , do I have to include a  filter to "reduce"  on a partial common Tokens ( or insert a non-fatal "syntax  error")  in 4f  to be graded from 1a to 4f (excluded) ?

I got the following message :
Timeout error happened during grading. Please review your code to be more efficient and submit the code again. Your submission token id is 1077954-06c411e3bcd5a08c3fb0fc4670d2791c:f7089c9836470accf2b42f468781db04:ip-172-31-8-188 Please include this submission token id when you need support for your code submission.
  I have spent almost a day for this 3- 1(c). Is it so difficult ? Or, I figured out in a wrong way ?This is my simpleTokenize and tokenize method.
 
quickbrownfox = 'A quick brown fox jumps over the lazy dog.'
split_regex = r'\W+'

def simpleTokenize(string):
    return sc.parallelize(re.split(split_regex,string)).map(lambda x:x.lower()).filter(lambda s:len(s)>0).collect()

print simpleTokenize(quickbrownfox)


-------------------
stopfile = os.path.join(baseDir, inputPath, STOPWORDS_PATH)
stopwords = set(sc.textFile(stopfile).collect())
print 'These are the stopwords: %s' % stopwords

def tokenize(string):
    return sc.parallelize(simpleTokenize(string)).filter(lambda s : s not in stopwords).collect()

print tokenize(quickbrownfox)
------------------------------------
The above code resolves all the tests for 1-a and 1-b. 1-c looks like this :
amazonRecToToken = amazonSmall.map(lambda (x,y):(x,tokenize(y)))
googleRecToToken = googleSmall.map(lambda (x,y):(x,tokenize(y)))

def countTokens(vendorRDD):
    return vendorRDD.map(lambda (key,value):len(value)).reduce(lambda x, y:x+y)

totalTokens = countTokens(amazonRecToToken) + countTokens(googleRecToToken)
print 'There are %s tokens in the combined datasets' % totalTokens
Which causes exception as below :
Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
Could you please make me understand, what's wrong with my code ? I am tired now. Please help and explain me the reason as well. Thank you so much in advance. For more about my tests and debugs please refer @2939 It is my opinion that there is a typo in the 2c directions and that the word "document" was intended to be "corpus". If there is not a typo, then the directions are self contradictory:
 
Create an RDD (*not a pair RDD*) containing the unique tokens from each document in the input `corpus`. For each document, you should only include a token once, *even if it appears multiple times in that document.*For each of the unique tokens, count how many times it appears in the document and then compute the IDF for that token: *N/n(t)*
 
If document was changed to corpus then the directions are inline with the earlier presentation of IDF and are internally consistent.

Edit after helpful feedback from other students--
The instruction could be restated as:
For each of the unique tokens, count the number of documents that contain the token. 
 
I solved 2c after assuming this was a typo and passed the local tests and submitted with 100%

#pin I have a one question regarding  4F

Consider the record from commonTokens RDD::

[(('b000fp0k0u', 'http://www.google.com/base/feeds/snippets/4036053921422681767'), ['access', 'without', 'never', 'easier', 'easy', 'software'])]


This will be the list of tokens in the record which will be passed to the function fastCosinesSimilarity
['access', 'without', 'never', 'easier', 'easy', 'software'])]

Following is my assumption. Please correct me if I am wrong?

1. For each record we will only get 1 list of common tokens.

2. So, basically I need to find cossimAB = dotProdAB / (normA * normB)

3. For dot product i.e.  a and b both refers to one common token list of tokens which is
['access', 'without', 'never', 'easier', 'easy', 'software'])]

4. If any one token is found in both the dictionaries then only I will take the dot product else not?

5. For the norm part the instructor has mentioned ::
   The only thing is you do not use the norm function anymore because you have pre-calculated all the norms and have those       available in a broadcast variable you should use. What is the way to extract the pre-calculated norms from braadcast variable?

Now my question are::

1.  Now what will happen if the token is not available in both the dictonaries? Then how to find dot product?

2. I couldn't get this part?
and computes the sum for each of the tokens in the token list of the products of the Amazon weight for the token times the Google weight for the token.

does it mean::

1. For each token in the list lookup  its weight in amazon dictionary and then in the google dictionary. If token is found in both then multiply their weights (weight in amazon * weight in google)
2. If the token is found only in one dictionary then just keep the weight as is for sum at the end.
3. In the end sum up all the weight of tokens.

Thanks Im prettty sure i didnt use the command "vagrant destroy", i tried "vagrant up" today just as always and it started downloading the vm all over again, then this error appeared to me: 'A VirtualBox machine with the name 'sparkvm' already exists.Please use another name or delete the machine with the existingname, and try again.' how to delete or (better) use the already existing aprkvm? I have tried 4e with join followed by both reduceByKey and combineByKey and this error shows up in both cases:RuntimeError: maximum recursion depth exceeded while pickling an objectMy previous step (4d) works fine and I didn't have this problem last night. What does this message mean? How can I overcome it?
Thanks. For some reason I get numbers in the range 50.7...., 82.27... for the IDFs of unique tokens.  I do pass tests 1 and 2 but not 3 because the scores are so high.

Any tips greatly appreciated.  Thanks

EDIT:  I was able to solve the problem.  My N was not calculated correctly. Like many, I initially used an sc context on 2A. That went fine until it chokes out 3C.

So, no I'm revisiting 2A, trying to accomplish it in pure python, as many others are suggesting is the only way forward.

I am trying to sum by key using a dictionary comprehension (which I am new to), but regardless of whether I feed it an INT or a FLOAT, it keeps returning "not iterable":

# TODO: Replace  with appropriate code def tf(tokens): """ Compute TF Args:      tokens (list of str): input list of tokens from tokenize Returns:      dictionary: a dictionary of tokens to its TF values """      tupled = dict(map((lambda x: (x,1.0)),tokens))           tot = dict((key, sum(vals)) for key, vals in tupled.items())      return tot print tf(tokenize(quickbrownfox)) # Should give { 'quick': 0.1666 ... }  
I am aware that I haven't divided by the length of tokens yet...I'll put that back in after succeed in summing by key.

Error is <big long blah blah blah>...TypeError: 'float' object is not iterableAdvice, please? There is an error in the documentation for the norm function in 3a that I believe is due to copy-and-paste.

        def norm(a):            """ Compute square root of the dot product            Args:            a (dictionary): a dictionary of record to value            Returns:            norm: a dictionary of tokens to its TF values            """

The part under "Returns" should be a single floating point value, not a dictionary of tokens to TF values, which doesn't make any sense. It turns out the string "a dictionary of tokens to its TF values" happens elsewhere in a totally different function, so this is probably just a case of copy-and-paste and and then forgetting to change part of it (although the writer did change the name of the return value, confusingly).

This documentation is pretty confusing, obviously, so it should probably be changed.

#pin I'm not sure of what's asked exactly.

For the dotprod function, that seems alright, I multiply the value of same keys (2*1 + 3*0 +5*20 = 102) and I return 102, that's ok.

But the norm function... If I see well the test result, I would have to find a dotprod for one dictionary only, that equals 38.
The dict is : testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }

Even the definition of the function is not clear to me: 

Define a function norm that returns the square root of the dot product of a dictionary and itself

Is anybody able to explain what's expected, and an example of the value expected to be returned.

I don't understand the concept of dotProd on one dictionary only.

Thanks for your time and help. Cosine Error

w1 = tfidf(tokenize(string1),idfsDictionary)  and so on 
my dot product and norm functions all tested correctly. Any Hints?

AttributeError                            Traceback (most recent call last)
<ipython-input-351-80ba49727d8a> in <module>()
     15 cossimAdobe = cosineSimilarity('Adobe Photoshop',
     16                                'Adobe Illustrator',
---> 17                                idfsSmallWeights)
     18 
     19 print cossimAdobe

<ipython-input-351-80ba49727d8a> in cosineSimilarity(string1, string2, idfsDictionary)
     11     w1 = tfidf(tokenize(string1),idfsDictionary)
     12     w2 = tfidf(tokenize(string2),idfsDictionary)
---> 13     return cossim(w1, w2)
     14 
     15 cossimAdobe = cosineSimilarity('Adobe Photoshop',

<ipython-input-349-354ce2229ff8> in cossim(a, b)
     36                 then by the norm of the second dictionary
     37     """
---> 38     return dotprod(a,b)/(norm(a)*norm(b))
     39 
     40 testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }

<ipython-input-349-354ce2229ff8> in dotprod(a, b)
     11     """
     12     i = 0
---> 13     results = [(i + v*b[k]) for (k,v) in a.items()]
     14     #results = map(int, results)
     15     return sum(results)

AttributeError: 'set' object has no attribute 'items' < Don't post solution code, it is a violation of honor code >
 My commontoken is
          map(swap).reduceByKey(lambda x,y : x + y)
The result of my
          print commonTokens.take(1)

is

[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), '120')]
s and value are
s =  dotprod(amazonWeightsBroadcast.value, googleWeightsBroadcast.value)    value = s/(norm(amazonWeightsBroadcast.value)*norm(googleWeightsBroadcast.value))

I am getting

TypeError: unsupported operand type(s) for *: 'dict' and 'float'

Help me to solve this error.


 Hi All,
 I get smallest IDF valie as 0.0025 and token value 'aided'

Here is my pseudocode:
uniqueTokens done as flatmap...(distinct)
tokenCountPairTuple doen as map to give tuples
tokenSumPairTuple reduceByKey( lambda a,b: a+b)

return tokenSumPairTuple.map( lambda s: (s[0], float(s[1]) / n) )

My lowest sum-pair value is 1. and 1/400 = 0.0025
So my lowest IDF value is 0.0025

Am I doing anything wrong here?

Please help.
Thanks
 Anybody can help me on this part. i am stuck on it more than hour but still not getting any result. Any directions please Guys, I must confess, SparkR needs an operator like %>% or with function urgently! I'm assuming that for instance we have one record from RDD:

[('id1', {'token a': 2.4051362683438153, 'token b': 56.65432098765432, 'token c': 254.94444444444443})]

we want to convert to:
[('token a': 2.4051362683438153), 'id1'], 
[('token b': 56.65432098765432), 'id1'], 
[('token c': 254.94444444444443), 'id1']

syntax wise: 
record[0] should be 'id1'
record[1][0] =  ('token a': 2.4051362683438153)
record[1][1] =  ('token b': 56.65432098765432)
record[1][2] =  ('token c': 254.94444444444443)

Am I right here?    Doesn't seem to work though....   Tx in advance for any pointer... The course description says:
"The lab is due Jun 26, 2015 at 00:00 UTC. There is a three day grace period for late submissions until Jun 29, 2015 at 00:00 UTC. Submissions after that time will lose 20 points."

Am I correct to assume that any submissions between Jun 26, 2015 at 00:00 UTC and Jun 29, 2015 at 00:00 UTC will be losing 20 points? Hi All,
 The word document in 2c can be interpreted in following possible ways
1. corpus is a union of amazon small and google small, so one document

2. corpus has 400 ids - 200 from amazon small and 200 google small - so 400 documents. That means each id is a document.

3. corpus is a union of two csv files - so 2 documents.

Please let me know which of the above is correct.

I already spent 6 hours on this alone. I wish there was more clarity - at least for me.

Thanks hi, 

i am using the below code.

return vendorRDD.takeOrdered(1,lambda x:(x[0],x[1]))
biggestRecordAmazon = findBiggestRecord(amazonRecToToken)print 'The Amazon record with ID "%s" has the most tokens (%s)' % (biggestRecordAmazon[0][0], len(biggestRecordAmazon[0][1])

Can any body suggest how to solve it as it is throwing error

File "<ipython-input-69-d798ba2a4c7a>", line 13
    len(biggestRecordAmazon[0][1])
                                  ^
SyntaxError: invalid syntax

  Dear support
I’m getting an error while submitting the Lab2. All tests are ok (100%) and I get no time-out errors (which seems to be the most common error on this lab) .
What should I do ? 
 
This is what I get from the autograder:



Data cleaning (1c)
------------------
Traceback (most recent call last):
File "", line 28, in
NameError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
name 'hostMoreThan10' is not defined
 
All tests passed
Top ten error endpoints (3a)
----------------------------
All tests passed
Number of unique hosts (3b)
---------------------------
All tests passed
Number of unique daily hosts (3c)
---------------------------------
All tests passed
Visualizing unique daily hosts (3d)
-----------------------------------
All tests passed
Average number of daily requests per hosts (3e)
-----------------------------------------------
All tests passed
Average Daily Requests per Unique Host (3f)
-------------------------------------------
All tests passed
Counting 404 (4a)
-----------------
All tests passed
Listing 404 records (4b)
------------------------
All tests passed
Top twenty 404 URLs (4c)
------------------------
All tests passed
Top twenty-five 404 response code hosts (4d)
--------------------------------------------
All tests passed
404 response codes per day (4e)
-------------------------------
All tests passed
Visualizing the 404 Response Codes by Day (4f)
----------------------------------------------
All tests passed
Five dates for 404 requests (4g)
--------------------------------
All tests passed
Hourly 404 response codes (4h)
------------------------------
All tests passed
Visualizing the 404 Response Codes by Hour (4i)
-----------------------------------------------
All tests passed
-- 16 cases passed (100.0%) --
 

 I got the following message while submitting lab3 to auto grader. I think I have optimized the code to be as efficient as possible. I am able to run the code locally. Some cells like 4f take over 5 min to complete, but others complete within a minute or two. I don't know what to do now.

Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 1084744-2fdac27b1e9da67d203b66d7bb679264:6fdc27643eaa506cc2b4c1b89aa77e68:ip-172-31-9-131
include this submission token id when you need support for your code submission. Hi, can anyone tell me what's wrong with my 3c, i have all the previous tests passed but i keep having this error :
I don't think i have a problem with the code(don't want to post it). My question is: Does the way we code our functions affect the outcome even if it passes the tests ? 
File "<ipython-input-49-d4198ceae546>", line 15, in computeSimilarityBroadcast
  File "<ipython-input-42-2c9d94075658>", line 11, in cosineSimilarity
  File "<ipython-input-38-315858681bf4>", line 12, in tfidf
KeyError: 'google'
Thanks,
PS: i have my (tf) written only in python. All my tests passed and I was given 100% credit on the progress page.  However, I did get this strange error.

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
All tests passed
Implement an IDFs function (2c)
-------------------------------
All tests passed
Implement a TF-IDF function (2f)
--------------------------------
All tests passed
Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
All tests passed
Perform Entity Resolution (3c)
------------------------------
All tests passed
Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
All tests passed
Perform a Gold Standard evaluation (3e)
---------------------------------------
All tests passed
Tokenize the full dataset (4a)
------------------------------
All tests passed
Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
All tests passed
Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
All tests passed
Create inverted indicies from the full datasets (4d)
----------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4e)
-------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4f)
-------------------------------------------------
All tests passed
-- 19 cases passed (100.0%) --


Your submission token id is 1087751-a7eafffdba3b8e1bc269c34610ceb6ab:66337455af515fa6bfece9b2a57bcaa7:ip-172-31-8-113
Please include this submission token id when you need support for your code submission. If there are 111387 Amazon inverted pairs and 77678 Google inverted pairs, how can there be 2441100 common tokens?

 Dear Professors,

Lab 4 is due on July 3rd. 
Then, three days grace period falls inside July 4th and July 5th.
Those of us in the USA,
on July 4th, we would like to have some fun,
and on July 5th, we would like to have some hangover.

Would it be possible to extend the grace period until July 7th?
 commonTokens.take(1) or record:[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120'])]amazonRec:b00005lzlygoogleRec:http://www.google.com/base/feeds/snippets/18376072611700638452tokens:['120']s:175.930835422Then the error below when trying to calculate the norm. I have been at this for hours and can't figure out what is wrong. I am trying to calculate the norm with elements like amazonNormsBroadcast.value[amazonRec][tokens].Please, anything to help me get over this would be greatly appreciated.---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)<ipython-input-82-de9c6eb143c9> in <module>()     27 inx=[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120'])]     28 iny= [(('b000bnb72g', 'http://www.google.com/base/feeds/snippets/6959766318020460134'), ['software'])]---> 29 fastCosineSimilarity(inx)     30 fastCosineSimilarity(iny)     31 #fastCosineSimilarity(commonTokens.take(1))<ipython-input-82-de9c6eb143c9> in fastCosineSimilarity(record)     18     s = sum([ (amazonWeightsBroadcast.value[amazonRec][token] * googleWeightsBroadcast.value[googleRec][token]) for token in tokens ])     19     print s---> 20     value = (s/((amazonNormsBroadcast.value[amazonRec][tokens])))/(googleNormsBroadcast.value[googleRec][tokens])     21     print value     22     key = (amazonRec, googleRec)TypeError: unhashable type: 'list'Thanks. In Part 0 at [4] a typical googleSmall line is thus:
google: <a target="_blank" href="http://www.google.com/base/feeds/snippets/11448761432933644608:">http://www.google.com/base/feeds/snippets/11448761432933644608:</a> spanish vocabulary builder "expand your vocabulary! contains fun lessons that both teach and entertain you'll quickly find yourself mastering new terms. includes games and more!" 
while one for amazonSmall is thus:
amazon: b00004tkvy: noah's ark activity center (jewel case ages 3-8)  "victory multimedia"
All tests for Parts 0 and Part 1a & b pass, I have in no way modified the regex expression, and the tokenize function seems to work well with diverse texts -- I just use nested list comprehensions (novel texts tested in Python, not on the notebook).

But I have not been able to get 1c to work, despite much effort (and many hints from the discussion board). 

When I prepare the vendorRDDs with flatMap, and insert above the function
print amazonRecToToken.take(5)
print googleRecToToken.take(5)
I get
['b000jz4hqo', ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund'], 'b0006zf55o', ['ca', 'international', 'arcserve', 'lap', 'desktop', 'oem', '30pk', 'oem', 'arcserve', 'backup', 'v11', '1', 'win', '30u', 'laptops', 'desktops', 'computer', 'associates'], 'b00004tkvy']
['http://www.google.com/base/feeds/snippets/11448761432933644608', ['spanish', 'vocabulary', 'builder', 'expand', 'vocabulary', 'contains', 'fun', 'lessons', 'teach', 'entertain', 'll', 'quickly', 'find', 'mastering', 'new', 'terms', 'includes', 'games'], 'http://www.google.com/base/feeds/snippets/8175198959985911471', ['topics', 'presents', 'museums', 'world', '5', 'cd', 'rom', 'set', 'step', 'behind', 'velvet', 'rope', 'examine', 'treasured', 'collections', 'antiquities', 'art', 'inventions', 'includes', 'following', 'louvre', 'virtual', 'visit', '25', 'rooms', 'full', 'screen', 'interactive', 'video', 'detailed', 'map', 'louvre'], 'http://www.google.com/base/feeds/snippets/18445827127704822533']
With a simple map function
[('b000jz4hqo', ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund']), ('b0006zf55o', ['ca', 'international', 'arcserve', 'lap', 'desktop', 'oem', '30pk', 'oem', 'arcserve', 'backup', 'v11', '1', 'win', '30u', 'laptops', 'desktops', 'computer', 'associates']), ('b00004tkvy', ['noah', 'ark', 'activity', 'center', 'jewel', 'case', 'ages', '3', '8', 'victory', 'multimedia']), ('b000g80lqo', ['peachtree', 'sage', 'premium', 'accounting', 'nonprofits', '2007', 'peachtree', 'premium', 'accounting', 'nonprofits', '2007', 'affordable', 'easy', 'use', 'accounting', 'solution', 'provides', 'donor', 'grantor', 'management', 're', 'like', 'nonprofit', 'organizations', 're', 'constantly', 'striving', 'maximize', 'every', 'dollar', 'annual', 'operating', 'budget', 'financial', 'reporting', 'programs', 'funds', 'advanced', 'operational', 'reporting', 'rock', 'solid', 'core', 'accounting', 'features', 'made', 'peachtree', 'choice', 'hundreds', 'thousands', 'small', 'businesses', 'result', 'accounting', 'solution', 'tailor', 'made', 'challenges', 'operating', 'nonprofit', 'organization', 'keep', 'audit', 'trail', 'record', 'report', 'changes', 'made', 'transactions', 'improve', 'data', 'integrity', 'prior', 'period', 'locking', 'archive', 'organization', 'data', 'snap', 'shots', 'data', 'closed', 'year', 'set', 'individual', 'user', 'profiles', 'password', 'protection', 'peachtree', 'restore', 'wizard', 'restores', 'backed', 'data', 'files', 'plus', 'web', 'transactions', 'customized', 'forms', 'includes', 'standard', 'accounting', 'features', 'general', 'ledger', 'accounts', 'receivable', 'accounts', 'payable', 'payroll', 'solutions', 'time', 'billing', 'job', 'costing', 'fixed', 'assets', 'analysis', 'reporting', 'customization', 'easily', 'convert', 'quickbooks', 'sage', 'software']), ('b0006se5bq', ['singing', 'coach', 'unlimited', 'singing', 'coach', 'unlimited', 'electronic', 'learning', 'products', 'win', 'nt', '2000', 'xp', 'carry', 'tune', 'technologies'])]
[('http://www.google.com/base/feeds/snippets/11448761432933644608', ['spanish', 'vocabulary', 'builder', 'expand', 'vocabulary', 'contains', 'fun', 'lessons', 'teach', 'entertain', 'll', 'quickly', 'find', 'mastering', 'new', 'terms', 'includes', 'games']), ('http://www.google.com/base/feeds/snippets/8175198959985911471', ['topics', 'presents', 'museums', 'world', '5', 'cd', 'rom', 'set', 'step', 'behind', 'velvet', 'rope', 'examine', 'treasured', 'collections', 'antiquities', 'art', 'inventions', 'includes', 'following', 'louvre', 'virtual', 'visit', '25', 'rooms', 'full', 'screen', 'interactive', 'video', 'detailed', 'map', 'louvre']), ('http://www.google.com/base/feeds/snippets/18445827127704822533', ['sierrahome', 'hse', 'hallmark', 'card', 'studio', 'special', 'edition', 'win', '98', '2000', 'xp', 'hallmark', 'card', 'studio', 'special', 'edition', 'win', '98', '2000', 'xp', 'sierrahome']), ('http://www.google.com/base/feeds/snippets/18274317756231697680', ['adobe', 'cs3', 'production', 'premium', 'academic', 'system', 'requirements', 'multicore', 'intel', 'processor', 'adobe', 'photoshop', 'extended', 'illustrator', 'flash', 'professional', 'effects', 'professional', 'universal', 'binary', 'also', 'work', 'powerpc', 'g4', 'g5', 'processor', 'adobe', 'onlocation', 'windows']), ('http://www.google.com/base/feeds/snippets/18409551702230917208', ['equisys', 'premium', 'support', 'zetafax', '2007', 'technical', 'support', '1', 'year', 'equisys', 'premium', 'support', 'zetafax', '2007', 'upgrade', 'license', '10', 'users', 'technical', 'support', 'phone', 'consulting', '1', 'year', '2', 'h'])]
Then a statement
There are PartitionerAwareUnionRDD[140] at union at NativeMethodAccessorImpl.java:-2 tokens in the combined datasets
or
There are PartitionerAwareUnionRDD[153] at union at NativeMethodAccessorImpl.java:-2 tokens in the combined datasets

Am I having problems because of the odd "ids" from the google dataset?

Thanks for any advice.

***

Sorry.  The magic of posting a question made the answer clear.









 Hi,

I am getting 22523 tokens in the output while the right answer is 22520.

I am not able to debug what's wrong

There are 22523 tokens in the combined datasets

Thanks in advance Would you please make available the Lab4 and lecture materials so that we can start? Hello everybody,

I would like to know if  it is possible flatten lists in a big list with spark . I try some ways but they don't work. 

I have the RDD ( different lists)

[['01', 'ssc', '6993', 'sonicwall', 'client', 'server', 'anti', 'virus', 'suite', 'subscription', 'license', '3', 'years', '50', 'sonicwall', '01', 'ssc', '6993', 'usually', 'ships', '24', 'hours', 'sonicwall', 'client', 'server', 'anti', 'virus', 'suite', 'leverages', 'award', 'winning', 'mcafee', 'netshield', 'groupshield', 'applications', 'networks', 'windows', 'based', 'file', 'print', 'exchange', 'servers'], ['10000', 'drink', 'recipe', 'snap', 'trying', 'find', 'perfect', 'dinner', 'drink', 'sure', 'mix', 'liquor', 'already', 'snap', '10000', 'drink', 'recipes', 'solution', 'beverage', 'needs', 'cocktails', 'bar', 'drinks', 'find', 'practically', 'recipe']]

I don't know what transformation I can use

Could somebody help me?

Thanks in advance

Carlota Vina






 Hello!

I think I did ok in the first two steps of the exercise, making a cartesian of googleSmall and amazonSmall in the first step, and defining well the computeSimilarity function (testing on the pair tested for asserting, it finds 0.000303171940451 as expected).

But for the variable similarities I tried this:

similarities = (crossSmall
                .map(computeSimilarity)
                .cache())

and it returned me an awful error.
However what I wanted is passing the computeSimilarty function to all the crossSmall RDD of the form

[ ((Google URL1, Google String1), (Amazon ID1, Amazon String1)), ((Google URL1, Google String1), (Amazon ID2, Amazon String2)), ((Google URL2, Google String2), (Amazon ID1, Amazon String1)), ... ]
allowing me to return an RDD like this

[ ((Google URL1, Amazon ID1, 0.0000335545), (Google URL2, Amazon ID2, 0.0000025444)), ...]

Any help?

Thanks I am getting this error in an python exception

AttributeError: 'Broadcast' object has no attribute 'get'

for idfsFullWeights i used collectAsMap() to create the dict and use sc.broadcast(idfsFullWeights) 
and try to print amazonFullRecToToken i see the error above mentioned .. any hints? Hi,

My idf values calculated are out by 0.25 and I'm not sure if the creation of uniqueTokens is correct as there is a lot of discussion here about document versus corpus. I have

  < Don't Post Solutions >
yielding very simple rdd creations...almost too simple:
  < Don't Post Solutions >
Can anyone point me in the right direction?

Kind regards,
Leanne Plz help:When I applied the following solution then it is displaying - "TypeError: 'PipelinedRDD' object is not iterable".What else can I do to solve this?<REDACTED> Can anyone explain how to approach this amazingly poorly worded problem? May some one tell what is wrong with this code 

# TODO: Replace <FILL IN> with appropriate codeamazonWeightsBroadcast = <REDACTED> I cannot for the life of me figure out how to get the averages to work correctly. I've tried pulling the values out and passing them through a sum function, but I still get a python error. I can't take any values from the RDD post-join (it refuses to print them) even though I still get the correct count, nor can I take out values from something like:
trueDupsRDD.map(lambda x:x[1]) which is what fails to pass through the sum function.

Help would be appreciated because this is very irritating, as pySpark is terrible for debugging, which leads me to believe that it is very immature in terms of technology.

EDIT: Nevermind, figured it out myself. Had to apply a map function to the join in order to find the useless extra 'gold' field and remove it. --- Local run passed all. 
But the autograder shows:

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
Py4JJavaError: 

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 923, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 712, in collect
    with SCCallSiteSync(self.context) as css:
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/traceback_utils.py", line 72, in __enter__
    self._context._jsc.setCallSite(self._call_site)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 536, in __call__
    answer = self.gateway_client.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 362, in send_command
    connection = self._get_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 318, in _get_connection
    connection = self._create_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 325, in _create_connection
    connection.start()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 432, in start
    raise Py4JNetworkError(msg)
Py4JNetworkError: An error occurred while trying to connect to the Java server

Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1174, in takeOrdered
    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 712, in collect
    with SCCallSiteSync(self.context) as css:
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/traceback_utils.py", line 72, in __enter__
    self._context._jsc.setCallSite(self._call_site)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 536, in __call__
    answer = self.gateway_client.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 362, in send_command
    connection = self._get_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 318, in _get_connection
    connection = self._create_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 325, in _create_connection
    connection.start()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 432, in start
    raise Py4JNetworkError(msg)
Py4JNetworkError: An error occurred while trying to connect to the Java server

Implement a TF-IDF function (2f)
--------------------------------
All tests passed
Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
All tests passed
Perform Entity Resolution (3c)
------------------------------
All tests passed
ERROR      | pid 14 | java_gateway.py, line 431: An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
ERROR      | pid 14 | java_gateway.py, line 431: An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
ERROR      | pid 14 | java_gateway.py, line 431: An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
All tests passed
Perform a Gold Standard evaluation (3e)
---------------------------------------
All tests passed
Tokenize the full dataset (4a)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 923, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 712, in collect
    with SCCallSiteSync(self.context) as css:
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/traceback_utils.py", line 72, in __enter__
    self._context._jsc.setCallSite(self._call_site)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 536, in __call__
    answer = self.gateway_client.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 362, in send_command
    connection = self._get_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 318, in _get_connection
    connection = self._create_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 325, in _create_connection
    connection.start()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 432, in start
    raise Py4JNetworkError(msg)
Py4JNetworkError: An error occurred while trying to connect to the Java server

Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 923, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 712, in collect
    with SCCallSiteSync(self.context) as css:
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/traceback_utils.py", line 72, in __enter__
    self._context._jsc.setCallSite(self._call_site)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 536, in __call__
    answer = self.gateway_client.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 362, in send_command
    connection = self._get_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 318, in _get_connection
    connection = self._create_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 325, in _create_connection
    connection.start()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 432, in start
    raise Py4JNetworkError(msg)
Py4JNetworkError: An error occurred while trying to connect to the Java server

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
All tests passed
Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 923, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
ERROR      | pid 14 | java_gateway.py, line 431: An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
ERROR      | pid 14 | java_gateway.py, line 431: An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 712, in collect
    with SCCallSiteSync(self.context) as css:
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/traceback_utils.py", line 72, in __enter__
    self._context._jsc.setCallSite(self._call_site)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 536, in __call__
    answer = self.gateway_client.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 362, in send_command
    connection = self._get_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 318, in _get_connection
    connection = self._create_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 325, in _create_connection
    connection.start()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 432, in start
    raise Py4JNetworkError(msg)
Py4JNetworkError: An error occurred while trying to connect to the Java server

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 923, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 712, in collect
    with SCCallSiteSync(self.context) as css:
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/traceback_utils.py", line 72, in __enter__
    self._context._jsc.setCallSite(self._call_site)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 536, in __call__
    answer = self.gateway_client.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 362, in send_command
    connection = self._get_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 318, in _get_connection
    connection = self._create_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 325, in _create_connection
    connection.start()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 432, in start
    raise Py4JNetworkError(msg)
Py4JNetworkError: An error occurred while trying to connect to the Java server

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 712, in collect
    with SCCallSiteSync(self.context) as css:
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/traceback_utils.py", line 72, in __enter__
    self._context._jsc.setCallSite(self._call_site)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 536, in __call__
    answer = self.gateway_client.send_command(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 362, in send_command
    connection = self._get_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 318, in _get_connection
    connection = self._create_connection()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 325, in _create_connection
    connection.start()
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 432, in start
    raise Py4JNetworkError(msg)
Py4JNetworkError: An error occurred while trying to connect to the Java server

-- 12 cases passed (63.0%) --


Your submission token id is 1094847-1ffcea508df8c50a6c2d5f5c1476687c:281bdd21495bf4fa3668d016d9f03bc7:ip-172-31-15-207
Please include this submission token id when you need support for your code submission. Struggling to make the averaging function for trueDups and nonDups. 

(nonDupsRDD.map(lambda x:x[1]).reduce(lambda x,y:x+y))/float(nonDupsCount)

is throwing errors that / cannot be between tuple and float. how is the first part of the expression a tuple when i have reduced it?  Hi I am having following partial code 

amazonNorms = apply map on rdd using (lambda (k,v) : (k,norm(v)))now using above norms in broadcast sc.broadcast(amazonNorms)

I am getting the following exception

---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-160-dd69bf9659e9> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
      2 amazonNorms = amazonWeightsRDD.map(lambda (k,v) : (k,norm(v)))
----> 3 amazonNormsBroadcast = sc.broadcast(amazonNorms)
      4 googleNorms = googleWeightsRDD.map(lambda (k,v) : (k,norm(v)))
      5 googleNormsBroadcast = sc.broadcast(amazonNorms)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in broadcast(self, value)
    642         be sent to each cluster only once.
    643         """
--> 644         return Broadcast(self, value, self._pickled_broadcast_vars)
    645 
    646     def accumulator(self, value, accum_param=None):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/broadcast.py in __init__(self, sc, value, pickle_registry, path)
     63         if sc is not None:
     64             f = NamedTemporaryFile(delete=False, dir=sc._temp_dir)
---> 65             self._path = self.dump(value, f)
     66             self._jbroadcast = sc._jvm.PythonRDD.readBroadcastFromFile(sc._jsc, self._path)
     67             self._pickle_registry = pickle_registry

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/broadcast.py in dump(self, value, f)
     80         else:
     81             f.write('P')
---> 82             cPickle.dump(value, f, 2)
     83         f.close()
     84         return f.name

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in __getnewargs__(self)
    169         # This method is called when attempting to pickle an RDD, which is always an error:
    170         raise Exception(
--> 171             "It appears that you are attempting to broadcast an RDD or reference an RDD from an "
    172             "action or transformation. RDD transformations and actions can only be invoked by the "
    173             "driver, not inside of other transformations; for example, "

Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063. Working for hours on debugging this weird error on lab 3, 3(c) exercise,

Help of pairs was helpful since some of them were coding the same way, with no errors.

Please help.

The exception:

Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
<tt></tt>

My code:

# TODO: Replace <FILL IN> with appropriate code
crossSmall = <REDACTED>




 The last line of code in the cells does not show and I cannot also scroll up to see. How to enlarge the cell size so we can see the full code? Thanks I'm thrilled to say that the autochecker has given me a 100 for the assignment. But I still have to go back and look at the read/run only section 5 to learn the material for the quiz questions. I am not able to run the code, for section 5 however. My computer runs out of memory even before completing 5a. I have no print statements, collect statements, groupByKeys, etc. Suggestions?
 I am completely stumped on Lab 3, 4e and can't get past it.  Therefore, I cannot complete any of the remaining parts of lab 3.
Will I be able to continue with Lab 4 next week if I can't finish Lab 3? Hello,
İ stuck with this error. My brain hurts.

<ipython-input-148-c96be971f764> in <module>()
     36             .collect()[0][2])
     37 
---> 38 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     39 print 'Requested similarity is %s.' % similarityAmazonGoogle
     40 

<ipython-input-148-c96be971f764> in similar(amazonID, googleURL)
     33     """
     34     return (similarities
---> 35             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     36             .collect()[0][2])
     37 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 355.0 failed 1 times, most recent failure: Lost task 1.0 in stage 355.0 (TID 978, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-148-c96be971f764>", line 23, in <lambda>
  File "<ipython-input-148-c96be971f764>", line 15, in computeSimilarity
AttributeError: 'tuple' object has no attribute 'map'
 
Consider this sample data for AmazonBrodcast variable:::

('b00005lzly': {'coach': 20.702255639097743, 'text': 0.26338747632439874, 'global': 0.37916219119226635, 'manager': 0.3594141604010025, 'guides': 1.3270676691729322, 'managerandquot': 34.503759398496236, 'writing': 0.9719368844646827, 'better': 0.228501717870836, 'platform': 0.4059265811587793, 'coaching': 11.50125313283208, 'samples': 1.189784806844698, 'employee': 12.177797434763379, 'get': 0.09611075041363855, 'assistant': 3.83375104427736, '120': 5.75062656641604})

Record is :: 
Record -- [(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120'])]Token = 120
AmazonRecID b00005lzly
GoogleRecID -- http://www.google.com/base/feeds/snippets/18376072611700638452Now what is the way I can extract the value of token 120 which is 5.75062656641604  ('120': 5.75062656641604)from the Broadcast variable amazonWeightsBroadcastI am using like this:: amazonWeightsBroadcast.value[googleRec][token]but its not working. I have also tried other combinations but all in vain :)This last question has taken the life out of me :)  I am very stuck on what I am supposed to do with the function: norm(a):
Can anybody please explain to me what we are supposed to do?  I am not looking for help with the Python code, I am just looking for a sequence of steps of what should be done.  I'll worry about Python as soon as I understand what the function is supposed to do. my following code is giving error. i am trying it from 2 hrs. plz help
<REDACTED>

I see in (x,y) 'y' has multiple tupple in it.
print amazonFullRecToToken.map(lambda (x,y): y).take(3) gives below result.
Please advise.

[['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund'], ['ca', 'international', 'arcserve', 'lap', 'desktop', 'oem', '30pk', 'oem', 'arcserve', 'backup', 'v11', '1', 'win', '30u', 'laptops', 'desktops', 'computer', 'associates'], ['noah', 'ark', 'activity', 'center', 'jewel', 'case', 'ages', '3', '8', 'victory', 'multimedia']] I have following and assume my swap method is fine:

commonTokens = (amazonInvPairsRDD.join(googleInvPairsRDD).reduceByKey(swap).cache())
print 'Found %d common tokens' % commonTokens.count()

Until this point I do not have problems running all the lab3 tests they run in reasonable time on my machine. Also I am using 2 GB memory in VM. I am getting below error. Is it related to my solution or I am getting some kind of memory issues. Only thing I did not shut down my VM for a week or so.

Thanks in advance for help

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-256-48f441e7534e> in <module>()
     13 commonTokens = (amazonInvPairsRDD.join(googleInvPairsRDD).reduceByKey(swap).cache())
     14 
---> 15 print 'Found %d common tokens' % commonTokens.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 483.0 failed 1 times, most recent failure: Lost task 0.0 in stage 483.0 (TID 2179, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 253, in mergeValues
    d[k] = comb(d[k], v) if k in d else creator(v)
TypeError: swap() takes exactly 1 argument (2 given)

 When calculating amazonWeightsRDD & googleWeightsRDD in cell 4b, the test only checks the count. The instructions say "For each of the Amazon and Google full datasets, create weight RDDs that map IDs/URLs to TF-IDF weighted token vectors", but no example is given. How are we to know what the output value looks like?

Here's my output from
print amazonWeightsRDD.first(), googleWeightsRDD.first()
...
('b000jz4hqo', {'rom': 2.4051362683438153, 'clickart': 56.65432098765432, '950': 254.94444444444443, 'image': 3.6948470209339774, 'premier': 9.27070707070707, '000': 6.218157181571815, 'dvd': 1.287598204264871, 'broderbund': 22.169082125603865, 'pack': 2.98180636777128}) ('http://www.google.com/base/feeds/snippets/11125907881740407428', {'quickbooks': 17.48190476190476, '2007': 4.985334057577403, 'learning': 5.932773109243698, 'intuit': 13.379008746355684})
This looks right to me, just wanted a gut check before I submit for grading. What are others seeing? Hello Jesus Ortega,

Can you please answer my post :: 
https://piazza.com/class/i9esrcg0gpf8k?cid=3234 I am not able to understand why output is not matching I am doing the following please guide

-as part of invert() I am getting index of dictionary and using keys() method to get list of keys and setting it as part of pairs
-I am using invert method as part of map operation on amazonInvPairsRDD 

Above logic seems correct dont understand why the following ouput is wrong and hence testcases failing

There are 1363 Amazon inverted pairs and 3226 Google inverted pairs. Sorry, I have to show you my code because, although I read other similar post, I couldn't figure out my mistake.
If some of you, guys, could help me, then instructors can delete everything I don't mind.
<< removed >>
And then I got this very long error text
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-44-bca4481e17a7> in <module>()
     21 
     22 similarities = (crossSmall
---> 23                 .map(computeSimilarity)
     24                 .cache())
     25 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in cache(self)
    189         """
    190         self.is_cached = True
--> 191         self.persist(StorageLevel.MEMORY_ONLY_SER)
    192         return self
    193 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in persist(self, storageLevel)
    205         self.is_cached = True
    206         javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)
--> 207         self._jrdd.persist(javaStorageLevel)
    208         return self
    209 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _jrdd(self)
   2286         command = (self.func, profiler, self._prev_jrdd_deserializer,
   2287                    self._jrdd_deserializer)
-> 2288         pickled_cmd, bvars, env, includes = _prepare_for_python_RDD(self.ctx, command, self)
   2289         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(),
   2290                                              bytearray(pickled_cmd),

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _prepare_for_python_RDD(sc, command, obj)
   2204     # the serialized command will be compressed by broadcast
   2205     ser = CloudPickleSerializer()
-> 2206     pickled_command = ser.dumps(command)
   2207     if len(pickled_command) > (1 << 20):  # 1M
   2208         broadcast = sc.broadcast(pickled_command)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py in dumps(self, obj)
    409 
    410     def dumps(self, obj):
--> 411         return cloudpickle.dumps(obj, 2)
    412 
    413 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dumps(obj, protocol)
    814 
    815     cp = CloudPickler(file,protocol)
--> 816     cp.dump(obj)
    817 
    818     #print 'cloud dumped', str(obj), str(cp.modules)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dump(self, obj)
    131         self.inject_addons()
    132         try:
--> 133             return pickle.Pickler.dump(self, obj)
    134         except RuntimeError, e:
    135             if 'recursion' in e.args[0]:

/usr/lib/python2.7/pickle.pyc in dump(self, obj)
    222         if self.proto >= 2:
    223             self.write(PROTO + chr(self.proto))
--> 224         self.save(obj)
    225         self.write(STOP)
    226 

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    560         write(MARK)
    561         for element in obj:
--> 562             save(element)
    563 
    564         if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    634                 write(APPENDS)
    635             elif n:
--> 636                 save(tmp[0])
    637                 write(APPEND)
    638             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    679                 for k, v in tmp:
    680                     save(k)
--> 681                     save(v)
    682                 write(SETITEMS)
    683             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    679                 for k, v in tmp:
    680                     save(k)
--> 681                     save(v)
    682                 write(SETITEMS)
    683             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    679                 for k, v in tmp:
    680                     save(k)
--> 681                     save(v)
    682                 write(SETITEMS)
    683             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    304             reduce = getattr(obj, "__reduce_ex__", None)
    305             if reduce:
--> 306                 rv = reduce(self.proto)
    307             else:
    308                 reduce = getattr(obj, "__reduce__", None)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in __getnewargs__(self)
    169         # This method is called when attempting to pickle an RDD, which is always an error:
    170         raise Exception(
--> 171             "It appears that you are attempting to broadcast an RDD or reference an RDD from an "
    172             "action or transformation. RDD transformations and actions can only be invoked by the "
    173             "driver, not inside of other transformations; for example, "

Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.
Thank you very much I've been stuck on 4E/F for a while now. It seems that all of my entries in commonTokens have only 1 token listed. I've tried to groupByKey, but get an error saying the system runs out of memory. Can't seem to find a way to implement reducebykey or combinebykey that will turn the single token strings into a list of tokens. Any hints on this? In the  function tfidf, token 'recb000hkgj8k' being passed was a list, but when I did this exercise I had to tokenize the string to form a dictionary tfidf(toke...(str),rdd), which I am trying to comprehend why is it different then the 2f implementation and still works. Any insights please? Auto grader returns
Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 7
    .)
     ^
SyntaxError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
invalid syntax
However in notebook everything runs correctly.
Any ideas ? I am not such a Pythonista that I know whatever the <bleep> "pickling" is supposed to mean...

I would REALLY like to get this 4f...not because I'm hung-up on getting the very last possible grade point, but because I don't get to see the graphs in section 5 without this...and the graphs would appear to be valuable for the learning objective of the lession, given that half the quiz is about the graphs.

This code is NOT working, so I trust it's permissible to post:

< redacted >
...and the resulting error:

PicklingError                             Traceback (most recent call last)
<ipython-input-53-cc43fe4945df> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
----> 2 amazonWeightsBroadcast = sc.broadcast(amazonWeightsRDD.collectAsMap)
      3 googleWeightsBroadcast = sc.broadcast(googleWeightsRDD.collectAsMap)
      4 
      5 def fastCosineSimilarity(record):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in broadcast(self, value)
    642         be sent to each cluster only once.
    643         """
--> 644         return Broadcast(self, value, self._pickled_broadcast_vars)
    645 
    646     def accumulator(self, value, accum_param=None):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/broadcast.py in __init__(self, sc, value, pickle_registry, path)
     63         if sc is not None:
     64             f = NamedTemporaryFile(delete=False, dir=sc._temp_dir)
---> 65             self._path = self.dump(value, f)
     66             self._jbroadcast = sc._jvm.PythonRDD.readBroadcastFromFile(sc._jsc, self._path)
     67             self._pickle_registry = pickle_registry

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/broadcast.py in dump(self, value, f)
     80         else:
     81             f.write('P')
---> 82             cPickle.dump(value, f, 2)
     83         f.close()
     84         return f.name

PicklingError: Can't pickle <type 'instancemethod'>: attribute lookup __builtin__.instancemethod failed I stuck in 3a. How to compute the norm?


Thanks, First of all my sincere thanks to all instructor and professors who have put together this course. I personally feel this is more more than a general certification where we are supposed to tick answers from multiple choice options.

I am not surprised by the overwhelming response of the students who are keen to make their career in SPARK.

I have a suggestion about this course (especially lab 3), may be we can give some tips (or probably change questions) to help students to test / execute the code quickly. As i see most of the time students spending lot of time to see results on the changes they are applying to the code. Just take this a suggestion ! Ignore if not applicable.

Thanks
Jaya I'm running the inverted pairs through the swap function after a join and then applying a map that converts the token to a list element. After that I attempt to run a reduceByKey(...a+b) or a groupByKey() to generate a list. Unfortunately, after doing so, neither the .take() nor the .count() functions work anymore which makes it both impossible to complete, or debug, or check my work.

Help would be appreciated, as it seems this is a common issue for me and I can't find a way to fix it.

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-328-71194ed94995> in <module>()
     17                 .reduceByKey(lambda (a,b):a+b)
     18                 .cache())
---> 19 commonTokens.take(50)
     20 #print 'Found %d common tokens' % commonTokens.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 576.0 failed 1 times, most recent failure: Lost task 0.0 in stage 576.0 (TID 3978, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 253, in mergeValues
    d[k] = comb(d[k], v) if k in d else creator(v)
TypeError: <lambda>() takes exactly 1 argument (2 given)

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
 I am getting this error when I submit Lab 3. I got a 94%, but because of this exception, I am docked 6%. I don't understand it, as it says the error is in part 1a (tokenize a string), but the code it shows is from part 5, which uses and accumulator 

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block Hello All, I am not able to proceed further due to this failed test. Could anyone please point the mistake in my code ?
<>

The last test fails. 
1 test failed. incorrect smallest IDF value
Am I missing anything ? Please suggest.Thank you in advance. I tried to export my IPython notebook as a PDF using the File|Download as|PDF via Latex menu option. But it failed with the message:

nbconvert failed: PDF creating failed

Based on some googling, I ssh'ed into by vagrant VM and did:

sudo apt-get install -y pandoc
sudo apt-get install -y pdflatex
sudo apt-get install -y texlive-latex-base
sudo apt-get install -y texlive-latex-extra

and then,

sudo pip install mistune --upgrade

Still same error! Does this work for anyone? Anything obvious I'm missing?

I can print-to-PDF in the browser, so it's no big deal, but still...

Thanks,

- Chris

 For the nonDupsRDD we can't just subtract the two relevant RDD's. Spark does let you add RDDs RDD1 + RDD2. 

But from what I can tell it will not let you subtract them? Hello all!

I would like to share just a couple thoughts that have been sitting inside of me.

First of all, I would like to say a BIG thank you to the Professor and the TA course staff. Their support on Piazza, production materials, and quality of lectures is unparalleled!
I learned a lot- more than I would have ever learned by going through a book by myself.

Also, the subtitles on the lecture videos save my life as I am a hard of hearing student, and I can read along with the lecturer speaking. Thanks so much!! 

I would like to make a couple of remarks though. Please accept these as positive criticisms coming from someone who has been breathing and living this course daily.

Criticism: 
1- The labs are murky in terms of documentation. Many people are wasting hours on reading and understanding the instructions and misleading variable names when they could be programming.

2- The labs are overwhelming, and they require many, many hours to complete. Being a high schooler, I am on summer vacation, so my home/family life is easier to juggle. I am not asking for the course staff to make the labs dumbed down, and I do not want to give the impression that I am against hard work. Rather, I am for it. However, I believe that most people in this class are adults who are taking this class to either (1) get some more information to supplement their college careers or (2) professionals that are trying to gain some experience with a tool that they plan to use in their line of work or (3) people that are here just for the love of programming and computer science. I would say that most of us students are adults and that we have to balance work/family life/etc. I would say that a 15-20 hour lab + 2 hours of viewing lectures is making their lives more difficult as either they are cutting down on sleep, or sanity, or both. I know that I have been cutting down on sleep, and I have way less distractions in terms of job and family life than they do.

3- The course description on edX is a bit misleading. It says that the effort needed is 5-7 hours each week, when in reality, we know that we spent more than 10 hours on lab1 and more than 15 on lab2. I know that the team is taking up a survey after we submit lab3 to see how long people are actually working, and I believe that the responses will reflect what I have been seeing on piazza. Maybe it should be updated to 12-17 hours per week? Just a suggestion in order to avoid misleading students who will then drop this class due to time constraints. I think this is just being fair.

4- Debugging messages are a pain in the but. I know that there are errors, but then you get the green py4java errors, which make it hard to cut through to the main error that caused it. Maybe, an exception can be thrown to py4Java errors in future offerings of this course to land gracefully, but only display the initial error. I am not a spark expert or python guru, but I am just suggesting.

5- I realize that we are the first cohort of students taking this course, and I realize that with that, there is bound to be some confusion that will be cleared up in later parts of this course. However, there are issues with memory exceptions, and the causes for this were glanced over in two/three lectures, but some questions were not answered. For example, how do we determine if collecting an RDD will cause a memory error due to the driver not having enough memory? This is a crucial question that I feel must be addressed in the future.

Please feel free to criticize my points of view as I encourage an open discussion.

Thanks.
 I have now gotten everything except the second test on 4f.  Like many people:





1 test passed.
1 test failed. incorrect similarityTest fastCosineSimilarity
1 test passed.





That's 20 of 21 tests passed.  And no error messages upon running the cell, to lend any additional insight.

The deadline has come and gone, but I am still trying. Surely this should count for *something*.

Not because I need that final grade point...I've already earned an A for this class. But because I want it for my own education.

I want to understand 4f, in case I need it for a job somebody. And I want to unlock section 5, for whatever value it brings to the learning objective.

So, dear instructors, please help the gaggle of us who came so very close - within a few percent of perfect - to either complete 4f, or, in the very least, give us screen shots of the correct Section 5, which required no coding input from us whatsoever, and would compromise nothing in terms of future classes receiving solutions. Who is correct? The professor says N is 4772 but the majority of students say 400 is N.
First I got 400 but because of the professor said N is 4772 I tried distinct transformation and I got 4772. Well, still I couldn't pass the last test though.

And why am I getting
idfsSmall.takeOrdered(1, lambda x: x[1])
=
[('software', 50.765957446808514)]
I still don't fully understand what the "document" means. Maybe N is 4772 but I am getting
n(t) wrong.
n(t), the number of documents in U that contain t
Hm, what is document and what is u? t is token so 'dog' is a one of t.
document is... id? Dear Instructors,
                           Even after the course is over we need to work a lot with Spark to become proficient with spark. May I request you to help us with some steps that can ensure installation on a windows machine (local installation) or some sandbox where we can upload our datasets easily and keep practicing. I saw one at cloudera and hortonworks but I need help in deciding if you were to recommend one.
I thank you for this lovely course (right course at right time!)

Sincerely,
PS Again, no answers contained:

Use leiningen

project.clj
(defproject br.net.bmobile/edx_lab2 "1.0.0-SNAPSHOT" ; version "1.0.0-SNAPSHOT"
  :description "A data analysis project"
  :dependencies [[org.clojure/clojure "1.6.0"]
                 [org.apache.spark/spark-core_2.10 "1.4.0"]
                 [org.apache.spark/spark-streaming_2.10 "1.4.0"]
                 [org.apache.spark/spark-streaming-twitter_2.10 "1.4.0"]
                 [org.apache.spark/spark-streaming-kafka_2.10 "1.4.0"]
                 [org.apache.spark/spark-sql_2.10 "1.4.0"]
                 [com.databricks/spark-csv_2.10 "1.0.1"]
                 [yieldbot/flambo "0.6.0-SNAPSHOT"]
                 [clj-time "0.8.0"]
                 [org.clojure/tools.trace "0.7.8"]
                 [clj-glob "1.0.0"]
                 [incanter "1.5.6"]]
   :profiles {:dev {
     :dependencies [[midje "1.6.3"]]
     :aot [flambo.api flambo.conf flambo.tuple flambo.sql]}})

Run lein deps and then lein repl to run the code below

exercises.clj
(ns br.net.bmobile.exercises
  (:use midje.sweet)
  (:use '(incanter core stats charts))
  (:require [flambo.conf :as conf]
            [flambo.api :as f]
            [flambo.tuple :as ft]
            [flambo.sql :as sql]
            [clj-time.core :as t])
  (:import [org.apache.spark.sql Row RowFactory]))

(def c (-> (conf/spark-conf)
  (conf/master "local")
  (conf/app-name "flame_princess")))

(def sc (f/spark-context c))

(def month_map {"Jan" 1 "Feb" 2 "Mar" 3 "Apr" 4 "May" 5 "Jun" 6 "Jul" 7
    "Aug" 8 "Sep" 9 "Oct" 10 "Nov" 11 "Dec" 12})
    

(defn parse_apache_time [s] 
  (t/date-time (Integer/parseInt (subs s 7 11))
    (get month_map (subs s 3 6))
    (Integer/parseInt (subs s 0 2))
    (Integer/parseInt (subs s 12 14))
    (Integer/parseInt (subs s 15 17))
    (Integer/parseInt (subs s 18 20)))
)

(def APACHE_ACCESS_LOG_PATTERN #"^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] \"(\S+) (\S+)\s*(\S*)\" (\d{3}) (\S+)")

(f/defsparkfn parseApacheLogLine [logline] 
  (def match (re-matches APACHE_ACCESS_LOG_PATTERN logline))
  (if (= nil match)
      [logline 0]
      (do
        (def size_field (get match 9))
        
        (if (= size_field "-")
          (def size 0)
          (def size (Long/parseLong (get match 9))))
        
        [{
          :host          (get match 1)
          :client_identd (get match 2)
          :user_id       (get match 3)
          :date_time     (parse_apache_time (get match 4))
          :method        (get match 5)
          :endpoint      (get match 6)
          :protocol      (get match 7)
          :response_code (Integer/parseInt (get match 8))
          :content_size  size
        } 1])))

(def logFile "hdfs://192.168.100.116:8020/user/edx/lab2/apache.access.log.PROJECT")

(defn parseLogs [] 
  (def parsed_logs (-> (f/text-file sc logFile)
    (f/map parseApacheLogLine)
    (f/cache)))

  (def access_logs (-> parsed_logs
    (f/filter (f/fn [logPair] (= (get logPair 1) 1)))
    (f/map (f/fn [logPair] (get logPair 0)))
    (f/cache)))

  (def failed_logs (-> parsed_logs
    (f/filter (f/fn [logPair] (= (get logPair 1) 0)))
    (f/map (f/fn [logPair] (get logPair 0)))))
    
  (def failed_logs_count (f/count failed_logs))
    
  (if (> failed_logs_count 0)
    (do
      (printf "Number of invalid logline: %d" (f/count failed_logs))
      (for [line (f/take failed_logs 20)]
        (printf "Invalid logline: %s" line))))

  (printf "Read %d lines, successfully parsed %d lines, failed to parse %d lines" (f/count parsed_logs) (f/count access_logs) (f/count failed_logs))

  [parsed_logs access_logs failed_logs]
)

(let [[parsed_logs access_logs failed_logs] (parseLogs)])

(if (= (f/count failed_logs) 0)
  (print "1 test passed")
  (print "incorrect (ft/count failed_logs)"))
  
(if (= (f/count parsed_logs) 1043177)
  (print "1 test passed")
  (print "incorrect (ft/count parsed_logs)"))

(if (= (f/count access_logs) (f/count parsed_logs))
  (print "1 test passed")
  (print "incorrect (ft/count access_logs)"))
 can someone elaborate.... in order to get the certification do you need a summary score percentage of 50%, or 50% on each lab? I have well above 50% summary, but had a family emergency and not sure how much time I will have to spend on the next lab. Thank you! I am running the 3c program and it is outputting:
Requested similarity is 0.000404873234237.

Can anybody give me advice on how to troubleshoot this?
This is what i am doing:
similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561') gets called

similar uses similarities (which I have coded using map lambda x passing it to computeSimilarity, which uses crossSmall.  I used cartesian to join both RDDs together.

computeSimilarity calls cosineSimilarity from the previous lab.  On the previous lab I get the correct results.  cosineSimiarity calls several functions from previous labs, labs in which I got all of the answers correct.

I have no idea what to do now, but I am extremely frustrated..

 I performed a join on the sims and goldStandard without problems. Then I need to get cosine similarity score. I think I am suppoosed to call the similarBroadcast function which needs to pass the amazonID and googleURL as parameters. I am new to python. Can someone please suggest in order to split the key of the joined RDD, if this is the way to do it below?:
<REDACTED> PLEASE DO NOT POST SOLUTIONS
 # this one will get to the unique token sets for each document
    uniqueTokensPerDoc = corpus.map(lambda t: (t[0], set(t[1])) )
# then I try to transpose it to (token, 1) serials.    tokenCountPairTuple = uniqueTokensPerDoc.map(lambda (id, tokens): (token,1.0) for token in tokens)
But it gives the error:
NameError: global name 'tokens' is not defined Hello

I have an issue to use Spark 1.4.0 on Jupiter.

We are testing lab assignment using scala-kernel, not python.

And we want to use spark 1.4.0 because it has good web UI.

But what I tried 'wget spark 1.4.0' on myvagrant and tried 'export path = ...' changing is failed.

Who can explain me how I can set up spark 1.4.0 and test it on notebook?

  Like others, I've been at this question for a long, long while.  It passes on two tests, then fails on "incorrect smallest IDF value".

On the subsequent item, I can see some of my IDFs match and some do not.  I would move on, and did, but later items rely on this function so I fear a domino effect of failed tests for the rest of the lab that will keep me from passing.

Here is generic code that reflects the logic I am using.  I cannot figure out the error here. (I'm concerned that my tokenizer earlier in the lab is wrong somehow even though I've passed all tests up to this point.)

Guidance needed!

N = 4
t = sc.parallelize(['a','b','c']) # unique tokens
d = sc.parallelize([('a','a','b'),('b','c','c','c','c'),('b'),('c')]) # document tokens
j = t.cartesian(d)
h = j.map(lambda p: (p[0], 1 if p[0] in p[1] else 0)) # token/document hits
s = h.reduceByKey(lambda a, b: a + b) # unique token sums of hits
i = s.map(lambda p: (p[0], float(N) / p[1])) # unique token IDF

print t.collect()
print d.collect()
print j.collect()
print h.collect()
print s.collect()
print i.collect()
Thx I am getting the following erros, even though the test cases are passing in my local vm. Please do help me to figure out the issueImplement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'rec_b000hkgj8k_weights' is not defined

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsSmallBroadcast' is not defined

  I got the following error when submitting Lab 3 (not influencing my 100% score for the lab):

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
---------------------------------- Hello,

The below code snippet throws exception and fails to execute.

# TODO: Replace <FILL IN> with appropriate code
<REDACTED>POSTING SOLUTIONS IS A VIOLATION OF THE HONOR CODE

def similar(amazonID, googleURL):
    """ Return similarity value
    Args:
        amazonID: amazon ID
        googleURL: google URL
    Returns:
        similar: cosine similarity value
    """
    return (similarities
            .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
            .collect()[0][2])

similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
print 'Requested similarity is %s.' % similarityAmazonGoogle

Exception :

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-27-534e58549a7f> in <module>()
     36             .collect()[0][2])
     37 
---> 38 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     39 print 'Requested similarity is %s.' % similarityAmazonGoogle

<ipython-input-27-534e58549a7f> in similar(amazonID, googleURL)
     33     """
     34     return (similarities
---> 35             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     36             .collect()[0][2])
     37 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 52.0 failed 1 times, most recent failure: Lost task 0.0 in stage 52.0 (TID 197, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-27-534e58549a7f>", line 19, in computeSimilarity
  File "<ipython-input-24-7741e37706de>", line 11, in cosineSimilarity
  File "<ipython-input-6-80d66f7b90d7>", line 13, in tokenize
  File "<ipython-input-4-4dac95fe2457>", line 12, in simpleTokenize
AttributeError: 'list' object has no attribute 'lower'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
<p><p>simpleTokenize() method : </p> <p></p> <p></p></p>
<pre style="white-space: pre-wrap; word-wrap: break-word;"># TODO: <tt>Replace</tt> <FILL IN> with appropriate code
quickbrownfox = 'A quick brown fox jumps over the lazy dog.'
split_regex = r'\W+'

def simpleTokenize(string):
    """ A simple implementation of input string tokenization
    Args:
        string (str): input string
    Returns:
        list: a list of tokens
    """
    return filter(lambda s:len(s)>0, re.split(split_regex,string.lower()))

print simpleTokenize(quickbrownfox) # Should give ['a', 'quick', 'brown', ... ]
<p><p>It says, there is some error in the simpleTokenize() method. Could anyone help me resolving this ? What's wrong with the lower() function ? It's really a nice course, where the effort makes you wise. I am enjoying at the fullest.</p></p> When calculating the avgSimNon  if you used reduce(lambda x,y: x + y) / float(noDupsCount)  your answer will be slightly different from .mean().

But slightly enough to not pass the test. So use .mean(). 

Maybe it is just my machine. Because I used reduce method for avgSimDups and got the exact answer as key.  Not a big deal, but just in case you get really close with brute force, just use .mean() Lab3 section 1a the regular expression "\W+" does not match the cell description (according to the python help).
\w - [a-zA-Z0-9_]
\W complements \w
So have I misunderstood the cell description, or is my python documentation wrong, or is the regular expression wrong, or just a cell description is wrong, or what do I miss?

'\w+' fits the task perfectly, or is it about strengthening our muscles and solving it with '\W+' ?

Thanks Ok, I have another issue that is stumping me with 3d.

I essentially copied the code from 3c and ran it for 3d, and received the following error:
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 72.0 failed 1 times, most recent failure: Lost task 0.0 in stage 72.0 (TID 441, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-105-c54af2e9bb82>", line 20, in <lambda>
  File "<ipython-input-105-c54af2e9bb82>", line 15, in computeSimilarityBroadcast
  File "<ipython-input-102-c6f0a17d7ba7>", line 14, in cosineSimilarity
  File "<ipython-input-66-e425d77e53a5>", line 33, in cossim
ZeroDivisionError: float division by zero
1) All functions written are working fine and passed each of the labs
2) When I say I copied the solution from 3c, basically the code in function computeSimilarityBroadcast is exactly the same in 3d as it was in 3c.  The only difference was the passing of the broadcast variable, which was already done (not done by me).   The same is true here:
similaritiesBroadcast = (crossSmall                         .map(lambda x: Same as 3c)                         .cache())

Clearly I am not doing something because I am getting the error.  What am I missing?  Do I need to do any checks in the cossim function?  It is not written to handle 0s in the divisor.  The divisor in cossim is norm(a)*norm(b)

Any help is appreciated. Hi All,
 I completed this after spending 18 hours spanned over 3 days.

Thanks everybody who provided their valuable input.

In my humble opinion, this course is for advanced python developers who are very well versed with python re module. Every lab has some regular expression part.
But I am going to be plugging along.

What is the deadline for completing all labs?
Can we get one or two weeks grace period after all lectures are completed?

Thanks

 How is average similarity supposed to be calculated?for 3e
Thank you! Is it normal that if I print one line of commonTokens, I have the following:

(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), <pyspark.resultiterable.ResultIterable object at 0xb0d5a06c>)

commonTokens is like this:

commonTokens = (<REDACTED>POSTING CODE IS A VIOLATION OF THE HONOR CODE)

Thanks for your help. Till yesterday, I had the problem with submissions. Autograder took more than 2 days, 6 attempts to start processing my .py file. No progress update(it says 0/100). I looked at the latest advice.

Now a new problem, I can't even see the submission button(no browse, no save, no check). And today is the last day.
 3d code is same as 3c except for using idfsSmallBroadcast. 3c passes all tests whereas 3d gives "KeyError: 'b000jz4hqo'"

Any help is greatly appreciated. Thanks.

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most erecent call last)
<ipython-input-34-573b5bb7de30> in <module>()
     33             .collect()[0][2])
     34 
---> 35 similarityAmazonGoogleBroadcast = similarBroadcast('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     36 print 'Requested similarity is %s.' % similarityAmazonGoogleBroadcast

<ipython-input-34-573b5bb7de30> in similarBroadcast(amazonID, googleURL)
     30     """
     31     return (similaritiesBroadcast
---> 32             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     33             .collect()[0][2])
     34 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 246, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-34-573b5bb7de30>", line 15, in computeSimilarityBroadcast
  File "<ipython-input-26-d9c3d83350ed>", line 12, in cosineSimilarity
  File "<ipython-input-22-43bf1e4aac9d>", line 11, in tfidf
KeyError: 'b000jz4hqo'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

</span></span></span></span></span></span></span></span></span></span></span> I think I double clicked one of my code chunks then the format of writing changed (with blue color of title). Usually if I just press ctrl + enter then it comes back but this time it looks like this

I only took the photo of the top of the chunk because of the Honors Code.
TODO... looks quite strange and my code doesn't execute anymore!!!!
This is question 3a...     w1 = tfidf(tokenize(string1),idfsDictionary)
    w2 = tfidf(tokenize(string2),idfsDictionary)
    print w1,w2
    return cossim(w1, w2)
Here is my code , I got the following result。 I don't know why it is wrong but 3a runs well.
{'photoshop': 22.22222222222222, 'adobe': 8.333333333333334} {'adobe': 8.333333333333334, 'illustrator': 50.0}
0.500277597875 Can this dictionary be made in one line with a dictionary comprehension? Thanks. i am doing join,map(swap) and reduceByKey(lambda x,y: (x+y)) the code is taking too long to return any idea?

i tried increasing the memory using virtualbox ui to 8 .. but vagrant up fail so had to revert back to 2gb ram.. and then it comes back up. any hints what i am missing? 

for lab3 question 2c
N = number of documents in the RDD
each element in the corpus RDD is ("id", [list of tokens with dupes] uniqueTokens =  a simple map will do by removing the duplicates in the tokens list
 tokenCountPairTuple = convert all the tokens from all the documents into a single list using "flatMap(lambda x: x[1]) "and then make each token a tuple.
   tokenSumPairTuple =  apply reduceByKey to sum the tokens based on keys.

 return tokenSumPairTuple is a tuple rdd same as above expect the tuple value is modded by N*(1/tuple value)

Hope that helps.

N is 400 by the way The test expects the token with the lowest idf to be 'software' with 4.25531914894. Since these are just numbers I have no idea how to find out where my implementation went wrong.

idfsSmall.takeOrdered(3, lambda s: s[1])

returns

[('adobe', 1.520912547528517), ('software', 2.1621621621621623), ('cs3', 2.9197080291970803)]

The implementation is quite straightforward, flatMap over the corpus and then apply the technique for occurrence counting we have used so often in Lab 2, then mapping again to the quotient of N and the count. The first test passes - there are 4772 unique tokens.

Thank you

 Hi,

I submitted my Lab twice and both time it failed due to missing /tmp file at the same place [3d]. The code runs fine from IPython notebook, ie all test passed. Do I need to make any changes to fix this?

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
All tests passed
Implement an IDFs function (2c)
-------------------------------
All tests passed
Implement a TF-IDF function (2f)
--------------------------------
All tests passed
Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
All tests passed
Perform Entity Resolution (3c)
------------------------------
All tests passed
Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/broadcast.py", line 106, in value
    self._value = self.load(self._path)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/broadcast.py", line 87, in load
    with open(path, 'rb', 1 << 20) as f:
IOError: [Errno 2] No such file or directory: u'/tmp/spark-2ba7eed5-815f-4620-8aca-152fff46bc66/pyspark-11a13aa8-7734-4ce9-a620-672bd849fc33/tmpT5FQpZ'

Perform a Gold Standard evaluation (3e)
---------------------------------------
All tests passed
Tokenize the full dataset (4a)
------------------------------
All tests passed
Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
All tests passed
Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
All tests passed
Create inverted indicies from the full datasets (4d)
----------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4e)
-------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4f)
-------------------------------------------------
All tests passed
-- 18 cases passed (94.0%) --


Your submission token id is 1111237-50fa315e592f92532818c4f570fa4eba:8a237747a59cea4e2aec09acb5ef59e7:ip-172-31-8-27
Please include this submission token id when you need support for your code submission.


 Hello Anthony / Instructors,

Till now I have not received any response for my queries :(. This is the only place where I got stuck in this entire lab and that too on the last question :)
(This is the first time in my life I did Python, Spark and Data Science -- Wow!!)

Please do not consider this to be duplicate :)

I have two questions:

1. Lets say if the token is not found in both amazonWeightsBroadcast and googleWeightsBroadcast. Then do I have to consider        including token weight for the sum?
What I am trying to say is that if the token is found in only one i.e.  amazonWeightsBroadcast or googleWeightsBroadcast. Then should I consider its weight or should I only consider the weight if token is found in both?
(I feel I should only consider the token weight if its found in both -- so that I can take its product for both the weights i.e. for amazon and google)

2. What if one of the norm is missing from norm Broadcast RDDs? Should we still find the cosine? If not, then what is the value we should return i.e. return (key, value)? should the value be returned as 0

Quiet Often my VM is going in "meditateguru" mode while working on 4F (due to lack of system RAM only 3GB) -- looks like I have to submit without 4F

Thanks,

Censored Code Snippet::

# TODO: Replace <FILL IN> with appropriate codeamazonWeightsBroadcast = sc.broadcast(amazonWeightsRDD.collectAsMap())googleWeightsBroadcast = sc.broadcast(googleWeightsRDD.collectAsMap())def fastCosineSimilarity(record):    """ Compute Cosine Similarity using Broadcast variables    Args:        record: ((ID, URL), token)    Returns:        pair: ((ID, URL), cosine similarity value)    """         amazonRec = record[0][0][0]    googleRec = record[0][0][1]    tokens =record[0][1]<code censored> key = (amazonRec, googleRec)
    return (key, value)
    

similaritiesFullRDD = (commonTokens
                       .map(fastCosineSimilarity)
                       .cache())

print similaritiesFullRDD.count()



While doing the Asserts I am getting following Error Trace 

1 test failed. incorrect len(similarityTest)  -- I am getting len(similarityTest) = 0
print similaritiesFullRDD.count() = 2441100
IndexError: list index out of range -- since my list is empty!!!

But, if I try to filter the record from commonTokens. Then the record is present in commonTokens

print commonTokens.filter(lambda ((aID, gURL), token): aID == 'b00005lzly' and gURL == 'http://www.google.com/base/feeds/snippets/13823221823254120257').collect()

[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), ['data', 'complete', 'includes', 'software'])]

If I try to execute code with the commonTokens RDD for example ::
rr = fastCosineSimilarity(commonTokens.filter(lambda ((aID, gURL), token): aID == 'b00005lzly' and gURL == 'http://www.google.com/base/feeds/snippets/13823221823254120257')
.collect())

Then I get this :: (('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), 4.286548413995203e-06) -- which is correct as this is what we want.

Strangely, this record is not coming in the ::: similaritiesFullRDD
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-76-cc8a5473fc5a> in <module>()
      2 similarityTest = similaritiesFullRDD.filter(lambda ((aID, gURL), cs): aID == 'b00005lzly' and gURL == 'http://www.google.com/base/feeds/snippets/13823221823254120257').collect()
      3 Test.assertEquals(len(similarityTest), 1, 'incorrect len(similarityTest)')
----> 4 Test.assertTrue(abs(similarityTest[0][1] - 4.286548414e-06) < 0.000000000001, 'incorrect similarityTest fastCosineSimilarity')
      5 Test.assertEquals(similaritiesFullRDD.count(), 2441100, 'incorrect similaritiesFullRDD.count()')

IndexError: list index out of range

 all the tests have passed on local system but getting the following error on submission...have gone through the posts on .collect(), prints and auto-grader...and maintained sanity on the same ...need pointers 

I can email the .py file if needed to the instructors.

Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 1110575-75a2b80853595544f143c1331d101bd1:4176c67c5fdfc997e83f1b60f8bf0009:ip-172-31-8-107
Please include this submission token id when you need support for your code submission. I have tried the whole day but still I am getting an error. pleas help me with it.
here is my code for  tokenize

<REDACTED>


here is my countTokens()

In [9]:


















# TODO: Replace <FILL IN> with appropriate code
amazonRecToToken = amazonSmall.map(lambda (x,y):(x,tokenize(y)))
googleRecToToken = googleSmall.map(lambda (x,y):(x,tokenize(y)))
​
def countTokens(vendorRDD):
    """ Count and return the number of tokens
    Args:
        vendorRDD (RDD of (recordId, tokenizedValue)): Pair tuple of record ID to tokenized output
    Returns:
        count: count of all tokens
    """
    #for key, value in vendorRDD.collect():
     #   print '{0}: {1}'.format(key, list(value))
    return vendorRDD.<REDACTED>
​
totalTokens = countTokens(amazonRecToToken) + countTokens(googleRecToToken)
print 'There are %s tokens in the combined datasets' % totalTokens










I am getting an error 
Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.

 What is googleRec? Is it a string for a specific google URL?
What is googleURL and amazonID in computeSimilarity function?


def computeSimilarity(record): """ Compute similarity on a combination record Args: record: a pair, (google record, amazon record) Returns: pair: a pair, (google URL, amazon ID, cosine similarity value) """ googleRec = record[0] amazonRec = record[1] googleURL = "http://www.google.com/base/feeds/snippets/17242822440574356561" amazonID = "b000o24l3q" googleValue = <FILL IN> amazonValue = <FILL IN> cs = cosineSimilarity(<FILL IN>, idfsSmallWeights) return (googleURL, amazonID, cs) pass


Are googleURL and amazonID just like that???? After having completed lab 3 I am wondering, would you write all this code in a real-world situation, or are there libraries with all these functions? My guess is it's all been done before and it would make sense to use a library. So, does it exist?

Also, I would suggest rewriting all of lab 3 explanations. Half the time I was guessing what I was supposed to do. It could be much more clear. As an example, in 4f 
...computes the sum for each of the tokens in the token list of the products of the Amazon weight for the token times the Google weight for the token. The sum should then be divided by the norm for the Google URL and then divided by the norm for the Amazon ID.
I had to read this like a 100 times. A formula would be much more readable. In 2f there is a problem of combining two dictionaries.
Is there any possibility to do that in elegant "lambda" way?
Since course show me how much I can do with lambdas I think now that simple "for" loop isn't elegant
But how can I handle with lambda with two or more iterables? 
For lists I can do zip() and get a list of tuples, but how about dicts? Is my invert function wrong?

Actually first test is passed.


def invert(record):    pairs = sc.parallelize([record]).map(lambda x : (x[1].keys(), x[0])).collect()    return (pairs)
invertedPair = invert((1, {'foo': 2}))Test.assertEquals(invertedPair[0][1], 1, 'incorrect invert result')




1 test passed.






---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-147-e440da328655> in <module>()
     14 Test.assertEquals(invertedPair[0][1], 1, 'incorrect invert result')
     15 
---> 16 amazonInvPairsRDD = (amazonWeightsRDD.flatMap(lambda x: invert(x)).cache())
     17 
     18 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in cache(self)
    189         """
    190         self.is_cached = True
--> 191         self.persist(StorageLevel.MEMORY_ONLY_SER)
    192         return self
    193 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in persist(self, storageLevel)
    205         self.is_cached = True

 Hi,

My sims is in this form:
[('b000jz4hqo http://www.google.com/base/feeds/snippets/11448761432933644608', 0.0)]
And my gold has this form:
[(u'b000jz4hqo http://www.google.com/base/feeds/snippets/18441480711193821750', 'gold')]
What to I need to do to create trueDupsRDD, I know that I need to join them later but at the first step when I need to map sims, what kind of approach do I need ?


Thank you.
 I get the below error where i try to submit my assignment. It runs find in notebook without any issues, but autogravder throws an error.

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 5
    .
     ^
SyntaxError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
invalid syntax

 
Your submission token id is 1114267-eee14ab88cbe58d65e7361ca8bc87960:6daa56776f4f4074ba3a4ee74fbac258:ip-172-31-8-188
Please include this submission token id when you need support for your code submission. Hi TA team,

With reference to my other post @3151, I would like to bring in your kind notice that I'm not in a position to spend more time in performance debugging of 4f in lab 3.

So I guess, I won't be able to submit the complete lab 3 of week4. Kindly suggest how can is submit the partial completed lab3 to autograder.

Also, I would request you/ your team to kindly see if you can review my code for any potential performance issues. I can share my code.

kindly suggest.

Regards,
Neeraj    Here is my top 5 counts in  tokenSumPairTuple 

[('adobe', 263), ('software', 185), ('cs3', 137), ('pro', 112), ('design', 111)]

and N/n[t] gives

[('adobe', 1.520912547528517), ('software', 2.1621621621621623), ('cs3', 2.9197080291970803), ('pro', 3.5714285714285716), ('design', 3.6036036036036037)]

But it seams value of adobe < software , which results in failing of test cases ! what could be the issue here ? Hi all! In case you'd like to play around with Spark in Scala, perhaps using the NASA web server log dataset from Lab 2, I've made this tiny library for parsing logs in the Common Log Format that you might find useful: https://github.com/stefanobaghino/clf4s Hope you enjoy! I'm stuck on calculating the sum of products within the fastCosineSimilarity function. Here's my pseudocode:
amazonRec = record...
googleRec = record...
tokens = record...
aW = aWBroadcast...
gW = gWBroadcast...
s = [aW[t] ... gW[t] for t in tokens]
When I run this I get a KeyError: '1', which is the first character in my tokens variable. The string '120' is present in both aW & gW. I've also tried this with other rows of the commonTokens RDD and get the same results.

I've tried:
Verified that the correct key appears in both aW & gW. They both have '120' (in this case).Verifying the values and variable types for each of the variables leading up to s. They all look exactly as they should. The tokens variable is always a string (even for the first row, where its string is '120').Forcing a type on tokens. I've tried list(tokens) & str(tokens).Adding a check to the list comprehension (if aW[t] in aW and gW[t] in gW).

I'm at a loss for where to go from here. If it were a KeyError for '120' then I'd be checking my expressions for aW & gW, but the error is always a single character. I'm exhausted & about to go to bed, but appreciate any tips and will reply as soon as possible in the morning. My code for Lab3 part 4f is taking too much time to complete.I am trying to debug/ optimize the code from past 2-3 days, but unable to do so.

My machine has around 4GB of RAM and 4 GB of swap space. In the first run it took around 50 minutes, after that I halted the vm. My implementation goes like this

amazonWeightsBroadcast:-created a broadcast varible by calling collectAsMap function on varible amazonWeightsRDD, similar approach for googleWeightsBroadcast variabledef fastCosineSimilarity(record):    """ Compute Cosine Similarity using Broadcast variables    Args:        record: ((ID, URL), token)    Returns:        pair: ((ID, URL), cosine similarity value)    """    amazonRec = extract "ID" from "record"    googleRec = extract "URL" from "record"    tokens = extract "token" from record #(token is of type <pyspark.resultiterable.ResultIterable object >)    # get dictionary corresponding to "ID" from amazonWeightsBroadcast variable    a_dict = amazonWeightsBroadcast.value.get(........)    # get dictionary corresponding to "URL" from googleWeightsBroadcast variable    g_dict = googleWeightsBroadcast.value.get(........)        # Calculate sum of weights of tokens from a_dict, g_dict    s = sum([a_dict.get(x,0) * g_dict.get(x,0) for x in tokens])      #get corresponding norms from amazonNormsBroadcast & googleNormsBroadcast variable    value = s / (amazonNormsBroadcast.value.get(......) * googleNormsBroadcast.value.get(.......))    key = (amazonRec, googleRec)    return (key, value)
What can be done to optimize this code I tried this code on a single record from commonTokens like this and I got the correct result.

test_data = commonTokens.filter(lambda ((aID, gURL), cs): aID == 'b00005lzly' and gURL == 'http://www.google.com/base/feeds/snippets/13823221823254120257').collect()
print fastCosineSimilarity(test_data[0])Output --(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), 4.286548413995203e-06)
The output is same as that in the test case. I think I need to make the above code fast enough to pass through the auto-grader.

What can/should I do to improve the run time of my code. Any help will be deeply appreciated. Thanks in advance!! In 4d, invert returns a list in a list, as shown below for one amazon record. Items within the internal list are correct. All it does is appends tuple to the list, starting with an empty list.

[[('rom', 'b000jz4hqo'), ('clickart', 'b000jz4hqo'), ('950', 'b000jz4hqo'), ('image', 'b000jz4hqo'), ('premier', 'b000jz4hqo'), ('000', 'b000jz4hqo'), ('dvd', 'b000jz4hqo'), ('broderbund', 'b000jz4hqo'), ('pack', 'b000jz4hqo')]]

Any idea why?

Thanks in advance. I think that the technique for getting document tokens can be used for document clustering too.


Document Clustering with Python

http://nbviewer.ipython.org/github/brandomr/document_cluster/blob/master/cluster_analysis_web.ipynb this is what  I get when I ask for the first item in commonTokens

commonTokens.first()
(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'),
 pyspark.resultiterable.ResultIterable at 0xb08ea9ec)

can anyone illuminate? Hello,

There seems to be a problem with the grader. It deducted my score despite there was an announcement in extending the grace period because of the problem during that time.

Here is my submission token id:
965107-5597de9998f070926b7542ee53503d1b:04eb6bb635014b798023b19ca80f7bc7:ip-172-31-35-47
 I used set subtraction to eliminate the stopwords. Naturally it also gets rid of the duplicates. I guess something is wrong with that because I know I am supposed to keep the duplicates. This simple approach is probably not going to work? Where to look for ideas?
 It seems after submitting autograder removes the score first and then grades and updates scores.

A better way would be to grade then take the best score and update.

Sorry if it is already doing that. :)

cheers!! I'm not totally understanding the description... so for example:

amazonInvPairsRDD contains:

[('foo', 'Aa'), ('foo', 'Ab')]

googlePairsRDD contains:

[('foo', 'Ga'), ('foo', 'Gb'), ('foo', 'Gc'), ('foo', 'Gd')]

What should commonTokens be, after the reduction?

Should it be:

A: [((Aa,Ga), (Aa,Gb), (Aa,Gc), (Aa,Gd), (Ab,Ga), (Ab,Gb), (Ab,Gc), (Ab,Gd)), 'foo']....

or

B: [(Aa, Ga), ('foo', 'bar', 'fox', 'jump', 'over', 'dog')], [(Aa, Gb), ('foo', 'over', 'dog')],....

My impression after reading the instruction for 4e, should be A but reading the question for 4f, I'm starting to think it should be B.   Either way, I'm getting a lot of conflicting information from different sources so I would greatly appreciate if the professor or TA can clarify this once & for all.    This 4f has been VERY painful and I needed to make sure I get the data set RIGHT before digging further If false-positives are considered much worse than false-negatives, shouldn't the optimal threshold value actually decrease?

When the threshold decreases, less pairs are predicted "positive".  We get less false-positive results at the expense of having more false-negative results.  If false-positives do have a bigger cost than false-negatives, then this is a good trade-off to make.  So it seems to me that we should decrease the threshold under the assumption given by the question.  

If I'm misinterpreting something here, please bring it to my attention. 

 We are using easy to use notebooks which has tutorials and we write where we learn the relevant stuff but shouldn't this be a part of notebook in maybe lab1/lab0 how to create sc objects. I read that they are at metric-learning project and can be found at:  cs100/lab3.
However, I can't seem to locate them at the metric-learning project.
Any tip/link greatly appreciated.
Thanks I have now submitted three times. All test pass when run locally in iPython Notebook but not with the Autograder with python script (.py). I get the following feedback:

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
All tests passed
Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect smallest IDF value

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 5, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect rec_b000hkgj8k_weights

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect cossimAdobe

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect similarityAmazonGoogle

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect similarityAmazonGoogle

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect avgSimDups

Tokenize the full dataset (4a)
------------------------------
All tests passed
Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
All tests passed
Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
All tests passed
Create inverted indicies from the full datasets (4d)
----------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4e)
-------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect similarityTest fastCosineSimilarity

-- 12 cases passed (63.0%) --


Your submission token id is 1119947-128c0b1f4b7a6593dfa577a72225b38d:ed0dadf5ce804d84b09ed128db599f31:ip-172-31-8-111
Please include this submission token id when you need support for your code submission.

But the notebook seems fine. As examples, here are screenshots for 2c, 3b and 3c.



And lastly


Has anyone had similar issues? I must be doing something silly. Thanks well in advance for your help...

V.
 I pass all the test when running my iPython notebook under Jupiter but il all fails when submitting it to the autogrador with following messages.

thnak you for your help

Your submission token id is 1097600-5e17d344c06ca1a82f6d86e97ec98ef2:d58029f4b351b72bd714cd78dba7d11b:ip-172-31-8-188
Please include this submission token id when you need support for your code submission.

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 481
SyntaxError: Non-ASCII character '\xc3' in file /ok/submission.py on line 482, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'simpleTokenize' is not defined

Removing stopwords (1b)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tokenize' is not defined

Tokenizing the small datasets (1c)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'totalTokens' is not defined

Amazon record with the most tokens (1d)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'biggestRecordAmazon' is not defined

Implement a TF function (2a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tf' is not defined

Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'corpusRDD' is not defined

Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueTokenCount' is not defined

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'rec_b000hkgj8k_weights' is not defined

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dp' is not defined

Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsSmallBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonFullRecToToken' is not defined

Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsFullCount' is not defined

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonNormsBroadcast' is not defined

Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'invert' is not defined

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'commonTokens' is not defined

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similaritiesFullRDD' is not defined

-- 0 cases passed (0.0%) --


Your submission token id is 1097600-5e17d344c06ca1a82f6d86e97ec98ef2:d58029f4b351b72bd714cd78dba7d11b:ip-172-31-8-188
Please include this submission token id when you need support for your code submission. I am currently persuing my B.Tech in computer science & engineering (4th year)I want to learn more about SPARK and learn better by building projects...........So how do i approach from here??Please help me... Thank you all for your help. HI 
need help with this exercise , i just spent too much time trying to solve it 
when i get the right result 
[('rom', 'b000jz4hqo'), ('clickart', 'b000jz4hqo'), ('950', 'b000jz4hqo'), ('image', 'b000jz4hqo'), ('premier', 'b000jz4hqo'), ('000', 'b000jz4hqo'), ('dvd', 'b000jz4hqo'), ('broderbund', 'b000jz4hqo'), ('pack', 'b000jz4hqo')]when running the test it gives me an error TypeError                                 Traceback (most recent call last)
<ipython-input-162-8accd0664df3> in <module>()
     21 amazonInvPairsRDD = invert(amazonWeightsRDD.take(1))
     22 
---> 23 invertedPair = invert((1, {'foo': 2}))

<ipython-input-162-8accd0664df3> in invert(record)
     11         #print dict(zip(line[1], [line[0]]*len(line[1])))
     12     for line in record:
---> 13         for c in line[1]:
     14             print c + ',' + line[0]
     15             pairs.append((c,line[0]))

TypeError: 'int' object has no attribute '__getitem__ each is taking 14 mins

#this is how broadcasting is done 
idfsSmallBroadcast = sc.broadcast(idfsSmallWeights)

# this is how similarity broadcast is calulated
similaritiesBroadcast = (crossSmall .map(computeSimilarityBroadcast) .cache())

# this is how it is used computeSimilarityBroadcast
cs = cosineSimilarity(googleValue,amazonValue, idfsSmallBroadcast.value)

#In 2f this is what was done
idfsSmallWeights = idfsSmall.collectAsMap()

28 mins for 3c and 3d and looks like this is why autograder throwing time out ...need assistance to understand what is going wrong 
  Hi,

I spent like 6 hours in this i m not sure what to do for the given instruction.

sims RDD is of the form [('amazonId googleURL', score)   ('amazonId googleURL', score).......]
goldRDD is of the form [('amazonId googleURL', gold)   ('amazonId googleURL', gold).......]
simbroadcast is of the form [('amazonId googleURL' , score)     ('amazonId googleURL' , score)]
Create a new nonDupsRDD RDD that has the just the cosine similarity scores for those "AmazonID GoogleURL" pairs from the similaritiesBroadcast RDD that do not appear in both the sims RDD and gold standard RDD
Can any one suggest the steps?

Regards,A My code results in this error-

[('b000o24l3q', 1547)]




---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-30-7e008f4bfd6c> in <module>()
     14 print biggestRecordAmazon
     15 print 'The Amazon record with ID "%s" has the most tokens (%s)' % (biggestRecordAmazon[0][0],
---> 16                                                                  len(biggestRecordAmazon[0][1]))

TypeError: object of type 'int' has no len()Iam using this in the code - eturn vendorRDD.takeOrdered(1,lambda x: -x[1])        biggestRecordAmazon = findBiggestRecord(amazonRecToToken)print biggestRecordAmazonPlease can someone help

 "
Requested similarity is 0.
"

This is the message I keep getting in 3c. I donot know where I am going wrong. All the tests have been passed. I was getting division by zero error and made corresponding changes and went through all tests. All tests have been passed but I keep getting similarity as zero. Can someone give me some idea where I might be going wrong? Hi All

This the output when I print daysWithHosts and dalyHosts respectively:

This is the same as the grader expects, but it fails both?

[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
[2582, 3222, 4190, 2502, 2537, 4106, 4406, 4317, 4523, 4346, 2864, 2650, 4454, 4214, 4340, 4385, 4168, 2550, 2560, 4134, 4456] Hi all,

by this stage I am more familiar with spark than I am with python, and so I am quite stuck with this exercise.

I have tried:

tfIdfDict = {token: v for v in tfs*idfs(tokens)}

but it isn't working. All my previous tests have passed, so tf is correctly defined. Can anyone give me some explicit examples on how to do this and tell me explicitly exactly what I am doing wrong? thanks! For the lab 3 2a, I have the right answer, but not the right format for the tests :
[('brown', 0.16666666666666666), ('lazy', 0.16666666666666666), ('jumps', 0.16666666666666666), ('fox', 0.16666666666666666), ('dog', 0.16666666666666666), ('quick', 0.16666666666666666)]instead of {'brown': 0.16666666666666666, 'lazy': 0.16666666666666666,
                             'jumps': 0.16666666666666666, 'fox': 0.16666666666666666,
                             'dog': 0.16666666666666666, 'quick': 0.16666666666666666}Any idea ? For common token I am getting the following

(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), '120')
instead of

(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120'])

and I am getting

Key error 1 in fastCosineSimilarity.

I like to know whether the following lines are correct?

s = sum([amazonWeightsBroadcast.value.get(amazonRec,0.0)[t] * googleWeightsBroadcast.value.get(googleRec,0.0)[t]] for t in tokens)
value = s/((amazonNormsBroadcast.value.get(amazonRec,0.0))*(googleNormsBroadcast.value.get(googleRec,0.0))) 

Help me to solve this issue. I tried many options but I failed.





 Hi, I am getting an error in the total count of Tokens for the 1c Tokenizing small datasets. It should get 22520, but I got 22566, which indicates that there might be an error with my tokenize function.

def tokenize(string):
    """ An implementation of input string tokenization that excludes stopwords
    Args:
        string (str): input string
    Returns:
        list: a list of tokens without stopwords
    """
    st = re.findall(r"[\w']+", string.lower())
    return #loop for excluding the words, this one works
Maybe something wrong with the findall function?

Thanks I've been going over this problem for many hours now and seem to be running into a dead-end.  In computing the similarity between Amazon record b000o24l3q and Google record http://www.google.com/base/feeds/snippets/17242822440574356561, I keep getting a value of 0.0 having traced through the entire process multiple times.  Looking at the actual tokenized documents, I notice that there are almost no common words between the two documents.  Anyone else experiencing the same problem?  Any hints I might have missed?  Thanks in advance for any assistance. Stuck on this particular code section. The psuedo code I am trying to implement is as follow : 

1) get a day, host pair.
2) aggregate by day. ex [(1,22345),(2,55446)]
3) Join on the unique host RDD calculated in previous exercise for example [(1,(22345,300)),(2,(55446,378))]

The problem is , I just cant find how to divide the total number and count in the tuple (1,(22345,300)) to get the average. Please suggest a hint. 

Please DO NOT post answer. Thanks in advance How do I save the top 100 rows of an rdd to a text file?

Lets say that I have an RDD (rdd) with 1000 rows

1. To save an rdd as a text file I can:
rdd.saveAsTextFile('/path/filename')

2. if I use take(100) I can't save:
top_100 = rdd.take(100)

However I can't save top_100 using:
top_100.saveAsTextFile('/path/filename')

Does anyone have a solution?
 I've just completed lab 3! It was really hard to store all these RDDs in mind! And now I feel great satisfaction cause the job is done. 

Positives:
Good exercise with python dict manipulations.I've read about Spark RDD's methods in manual yet some times, now most of them in my memory.
Negatives:
I spent a lot of time trying to understand lab explanations.Sometimes it was necessary to reboot VM and re-execute all cells to make Spark finish evaluations. How much time do they take to in verification purpose ? It's been close to 24hrs and mine still shows pending. Here's an example to illustrate my point
kl = sc.parallelize([('a', '1'), ('a','3'), ('a','5'),('b',3)])
I want an RDD from this, where all values of identical keys are combined in a list
this works fine:
ka = kl.map(lambda (x,y) :(x, [y,])).reduceByKey(lambda a,b:a+b)
However, when I try the following
ka = kl.combineByKey(lambda x:[x,], # createCombiner                     lambda alist, avalue : alist.append(avalue), #mergeValue                     lambda alist, blist : alist + blist #mergeCombiner                     )
A 'NoneType has no attribute append' error is all I get

Could anybody please assist with this
Thanks in anticipation 


We will reuse your code from above to compute norms of the IDF weights for the complete combined dataset. The steps you should perform are:
Create two collections, one for each of the full Amazon and Google datasets, where IDs/URLs map to the norm of the associated TF-IDF weighted token vectors.Convert each collection into a broadcast variable, containing a dictionary of the norm of IDF weights for the full dataset





In [39]:

Edit:

Norm wasn't defined. It's really confusing as I had to dig through the error to find that it wasn't defined and literally wasted hours on it.
















# TODO: Replace <FILL IN> with appropriate code
amazonNorms = amazonWeightsRDD.map(lambda l: (l[0],norm(l[1])))
amazonNormsBroadcast = sc.broadcast(amazonNorms.collectAsMap())
googleNorms = googleWeightsRDD.map(lambda l: (l[0],norm(l[1])))
googleNormsBroadcast = sc.broadcast(googleNorms.collectAsMap())

















---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-39-a44d2edad028> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
      2 amazonNorms = amazonWeightsRDD.map(lambda l: (l[0],norm(l[1])))
----> 3 amazonNormsBroadcast = sc.broadcast(amazonNorms.collectAsMap())
      4 googleNorms = googleWeightsRDD.map(lambda l: (l[0],norm(l[1])))
      5 googleNormsBroadcast = sc.broadcast(googleNorms.collectAsMap())

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collectAsMap(self)
   1443         4
   1444         """
-> 1445         return dict(self.collect())
   1446 
   1447     def keys(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args




 Create a new nonDupsRDD RDD that has the just the cosine similarity scores for those "AmazonID GoogleURL" pairs from the similaritiesBroadcast RDD that do not appear in both the sims RDD and gold standard RDD.
Can someone give a hint for this question? I am trying to solve 3e I am not able to understand the following

Combine the sims RDD with the goldStandard RDD by creating a new trueDupsRDD RDD that has the just the cosine similarity scores for those "AmazonID GoogleURL" pairs that appear in both the sims RDD and goldStandard RDD. Hint: you can do this using the join() transformation.¶

I know we have to use join but how do I apply condition of matching keys from both rdd?

trueDupsRDD = (sims.join(goldStandard)).filter(???)





 Help ! i spend 8 hours trying to figure out how to create the swap function ... 
no luck .... i got a good result but i didnt include the list of tokens was just an RDD of ID,URL   










# TEST Compute Norms for the weights from the full datasets (4c)
Test.assertTrue(isinstance(amazonNormsBroadcast, Broadcast), 'incorrect amazonNormsBroadcast')
Test.assertEquals(len(amazonNormsBroadcast.value), 1363, 'incorrect amazonNormsBroadcast.value')
Test.assertTrue(isinstance(googleNormsBroadcast, Broadcast), 'incorrect googleNormsBroadcast')
Test.assertEquals(len(googleNormsBroadcast.value), 3226, 'incorrect googleNormsBroadcast.value')












I don't know if there is a solution to this :/. The other two tests are passing.




---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-45-1095dbd1bf65> in <module>()
      1 # TEST Compute Norms for the weights from the full datasets (4c)
----> 2 Test.assertTrue(isinstance(amazonNormsBroadcast, Broadcast), 'incorrect amazonNormsBroadcast')
      3 Test.assertEquals(len(amazonNormsBroadcast.value), 1363, 'incorrect amazonNormsBroadcast.value')
      4 Test.assertTrue(isinstance(googleNormsBroadcast, Broadcast), 'incorrect googleNormsBroadcast')
      5 Test.assertEquals(len(googleNormsBroadcast.value), 3226, 'incorrect googleNormsBroadcast.value')

NameError: name 'Broadcast' is not defined

 



 Hi I was just wondering when the course's final deadline would be. Trying to figure out whether to sign up to the verified version and whether I would have time to go through all the work required. Hi,

When I look at the IDF Histogram I can't understand the relation between this graph and the data. can somebody explain what values are related to x-axis and y-axis ? Why do we use dictionary in dot product?
Isn't dictionary unordered in memory?
Maybe I'm wrong but dict {'a': 1, 'b': 2, 'c': 3} does not have to be ordered that way while we call it.
But I think this is crucial in dot product
 I am stuck at 4e for a long time. I am writing the following code

<SNIP honor code violation> .reduceByKey(lambda x,y: x + ' ' + y) .map(lambda x: (x[0], x[1].split()))

and getting the following error

[('r', 'b000jz4hqo'), ('c', 'b000jz4hqo'), ('9', 'b000jz4hqo')]
[('q', 'http://www.google.com/base/feeds/snippets/11125907881740407428'), ('2', 'http://www.google.com/base/feeds/snippets/11125907881740407428'), ('l', 'http://www.google.com/base/feeds/snippets/11125907881740407428')]
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-48-68cdbd79d0b2> in <module>()
     24  #               .cache())
     25 
---> 26 print 'Found %d common tokens' % commonTokens.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 111.0 failed 1 times, most recent failure: Lost task 1.0 in stage 111.0 (TID 509, localhost): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:172)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:108)
<p><p></p></p> Hello,
I'm getting stuck on the following explanation. Someone could help me?
Define a worker function that given an element from the combination RDD computes the cosineSimlarity for the two records in the element

Thanks for help Hi everybody, I'm getting stuck with question 4f when it come to appling the fastCosineSimilarity as a lambda.

Here is what I'm trying to do :

First run the function alone on a subset of commonTokens (the same used for verification in the next cell)

filteredTokens = commonTokens.filter(lambda x: (x[0][0] == 'b00005lzly' and                                                 x[0][1] == 'http://www.google.com/base/feeds/snippets/13823221823254120257'))print filteredTokens.take(1)print fastCosineSimilarity(filteredTokens.take(1))

The output is :

[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), ['data', 'complete', 'includes', 'software'])]
amazonNorm: 115.566180222
googleNorm: 389.96983923
(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), 4.286548413995203e-06)
It's all right the function returns the expected value.

But when I do :

similaritiesFullRDD = (filteredTokens                           .map(fastCosineSimilarity)                       .cache()                      )print similaritiesFullRDD.count()

I get this

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1691.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1691.0 (TID 2410, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-196-258c7622053b>", line 30, in fastCosineSimilarity
KeyError: 'b'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
<p></p>
<p>Any toughts ?</p> I need more hint about commonTokens...
I am also wondering amazonInvPairsRDD.count() = 111387
googleInvPairsRDD.count() = 77678

Why commonTokens.count() is 2441100??
 I would like to download some data into the VM created for this course and write a Spark program to process the data.
Am able to log into the VM by typing "vagrant ssh". How can I copy files into the "data" folder?

Thanks I'm also getting adobe :-( instead of software as smallest value
 Solved the problem. So there I was, stuck on 1b, reading through a bunch of threads about list comprehension and wanting to use that as a solution. 

One thing that went right over my head was that I still needed to use what was returned in 1a in order to implement the list comprehension in 1b. I started out with @3039 and then posted my frustration with not being able to understand why my list was returning characters using:

return [w for w in string if not w in stopwords]
the return was: 
['A', ' ', 'q', 'u', 'c', 'k', ' ', 'b', 'r', 'o', 'w', 'n', ' ', 'f', 'o', 'x', ' ', 'j', 'u', 'm', 'p', ' ', 'o', 'v', 'e', 'r', ' ', 'h', 'e', ' ', 'l', 'z', 'y', ' ', 'd', 'o', 'g', '.']
So it looks like i was filtering out stop words like 'I' and 'a'... (and oddly enough 't' and 'ed')

At any rate, @gokul very eloquently pointed out that a string does not *automatically* get split in to words... Rereading it a couple of times made me realize that what he meant was that a bunch of characters doesn't automatically get compiled into a string, meaning I needed to do that work before the list comprehension piece would work. We've already done that work in 1a. I just needed to add that part to my list comprehension logic, and now,  KABOOM! works like a charm.    I find some ambiguities in the statement of the problem we are trying to solve:
 
"Create a fastCosinesSimilarity function that takes in a record consisting of the pair ((Amazon ID, Google URL), tokens list) and computes the sum for each of the tokens in the token list of the products of the Amazon weight for the token times the Google weight for the token. "

Since the dictionaries created from amazonWeightsRDD and googleWeightsRDD are by document ID/URL, information on a token can only be easily retrieved for the specific document ID/URL referenced in the record being processed.  Is that what is desired from the part of the problem definition " computes the sum for each of the tokens in the token list of the products of the Amazon weight for the token times the Google weight for the token"  to use the tokens' weights for the specific document ID/URL in the record being processed by the function fastCosineSimilarity()?  

The wording of the problem statement is ambiguous as to whether the weights involved in the calculation are specific to the document ID/URL for this specific record being processed or should be calculated globally across all documents. Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 1131577-8d94a95482cc2f0a344745cb9165f022:a149730c2587444acdcb087be56e08ce:ip-172-31-8-113
Please include this submission token id when you need support for your code submission.
 Fantastic!

I am able to get the tokens.  However, I get a total of 22,493 and the answer is 22,520.
Missing 28 tokens.
The regext and the tokenizer passed the tests

Anyone getting this issue Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
All tests passed
Implement an IDFs function (2c)
-------------------------------
All tests passed
Implement a TF-IDF function (2f)
--------------------------------
All tests passed
Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
test 1a fail in autograder, while the sebsequent tests passes. the simpleTokenize works fine in my notebook, any suggestion? Working on Lab 3, problem 1(c) and I can't pass it.

I'm getting 33414 tokens both by using a flatMap then count method and by using a map then reduce method. Clearly I feel like my tokenize function is wrong but it passed the tests. I can also pass 1(d).

I viewed a couple of the entry's from Amazon using taken. My only concern is that tokenize might be splitting some words with weird punctuation such as 'v11.1' into 'v11' '1'. Any advice? My commontoken is<SNIP Honor code violation>

I need to get
commonTokens.take(1) or record:
[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120'])]

but I am getting

commonTokens.take(1) or record:[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), '120')]

How to convert '120' to ['120']?
I completed everything. Because of this issue I am unable to submit my solution. I am doing 4f and I get this stack print out which says RDD is empty, not sure why. Would this stack print out be a help, can't see much myself. Thanks very much.


---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
 in ()
     42                        .map(fastCosineSimilarity)
     43                        .cache())
---> 44 print similaritiesFullRDD.first()
     45 print similaritiesFullRDD.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in first(self)
   1240         ValueError: RDD is empty
   1241         """
-> 1242         rs = self.take(1)
   1243         if rs:
   1244             return rs[0]

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 116.0 failed 1 times, most recent failure: Lost task 0.0 in stage 116.0 (TID 386, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "", line 34, in fastCosineSimilarity
KeyError: '1'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) Why is there no test for cossim()?  I know it is straightforward but its also the function we use going forward in 3b.

Does anyone have a test for cossim they already tested on a correct implementation?  thx Conceptually is this correct?

amazonWeightsRDD  elements are tuples.  The first element being the url/id the second is a dictionary.

After our transformation we want back another tuple with the url/id and norm(of dictionary)  Right?  This should be easy. I have problem in  4e and I don't know what to resolve it.

<REDACTED>


this is an exception:
ypeError                                 Traceback (most recent call last)
<ipython-input-93-dc850ccecedf> in <module>()
     31 googleRec = a[0][1]
     32 tokens = a[1]
---> 33 s = sum( [(amazonWeightsBroadcast.value[amazonRec][token] * googleWeightsBroadcast.value[googleRec][token])] for token in tokens)
     34 value = s/((amazonNormsBroadcast.value[amazonRec])*(googleNormsBroadcast.value[googleRec]))
     35 key = (amazonRec, googleRec)

TypeError: unsupported operand type(s) for +: 'int' and 'list'

Can somebody explain me the error and how to fix it ? Your submission token id is 1117699-be768b4cc6fc662d81cb7969cda225da:9f4a76ef2d8050d505ee1bb542e12d68:ip-172-31-8-109


I have already tried 4 times... Could anyone help me submit my codes? Thanks. I have worked all week on this and spent many hours trying to debug 4f with print statements everywhere. Using individual commonToken values seems to work fine ... tokens are parsed fine and look-ups on dictionaries seem correct, etc..  I am, however, getting KeyErrors when trying to use the full commonTokens data set.  Prior count tests have passed. I am left believing that in some way the content of a dictionary is not correct and this is frustrating because it now calls into question the enormous amount of work I have done just to get to 4f.I have broadcast variable amazon and google dictionaries of the form:{'b000jz4hqo': {'rom': 2.4051362683438153, 'clickart': 56.65432098765432, '950': 254.94444444444443, 'image': 3.6948470209339774, 'premier': 9.27070707070707,'000': 6.218157181571815, 'dvd': 1.287598204264871, 'broderbund': 22.169082125603865, 'pack': 2.98180636777128}}So a dictionary (call it d2), nested within a dictionary (d1).I have checked that there are no dictionary keys in d1 which are amazon IDs or google IDs corresponding the key value reported in the "KeyError" message. For example, one case was KeyError 'b'. In fastCosineSimilarity I essentially blocked this key from being used as a lookup in both the amazon and google Weights/Norms dictionaries i.e. don't allow this key to be used as a lookup in d1 or amazon.value['b'][ ]. In doing so, my next run gave a KeyError '1'. So, the KeyError would then seem to be coming from the d2 lookup . I have no idea how I could get incorrect entries for the d2 dictionary. (btw - I checked the keys in the norms dictionary and the counts for key 'b' were zero ).  The d2 dictionary takes me back to 4b where I map the weightsRDDs of k,v to k, tfidf(v,idfsFullBroadcast.value) . I am just at a complete loss at this point after many many hours. If there is anyone who has some magical insight I would greatly appreciate it. Thx. How do you apply combineByKey to create a list of tokens, but not count them?

I use reduceByKey and i am getting this 
[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), '120')] can someone give me a explanation on on to get those values my current try is this:

nonDupsRDD = sims.subtractByKey(goldStandard).union(goldStandard.subtractByKey(sims))
nonDupsRDD.take(5)

but i get a Error trying for different types

TypeError: unsupported operand type(s) for -: 'PipelinedRDD' and 'float


thanks
RF I'm getting an incorrect result in 3c, Is there a hint someone could provide? thanks

I'm getting 
Requested similarity is 0.000720187336971.

Sorry for providing so little info, I didn't know if perhaps someone else had gotten the same result. It's a little tricky to post enough info without posting too much info. 
In 3c as the steps outline, I'm creating crossSmall by combining the other 2 Small RDDs.
Then in computeSimilarity, I'm pulling out the parts of the Pair tuple (which are also in tuples) to pass Values to cosineSimilarity.
To come up with similarities, I'm doing a transformation over the crossSmall RDD to call the computeSimilarity function for each of the RDD elements. 
If you need more info to know how to help, please let me know what I can provide. Thanks again.

I was looking back at 3a at cossim, dotprod and norm functions. 
I added the extra print to check the output of cossim and it looked right, based on this post -  https://piazza.com/class/i9esrcg0gpf8k?cid=3350  I initially implemented 4e using groupByKey then realized it's not quite efficient. So I reimplemented 4e using reduceByKey which significantly improved the performance: 2.3 minutes vs 1.0 minutes on my PC. The stats attached below also show rBK has much less shuffles than gBK does.
 

 Now I know I did not get the submission ID.

Do I need to submit again?

Thanks. Hi,

Getting a list within list when I run the following command.
uniqueTokens = corpusRDD.map(lambda x:(x[1]))

Example: [['clickart','950'....]]

How do I get an RDD containing just ['clickart','950'...]?

 If I accidentally used print for long list, Lab notebook page hangs. I tried killing the pages, restarting vagrant. It didn't fix. Kernel continues to be busy. I am using chrome browser.

Any fix for this issue? Hi Guys, 

I am also stuck on lab3 2c, but I think I am almost. Hope my thoughts would help others and hope those who passed could share some hints. 

Here are my steps (not actual code):

    N: count corpus
    
    uniqueTokens: flatMap, list and set together
    
    tokenCountPairTuple: map(lambda a: (a,1))
    
    tokenSumPairTuple: reduceByKey(lambda a,b : (a+b))
    
    return: tokenSumPairTuple.map(lambda a, b: (a, float(N) / b))
and I get a huge error when running, which core part is follow:

An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 250.0 failed 1 times, most recent failure: Lost task 0.0 in stage 250.0 (TID 800, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):

I guess I could mixed up python and spark objects agains, so please share some hints. :)

BTW my tokenSumPairTuple.take(10) before return and apply idfs are:

[('aided', 1),
 ('precise', 4),
 ('duplex', 1),
 ('dance', 1),
 ('breath', 2),
 ('themes', 3),
 ('known', 4),
 ('verses', 1),
 ('battle', 4),
 ('9999', 1)]
Thanks again!  I am returning the following for trueDupsRDD, can somebody confirm if this is correct?
'b0009jlux8 http://www.google.com/base/feeds/snippets/4923451909983851979', (0.036762553443747806, 'gold')

I feel like it shouldn't be nested and 'gold' shouldn't be there.

 I passed 4a, and when I run 4b I got the following:

Can someone please guide me how to get 17078 for idfsFullCount?

There are 4772 unique tokens in the full datasets.
There are 1363 Amazon weights and 3226 Google weights.1 test failed. incorrect idfsFullCount
1 test passed.
1 test passed. returns a list of pairs of (token, ID/URL).

So this should be a list of tuples?  I am passing the test for number of elements but I am failing the first test. Hello TA's

I submitted my lab 3 assignment and got the below response. Could you please check and see if anything is wrong
Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
All tests passed
Implement an IDFs function (2c)
-------------------------------
All tests passed
Implement a TF-IDF function (2f)
--------------------------------
All tests passed
Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
All tests passed
Perform Entity Resolution (3c)
------------------------------
All tests passed
Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
All tests passed
Perform a Gold Standard evaluation (3e)
---------------------------------------
All tests passed
Tokenize the full dataset (4a)
------------------------------
All tests passed
Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
All tests passed
Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
All tests passed
Create inverted indicies from the full datasets (4d)
----------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4e)
-------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4f)
-------------------------------------------------
All tests passed
-- 19 cases passed (100.0%) --


Your submission token id is 1140850-b941c6ad146ec282b595ed4ace54a96a:ac4a453109a0c3b7d8fa363726d38bbf:ip-172-31-23-148
Please include this submission token id when you need support for your code submission.
 their expected structure, values?
Reading quite a bit of posts but there is no clear answer for this item.
What do we suppose to construct it?
Many thanks. My question is that, the mentioned timeline is Jun 26 00:00 UTC (which is alreary over). What will be the case, if I submit the assignments now (Do I get 20% penalty ? ) or by detault we would be getting 3 days grace period, so that my submission would be still acceptable.

Start the Lab 3 Text Analysis and Entity Resolution Exercise. The lab is due June 26, 2015 at 00:00 UTC with an automatic 3-day grace period to the following Monday at 00:00 UTC (one minute after Sunday at 23:59 UTC). 

My general question is that, which score would be considered (before the deadline submission) ? or the recent score (if it is missed the deadline and got less than the previous one) I want to confirm the meaning of document.
U is set of document, mean we have 2 docs {amazon, google}.
Thus the total # of docs N in U is 2? amazon and google?

So the way to recognize the doc is by the key?

Thanks

ram I implement the function
def cosineSimilarity(string1, string2, idfsDictionary):
       w1 = tfidf(string1, idfsDictionary)


cossimAdobe = cosineSimilarity('Adobe Photoshop',
                                                   'Adobe Illustrator',                                                   idfsSmallWeights)
print cossimAdobe

and I got the following error
KeyError                                  Traceback (most recent call last)
<ipython-input-30-0dae1368ac35> in <module>()
     15 cossimAdobe = cosineSimilarity('Adobe Photoshop',
     16                                'Adobe Illustrator',
---> 17                                idfsSmallWeights)
     18 
     19 print cossimAdobe
<ipython-input-22-e0f66a4c75f2> in <dictcomp>((k, v))
      9     """
     10     tfs = tf(tokens)
---> 11     tfIdfDict = {k: v * idfs[k] for k, v in tfs.iteritems()}
     12     return tfIdfDict
     13 

KeyError: 'A'

Seems that the dictionary can not find a key, so the dictionary idfsSmallWeights may not have the key 'Adobe'. The word 'Adobe' is not in the corpus that creates the IDF dictionary. And my IDF dictionary does not know how to deal with the case where no key is found. How to solve this? I'm a consultant based in London.

Anybody fancy a discussion about the course over a pint? Hi ... I get the rights counts for amazon and google inverted pairs in 4d, but when I join, sway and group by key in 4f I do not get the correct number of commonTokens.

Any help would be appreciated, thx. Is anyone facing edx login issues right now. When I open the browser console, I see cross-origin policy bugs from edx servers.

I reported the issue to edx. Just wanted to find out if others are facing the same issue.
I did the following:

Logout > Clear browser > Try to login again. Can someone post the correct output for swap for the first 5 rows? Mine is below.

[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120']), (('b000bnb72g', 'http://www.google.com/base/feeds/snippets/6959766318020460134'), ['software']), (('b00009apna', 'http://www.google.com/base/feeds/snippets/13986409474793202514'), ['business']), (('b00004ochi', 'http://www.google.com/base/feeds/snippets/7147648211015076337'), ['rom']), (('b000gcgqvy', 'http://www.google.com/base/feeds/snippets/6874875179525744781'), (['win'], ['xp']))]

Notice the last line, I believe (['win'], ['xp']) is incorrect. I'm not positive but I think this should be (['win', 'xp']). I'm also not sure how to get that as the output. I've run into low RAM issues in several parts of Lab3.

In retrospect I would have been happy to pay a nominal fee for access to a Databricks cluster.

Perhaps it could be an option for the next iteration of the course. Going into the last instruction I have something like this:

[(['b0007yepy6', 'b000hlt5j4', 'b000dz9yoa', 'b00004ochi'], 'aided')]
Finally, create an RDD consisting of pairs mapping (ID, URL) to all the tokens the pair shares in common.  Can somebody explain what this means in english?


 Can sombody explain in english what we are suppose to do for his 4E.

I just can't understand the language that is used in the description?

Thanks.

 Hi all. Whenever I have to sort an RDD consisting of (key,value) pairs I always first flip it ( (a,b) -> (b,a) ), then use sortbyKey, then put it back in the right order and it just seems like such an inefficient way... Any tips?

Thanks a lot! Hi all,
when i'm doing join between amazonInvPairsRDD and googleInvPairsRDD, then map it with swap function and do groupByKey(). the kernel hangs down and doesn't produce out put. what am i doing wrong here? a hint will be helpful.Thanks  I consistently get this error when trying to invoke the line `idfsSmallBroadcast = sc.broadcast(idfsSmallWeights)` from 3.3d.

I have tried re-executing all cells top-down, restarting the kernel, restarting the kernel and then re-executing the cells, still the error persists. All earlier calls to `sc` like `sc.textFile` seem to be alright, but only before I try to execute 3.3d. Afterwards, they also produce the same error complaining about 'float' not having the 'broadcast' or `textFile` attribute.

I am not introducing any new variables, so the sc should not be overridden.

Anybody else experiencing the error? Any hints to a solution?

The full error message:

AttributeError                            Traceback (most recent call last)
<ipython-input-28-a69aa7cf18c6> in <module>()
     16     return (googleURL, amazonID, cs)
     17 
---> 18 idfsSmallBroadcast = sc.broadcast(idfsSmallWeights)
     19 similaritiesBroadcast = (crossSmall
     20                          .map(computeSimilarityBroadcast)

AttributeError: 'float' object has no attribute 'broadcast'
 We need a mapping from (ID, URL) to token, so create a function that will swap the elements of the RDD you just created to create this new RDD consisting of ((ID, URL), token) pairs.

Would a better way to say this be insert the workd 'iterable' ? ie:
(iterable(ID, URL), token Hello,

I have spent hours on this part to no avail. I have read all the threads including the pinned one @2967 and cannot figure out what i am doing wrong.

Here is what i have:

amazonWeightsBroadcast = sc.broadcast(amazonWeightsRDD.collectAsMap())googleWeightsBroadcast = sc.broadcast(googleWeightsRDD.collectAsMap())
    amazonRec = record[0][0]    googleRec = record[0][1]    tokens = record[1]        s = sum(amazonWeightsBroadcast.value[amazonRec][t] * googleWeightsBroadcast.value[googleRec][t] for t in tokens)         value = s / ((amazonNormsBroadcast.value[amazonRec]) * googleNormsBroadcast.value[googleRec])

similaritiesFullRDD = (commonTokens .map(lambda x: fastCosineSimilarity(x)) .cache())

I get this really long error message that I cannot decipher:

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-101-5f7b5c1dd17f> in <module>()
     25                        .cache())
     26 
---> 27 print similaritiesFullRDD.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 315.0 failed 1 times, most recent failure: Lost task 0.0 in stage 315.0 (TID 1942, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-101-5f7b5c1dd17f>", line 24, in <lambda>
  File "<ipython-input-101-5f7b5c1dd17f>", line 18, in fastCosineSimilarity
TypeError: unsupported operand type(s) for *: 'dict' and 'dict'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
<p></p>
<p>I have spent far too much time on this lab and I have found the instructions to be quite convoluted as have many others. </p>
<p></p>
<p>I have passed all the other tests and am stuck on this last one. Any help would be appreciated. </p>
<p></p>
<p>Thanks in advance.</p> Hi friends,

I could not figure out, what to be done in Lab -3 4(d) . I followed "Create an invert function that given a pair of (ID/URL, TF-IDF weighted token vector), returns a list of pairs of (token, ID/URL). Recall that the TF-IDF weighted token vector is a Python dictionary with keys that are tokens and values that are weights."  and jumped in to the code to achieve. I tried the below to create an inverted list .

def invert(record):
    pairs = list() #create a blank list
    for i in range (0,len(record)-1): #iterate the list starting from 0 to the len(record)
        for k in record[i][1]:		# get the dictionary from each records corresponding value
            pairs.append(({k:record[i][1][k]},record[i][0])) #append to the list with the dictionary 								#and record key
    return pairs

invert(amazonWeightsRDD.collect())

amazonWeightsRDD.collect()  :-------------------------------------[('b000jz4hqo', {'rom': 2.4051362683438153, 'clickart': 56.65432098765432, '950': 254.94444444444443, 'image': 3.6948470209339774, 'premier': 9.27070707070707, '000': 6.218157181571815, 'dvd': 1.287598204264871, 'broderbund': 22.169082125603865, 'pack': 2.98180636777128}), ('b0006zf55o', {'laptops': 11.588383838383837, 'desktops': 12.74722222222222, 'computer': 0.6965695203400122, 'backup': 2.8015873015873014, 'win': 0.501859142607174, 'ca': 9.10515873015873, 'v11': 50.98888888888888, '30u': 84.98148148148148, '30pk': 254.94444444444443, 'desktop': 2.23635477582846, '1': 0.3231235037318687, 'arcserve': 24.28042328042328, 'associates': 7.284126984126985, 'oem': 46.35353535353535, 'international': 9.44238683127572, 'lap': 127.47222222222221}), ('b00004tkvy', {'case': 5.28078250863061, 'center': 6.953030303030303, 'noah': 208.5909090909091, 'jewel': 7.192789968652038, 'multimedia': 7.070878274268105, 'ages': 7.871355060034306, '3': 0.6964638033085445, 'victory': 34.765151515151516, 'activity': 10.175166297117517, '8': 1.2641873278236915, 'ark': 208.5909090909091})]

invert(amazonWeightsRDD.collect())  :-----------------------------------------------------[({'rom': 2.4051362683438153}, 'b000jz4hqo'), 
({'clickart': 56.65432098765432}, 'b000jz4hqo'), 
({'950': 254.94444444444443}, 'b000jz4hqo'), 
({'image': 3.6948470209339774}, 'b000jz4hqo'), 
({'premier': 9.27070707070707}, 'b000jz4hqo'), 
({'000': 6.218157181571815}, 'b000jz4hqo'), 
({'dvd': 1.287598204264871}, 'b000jz4hqo'), 
({'broderbund': 22.169082125603865}, 'b000jz4hqo'), 
({'pack': 2.98180636777128}, 'b000jz4hqo'), 
({'laptops': 11.588383838383837}, 'b0006zf55o'), 
({'desktops': 12.74722222222222}, 'b0006zf55o'), 
({'computer': 0.6965695203400122}, 'b0006zf55o'), 
({'backup': 2.8015873015873014}, 'b0006zf55o'), 
({'win': 0.501859142607174}, 'b0006zf55o'), 
({'ca': 9.10515873015873}, 'b0006zf55o'), 
({'v11': 50.98888888888888}, 'b0006zf55o'), 
({'30u': 84.98148148148148}, 'b0006zf55o'), 
({'30pk': 254.94444444444443}, 'b0006zf55o'), 
({'desktop': 2.23635477582846}, 'b0006zf55o'), 
({'1': 0.3231235037318687}, 'b0006zf55o'), ({'arcserve': 24.28042328042328}, 'b0006zf55o'), ({'associates': 7.284126984126985}, 'b0006zf55o'), ({'oem': 46.35353535353535}, 'b0006zf55o'), ({'international': 9.44238683127572}, 'b0006zf55o'), ({'lap': 127.47222222222221}, 'b0006zf55o'), ({'case': 5.28078250863061}, 'b00004tkvy'), ({'center': 6.953030303030303}, 'b00004tkvy'), ({'noah': 208.5909090909091}, 'b00004tkvy'), ({'jewel': 7.192789968652038}, 'b00004tkvy'), ({'multimedia': 7.070878274268105}, 'b00004tkvy'), ({'ages': 7.871355060034306}, 'b00004tkvy'), 
({'3': 0.6964638033085445}, 'b00004tkvy'), ({'victory': 34.765151515151516}, 'b00004tkvy'), 
({'activity': 10.175166297117517}, 'b00004tkvy'), ({'8': 1.2641873278236915}, 'b00004tkvy'), 
({'ark': 208.5909090909091}, 'b00004tkvy'), ({'restore': 1.2044619422572178}, 'b000g80lqo'), ({'sage': 3.4413198350206224}, 'b000g80lqo'), ({'businesses': 0.6339273380301147}, 'b000g80lqo'), ({'solutions': 0.46325459317585305}, 'b000g80lqo'), ({'integrity': 3.6133858267716534}, 'b000g80lqo'), ({'costing': 7.226771653543307}, 'b000g80lqo'), ({'web': 0.12084902430674427}, 'b000g80lqo'), ({'wizard': 0.9033464566929134}, 'b000g80lqo'), ({'operational': 7.226771653543307}, 'b000g80lqo'), ({'forms': 0.44065680814288455}, 'b000g80lqo'), ({'peachtree': 2.627916964924839}, 'b000g80lqo'), ({'easy': 0.06522357088035476}, 'b000g80lqo'), ({'snap': 0.6339273380301147}, 'b000g80lqo'), ({'thousands': 0.5735533058367703}, 'b000g80lqo'), ({'premium': 0.48501823178143}, 'b000g80lqo'), ({'provides': 0.13584157243502457}, 'b000g80lqo'), ({'dollar': 7.226771653543307}, 'b000g80lqo'), ({'trail': 12.044619422572179}, 'b000g80lqo'), ({'choice': 0.4153317042266268}, 'b000g80lqo'), ({'tailor': 5.161979752530933}, 'b000g80lqo'), ({'every': 0.20647919010123736}, 'b000g80lqo'), ({'report': 1.2044619422572178}, 'b000g80lqo'), ({'password': 1.5055774278215224}, 'b000g80lqo'), ({'advanced': 0.12813424917629976}, 'b000g80lqo'), ({'easily': 0.11618603944603387}, 'b000g80lqo'), ({'like': 0.17712675621429674}, 'b000g80lqo'), ({'solution': 0.3197686572364295}, 'b000g80lqo'), ({'shots': 2.0074365704286965}, 'b000g80lqo'), ({'rock': 0.7527887139107612}, 'b000g80lqo'), ({'small': 0.242509115890715}, 'b000g80lqo'), ({'fixed': 4.516732283464567}, 'b000g80lqo'), ({'set': 0.1411478838582677}, 'b000g80lqo'), ({'financial': 0.7688054950577986}, 'b000g80lqo'), ({'customization': 1.4453543307086614}, 'b000g80lqo'), ({'challenges': 0.6339273380301147}, 'b000g80lqo'), ({'individual': 0.3284896206156049}, 'b000g80lqo'), ({'result': 1.0323959505061868}, 'b000g80lqo')]
Am I correct ?
Is this a right approach ?
Please suggest me or explain me in language, what to be followed next ? what are we trying to achieve in this ? Am lost a bit here.
Is using collect() method here, a good approach ? Any suggestions ? ?
Sorry to say, I am not a very good python programmer.
-------Thank you in advance 

Hello friends..!! i am getting the error when i give the command vagrant up command in my windows system..!!
Need your help to resolve this..!! Hello! I have a problem with  trueDupsRDD: 
I apply join method (trueDupsRDD = sims.join(goldStandard) ), but this return the empty RDD (i.e. trueDupsRDD.collect()=[ ] ). Why does this may be occur if  the sims RDD and goldStandard are non empty?? 
Thanks in advance Using the two inverted indicies (RDDs where each element is a pair of a token and an ID or URL that contains that token), create a new RDD that contains only tokens that appear in both datasets. This will yield an RDD of pairs of (token, iterable(ID, URL)).¶

Can somebody explain this. I don't understand what it means nor do I understand how to do it?

(token1, url) , (token1, id)  so we want (token1, (url, id)  Is that what he wants?

 how can I see what the values of key and tokens are in this code. Thanks.

def swap(record):
    """ Swap (token, (ID, URL)) to ((ID, URL), token)
    Args:
        record: a pair, (token, (ID, URL))
    Returns:
        pair: ((ID, URL), token)
    """
    token = record[0] #[0] 
    keys = record[1]  #[1] 
   
    return (keys, token) I am not sure the intention why we need to create the invert() function, when flatMapValues() does the same thing.

I just did the flatMapValues() and got the correct count for amazonInvPairsRDD & googleInvPairsRDD The Lectures do not teach the concepts that are needed to complete the labs I am getting an error when using my broadcast variable which I am not getting with a normal variable.

Here is how I create the broadcast variable:

idfsFullBroadcast = sc.broadcast(idfsFullWeights)

So using idfsFullWeights in the tfidf function works fine.  Using idfsFullBroadcast however generates the following error.  How can I fix it?



There are 17078 unique tokens in the full datasets.






---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-420-e3d08bbb7239> in <module>()
     50 
     51 
---> 52 print 'There are %s Amazon weights and %s Google weights.' % (amazonWeightsRDD.count(),
     53                                                               googleWeightsRDD.count())

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1818.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1818.0 (TID 10782, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <genexpr>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "<ipython-input-420-e3d08bbb7239>", line 46, in <lambda>
  File "<ipython-input-395-ee459492d782>", line 13, in tfidf
TypeError: 'Broadcast' object has no attribute '__getitem__'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

 should I pay for a verified statement of accomplishment?  is it worth it? why the certificate doesn't include grades?  Greetings:

When I create a broadcast variable such as

amazonNormsBroadcast = sc.broadcast(amazonNorms)

I get the following error message.  Unable to solve.  Do I need to collect() the data first as in amazonNormsBroadcast = sc.broadcast(amazonNorms.collect())?

Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.

 

Thanks I had a clean Autograder submission except for section 1A in which it flagged:

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

I have only 14 lines of code in my first block of code (marked as In[49]: in the margin next to the code cell).  All of the code that I added is embedded in the return line as in:

# TODO: Replace <FILL IN> with appropriate code
quickbrownfox = 'A quick brown fox jumps over the lazy dog.'
split_regex = r'\W+'

def simpleTokenize(string):
    """ A simple implementation of input string tokenization
    Args:
        string (str): input string
    Returns:
        list: a list of tokens
    """
    return filter(None,re.split(split_regex, string.lower()))

print simpleTokenize(quickbrownfox) # Should give ['a', 'quick', 'brown', ... ]

This function passes all four tests in the next block of code and there are no indentation errors and I do not have an 
VectorAccumulatorParam(AccumulatorParam)
in my code.  So what is is flagging here?  is this an internal Autograder issue?

All the other sections of code that I wrote passed although it took some time to get through it all.

I'm going to try resubmitting to see what happens.


 
How can i create pair tuple  with unique tokens.

the below code not  working .

tokenCountPairTuple = uniqueTokens.flatMap(lambda x:(x,1)) .  Say I have amazon rdd:

(cat, id1) , (dog, id2) , (hamptster, id3)

Google rdd:

(cat, url1), (dog url2), (duck, url3)

I want to end up with ( cat ,(id1,url1), dog ,(id2, url2)  

Is that the idea? I am stuck in lab 3, 4f.  When I do

similaritiesFullRDD = (commonTokens.map(fastCosineSimilarity).cache())

I get: 'list' object has no attribute 'map'.  How is commonTokens a list?  When I print it out it has values like this:

[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), '120')]

seems like an RDD to me... Should the idfs be returning an integer or a float?  Seems to me it should be an int because it's a count of how many times each token appears in its original document.  Yet I see comments in the forums that N should be a float?  Which means the return would be a float.

Please explain. Either taking too long or dumping errors.
But the groupByKey works fine though.

Why?
I am new to python, 

mapping with lambda function returning (x[0], [x[1]])
and reduce it by a + b,
too expensive for python?
 "mods please delete" I have  a list of tuples
yy=[ (token1, idf1),(token2,idf2)...]\
How do I convert this to RDD.
I did map of xxRDD=yy.map(lambda a,b: (a,b)]

and I get an error list has no attribute map

thanks how to reduce the tuples share the same key into tuple of that key with list containing the values from each previous tuple? I think the instructors have done a good job with the autograder which can scale for such a huge class. Is it possible the share the architecture and implementation details may be after the course.

Thanks. Hi, I am able to use re.split to filter the string but not able to use tokenize funtion in mapper. Please help Hi everybody. I have a question about how to approach this course, and in general anything that involves programming.

I have had some experience programming, I am not new to the concepts. However I don't practice a specific language enough so that I can say I am proficient.

My problem is I basically think in "if"s and "for"s. And you can do a lot with that, but it is never elegant nor efficient. Let me give you an example: in lab three at one point you end up with two dictionaries with the same keys but different values and you want to multiply them. You can do this just with ifs and fors but again, long and not elegant. Or you can jump online and find that there is a simple way using braces (I don't wanna give up too much) that is more elegant but if you don't know it.. well you don't!

So my question is: what is the best approach? Should I keep writing inefficient code that is "mine" or should I look up stuff that I obviously try to understand before I implement but that I might forget in 2 days if I don't use it again?

Thanks a lot in advance for any insights! After spending so much time solving lab3 4e, yesterday complete day, and I am struggling with 4f now. I had an issue with 4d (even though it passed the test and it caused all kinds of problems in 4d. I have seen several posts and  many have to go back and fix issues with previous sections while solving 4f.

In my opinion if the problem set with the associated tests are designed properly in lab3 it would have saved lot of hours of many.
 This is what I see in localhost:4040/stages timing.html

it took 58 minutes ...I have removed all print statements ...and collect() including the ones given to us as part of note book 
all tests are passed in local machine ...is it worth submitting ... Hi
I'm trying to calculate values of 2c in Lab3, but I thing I miss something obvious...
First of all - N - it is described as number of documents in set of documents. Is it a total number of elements in corpus? 
Than:
uniqueTokens - I take corpus and flatMap tokens, and than distinct. Result seems to be OK, cause I pass first of the tests in next cell

tokenCountPairTuple = uniqueTokens.<FILL IN> - I don't undestarnd what needs to be done here and in next steps. If we take RDD uniqueTokens there is no point counting tokens, cause they unique so appear only once.

Can anyone explain the 2c exercise?

Regards
Pawel
 
Hello All,
I'm facing the below error while uploading the file lab3_text_analysis_and_entity_resolution_student.ipynb

Any advice?

Cannot upload invalid Notebook


The error was: SyntaxError: Unexpected token c
 Is there a way to get partial credit for the lab without doing lab 3 4e and 4f. 

I have been waiting for two hours this to run, and it is not even close to finish. I am afraid I have to reapeat this after I see my errors!!

Thanks. So how doe sone resubmit a modified Lab3?
The autograder keeps evaluating the old version event though I have changed the file (namely  the original typo 'rec_b000hkgj8k_weights' versus 'recb000hkgj8kWeights'.

I've tried three times from two different browsers and it still grades the original submission that has the unfixed typo.

Am I to expect to lose points for a no-functional file upload on the submission page?
With the mistakes and typos and misleading docstrings in the notebook this course is quickly losing attraction.
 Hello can some help me with 3a 
I am confused on how to calculate the following
1-dotproduct
2-norm
3-cossim

Any help would be greatly appreciated. Hi, I dont know if Im using the correct approach, but when trying to execute 4e it takes forever and have to restart the kernel to run again all the previous cells!!  

If I do this:
commonTokens2 = amazonInvPairsRDD.join(googleInvPairsRDD).map(swap)

I get this output:
[(('b0007yepy6', 'http://www.google.com/base/feeds/snippets/17293379770636740081'), 'aided'), (('b000hlt5j4', 'http://www.google.com/base/feeds/snippets/17293379770636740081'), 'aided'), 

then when I tried to groupbykey, to obtain the list of tokens:
commonTokens = commonTokens2.groupByKey()

and do a
print commonTokens.take(5) 
is when the spark VM looks like is on a infinite loop, I see my CPU is 100% executing that line, but It passed about 5 minutes and still no result, 
Where is my mistake?

thanks!




 I recalled it was either 20% off the total score (X - 20%) or 80% of the total score (X * 80%).   In any case, does that mean if one can achieve better than 80% by deadline, then there's no point re-submitting after the deadline as the new score would be almost certainly lower?   

Unless you mean you'll grant 80% of the score differential?    i.e. before deadline score = 90.  After deadline score = 100, improvement = 10, taking 20% off one achieve 98 afterall? Can someone confirm if the following idfsSmall.take(7) smallest IDFs  values are correct.

I got cnt = 22520
There are 4772 unique tokens in the small datasets.print idfsSmall.take(7)
[('aided', 400.0), ('precise', 44.44444444444444), ('duplex', 400.0), ('dance', 400.0), ('breath', 200.0), ('themes', 133.33333333333334), ('known', 57.142857142857146)]smallIDFTokens = idfsSmall.takeOrdered(11, lambda s: s[1])
print smallIDFTokens
#print idfs(amazonRecRDD).take(10)
[('adobe', 1.520912547528517), ('software', 2.1621621621621623), ('cs3', 2.9197080291970803), ('pro', 3.5714285714285716), ('design', 3.6036036036036037), ('new', 3.6363636363636362), ('0', 4.166666666666667), ('3', 4.25531914893617), ('tools', 4.3478260869565215), ('windows', 4.545454545454546), ('create', 4.597701149425287)] Can someone please explain me why if I print the fastCosineSimilarity with a "pure" element of commonTokens RDD (commonTokens.take(1)) returns the expected result:

print fastCosineSimilarity((('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), '120'))


(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), 0.004328984263097415)


But when I try to map de hole RDD, count() method it's not working?

smilaritiesFullRDD = (commonTokens .map(fastCosineSimilarity) .cache())

print similaritiesFullRDD.count()

Stack trace is ancient Chinese to me, but here goes in case someone does understand...

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-132-31b111bcb6fe> in <module>()
     43 print fastCosineSimilarity((('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), '120'))
     44 
---> 45 print similaritiesFullRDD.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 351.0 failed 1 times, most recent failure: Lost task 0.0 in stage 351.0 (TID 650, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-44-27f6290f883a>", line 19, in fastCosineSimilarity
  File "<ipython-input-44-27f6290f883a>", line 19, in <genexpr>
KeyError: '1'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
<p></p>
<p></p> Your submission token id is 1160030-d650e19d07372be1a15b0d9476409009:6c5272ffdb42fb535bf4a3bd8b4b6a0b:ip-172-31-8-110
Please include this submission token id when you need support for your code submission.when I run in my virtual machine is Ok and passes the test.I do not know what happenThanks  

I have followed and tried all the steps given in autograder page. It also works fine on my VM.
How do I fix this problem ?
 Hi there for some reason I am unable to export the lab3 to *.py
opened lab2 and the option to export to py is there...



vagrant halt/up solved the issue!!!

    
idfsFullCount = idfs(amazonFullRecToToken).count() 
evokes an error message:
----> 1 idfsFull = idfs(amazonFullRecToToken).count()
      ...
ValueError: too many values to unpack
On the other hand, idfs() on the small tokenized dataset,
idfsFullCount = idfs(amazonRecToToken).count()
works fine.

I am perplexed by the "too many values to unpack" error. Here is what I am doing and I am very new to python programming ..Any help with examples would be greatly appreciated.

dotprodcut(a,b)=sum((x*y) for x, y in zip(a, b))
norm(a)=sqrt(dotproduct(a, b))
cossim(a,b)=(dotprod(a, b) / (norm(a) * norm(b)))

I am getting the following error

TypeError                                 Traceback (most recent call last)
<ipython-input-193-6b8b077a9089> in <module>()
     34 testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }
     35 testVec2 = {'foo': 1, 'bar': 0, 'baz': 20 }
---> 36 dp = dotprod(testVec1, testVec2)
     37 nm = norm(testVec1)
     38 print dp, nm

<ipython-input-193-6b8b077a9089> in dotprod(a, b)
     10         dotProd: result of the dot product with the two input dictionaries
     11     """
---> 12     return sum((x*y) for x, y in zip(a, b))
     13 
     14 def norm(a):

<ipython-input-193-6b8b077a9089> in <genexpr>((x, y))
     10         dotProd: result of the dot product with the two input dictionaries
     11     """
---> 12     return sum((x*y) for x, y in zip(a, b))
     13 
     14 def norm(a):

TypeError: can't multiply sequence by non-int of type 'str'

  since last night, I have tried five times to submit the lab 3 with no luck; please see below. I have followed all instructions for coding and submitting.

Please advise!

Thanks.
Your submission token id is 1159636-44ead7c0959df3fc3a2f53b54b81c87a:dc89189a3c74604cf3d39aceb512219f:ip-172-31-20-42
Please include this submission token id when you need support for your code submission.

 Hello,

All my tests are successful, but I get error for each of my cells. The first one is just below the token:

Your submission token id is 1163281-8d345e8a630d3e4fb7c25cb38dedfab0:2236ed4b23adbdfa7195c53d0f811b3f:ip-172-31-8-113
Please include this submission token id when you need support for your code submission.

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 619
SyntaxError: Non-ASCII character '\xc3' in file /ok/submission.py on line 620, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'simpleTokenize' is not defined Hello,
I am stuck in this section , need help please:

TypeError                                 Traceback (most recent call last)
<ipython-input-42-5f0b95d91b88> in <module>()
     34 testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }
     35 testVec2 = {'foo': 1, 'bar': 0, 'baz': 20 }
---> 36 dp = dotprod(testVec1, testVec2)
     37 nm = norm(testVec1)
     38 print dp, nm

<ipython-input-42-5f0b95d91b88> in dotprod(a, b)
     10         dotProd: result of the dot product with the two input dictionaries
     11     """
---> 12     return sum(p*q for p,q in zip(a,b))
     13 
     14 def norm(a):

<ipython-input-42-5f0b95d91b88> in <genexpr>((p, q))
     10         dotProd: result of the dot product with the two input dictionaries
     11     """
---> 12     return sum(p*q for p,q in zip(a,b))
     13 
     14 def norm(a):

TypeError: can't multiply sequence by non-int of type 'str'

 Hi all,

I'm half way through lab 3 and, my PC crashed this morning so I had to reimage the PC.  I have re-installed virtual box and vagrant.  

When I ran "vagrant up --provider=virtualbox" for the first time, it downloaded a file https://altas.hashicorp.com/sparkmooc/boxes/base/versions/0.0.7.1/providers/virtualbox.box.   I assume this is the new image file since it's really large.

I still have the previous box-disk1 file (>1.5G) and sparkvm(8KB) which are intact.  How do i get the auto-saved lab3 notebook from the old spark vm?  Or how to boot to the previous spark vm?  

I have tried to swap out the newly created box-disk1 and sparkvm files with my previous ones.  But "vagrant up" gives an error message "Your VM has become inaccessible.  Unfortunately this is a critical error with VirtualBox that Vagrant can not cleanly recover from.  Please open VirtualBox and clear out your inaccessible virtual machines or fine a way to fix them."

Thanks a lot for your help!




 Can't believe how much effort I had to put into this lab and still could not finish due to some KeyError issue  that I spent 16 hours investigating alone... very frustrating. This course might be fine for people who don't have to hold down a full time job at the same time. It also seems to be more about wrangling data with Python than gaining a good experience with developing Spark solutions. Anyway, would love to see the completion stats on Lab3 compared to prior labs. That is my rant and I am sticking to it. Hi:

I solved Lab 3 3e by an ugly way , but wondering a more elegant way to calculate the average. My major hurtle was to calculate the sum of all scores from RDD contains ('joinedID', (score, 'gold')).  I used a map and a reduce to calculate sum of all scores.  I tried to calculate by just one reduce function but failed like this:

>>> b=('a b',(1,'gold'))>>> b[1][0]1   #very good!
>>> rdd=sc.parallelize( [('a b',(1,'gold')), ('a b',(2,'gold')), ('a c',(4,'gold'))] )
>>> rdd.reduce( lambda a, b: a[1][0]+b[1][0] )
File "<stdin>", line 1, in <module> File "/Users/j/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 741, in reduce return reduce(f, vals) File "<stdin>", line 1, in <lambda>TypeError: 'int' object has no attribute '__getitem__'

I am guess it's because tuples are not mutable.  So I tried fold(0,lambda a,b: a[1][0]+b[1][0] ) (didn't work), and looking at aggregate() now.  Hope somebody can hint me. Have a nice weekend! 
 I looked that the following to run some basic test of cosineSimilarity.. All my test upto this point have passed.

https://piazza.com/class/i9esrcg0gpf8k?cid=3350

testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }
testVec2 = {'FOO': 1, 'BAR': 0, 'BAZ': 20 }
testVec3 = {'foo': 1, 'BAR': 0, 'BAZ': 20 }
dp = dotprod(testVec1, testVec2)
dp2 = dotprod(testVec1, testVec3)
nm = norm(testVec1)
print dp, dp2, nm
0 2 6.16441400297

But than I run into following error which looks to me I am getting some null value... I am at wits end debugging this... 

IndexError: list index out of range
============
One place I have some doubt is the way I am creating the string to be passed to the function cosineSimilarity() as follow..
 googleValue = " ".join(googleRec[1:])
 
Any suggestion about how to debug or proceed? What about releasing the best/efficient solution at the end of the course? I think it might be helpful to many of us. Dear TA,
 
I submitted my lab3 python file three hours ago, but the grading job seems still running and there is still no feedback yet. It is the second time I have tried.
Should I wait for more time or something weird has already happened.
Thanks in advance!
Best regards,
Llanos Tobarra So long story short:

1. Have ran the WHOLE book successfully in 15 minutes(including Part 5) in my VM.
All tests pass successfully.

2. Trying to submit to Autograder and getting the following error(after around 50 minutes):

Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 1168769-3168f07d285539f507a4cb5bc55c6a08:b23d6c5caae8258db4b9dd5b80dc9687:ip-172-31-25-62
Please include this submission token id when you need support for your code submission.


Note#1: This is the 2nd time that failed with this configuration. I have also tried restarting and rerunning the whole notebook.
Note#2: I have followed all the steps listed in the section "Before you submit your lab 3, make sure you follow these steps:"

Can an instructor have a look at this? I have used 9 of my 10 submissions and I don't want to not get a grade for a lab I have completed! Thanks in advance! similarities = (crossSmall.map(computeSimilarity).cache())

gives a coredump. Anyone know whet is the issue?
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-541-7ed32d8b057f> in <module>()
     41             .collect()[0][2])
     42 
---> 43 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     44 print 'Requested similarity is %s.' % similarityAmazonGoogle

<ipython-input-541-7ed32d8b057f> in similar(amazonID, googleURL)
     38     print "got params as %s" %amazonID
     39     return (similarities
---> 40             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     41             .collect()[0][2])
     42 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 649.0 failed 1 times, most recent failure: Lost task 0.0 in stage 649.0 (TID 1858, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-541-7ed32d8b057f>", line 22, in computeSimilarity
  File "<ipython-input-390-3559585dad17>", line 13, in cosineSimilarity
  File "<ipython-input-389-bcf45224170d>", line 38, in cossim
  File "<ipython-input-389-bcf45224170d>", line 13, in dotprod
KeyError: 'lessons'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span> Hello,

I've tried to do the following with a pair RDD (with the format (int, [int,int,..., int]) ), but I got an error from the Apache Spark:
pairRDD.reduce(lambda a,b: len(a[1])+len(b[1]))
Then when I changed to the following it worked (and I don't know why):
pairRDD.map(lambda item: len(item[1])).reduce(lambda a,b: a+b)
I think this has something to do with the accumulator used to process the reduce operation (with the second code Spark can use either a or b as accumulators, and with the first it can't do so because len is not a mutable object).

Thanks for the help i had a really hard week trying to finish previous labs to get this one before the dealine and while doing the last function of the last part the vm crashed! i tried everything and then one note of the errors said i should remove some file, i did, and all my work is gone! In 3(c), to combine the 2 RDDs, i am planning to use zip() function. Here is the summary of errors and what I tried. Any suggestion what is being overlooked?

Error:
(define) crossSmall = googleSmall.zip(amazonSmall).cache()
Now: print crossSmall.count() throws error
Any action on crossSmall throws error

I tried:
* print crossSmall: shows that this is a JavaRDD

* print googleSmall.map(lambda x: x[0]).count(), googleSmall.map(lambda x: x[1]).count(), and same with amazonSmall.. all of them are 200

* The following thing to identify if there is some particular combination that is causing problem, but this also works properly (Sorry about indentation, not able to do it in plain text editor):
list1 = googleSmall.collect()
list2 = amazonSmall.collect()
i = 0
for l in list1: #start of for loop
rdd1 = sc.parallelize([list1[i]])
rdd2 = sc.parallelize([list2[i]])
j = rdd1.zip(rdd2).count()
i=i+1
print i #end of for loop
#The above code runs displaying 1 to 200 properly, without any error

* Similar to above, if I define: 
rdd1 = sc.parallelize([(1,'a'),(2,'b')])
rdd2 = sc.parallelize([(3,'c'),(4,'d')])
print rdd1.zip(rdd2).count() #this works

* I also tried the following to check if it may work when instead of having tuple-of-tuples, there is just one tuple. It still errors:
crossSmall = googleSmall.zip(amazonSmall).map(lambda x: (x[0][0], x[0][1], x[1][0], x[1][1]))
crossSmall.count() #Throws error if it happens that you get 4f test 3 failed, just as I did, the cause might be in 4b , in which you may want to check your amazonWeightsRDD values. add below test to 4b might help:
Test.assertEquals(amazonWeightsRDD.filter(lambda x:x[0]=='b000jz4hqo').flatMap(lambda (x,v):v.items()).filter(lambda x:x[0]=='rom').collect()[0][1], 2.4051362683438153, 'incorrect amazonWeightsRDD value')

also attached the take ordered 1 of my amazonWeightsRDD & googleWeightsRDD

print amazonWeightsRDD.takeOrdered(1)print googleWeightsRDD.takeOrdered(1)

[('1931102953', {'easily': 0.7766119478761211, 'instant': 3.773848684210526, 'language': 5.084764542936288, 'entertainment': 1.3270676691729324, 'immersion': 9.289473684210526, 'win': 0.47544550352258597, 'topics': 1.725187969924812, 'american': 23.00250626566416, 'sign': 26.83625730994152, 'beginners': 8.625939849624059, 'mac': 0.2956258455195516, 'quickly': 1.166793796084414, 'teaches': 3.0190789473684205, 'asl': 241.52631578947367})]
[('http://www.google.com/base/feeds/snippets/10004325115759097777', {'rom': 0.6012840670859538, 'topics': 0.9105158730158731, 'cd': 0.28645443196004994, 'learning': 0.576797385620915, 'ready': 1.09889846743295, 'features': 0.19550954328561687, 'dlux': 42.49074074074074, 'entertainment': 0.7003968253968254, 'multimedia': 2.1605461393596985, 'commence': 42.49074074074074, '0': 0.4335789871504157, '2': 0.21069788797061523, 'new': 0.1911127769448609, 'first': 0.8067862165963432, 'vendor': 3.1090785907859075, 'immersion': 4.902777777777778, 'collection': 0.712135319677219, 'v2': 12.74722222222222, 'journey': 4.902777777777778, 'class': 2.451388888888889, 'instant': 2.987630208333333, 'language': 1.341812865497076, 'topcd': 63.73611111111111, 'immers': 25.49444444444444, '80696wi': 127.47222222222221, 'model': 2.4994553376906317, 'deluxe': 1.2256944444444444, 'italian': 17.382575757575758})] trying to do
pairs=( <list comprehension> )

getting errors in tests
obviously, i'm treating this data structure improperly.
but since i'm new to Python i just can't fancy the RIGHT way to do it. Is this correct:?
N = 400
unique Tokens = 4772
as an example tuple ('aided',1)
meaning the IDF = 400.0/1.0
The unique counts match
However, the smallest IDF token I get is 'adobe'  and value of 1.52  but  the answer is 'software' of 4.2
 
  I don't see any other posts confirming _Norms before creating the broadcast var, so can I get a confirmation here?  (Like 4b nothing besides length of structure is tested)

print amazonNorms.first()
('b000jz4hqo', 262.3974984324429)
print googleNorms.first()
('http://www.google.com/base/feeds/snippets/11125907881740407428', 23.338085216460595) All my tests passed locally and i submitted 4 times already. Everytime Autograder is giving 0 test passed with a invalid syntax error in one of the first tests where as i do not have any failed test or errors on my local run. All my tests also ran pretty quick. Can one of the TAs help out?

Your submission token id is 1174319-60721263e3c7c1f8f6c653893cfc6670:180e2f970ce7c214c4866ad21c8b83e3:ip-172-31-28-13 Executed same job multiple times and their execution time varies too much...why?
No changes to vm/physical machine. may be few background processes in physical machine. but vm should be stable and constant, as this is only the job i am running.
What are the possible causes?


 the ID for each line in the google dataset is an URL 
http://www.google.com/base/feeds/snippets/11448761432933644608:
which leads to a 404 error The amazonWeights broadcast variable is a dictionary with a nested dictionary ... something like this>>> awrddbc = sc.broadcast(awrdd.collectAsMap())>>> awrddbc.value{'b000jz4hqo': {'rom': 2.40513653, 'clickart': 56.6543432}, 'b0006zf55o': {'laptops': 11.58838837, 'desktops': 12.1}}>>> awrddbc.value['b0006zf55o']['desktops']12.1>>> awrddbc.value.viewkeys()dict_keys(['b000jz4hqo', 'b0006zf55o'])>>> for key in awrddbc.value.viewkeys(): print awrddbc.value[key]{'rom': 2.40513653, 'clickart': 56.6543432}{'laptops': 11.58838837, 'desktops': 12.1}So in this dictionary the values are dictionaries. Is there a way to reference the 'values dictionary' i.e the nested dictionary?In 4f I have tested my code, I believe successfully, with individual commonToken values (I have matched the values of various outputs that other students have posted). However, I am getting KeyErrors in 4f when trying to run with full commonTokens RDD. As part of my debugging I am now trying to validate that the keys in both these dictionaries exist for the amazon and google broadcasts.  Look correct?

print amazonInvPairsRDD.first()

(('rom', 2.4051362683438153), 'b000jz4hqo')

googleInvPairsRDD
(('quickbooks', 17.48190476190476), 'http://www.google.com/base/feeds/snippets/11125907881740407428')

Now that I have proceeded to 4e it seems that it should just be (word, id) ... ???? I am mapping to get weights like this:

idfsFullWeights = fullCorpusRDD.map(lambda (x,y): (x , tfidf(y,idfsFull))) 

but the following print is giving error:

print idfsFullWeights.take(3)  

Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.

Where is my mistake? >Count the number of true duplicate pairs in the trueDupsRDD dataset
>Compute the average similarity score for true duplicates in the trueDupsRDD datasets. Remember to use float for calculation

Can someone please give me a example of a trueDupsRDD entry( actual entry )
.. Conceptually its not clear to me what is meant by "true duplicate".. Is it defined by the cosineSimilarity score ==cos(0)==1? 4e , looks like its going in to infinite loop - need help

def swap(record):token = record[0] keys = record[1] return (keys, token)fullTokens = amazonInvPairsRDD.join(googleInvPairsRDD).map(swap)commonTokens = (commonTokens2.reduceByKey(lambda x,y: x + ' ' + y).map(lambda x: (x[0], x[1].split())).cache()) 

4f - need help in fastCosineSimilarity 
I'm struggling with this lab…
  File "<ipython-input-53-0ed5a02455e0>", line 28, in <lambda>
  File "<ipython-input-53-0ed5a02455e0>", line 12, in fastCosineSimilarity
ValueError: need more than 1 value to unpack



On line 12 I'm doing something like this, but it doesn't quite seem right to me:  amazonRec, googleRec = record[0][0]

So to start I run this:

print commonTokens.take(1)
[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120'])]


(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), 10.266222905424064)
print fastCosineSimilarity(commonTokens.take(1))

If I add debug prints for amazonRec and googleRec they print out as expected.  I get no errors and the results in the format I would expect. (10.266222905424064 is most likely wrong but I'm cool with that for now).

With this code in place I create the similaritiesFullRDD from commonTokens and mapping the following function (lambda x : fastCosineSimilarity(x)) this creates the RDD.. but if I try one of the following:

print similaritiesFullRDD.count()
print similaritiesFullRDD.take(1)

I receive the above exception.  Am I passing the wrong value into fastCosineSimilarity in my lambda function?
 I am getting a trueDupsCount = 167 instead of 146, Can't figure what could be wrong.

All tests upto 3d have passed.

I used cartesian() to produce crossSmall in part  3c.
I have 272600 records in sims. Can someone confirm if this number is correct for sims?

trueDups.take(2) gives the following:

[('b0006b63rw http://www.google.com/base/feeds/snippets/12911813140487249020', 0.28983648502532805), ('b000g80lqo http://www.google.com/base/feeds/snippets/18441188461196475272', 0.4575582894588214)]
 I passed all tests so far and I was confident to pass this question  but I got this error.
4a was
amazonFullRecToToken = amazonmap(lambda (x, y): (x, tokenize(y)))
just like 1c. map, lambda, and make a tuple (id, tokenize)
fullCorpusRDD : used union
idfsFull is given... idfs(fullCorpusRDD)  

idfsFullWeights is also kind of given.. used collectAsMap
previously given how to make broadcast
idfsFullBroadcast = sc.broadcast(idfsFullWeights)

and the problem is
amazonWeightsRDD = tfidf(amazonFullRecToToken, idfsFullWeights)
it causes this error

 I'm having some trouble with 4b. First of all, I can't seem to get the broadcast variable to work.

I run idfs on fullCorpusRDD then use the sc.broadcast method in the same manner as in (3d) but every time I end up with an error. I am getting the correct number of entries for fullCorpusRDD.

I'm wondering if I have the format wrong? Should fullCorpusRDD be a (key, value) pair with key being the url/ID and values being a list of tokens? Or should it purely be a lists of tokens?

Similarly, what's the desired format for amazonWeightsRDD? Is it (k, v) where k is the id and v is a dictionary with tokens mapping to numbers?
 I am getting different result. amazonWeightsRDD counts are correct.
If I select first two records, I am getting below records. List brackets exit for every source record.

Any suggestion?

amazonWeightsRDD.map(lambda x: invert(x)).take(2)
[[['rom', 'b000jz4hqo'],
  ['clickart', 'b000jz4hqo'],
  ['950', 'b000jz4hqo'],
  ['image', 'b000jz4hqo'],
  ['premier', 'b000jz4hqo'],
  ['000', 'b000jz4hqo'],
  ['dvd', 'b000jz4hqo'],
  ['broderbund', 'b000jz4hqo'],
  ['pack', 'b000jz4hqo']],
 [['1', 'b0006zf55o'],
  ['desktops', 'b0006zf55o'],
  ['computer', 'b0006zf55o'],
  ['win', 'b0006zf55o'],
  ['ca', 'b0006zf55o'],
  ['v11', 'b0006zf55o'],
  ['30u', 'b0006zf55o'],
  ['30pk', 'b0006zf55o'],
  ['desktop', 'b0006zf55o'],
  ['laptops', 'b0006zf55o'],
  ['arcserve', 'b0006zf55o'],
  ['associates', 'b0006zf55o'],
  ['lap', 'b0006zf55o'],
  ['oem', 'b0006zf55o'],
  ['international', 'b0006zf55o'],
  ['backup', 'b0006zf55o']]]

There are 90427 Amazon inverted pairs and 62987 Google inverted pairs.

 I am getting the following error even though i think the pair I am forming is like the following

[('rom', 'b000jz4hqo'), ('clickart', 'b000jz4hqo'), ('950', 'b000jz4hqo'), ('image', 'b000jz4hqo'), ...]

Any insights? 


Error I am getting:

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-531-ade5118ab5c2> in <module>()
     23                     .cache())
     24 
---> 25 print 'There are %s Amazon inverted pairs and %s Google inverted pairs.' % (amazonInvPairsRDD.count(),
     26                                                                             googleInvPairsRDD.count())

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1093.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1093.0 (TID 7689, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-531-ade5118ab5c2>", line 11, in invert
AttributeError: 'tuple' object has no attribute 'collect'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> There is     sc.textFile(filename, 4, 0)    on lab3,
I know that the first arg is the path, the 2nd is the number of partitions, what does the 3rd argument mean?
Thank you. so, the tfidf(tokens, idfs) function in 2f returns a dictionary.

when I use is like this 

amazonWeightsRDD = tfidf(amazon.......)
googleWeightsRDD = tfidf(google........)

print 'There are %s Amazon weights and %s Google weights.' % (amazonWeightsRDD.count(), googleWeightsRDD.count())

The print statement fails with 'dict' object has no attribute 'count' error.

Any clues how I can convert dictionary to a RDD? Hello All,
I have started this lab on Tuesday and I am having the hardest time.
I am currently on 2c ( but I am close)
I cannot see my error

1    N = corpus.count()
2    uniqueTokens = corpus.flatMap(lambda a: a[1]) """Tried ending this line with .distinct()"""
3    tokenCountPairTuple = uniqueTokens.map(lambda a: (a, 1))
4    tokenSumPairTuple = tokenCountPairTuple.reduceByKey(lambda a,b: a + b)
6    return tokenSumPairTuple.map(lambda (a,b): (a,N/b))

I believe my error is on line 2. but I try to add a .distinct() at the end but I am still not passing the test. When I add distinct all of the Values become 400, because line 6 (N/b) = 400/1

Does anyone see my error?
Thanks
 explode = (0.05, 0.05, 0.1, 0, 0, 0, 0)patches, texts, autotexts = plt.pie(fracs, labels=labels, colors=colors,                                    explode=explode, autopct=pie_pct_format,                                    shadow=False, startangle=125)for text, autotext in zip(texts, autotexts):    if autotext.get_text() == '':        text.set_text('') # If the slice is small to fit, don't show a text label    plt.legend(labels, loc=(0.80, -0.1), shadow=True)

can someone explain me the above code . what is the use of explode and how patches ,texts ,autotexts defined within a single peace of code ? i mean i know we can define a,b=3,4 in python but we can't define its as a,b=3 ? seems like this is the case above . please clarify it and correct me if i am wrong  Plz help me:My code for this problem is:<REDACTED> But it is showing following error:Py4JJavaError Traceback (most recent call last) in () 24 .cache()) 25 ---> 26 print similaritiesFullRDD.count()/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self) 930 3 931 """--> 932 return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum() 933  934 def stats(self):/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self) 921 6.0 922 """--> 923 return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add) 924  925 def count(self):/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f) 737 yield reduce(f, iterator, initial) 738 --> 739 vals = self.mapPartitions(func).collect() 740 if vals: 741 return reduce(f, vals)/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self) 711 """ 712 with SCCallSiteSync(self.context) as css:--> 713 port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd()) 714 return list(_load_from_socket(port, self._jrdd_deserializer)) 715 /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args) 536 answer = self.gateway_client.send_command(command) 537 return_value = get_return_value(answer, self.gateway_client,--> 538 self.target_id, self.name) 539  540 for temp_arg in temp_args:/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 298 raise Py4JJavaError( 299 'An error occurred while calling {0}{1}{2}.\n'.--> 300 format(target_id, '.', name), value) 301 else: 302 raise Py4JError(Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 186.0 failed 1 times, most recent failure: Lost task 0.0 in stage 186.0 (TID 722, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last): File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main process() File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process serializer.dump_stream(func(split_index, iterator), outfile) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream vs = list(itertools.islice(iterator, batch)) File "", line 16, in fastCosineSimilarityTypeError: unsupported operand type(s) for /: 'float' and 'dict' at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
 at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70) at org.apache.spark.rdd.RDD.iterator(RDD.scala:242) at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
 at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
 at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
 at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
 at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
 at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)<br /><br /><br /> Please help friends. I have already spend my 2 hours on this problem, still unable to get out of it. :(</p> Hello Friends,

The commonTokens looks good to me, because, I have all the tests till 4(e) .

commonTokens:-------------------[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), '120'), (('b000bnb72g', 'http://www.google.com/base/feeds/snippets/6959766318020460134'), 'software'), (('b00009apna', 'http://www.google.com/base/feeds/snippets/13986409474793202514'), 'business')]

This is how my 4f looks like :

amazonWeightsBroadcast = sc.broadcast(amazonWeightsRDD.collectAsMap())
googleWeightsBroadcast = sc.broadcast(googleWeightsRDD.collectAsMap())

def fastCosineSimilarity(record):
    amazonRec = record[0][0]
    googleRec = record[0][1]
    tokens = record[1]
    s = sum([amazonWeightsBroadcast.value.get(amazonRec)[t]
                 * googleWeightsBroadcast.value.get(googleRec)[t]] for t in tokens)
    value = s/(math.sqrt(amazonNormsBroadcast.value[amazonRec]*amazonNormsBroadcast.value[amazonRec]))/(math.sqrt(googleNormsBroadcast.value[googleRec]*googleNormsBroadcast.value[googleRec]))
    key = (amazonRec, googleRec)
    return (key, value)

similaritiesFullRDD = (commonTokens.map(fastCosineSimilarity).cache())

print similaritiesFullRDD.count()

Exception Thrown is :


---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-63-6258896cfdcf> in <module>()
     23                        .cache())
     24 
---> 25 print similaritiesFullRDD.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 152.0 failed 1 times, most recent failure: Lost task 0.0 in stage 152.0 (TID 597, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-63-6258896cfdcf>", line 16, in fastCosineSimilarity
  File "<ipython-input-63-6258896cfdcf>", line 16, in <genexpr>
KeyError: '1'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
<p>Questions :</p>
<p>--------------------</p>
<p>1. Are my formulae implemented correct ?<br />2. When I ask for type(similaritiesFullRDD) ; I get a pythonRDD. Then what causes this error ?</p>
<p></p>
<p>Regards</p> Hello everbody,

In exercise 3c I have

 print "count crossSmall"print crossSmall.count()

40000

I did the following test. I wrote the function prueba


def prueba(record): print "prueba" return "a"

Then I call prueba
print "before similarities3"
similarities = (map1 .map(prueba) .cache())print "after similarities3"

I printed a trace:

before similarities3after similarities3

Why my code don't execute prueba?

Could somebody help me?

Thanks in advance

Carlota Vina



 Hi, I've not been able to submit last two week's lab assignment because vagrant on my machine(2gb ram) becomes very slow and makes system unresponsive, it worked fine for first lab but while running the parselogs() function in lab2 it crashed and became unresponsive, I had to restart my labtop. What can I do to complete the course and take this course and I had also planned to do the next course in this series i.e. CS 190.1x but how will I be able to do with this system? Help me? I can't seem to understand the meaning of "Pre-compute TF-IDF weights. Build mappings from record ID weight vector." 

What is the record ID weight vector

Also, can someone please show me the  first line of the amazonWeightsRDD that is required, so that I know what is the required output.  I used below process1) Use map and lanbda with log.endpoint on badRecords to get badEndpoints2) use distinct() on badEndpoints to get badUniqueEndpoints3) print badUniqueEndpoints.take(2) gives me below errorAttributeError: 'unicode' object has no attribute 'endpoint'when I give print badRecords.take(2), i get below 2 records. Are these correct?[u'/shuttle/resources/orbiters/discovery.gif', u'/pub/winvn/release.txt']So where is the error?. My 4a is clear. But stuck in 4b. Bascially take() is not working on badEndpoints, badUniqueEndpoints Hello,

Can you please reset my autograder counter so that I can resubmit my lab 3 again.

Yesterday, I have submitted without 4F. Today, I have solved the problem and I want to re-submit it again.

token id is 1152755-fbe61fc061a6aa87fc191388d7ab7627:50eeba1795f0f8ebcb00f0c84ce2d89b:ip-172-31-8-112

Thanks,


 Hi,

I have managed to solve 4rth exercise till 4e, but when I am running 4e my machine is hanged.
I am thinking that the issue is from commonTokens ....cache() so my question is do I need to cache commonTokens ?

MY VM run using 4Gb RAM.

Thanks in advance. Hello Instructors / Support Team,

Can you please reset my autograder  so that I can resubmit my lab 3 again.
 
Yesterday, I have submitted without 4F. Today, I have solved the problem and  want to re-submit it again.
 
token id is 1152755-fbe61fc061a6aa87fc191388d7ab7627:50eeba1795f0f8ebcb00f0c84ce2d89b:ip-172-31-8-112
 
Thanks, i got the below message. do i need to change anything in the note book and submit again?

Thank You!

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 4
    class VectorAccumulatorParam(AccumulatorParam):
                                                  ^
IndentationError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
expected an indented block

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
All tests passed
Implement an IDFs function (2c)
-------------------------------
All tests passed
Implement a TF-IDF function (2f)
--------------------------------
All tests passed
Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
All tests passed
Perform Entity Resolution (3c)
------------------------------
All tests passed
Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
All tests passed
Perform a Gold Standard evaluation (3e)
---------------------------------------
All tests passed
Tokenize the full dataset (4a)
------------------------------
All tests passed
Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
All tests passed
Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
All tests passed
Create inverted indicies from the full datasets (4d)
----------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4e)
-------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4f)
-------------------------------------------------
All tests passed
-- 19 cases passed (100.0%) --


Your submission token id is 1178845-2741e3e822c883d4f44e7a4f1641d7a3:b9372289a558279354be257d0a8e92b7:ip-172-31-19-75
Please include this submission token id when you need support for your code submission. similarities = (crossSmall.map(lambda x : computeSimilarity(x)).cache())

Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063. Hi,

To solve 1(a) for lab3 I applied re.split first and later used filter to remove empty tokens.

Is this a good approach and what are the other possible solutions for this. # TEST Implement a cosineSimilarity function (3b)
Test.assertTrue(abs(cossimAdobe - 0.0577243382163) < 0.0000001, 'incorrect cossimAdobe')

My value is 0.500277597875

Appreciate any input I have made an app for pyspark RDD methods for convenience. Its free with no ads. The data is from the official documentation.
Get it from: https://play.google.com/store/apps/details?id=nbsu.docsforpyspark
or http://www.amazon.com/techcrater-Docs-for-pySpark-RDD/dp/B010G9EWIK/

Thanks to the instructors for this great course. Hello
How does hardware can roughly speed up calculation for such task more exactly?
-Ram: 1600 vs 2400 Mhz/CL?
-CPU: i5 vs i7? Ghz?
- Nb of core?
-SSD: speed?
-GPU? Hi,
please find the error message during my Lab3 submission. I have just checked my code as mentioned in more posts.. no collect() and print() function added in my code. I have tryed to optimize my code, following your staff suggestions but I always have the same problems. I passed all tests in my notebook without errors  but when I submit my lab.py into autograder I wait for 45-60 min and then the following error message.. 

Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token id is 1169705-c9bf2f9cd3aaf72004d20247683e2b88:105633b2792b264588d0d11b12d5bffc:ip-172-31-8-107
Please include this submission token id when you need support for your code submission. i'm very late to join cs1001x (fortunately early for machine learning), I see the registration is closed an hour back, any possibility of applying now or any offline process to register?  
I have been able to calculate
trueDupsCount - 146
avgSimDups - 0.2643

But when I use same logic to calculate avgSimNon, it gives me error.
I am using fullOuterJoin to get nonDupsRDD

Sample nonDupsRDD is
[0.0, 0.0, 0.0, 0.00035705334436562144, 0.0, 0.0, 0.0030233340055918307, 1.0177137663742169e-05, 0.0, 0.04172744337824608, 0.0, 0.0, 0.00016786202987308414]
Error Trace :-

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-50-46ace9680b68> in <module>()
     10               .fullOuterJoin(goldStandard).map(lambda x: x[1][0]))
     11 print nonDupsRDD.take(100)
---> 12 avgSimNon = (nonDupsRDD.reduce(lambda x,y:x+y))/float(nonDupsRDD.count())
     13 
     14 print 'There are %s true duplicates.' % trueDupsCount

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 160.0 failed 1 times, most recent failure: Lost task 0.0 in stage 160.0 (TID 1224, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 737, in func
    yield reduce(f, iterator, initial)
  File "<ipython-input-50-46ace9680b68>", line 12, in <lambda>
TypeError: unsupported operand type(s) for +: 'float' and 'NoneType'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) Hi,

I am trying to print some result to check if everything is ok:
print commonTokens.take(1)[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), '120')]
print filteredTokens.take(1)[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), 'datacompleteincludessoftware')]

As I have recognized from the print filteredTokens 'datacompleteincludessoftware' is not right because all the tokens are not separated, am I right ?


Thanks in advance for any help or suggestion. As the lab exercise suggested not to include those values that contains 200 i am using lambda x:x!200 but it gives we a Boolean value (True or False ) . please guide me a way where i can only select a tuple value of my requirement by using lambda function . i also applied filter(lambda x:x!=200)  but its also not working    I absouely just don't get this part.  Mostly because even though we are told the broadcast variable is a dict, it is not...

googleWeightsBroadcast[t]TypeError: 'Broadcast' object has no attribute '__getitem__'

I have tried scrolling up and down this absurdly long lab file trying to figure out how to use this but I am stumped.   How do we use this?

EDIT:  I have calculated a sum of products after finding  a relevant code post in the forums.  However since I read that we don't use norm, what do we divide by when it says 
The sum should then be divided by the norm for the Google URL and then divided by the norm for the Amazon ID.
?

EDIT2: for those also having this question here is the format

Broadcast.value[ID][t]

 I'm still in week 3, I  tried  downloading the 'apache.access.log.PROJECT' provided but it was very small, I believe it was compressed and if so, what was the original size?  Same for the wordcount problem in lab1. Also, whenever I run problems on Spark virtual machine, it slows down my everything on my computer (which is 4 Gig RAM and 500 Gig Hard disk) and processing isn't as fast as I expected, why is that and how can I improve the speed of the computations like that of lab2 question 1c?  I've been trying to load the localhost home page for over 1 hr now but it shows just a blank screen, why? In the localhost, what are the other tabs doing and how can we use them? How can I use Databricks cloud with my localhost server? I am getting an error at the line containing takeOrdered().
The same code was executing in 2f. Please help.

# TODO: Replace <FILL IN> with appropriate code# HINT: Each of these <FILL IN> below could be completed with a single transformation or action.# You are welcome to structure your solution in a different way, so long as# you ensure the variables used in the next Test section are defined (ie. endpointSum, topTenErrURLs).from operator import addnot200 = access_logs.filter(lambda log : log.return_code != 200)
endpointCountPairTuple = not200.map(lambda log : (log.endpoint,1))
endpointSum = endpointCountPairTuple.reduceByKey(add)
topTenErrURLs = endpointSum.takeOrdered(10, lambda s: -1 * s[1]) #giving error at takeOrderedprint 'Top Ten failed URLs: %s' % topTenErrURLs I'm calling map on fullCorpusRDD, passing each list of tokens, and the idfsFull.collectAsMap() to the tfidf function.

However, when I collect the idfsFullWeights I see that it's a list of dictionaries.

This doesn't seem correct.

I'm calling mapValues on amazonFullRecToToken and passing the value to tdifs, with the broadcasted idfsFullWeights lists of dicts as a parameter. This is failing because tdifs is expecting a single dict.

What am I doing wrong here? Hello!
I'm stuck in the fastCosineSimilarity function now. All tests are passing for me so far, I have also checked the outputs of the 4b weights (some other students posted their results). So far, all is good, but test 2 fails in 4f - I get bad cosine similarity value.
Could you please help me figure it out? Blow you can find code sample (part of my solution) and the result for the test case:

  s = sum([amazonWeightsBroadcast.value[amazonRec][token] * googleWeightsBroadcast.value[googleRec][token] 
             for token in tokens])    
    
    amazonNorm = amazonNormsBroadcast.value.get(amazonRec)
    googleNorm = googleNormsBroadcast.value.get(googleRec)

    value = s/(amazonNorm * googleNorm)
And below is the output for the test case:

Total count: 2441100
test case result: [(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), 4.3321433023045483e-07)]

Thank you in advance! def parseData(filename):    return (sc.textFile(filename, 4, 0).map(parseDatafileLine).cache())
def loadData(path):    filename = os.path.join(baseDir, inputPath, path)    raw = parseData(filename).cache()
parseData returns a RDD cached, that is cached again in loadData. 
I believe it is redundant, but as i am new in Spark, maybe it is a needed trick.
Anybody understand it? Hi,

I have written code for 1(c) as below:

amazonSmall.map(lambda x: (x[0],tokenize(x[1])))


And in the function I have written it as :
return vendorRDD.mapValues(lambda x: sum(x))

I am getting ouput as

There are UnionRDD[72] at union at NativeMethodAccessorImpl.java:-2 tokens in the combined datasets
Please let me know to fix this. hostsPick20 = (hostMoreThan10 .map(lambda s: s[0]) .take(20))
please help me to understand code in bold. s:s[0] what does it mean I am attaching a screenshot of the problem faced during the typing of the command in cmd. I have successfully completed lab 1,2 and almost 90% of my lab3. I am facing timeout issues while waiting for the machine to boot. Any sort of help would be greatly appreciated. 

 not200 = access_logs.map(lambda log:(log.response_code,1)).reduce(lambda a,b:a+b)
print not200.top(10)

above code is seems to be not executing . its is not displaying any info . 
i am on win8 and using chrome . 
i also restart my machine but again when i reached to the above code first time it shows busy on the top end of the browser but then it start displaying nothing . please help me on this 

i also tried 
not200 = access_logs.map(lambda log:(log.response_code,1)).reduceByKey(lambda a,b:a+b)
print not200.top(10)
 Hi

I have a question for the academic staff:

I want to take this course to earn the XSeries Big Data Certificate, how do I proceed to do that?

Best regards

Ole C. Brudvik When I try to run this line
similarities = (crossSmall .map(lambda record: computeSimilarity(record)) .cache())

I get the following error:
Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.

 
Plz suggest how can similarities be constructed. Basically for every record present in crossSmall I want the record to get mapped to the output of computeSimilarity() Hello! 
Kernel always die when I execute cell 4f (where fastCosineSimilarity function is defined). This occurs even when i comments all calculation operators and function must be only return key (see next function code):

amazonRec = record[0][0] googleRec = record[0][1] tokens = record[1]
key = (amazonRec, googleRec) return key

why does kernel always die ? Is it wrong function code or this occurs by reason wrong definition of broadcast variables that also are defined in this cell ?
I define  broadcast variables like these:

amazonWeightsBroadcast = sc.broadcast(amazonWeightsRDD.collectAsMap())googleWeightsBroadcast = sc.broadcast(googleWeightsRDD.collectAsMap())

Thanks in advance Hi,

Takeordered returns the record id as well as count of the value.

[('b000o24l3q', 1547)]

How to sortbyvalue and get the top value. I pass the tests all the way to 4f but %100 of my grading fails with this message

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 1250
SyntaxError: Non-ASCII character '\xe2' in file /ok/submission.py on line 1251, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'simpleTokenize' is not defined

Removing stopwords (1b)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tokenize' is not defined

Tokenizing the small datasets (1c)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'totalTokens' is not defined

Amazon record with the most tokens (1d)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'biggestRecordAmazon' is not defined

Implement a TF function (2a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tf' is not defined

Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'corpusRDD' is not defined

Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueTokenCount' is not defined

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'rec_b000hkgj8k_weights' is not defined

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dp' is not defined

Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsSmallBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonFullRecToToken' is not defined

Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsFullCount' is not defined

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonNormsBroadcast' is not defined

Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'invert' is not defined

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'commonTokens' is not defined

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similaritiesFullRDD' is not defined In 2b we created a pair RDD, In 2c we start with an RDD which is not a pair. 
What was the purpose of 2b. 
Will we know the answer in a later lab? After doing the join and the swap and reduceby I am trying to do a map to get the data into a list as follow:

[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120'])]

But my map below not quite right, any thoughts, thanks a lot.
map(lambda (x,y):x[0][1]+y[1][1],token) It certainly does not recognise 'token'

My output before the final map is as such:
[('rom', 'b000jz4hqo')]
[('quickbooks', 'http://www.google.com/base/feeds/snippets/11125907881740407428')] Hello everyone, I've been stuck at part 4e for almost two days, still can't figure out what's wrong. I try to do a join on the google and amazon inverted pairs, and I get a crash.

Here's my code
commonTokens =  googleInvPairsRDD.join( amazonInvPairsRDD, numPartitions=None )

I'm not sure how to debug this one as well. Here's my output:

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-224-6a5ef2fe9708> in <module>()
     25                )
     26 
---> 27 print 'Found %d common tokens' % commonTokens.count()
     28 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 827.0 failed 1 times, most recent failure: Lost task 7.0 in stage 827.0 (TID 3158, localhost): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:172)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:108)
	... 10 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
<p></p>
<p></p>
<p>I pass all tests before 4e, here's some output I get</p>
<p></p>
<p>The tf-idf dictionary seems correct, this is what I get:</p>
<p></p>
<p>print amazonWeightsRDD.take(1)<br />print googleWeightsRDD.take(1)</p>
<p>this returns</p>
[('b000jz4hqo', {'rom': 2.4051362683438153, 'clickart': 56.65432098765432, '950': 254.94444444444443, 'image': 3.6948470209339774, 'premier': 9.27070707070707, '000': 6.218157181571815, 'dvd': 1.287598204264871, 'broderbund': 22.169082125603865, 'pack': 2.98180636777128})]
[('http://www.google.com/base/feeds/snippets/11125907881740407428', {'quickbooks': 17.48190476190476, '2007': 4.985334057577403, 'learning': 5.932773109243698, 'intuit': 13.379008746355684})]
<p>The inverse pairs seem correct as well, here's what I get:</p>
<p></p>
<p>print googleInvPairsRDD.take(2)<br />print amazonInvPairsRDD.take(2)</p>
<p>this will return: </p>
[('quickbooks', 'http://www.google.com/base/feeds/snippets/11125907881740407428'), ('2007', 'http://www.google.com/base/feeds/snippets/11125907881740407428')]<br />[('rom', 'b000jz4hqo'), ('clickart', 'b000jz4hqo')]
<p></p>
<p>Thanks a lot in advance?</p>
<p></p> Hi,

I'm stuck at lab3 4b i'm jsut doing a union of amazonFullRecToToken on googleFullRecToToken and then calling idfs but it never stops.
My computer is a mac fairly recent.

I must doing something worng but i have no idea.

How long does it take approximatively?
thanks when week 5 course videos and Lab available? Im stuck with the following logic:
  return filter(None, re.split(split_regex, quickbrownfox.lower()))

The first test is passing, but the rest of the 3 assert tests are failing.

Please help. Hi, 

I know there are numerous threads on the issue related to 4e but please can someone provide clues or point the problem in my code below - 

def swap(record):
 """ Swap (token, (ID, URL)) to ((ID, URL), token)
 Args:
 record: a pair, (token, (ID, URL))
 Returns:
 pair: ((ID, URL), token)
 """
 token = record[0]
 keys = record[1]
 return (keys, [token])
commonTokens = (amazonInvPairsRDD.join(googleInvPairsRDD).map(swap).reduceByKey(lambda t1,t2: t1+t2).cache())
keys need to be in parentheses here: try keys=(record[1])

When I run this it's failing with the error below - 
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-398-937c22378562> in <module>()
     14 #print amazonInvPairsRDD.join(googleInvPairsRDD).take(1)
     15 
---> 16 commonTokens = (amazonInvPairsRDD.join(googleInvPairsRDD).map(swap).reduceByKey(lambda t1,t2: t1+t2).collect().cache())
     17 
     18 print commonTokens.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 931.0 failed 1 times, most recent failure: Lost task 0.0 in stage 931.0 (TID 3812, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 253, in mergeValues
    d[k] = comb(d[k], v) if k in d else creator(v)
MemoryError

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

The code works if I remove reduceByKey but then it's not the right answer. 

Any help would be appreciated.

Cheers,
 Is performance of Spark running in VM better than that running not in the distributed way?
If I have just one laptop, how to reduce the running time? 
In lab4 , we will do collaborative filtering, would it be faster with spark in VM compared without spark? I wrote my code for lab3 2c and got the answer as:

There are 4772 unique tokens in the small datasets.

But when I run the test i get the following:






1 test passed.






---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-127-3fe135a7fbe5> in <module>()
      1 # TEST Implement an IDFs function (2c)
      2 Test.assertEquals(uniqueTokenCount, 4772, 'incorrect uniqueTokenCount')
----> 3 tokenSmallestIdf = idfsSmall.takeOrdered(1, lambda s: s[1])[0]
      4 Test.assertEquals(tokenSmallestIdf[0], 'software', 'incorrect smallest IDF token')
      5 Test.assertTrue(abs(tokenSmallestIdf[1] - 4.25531914894) < 0.0000000001,'incorrect smallest IDF value')

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in takeOrdered(self, num, key)
   1172             return heapq.nsmallest(num, a + b, key)
   1173 
-> 1174         return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
   1175 
   1176     def take(self, num):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 180.0 failed 1 times, most recent failure: Lost task 0.0 in stage 180.0 (TID 559, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1174, in <lambda>
    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
  File "/usr/lib/python2.7/heapq.py", line 412, in nsmallest
    return [min(chain(head, it), key=key)]
  File "<ipython-input-127-3fe135a7fbe5>", line 3, in <lambda>
TypeError: 'int' object has no attribute '__getitem__'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

I dont know whats wrong?




 This part asks for computing the product of TF and IDF weights.
Recommend to tackle this as pure python code over two dictionaries.
 Hello everybody,

I don't understand what is a dictionary , tuple, RDD.

My crossSmall has the format

[(('http://www.google.com/base/feeds/snippets/9997282084409978802', 'dr. seuss reading learning system 2008 encore "key features: develop reading skills dr. seuss classic books ..." '), ('b000s8jxpc', 'gods: lands of infinity - special edition "the ancient world of bellarion struggled in the never ending war of gods. the conflict began when xarax ruler of the gods was murdered. each of the remaining gods claimed the right to the divine throne. after many battles the defeated gods refused to submit to the victors and instead summoned their armies to attack the worshippers of other gods. the deaths of those worshippers weakened the power of the conquering gods. after 2000 years of battles bellarion lay in ruins. no one was able to achieve victory until the god of darkness mortagon found a new weapon which resisted all magic and divine powers. as the last hope arswaargh the god of fire created vivien who he fashioned after his own astral body. arswaargh sent vivien through space and time to seek the counter-weapon needed to defy mortagorn but unfortunately she lost her divine essence while traveling through the magical portal to another world. vivien must now seek the gods of the new world gain back her power and search for the counter-weapon against mortagorn. gods: lands of infinity special edition is a blend of adventure and action in a turn-based setting. there is an array of missions and npc (non player characters) that players can add to their team. players must fight gain experience level and acquire new skills. the game is played out in the first-person perspective until gamers enter a battle encounter at which point the game goes into third-person turn-based battle. gods: lands of infinity special edition invites you into an exciting world full of adventures battles and quests of unearthly affairs. spend hours discovering the world of antasion trading and completing the lead quests of each race or kingdom and many side quests. highly advanced shader system providing realistic effects of natural ice water grass and other nature elements. animated environment with high-resolution photo-realistic textures over two hundred" "strategy first"'))]

What format does crossSmall have? it's tuple, RDD, dictionary

In the other hand  in computeSimilarity(record). 

What is record?

Could somebody explain me this question?

Thanks in advance

Carlota Vina




 it's been 2 hours since i submitted lab3, but it's not graded yet, yes i've read @3109 and other posts regarding the autograder, and did what youre suggesting, but it doesnt seem to resolve the problem I am stumped on the dictionary operation.

def tfidf(tokens, idfs):

tfIdfDict = {}    tfs = tf(tokens)        for (k, v) in tfs.items():        tfIdfDict[k] = v * idfs(k)        return tfIdfDict

The logic looks right. However, it gives error for tfIdfDict[k] = v * idfs(k)

Please help.
 I am stuck with lab3 4d. Getting differnt token counts. I checked back both WeightsRDD count and getting same count.

Please check the counts that I generated and provide hints to fix this.

There are 90427 Amazon inverted pairs and 62987 Google inverted pairs.

1 test passed.1 test failed. incorrect amazonInvPairsRDD.count()1 test failed. incorrect googleInvPairsRDD.count()

print countTokens(amazonFullRecToToken)print countTokens(amazonWeightsRDD)print countTokens(googleFullRecToToken)print countTokens(googleWeightsRDD)print googleWeightsRDD.take(3)
197846904279952762987[('http://www.google.com/base/feeds/snippets/11125907881740407428', {'quickbooks': 38.095238095238095, '2007': 6.015037593984962, 'learning': 6.015037593984962, 'intuit': 19.047619047619047}), ('http://www.google.com/base/feeds/snippets/11538923464407758599', {'better': 1.5384615384615385, 'kids': 1.666666666666667, 'help': 0.8695652173913043, 'read': 4.0, 'puzzle': 10.0, 'writing': 15.0, 'write': 6.666666666666668, 'exercises': 10.0, 'designed': 0.9523809523809524, 'learn': 1.25, 'fun': 2.0, 'reading': 5.0, 'creative': 1.5384615384615385}), ('http://www.google.com/base/feeds/snippets/11343515411965421256', {'customer': 6.060606060606061, 'tracking': 9.090909090909092, '6': 1.8181818181818183, 'pos': 18.181818181818183, 'sales': 9.090909090909092, '0': 0.7905138339920948, 'qb': 36.36363636363637, 'inventory': 18.181818181818183, 'retailers': 18.181818181818183, 'basic': 9.090909090909092, 'need': 0.47846889952153115, 'mngmt': 18.181818181818183, 'retail': 3.0303030303030307, 'intuit': 6.060606060606061, 'software': 0.3868471953578337})] Like many others, I've struggled to understand and write the code for these 2 parts.  I've had to read the forum to get an inkling on how to approach and reread list comprehension to complete it.  I usually try to finish the lab BEFORE I get on the forum as answers end up getting posted.  Anyways, here's one approach, where I post test data and results that should allow you to get at the solution.  There will be no code. As the instructors suggested, please instrument your code with print statements or call with test data to figure out what's going on.

# with made up info; swap should return in the suggested data structureprint swap(('aided 120', ('b0007yepy6', 'http://www.google.com/base/feeds/snippets/17293379770636740081')))(('b0007yepy6', 'http://www.google.com/base/feeds/snippets/17293379770636740081'), ['aided', '120'])
print swap(('aided 120 45 aa', ('b0007yepy6', 'http://www.google.com/base/feeds/snippets/17293379770636740081')))(('b0007yepy6', 'http://www.google.com/base/feeds/snippets/17293379770636740081'), ['aided', '120', '45', 'aa'])print swap(('aided', ('b0007yepy6', 'http://www.google.com/base/feeds/snippets/17293379770636740081')))(('b0007yepy6', 'http://www.google.com/base/feeds/snippets/17293379770636740081'), ['aided'])#do note how the 2nd element is, especially with multiple text
You need to make 4e spit out about for swap().

Then you apply it on the RDD, with the usual semantics of merging,getting values of the same key into a RDD

[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120']), (('b000bnb72g', 'http://www.google.com/base/feeds/snippets/6959766318020460134'), ['software']), (('b00009apna', 'http://www.google.com/base/feeds/snippets/13986409474793202514'), ['business']), (('b00004ochi', 'http://www.google.com/base/feeds/snippets/7147648211015076337'), ['rom']), (('b000gcgqvy', 'http://www.google.com/base/feeds/snippets/6874875179525744781'), ['win', 'xp'])]
Note how 'win' 'xp' is done, it comes out as list

Now for 4f, you need to split the record and once you identify tokens, you will see it as a list. Use list comprehension to accumulate appropriately to compute s and then sum the new list. I presume you are using the RDD datasets generated a few steps before, for the calculation of s and value

At least for me, the entire edifice of the solution, was how swap spit out the list.  That took a few hours.  When generating commonTokens, I get an error when i use the reduceByKey to create a list of common tokens.

Here's a sample after my map function:
[(('b0007yepy6', 'http://www.google.com/base/feeds/snippets/17293379770636740081'), ['aided']), (('b000hlt5j4', 'http://www.google.com/base/feeds/snippets/17293379770636740081'), ['aided']), (('b000dz9yoa', 'http://www.google.com/base/feeds/snippets/17293379770636740081'), ['aided']), (('b00004ochi', 'http://www.google.com/base/feeds/snippets/17293379770636740081'), ['aided']), (('b000ap422c', 'http://www.google.com/base/feeds/snippets/13676550626962262162'), ['four'])]

This is follwerd by reduceByKey(lambda a,b: a+b) to create a list containing common terms but I get the error below. Can somone please suggest what I might be doing wrong?

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 876.0 failed 1 times, most recent failure: Lost task 7.0 in stage 876.0 (TID 4393, localhost): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:172)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:108)
	... 14 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span> yesterday I submitted the lab result and got 94%, but today it disappeared. I got the submmit error message 

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 1177
SyntaxError: Non-ASCII character '\xc2' in file /ok/submission.py on line 1178, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'simpleTokenize' is not defined

Removing stopwords (1b)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tokenize' is not defined

Tokenizing the small datasets (1c)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'totalTokens' is not defined

Amazon record with the most tokens (1d)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'biggestRecordAmazon' is not defined

Implement a TF function (2a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tf' is not defined

Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'corpusRDD' is not definedany ideas? thanks. The docstring for (part 0) parseDatafileLine should read something like this:
""" Parse a line of the data file using the specified regular expression pattern    Args:        datafileLine (str): input string that is a line from the data file    Returns:        a tuple consisting of:
   - string parsed using the given regular expression and without the quote characters
   - one of (-1, 0, 1) where -1 indicates an error, 0 the header line and 1 a successfully parsed line    """ return string.lower().split(' ')
This is my code,


['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']This is the response. help me remove the punctuation and empty spaces in between
 I didn't realize this course can get this tougher. It took more than 5 days and nights to complete it. This course started out good and soon got more and more difficult particularly for working professionals. Even though I completed this lab I can't say I understood the concepts and might take more time for me to understand them. I would think this course is more suitable for a semester course than a 4-5 weeks course and I would prefer I learn the concepts better than to just complete the labs hence my suggestion is to make this a longer course or split them to two or more sub courses so that we can benefit learning better.

 I don't quite understand what is happening in this part of code.


For here https://docs.python.org/2/library/os.path.html#os.path.join I am reading that it somehow operates on the path. But the meaning is so blurry, especially, in the context of the notebook code.

Could anyone explain this in plain text please? Would appreciate. I feel (maybe I am) a complete noob. But in the exercise it says 

Where can I notice that? I have run 1a and 1b, but I have no output. Am I missing something here? I am very new to python, so please kindly help, !! Don't understand what is wrong with my lab 3 3c.
I am newbie in Python.
I read almost all posts about lab 3 3c.
My code is:
crossSmall = <REDACTED>
I got error:
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-30-bca4481e17a7> in <module>()
     36             .collect()[0][2])
     37 
---> 38 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     39 print 'Requested similarity is %s.' % similarityAmazonGoogle

<ipython-input-30-bca4481e17a7> in similar(amazonID, googleURL)
     33     """
     34     return (similarities
---> 35             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     36             .collect()[0][2])
     37 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 55.0 failed 1 times, most recent failure: Lost task 0.0 in stage 55.0 (TID 216, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-30-bca4481e17a7>", line 19, in computeSimilarity
  File "<ipython-input-24-d9c3d83350ed>", line 13, in cosineSimilarity
  File "<ipython-input-22-f3d58cc3f8c4>", line 32, in cossim
  File "<ipython-input-22-f3d58cc3f8c4>", line 12, in dotprod
TypeError: reduce() of empty sequence with no initial value

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
<p><p>idfsSmallWeights type is dictionary.</p> <p>All earlier tests are passed.</p> <p>I restarted my computer.</p></p> I've been working on this for awhile and am stuck on why the number of distinct values comes back as 15144 and not 4772.  I'm not sure where the problem is as I'm not getting the desired feedback from the tokenSumPairTuple variable.

    uniqueTokens = corpus.flatMapValues(lambda x: x).distinct()       tokenCountPairTuple = uniqueTokens.map(lambda x: (x, 1))        tokenSumPairTuple = tokenCountPairTuple.reduceByKey(lambda a,b: a + b) When i use : amazonRecToToken = amazonSmall.take(1) and print it, i get 
[('b000jz4hqo', 'clickart 950 000 - premier image pack (dvd-rom)  "broderbund"')]
It is a RDD in the format (key,value)For tokenizing the value do I have to use a map or lambda function ?Thanking you in Anticipation I have a corpus which is the combination of amazonRecToToken and googleRecToToken, but I don't understand what is meant by the number of documents in the corpus... Does each entry constitute a document (i.e. one item from the Amazon or Google database), or does the whole Amazon / Google file count as the document...? Really confused... Hi,

I'm trying to upload a new version of my homework, but i didn't get any response from grader. It's only shows me a yellow square but doesn't give any response or takes a any of avaiables submissions, always prompt 1 of 10.

Any help?

Thanks in advance Good evening,

I was not able to respond to Lab3 4e and 4f. But when I did send to the autograder my python file (
b00ce7f7f9e4c9fa96722f015361a14a:2e0f9f73de73c2a21cc0847655ce578a:ip-172-31-18-174
)
I did receveid a lot of errors .. and my file is not being validated at all.

I have worked very hard for that lab.. And seeing no "points" for that is making me very sad.

How could I send partial Lab ?

Thanks for your feed back

Benoit What should be a simple function that puts to RDDs together, I get the following error:

AttributeError                            Traceback (most recent call last)<ipython-input-96-3abb91a73f2e> in <module>()      1 # TODO: Replace <FILL IN> with appropriate code----> 2 corpusRDD =  amazonRecToToken.union(googleRecToToken)      3 test = corpusRDD.count()      4 print 'There are %s tokens in the combined datasets' % test/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in union(self, other)    474         """    475         if self._jrdd_deserializer == other._jrdd_deserializer:--> 476             rdd = RDD(self._jrdd.union(other._jrdd), self.ctx,    477                       self._jrdd_deserializer)    478         else:/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _jrdd(self)   2291                                              env, includes, self.preservesPartitioning,   2292                                              self.ctx.pythonExec,-> 2293                                              bvars, self.ctx._javaAccumulator)   2294         self._jrdd_val = python_rdd.asJavaRDD()   2295 /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)    690     691         args_command = ''.join(--> 692                 [get_command_part(arg, self._pool) for arg in new_args])    693     694         command = CONSTRUCTOR_COMMAND_NAME +\/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_command_part(parameter, python_proxy_pool)    263             command_part += ';' + interface    264     else:--> 265         command_part = REFERENCE_TYPE + parameter._get_object_id()    266     267     command_part += '\n'AttributeError: 'function' object has no attribute '_get_object_id'
 HI, Im trying to solve the 4f, all the previous test passed succesfully, this mornig while looking at the forum I see differences from the output of googleWeightsRDD!!
So Im very stunned!, because in my values are wrong it means some function before has to be bad.
I really appreciate the help for fix this, cos I dont know where to start!! 
Its very weird but is like the  tfidf and idfs functions are incorrect, but in each test they output the correct results, I restarted the kernel but the results are the same, below I made a test to see, and you can compare with your results:

print amazonWeightsRDD.filter(lambda x:x[0]=='b00005lzly').collect()
print amazonNorms.filter(lambda x:x[0]=='b00005lzly').collect()



[('b00005lzly', {'coach': 1.8045112781954886, 'text': 0.022958158755667795, 'global': 0.033049657109807486, 'manager': 0.03132832080200501, 'guides': 0.1156737998843262, 'managerandquot': 3.007518796992481, 'writing': 0.08471883935190087, 'better': 0.019917343026440274, '0': 0.015344483658124903, 'coaching': 1.0025062656641603, 'samples': 0.10370754472387865, 'employee': 1.0614772224679345, 'get': 0.008377489685215825, 'assistant': 0.3341687552213868, '120': 0.5012531328320802, 'new': 0.004509023683646899, 'day': 0.08020050125313283, 'appraiser': 6.015037593984962, 'necessary': 0.09113693324219639, 'name': 0.022784233310549098, 'specific': 0.04850836769342711, 'templates': 0.0639897616381379, 'library': 0.027341079972658916, 'export': 0.03341687552213868, 'descriptions': 1.5037593984962405, 'progress': 0.07914523149980214, 'recognition': 0.12531328320802004, 'sub': 0.058970956803774136, 'memos': 3.007518796992481, 'review': 0.0639897616381379, 'libraries': 0.10741138560687433, 'import': 0.04971105449574349, 'business': 0.008285175749290581, 'marketing': 0.07247033245765015, 'job': 0.18413380389749884, 'benefits': 0.03957261574990107, 'language': 0.0633161851998417, 'action': 0.02506265664160401, 'makes': 0.015037593984962405, 'objectives': 0.6015037593984962, 'software': 0.003240860772621208, 'features': 0.013838276673278287, 'tools': 0.006412620036231303, 'management': 0.013859533626693461, 'platform': 0.03538257408226449, 'support': 0.021714937162400586, 'due': 0.2313475997686524, 'start': 0.020741508944775732, 'notices': 3.007518796992481, 'type': 0.04296455424274973, 'mailer': 3.007518796992481, 'complete': 0.0151894888736994, 'form': 0.051853772361939325, 'hr': 3.007518796992481, 'company': 0.036235166228825076, 'license': 0.034047382607462054, 'pc': 0.01145721446473326, 'reviews': 0.20050125313283207, 'corrective': 1.0025062656641603, 'scan': 0.08845643520566121, 'calculator': 1.0025062656641603, 'toolkit': 0.09398496240601503, 'sample': 0.050974894864279335, 'information': 0.0296794618124258, 'documents': 0.04591631751133559, '1': 0.0038118108960614465, '5': 0.01965698560125805, 'objective': 3.007518796992481, 'performance': 0.04626951995373047, 'development': 0.04971105449574349, 'product': 0.032338911795618076, 'finish': 0.06137793463249962, 'plans': 0.09398496240601503, 'ratings': 0.18796992481203006, 'track': 0.017691287041132243, 'miscellaneous': 0.37593984962406013, 'includes': 0.00895094880057286, 'helping': 0.10370754472387865, 'notebook': 0.13076168682576003, 'user': 0.006653802648213454, 'date': 0.0385579332947754, 'data': 0.012127091923356778, 'compatibility': 0.04119888763003399, 'tune': 0.07335411699981662, 'faster': 0.0151894888736994, 'reminders': 0.3341687552213868, 'english': 0.025705288863183597, 'deluxe': 0.0578368999421631, 'quantity': 0.12030075187969924})]
[('b00005lzly', 10.07332144010543)]
 The 2 EdX Spark courses are amazing. To complete the series I would like to suggest a 3rd course, where the focus is Spark Streaming, which could include labs such as:
- Real time Twitter sentiment analysis
- Real time analysis of error logs
- Real time analysis of SmartPhone data (It could be cool to stream smartphone data real time to a server that could be analyzed with Spark)
- Real time fraud analysis
- Etc.
I'm looking forward to the next Spark courses from EdX / DataBricks / Berkeley. 

 From the documentation https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cache
it reads that cache(): Persist this RDD with the default storage level (MEMORY_ONLY_SER)
But isn't this 'state' of 'resiliency' of RDD 'persistent' by definition of RDDs?
I am a bit confused here. What difference does caching mean if RDDs are in memory anyway and are resilient, i.e. persistent by definition?
Or probably am I missing a crucial difference between persistence and resilience in terms of datasets?
Would be grateful for hints.

EDIT: and if cache() is an action like collect(), then what is the difference between them both?
 Hi,
 
I am trying to get dotProd using below code :
return {k:sum(v*b[k]) for k,v in a.items() if k in b}
 
but it is giving me the following error. I am just iterating here over keys which are string , then why is it giving error on int??
 
Please help.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-73-43b2425fbd2d> in <module>()
     34 testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }
     35 testVec2 = {'foo': 1, 'bar': 0, 'baz': 20 }
---> 36 dp = dotprod(testVec1, testVec2)
     37 nm = norm(testVec1)
     38 print dp, nm

<ipython-input-73-43b2425fbd2d> in dotprod(a, b)
     10         dotProd: result of the dot product with the two input dictionaries
     11     """
---> 12     return {k:sum(v*b[k]) for k,v in a.items() if k in b}
     13 
     14 def norm(a):

<ipython-input-73-43b2425fbd2d> in <dictcomp>((k, v))
     10         dotProd: result of the dot product with the two input dictionaries
     11     """
---> 12     return {k:sum(v*b[k]) for k,v in a.items() if k in b}
     13 
     14 def norm(a):

TypeError: 'int' object is not iterable I did it, and got 
There are 22520 tokens in the combined datasets This is my token for the submission:

Your submission token id is 1193170-0ab719d5701035e00dc5f999eddbbd84:bc4e81a8f530b7ed122f1566f1718bcf:ip-172-31-29-173

Though I pass the the tests and the output looks fine, I seem to be getting %100 failure. I did copy and paste some of the parts just to remove some unforeseen characters. this is my output, seems to be complaining from 1a and can not see a file and simpleTokenize

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 1254
SyntaxError: Non-ASCII character '\xe2' in file /ok/submission.py on line 1255, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'simpleTokenize' is not defined

Removing stopwords (1b)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tokenize' is not defined

Tokenizing the small datasets (1c)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'totalTokens' is not defined

Amazon record with the most tokens (1d)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'biggestRecordAmazon' is not defined

Implement a TF function (2a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tf' is not defined

Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'corpusRDD' is not defined

Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueTokenCount' is not defined

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'rec_b000hkgj8k_weights' is not defined

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dp' is not defined

Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsSmallBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonFullRecToToken' is not defined

Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsFullCount' is not defined

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonNormsBroadcast' is not defined

Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'invert' is not defined

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'commonTokens' is not defined

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similaritiesFullRDD' is not defined

-- 0 cases passed (0.0%) -- When i use .map() with my RDD it gives me 200 tokens for amazonRecToToken but when I use .flatMap() I get 16707 token for amazonRecToToken.Can please anyone explain me the reason behind this ? I am not sure how to get around this. Any pointers will be appreciated!

5a & b passed.
Lab3/5c fails with the following stack trace:(I thought it has got something to do with the parenthesis on the indicated line, so I added a couple of good measure but that did not help )
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-88-6cb7b4e378d1> in <module>()
      1 thresholds = [float(n) / nthresholds for n in range(0, nthresholds)]
----> 2 falseposDict = dict([(t, falsepos(t)) for t in thresholds])
      3 falsenegDict = dict([(t, falseneg(t)) for t in thresholds])
      4 trueposDict = dict([(t, truepos(t)) for t in thresholds])
      5 

<ipython-input-86-3c61f0dc3f4d> in falsepos(threshold)
     48 def falsepos(threshold):
     49     fpList = fpCounts.value
---> 50     return sum([fpList[b] for b in range(0, BINS) if ((float(b) / nthresholds) >= threshold)])
     51 
     52 def falseneg(threshold):

TypeError: 'float' object is not callable Hi,

I'm having this exception execution code from 3c. I don't know how interpret this trace to fix my error, any idea?

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-317-d2c402ef69fd> in <module>()
     36             .collect()[0][2])
     37 
---> 38 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     39 print 'Requested similarity is %s.' % similarityAmazonGoogle

<ipython-input-317-d2c402ef69fd> in similar(amazonID, googleURL)
     33     """    
     34     return (similarities
---> 35             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     36             .collect()[0][2])
     37 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 300.0 failed 1 times, most recent failure: Lost task 0.0 in stage 300.0 (TID 926, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-317-d2c402ef69fd>", line 19, in computeSimilarity
  File "<ipython-input-315-d9c3d83350ed>", line 11, in cosineSimilarity
  File "<ipython-input-311-f5c5d045b2e1>", line 13, in tfidf
KeyError: 'google'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

<p></p>
<p>Thanks,</p>
<p></p>
<p>Carlos</p> what does the certification of  50 dollars valid for one year?
after one year, I lost y certification? This link:
https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/courseware/80e5487245874eeba0447fbf9e133faf/f57237079dcd4c6f8945284a11509766/

is not working for me The approach I'm using is mapping the similairty scores within trueDupsRDD, doing a sum and division casting as float.

Although I am able to code this without getting an error, not able to extract the float since it is returning as RDD. So assuming my general approach is correct, how do if return the result as a value  which can be printed rather than as a RDD?

Or is there a way to convert the RDD - take(), collect(), etc. don't work. If I do that I receive
    TypeError: 'float" object is not interable

OK - figured it out, was attempting to do the division within the map moved it outsied and it works correctly now. However any general information on converting RDDs to values would be appreciated.


   example -  "....of true duplicates is PythonRDD at RDD at PtyhonRDD.scala:43. count N cast floatflatMapValuesmap (a,1.0)reduceByKey()at last map (a, N/b)


error says me  this why I couldn't understand
File "<ipython-input-25-e94906d9f70d>", line 32
    idfsSmall = idfs(amazonRecToToken.union(googleRecToToken))
            ^
SyntaxError: invalid syntax Hi, I need some help in 3b because my result is not ok.
The value that I got for 
[cossim(w1, w2), dotprod(w1,w1), norm(w1), norm(w2)]
is
[0.5002775978746291, 563.2716049382716, 23.73334373699314, 50.68968775248516]
and cossim(w1,w2) should be 0.0577243382163
any ideas?

EDIT
I realized that 
>>>print w1{'photoshop': 22.22222222222222, 'adobe': 8.333333333333334}
so my function tfidf gives me a dictionary but in other order. Why? I try to run inverse function, I checked the code in python and it works.  The problem is when I try to run it in lab 3 4 d, 

'PipelinedRDD' object is not iterable 

It's a concept that I couldn't grasp yet.  As map from the RDD I has this code:

...map(lambda x: (x[0],x[1].items())) 

I think I need add some kind of action, but something that in my opinion could be redundant, like groupeByKey().collect(), etc. But I've not been able to find the solution yet.  Can somebody help a hint o clue about it? Thanks in advance!

Santiago The first command in the DBC version of lab 4 is not working:
display(dbutils.fs.ls('/mnt/spark-mooc/cs100/lab4/small'))
(Oddly enough, on Databricks, I'm unable to copy and paste any lines that are output by a command, including the error message for the one above.)

The line reading in the files also does not work.  Is there some edit to the path that has to be made?

Thanks! Nothing happens when I run  3c Cell.Even if I print "Hello", it doesnt run. I tried restarting but nothing works.
Can anyone help? Can somebody explain to me what I'm supposed to do for Norm the function states

def norm(a):
    """ Compute square root of the dot product
    Args:
        a (dictionary): a dictionary of record to value
    Returns:
        norm: a dictionary of tokens to its TF values
    """
Yet the instructions before it say "a function `norm` that returns the square root of the dot product of a dictionary and itself" One says tf value the other says get the sqrt of the dictionary, I really would like some clarification on this

And I figure this code would work but no dice 

def norm(a):    x = [a for a in a.values()]        x = reduce(lambda j,y: j*y, x)        return math.sqrt(x)
 Hi everybody,

I get the following error when running 3c. If I am reading the error correctly the problem is at the very beginning in the function in simpleTokenize where I use string.lower()... All other test are ok. Any ideas? Thanks a lot!

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-58-f1245f3c3119> in <module>()
     35             .collect()[0][2])
     36 
---> 37 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     38 print 'Requested similarity is %s.' % similarityAmazonGoogle

<ipython-input-58-f1245f3c3119> in similar(amazonID, googleURL)
     32     """
     33     return (similarities
---> 34             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     35             .collect()[0][2])
     36 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 1 times, most recent failure: Lost task 0.0 in stage 81.0 (TID 324, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-58-f1245f3c3119>", line 18, in computeSimilarity
  File "<ipython-input-53-80ba49727d8a>", line 11, in cosineSimilarity
  File "<ipython-input-35-1004d5c514b2>", line 13, in tokenize
  File "<ipython-input-33-6f5c9c993eee>", line 12, in simpleTokenize
AttributeError: 'list' object has no attribute 'lower'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span> Hello,
By mistake, I uploaded lab 1 in the setup autograder section; previously this setup was uploaded in time, and graded with maximum mark.

As this happened after the deadline for setup autograder, I was downgraded to 80%. Is it possible that the first grade to be considered?

Thanks. I need some help/hint for lab 3 - 1(d)

When I do takeOrdered(1,..) on the previous RDD, my code correctly gets the record with key as 1547, and I can see that the corresponding value has the id 
b000o24l3q
& all the tokens for that id.

However  when I do takeOrdered, it gives me a list. I am not clear how to return just that id/tokens as an RDD from the function. Do I need to do some other RDD transformation?

('b000o24l3q', ['adobe', 'premiere',.....]

    s1 = r.takeOrdered(1, lambda s: -1* s[0])    print type(s1)  ==> This is list    print s1

<type 'list'>
[(1547, ('b000o24l3q', ['adobe', 'premiere', 'pro', 'cs3', 'upgrade', 'note', 'upgrade', 'version', 'adobe', 'premiere', 'pro', 'cs3', 'tell', 'story', 'maximum', 'impact', 'using', 'adobe', 'premiere', 'pro', 'cs3', 'upgrade', 'software', 'start', 'finish', 'solution',.........'windows', 'based', 'computer', 'adobe'])) I did detailed notes for Lab 2 (@2152). Here are my Notes on the Lab 3:Intended Audience is people who started late and don't have the time to make too many false starts.Important stuff:
This, for me was a Lab from hell. Completion time 13 hours. Hope you do better.Read through @3109 before you begin.Biggest Pain in the necks: 2c, 3c, 4e, 4f
#0
Read through, carefully.#11a)
Straight forward.Hints in @3109 basically solve this for you.Look here: https://docs.python.org/2/tutorial/datastructures.html#functional-programming-tools. It says Python lists have map(), reduce() & filter() methods available. Pick what you may need.
1b)
Don't overthink and build over simpleTokenize() you wrote above.Notice variable stopwords is a set. Set provides "in", "not in" constant time operators.
1c)
My mind went blank for sec, then it came to me. We are trying to get a number out, hence we should use an "action" ultimately. Easy when you "get" it.N ways you could solve for # of tokens, but ensure that amazonRecToToken and googleRecToToken are what the question wants it to be. They are used a bunch of times later.

1d)
Follows from c, this one is easy, only one method we came across does let you specific a sorting method & the # of elements to pick.Remember what the method is meant to return.
#22a) Easy, No hints needed if you know Python. I don't so I took a while to find the right operators.2b) Easy, Look for the right transformation in Spark Programing Guide: https://spark.apache.org/docs/latest/programming-guide.html#transformations . Also, solved for you in 2c! :P2c)
There is a pinned discussion about this: @3189 & @3042. Read them, just in case.This one was hard for me. Calculating N is trivial.After that, if you follow the instructions, calculating uniqueTokens is easy too. But after that I couldn't figure out what to do based on the instructions.So I did something I could follow. Hints:
Since there is counting invovled, I wanted to create tuples like [(token1, 1), (token2, 1), ...] which I could run a reduceByKey() on.So, in the corpus RDD, the 2nd element is a list, I wrote a Python method to return a unique token tuple list per document and then flatten it.Then follow the logical progression and complete.

2d, 2e) Nothing to do.2f) is actually easy, takes 4-5 lines of code to do. Just follow the instructions and implement what it takes according to the method signature.#3:#2 needs to be completed to attempt this.
3a)
Straight-forwardmath.pow/math.sqrtI used Python set intersection for dotprod. Made most sense to me.
3b)

Also, easy. As they say in the instructions, use tokenize() and tfidf(). Look at code in 2f to see how to call tfidf.In retrospect, I made a mistake here and paid the price in 3c when my math went haywire.

3c)
https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD . Find a transformation that gives EXACTLY what crossSmall is supposed to be.All straight forward after that.I spent 2 hours struck on this as my cosineSimilarity method was incorrect.Or your cosine calculator methods might be buggy. See here for some excellent help @3198.
3d) Same as c3e)

For trueDupsRDD, it says use join() so use it.For, nonDupsRDD go here and find a transformation that can work: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD
#44a) Like how we did 1c.4b) Just run through what the steps suggest, no surprises there. Apply tfidf() correctly.4c) Easy-peasy.4d) Write invert() correctly and use a single flattening transformation.4e)
Runs long. Be patient.swap() is easy.Look at @3232What wasn't very clear to me: commonToken is meant to look like: [((ID, URL), [list of tokens they share in common])].Obviously the above calls for one of those Bykey() ops.
4f)
I found this hard: instructions seemed very vague to me.Creating/Declaring the broadcast variables was easy: I couldn't figure out how to use them right.@3484 has code snippets in the question by a "saint" which basically showed me how to use those broadcast variables.
#5

Nothing to do here, if you are done everything corretly until this moment, the universe shall reward you. If you did and it doesn't, then restart your kernel and run through all the code again. That worked for me.Most of the stuff here takes a while to run. Be Patient.
BEST OF LUCK! :)
#pin Hi all,
I don't see the option to download as python .py any idea why ?

 I get an unhashable type: list error in the following line:

for each in tokens: s = s + amazonWeightsBroadcast.value[amazonRec][each] * googleWeightsBroadcast.value[googleRec][each]

I checked some of the other posts for the same error but still am clueless about how to fix this. Pls help. 

My stack trace is like this:
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-70-d7e24e105194> in <module>()
     29                       .cache())
     30 
---> 31 print similaritiesFullRDD.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 184.0 failed 1 times, most recent failure: Lost task 0.0 in stage 184.0 (TID 718, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-70-d7e24e105194>", line 28, in <lambda>
  File "<ipython-input-70-d7e24e105194>", line 20, in fastCosineSimilarity
TypeError: unhashable type: 'list'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) 

 I think I am missing something fundamental. If the number of inverted token pairs are 
111387 Amazon inverted pairs and 77678 Google
how can the common tokens be 2441100? I would expect the common tokens to be a subset of the sum of the 11387 and 77678. Hi - I see there are elements in "sims" RDD and "goldStandard" RDD. But when I join them and try to print out trueDupsRDD.take(1), I get an empty list. What am I doing wrong? I use the statement given in the hint to join.
Ignore the question please. Make sure to include a space between AmazonID and GoogleURL, that resolved the issue for me. in the ipython notebook I downloaded, in lab4-3b it uses an undeclared variable " training".
It should read like this:

print ('The training dataset now has %s more entries than the original training dataset' %       (trainingWithMyRatingsRDD.count() - trainingRDD.count()))assert (trainingWithMyRatingsRDD.count() - trainingRDD.count()) == myRatingsRDD.count()


The original lines as written:

print ('The training dataset now has %s more entries than the original training dataset' %       (trainingWithMyRatingsRDD.count() - training.count()))assert (trainingWithMyRatingsRDD.count() - training.count()) == myRatingsRDD.count()

will throw this error:
NameError: name 'training' is not defined Hi all,

I followed all the settings mentioned in lab1 but don't see the option to download as python i.e. .py ca you help me where I am missing out on.

 In the header of 1cMovies with more than 500 reviewsand in the textMovies with at least 500 reviews

In most cases this will not make any difference
but unfortunately there are two movies with exactly 500 reviews

Thanks You write "Use the `testRDD` to create an RDD with entries of the form (userID, movieID, rating)."

That is already the form of testRDD, so I'm not sure what we should do to create testForRMSERDD...if I just set it equal to testRDD it seems to pass the test but I don't understand if this is what we want. Team

I submitted code and I am not sure if it made it. 

When I ran code in iPython there were no issues when submitted to auto-grader got a pickle error. But the end message says all 19 test cases passed .

Please advise

sudhir

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
PicklingError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
Can't pickle builtin 

-------------------------------------------------
All tests passed
-- 19 cases passed (100.0%) --


Your submission token ID is 1194138-c5645ec2e6507a3fc03bb3ad0e230a07:ip-172-31-19-38 My RDD is of the type : 
[('b000jz4hqo', ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund'])]

for this part, I am unable to figure out the solution. I've tried using len, but it doesn't help me. In lab 4 -- 3d 's explanation, it tells you to reuse the variable: testForPredictRDD in the 3d cell.

This variable was never created - back in question 2b, the variable 'testForPredictingRDD' is created in the cell ( this is the variable name already populated in the 2b cell, although it's referred to in the description for 2b as testForPredictRDD ) .

Just seems to be an unintentional inconsistency in the variable names .. Hi. I got the correct answer for the number of commonTokens but when i run the test it fails. I have been looking for an similar issue here but couldn't find it.
If you have any ideas, please share.  Lab 2 was pretty bad.  Lab 3 was ridiculous.

Please,  more teaching, less "figure it all out on your own."  How?  Oh, maybe, make lectures more than 1-2 minutes long.  Talk about this stuff.  You know, stuff like this material, in some more detail sufficient to solve labs like Lab 3 in less than 50 hours of effort.

Thanks. This is just, like, my opinion, man. Instructors: I think the RDD in Lab 4, Part 3a isn't what it should be. The print statement claims that the RDD items are (number of ratings, (movie name, movie ID)). However, movieLimitedAndSortedByRatingRDD is (rating float, (movie name, number of ratings))

For those of us students who are curious about what movies would be recommended to us by MLlib, that error would prevent us from rating the correct movie IDs. Fyi for course staff:

"training" should be "trainingRDD"

 When I submit, my result for 1A is shown as :
Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
  File "", line 13
    googleURL = 
                ^
SyntaxError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
invalid syntax
But my code in 1a has no 'googleURL = ' statement in it:
# TODO: Replace <FILL IN> with appropriate codequickbrownfox = 'A quick brown fox jumps over the lazy dog.'split_regex = <<removed for honor code>>
def simpleTokenize(string): """ A simple implementation of input string tokenization Args: string (str): input string Returns: list: a list of tokens """ out = re.<<removed for honor code>> clean = <<removed for honor code>> for itm in out: 	if len(itm) > <<removed for honor code>>: 	clean.<<removed for honor code>> return clean
Can anyone tell me why this would fail the autograder when I have no 'googleURL = ' statement in it: Just starting on Lab 3, I'm likely missing the obvious but I'd like to look at the files used and can't figure where they are instructions to get it from metric-learning github are not clear, I check all the source tree and can't find the files?   No link in the exercise as well, I'm sure they're somewhere on the VM but is there a link to download them?

Data files for this assignment are from the metric-learning project and can be found at:¶
cs100/lab3 Feels like almost there but small hurdle. 

My amazonInvPairsRDD.take(10) is:
[('rom', 'b000jz4hqo'), ('clickart', 'b000jz4hqo'), ('950', 'b000jz4hqo'), ('image', 'b000jz4hqo'), ('premier', 'b000jz4hqo'), ('000', 'b000jz4hqo'), ('dvd', 'b000jz4hqo'), ('broderbund', 'b000jz4hqo'), ('pack', 'b000jz4hqo'), ('1', 'b0006zf55o')]

It has  count of 111387

My resulting  commonTokens.take(10)
['b000jz4hqo', 'rom', 'b000jz4hqo', 'clickart', 'b000jz4hqo', '950', 'b000jz4hqo', 'image', 'b000jz4hqo', 'premier']
But the count is 
Found 222774 common token

I am short of some tokens ...not sure what could be the issue.. this should be a straight forward swapping... i got 45040 words by using below code

amazonRecToToken = amazonSmall.flatMap(lambda x:  tokenize(x[1])).count()googleRecToToken = googleSmall.flatMap(lambda x:  tokenize(x[1])).count()

what's wrong here It seems just is testRDD to me... We have a new revision of lab4 - the current version is 1.0.2
https://raw.githubusercontent.com/spark-mooc/mooc-setup/master/lab4_machine_learning_student.ipynb

1.0.1 : These changes improve clarity of descriptions or coding questions, however, does not affect your score or your ability to take the full score (with small changes outside of <FILL IN> blocks). In other words, if you are already passing every test then you _do not_ need to do anything.

1.0.2 : We have increased the tolerance value for 2c 2d, if your tests are passing locally, you _do not_ need to do anything, simply re-submit if you are running into problems in the autograder. It tests are not passing locally, you should check with the latest version of this lab.

For details, please see git history: https://github.com/spark-mooc/mooc-setup/commits/master/lab4_machine_learning_student.ipynb

Thanks for those reporting issues:
@3552
@3554
@3555
@3558
@3561
@4078

#pin can someone please point me to a post where the correct output is expected from this step? I am not clear about 4d.
An example would be great.
Thanks! 1c

Counts - 6443
	20981
There are 27424 tokens in the combined datasets

I removed stop words and empty strings also, I'm using tokenize function.
I tried with both "\W+" and "[^a-zA-Z0-9\_]" in regex. But i'm not getting exact count. Please help Hi, coud you share the output for: 

print idfsFullWeights["software"]

mine is:
0.215517241379
thanks! Hi all,
I cannot understand the correct return formula. I read the text differently I suspect.
def cossim(a, b):    return dotprod(a,b)/norm(a)/norm(b)

I really need to move on..... Can some one tell me the correct formula?

 Thank you guys for starting this course.
A special thanks to the course staff for guiding us throughout the course and their prompt replies to all the queries
Really enjoyed it.
Wished the course had been more longer.
Hope to see you in the scalable ml class.

Thanks Hi Tutors,

I have been recently spending time on SAAS course where the instructor introduced me to IBM Watson.
After doing labs 2 and 3 , I feel I was doing the exact same procedure that IBM Watson is doing for Natural Language
processing. awesome

The way the course is structured is great.Thanks,

It was an awesome experience.
Looking forward to Lab4 and verified Cert.

Thanks,
Anoop Using string in sort function makes me feel uncomfortable. Is it guaranteed to work?

When I ask python
>>> '100' < '99'
True
>>> '10.1' < '9.1'
True
Isn't it better to just compare the tuple?
>>> (100, 'this') < (99, 'this')
False
>>> (10.1,'this') < (9.1, 'this')
False
I see that we are just trying to get a fixed order, so, it probably does not matter in this case. But, isn't it better to just compare the whole tuple?

 Hi, 

I must be making some silly mistake but unable to figure out. Need help.

I am getting the following error:

(1, 2, 3, 4)
4
10
2.5
1 test failed. incorrect getCountsAndAverages() with integer list
(10.0, 20.0, 30.0)
3
60.0
20.0
1 test failed. incorrect getCountsAndAverages() with float list
xrange(20)
20
190
9.5
1 test failed. incorrect getCountsAndAverages() with xrange I am getting the following error. Any pointers ?






(1, 2, 3, 4)
4
10
2.5
1 test failed. incorrect getCountsAndAverages() with integer list
(10.0, 20.0, 30.0)
3
60.0
20.0
1 test failed. incorrect getCountsAndAverages() with float list
xrange(20)
20
190
9.5
1 test failed. incorrect getCountsAndAverages() with xrange



 Hi everyone,

Quick question. Is it allowed to call a python defined function from a lambda function?

Thanks Hi 

I have cleared al the tests till 4b. So I assume all my functions work as expected. Some how i am getting idfsFull.count() as 4772 instead of 17078.Can some help , I am stuck in this for long time. Some one had suggested a GiveUp button in Lab2. I think that would come in handy right now for me struggling at 4d! :)
This lab has really been something. One would think after progressing till here it should be a breeze but for me it has not. def idfs(corpus):

return (tokenSumPairTuple.map(lambda x: (x[0],N/float(x[1]))))

where the tuple is reduced value of a flatmapped set of tokens from corpus.

so Adobe is smallest and not software. Do you see any issue?


:


















print idfsSmall.take(7)
print idfsSmall.takeOrdered(7, lambda s: s[1])[0]
print idfsSmall.takeOrdered(2,lambda s: s[1])[1]
aa = idfsSmall.takeOrdered(2,lambda s: s[1])[1]
Test.assertTrue(abs(aa[1] - 4.25531914894) < 0.0000000001,
                'incorrect smallest IDF value')

















[('aided', 400.0), ('precise', 44.44444444444444), ('duplex', 400.0), ('dance', 400.0), ('breath', 200.0), ('themes', 133.33333333333334), ('known', 57.142857142857146)]
('adobe', 1.520912547528517)
('software', 2.1621621621621623)
1 test failed. incorrect smallest IDF value






 Hi guys, 

I am new to python dictionary operations. When I try to calculate the averaged result, it returns zero!

Test like: 
input: print 1/5    # or  others like 1/10, 1/2 etc
output: 0

What's going on here? Thanks.

 have problem in forming "tuples of the form (average rating, movie name, number of ratings)"
1. first after  combined  the moviesRDD with movieIDsWithAvgRatingsRDD, I got:
(2049, (u'Happiest Millionaire, The (1967)', (22, 3.6818181818181817)))
2. Then I use following code:
   .map(lambda s: (s[1][1][0],s[1][0],s[1][1][1])) 
But it didn't work. s[1][0] return the movie name, but s[1][1][1]/[0] didn't yield errors. 

Any help??
Thank you :)
 Here is is written that ,

 Use the Python list `myRatedMovies` to transform the `moviesRDD` into an RDD with entries that are pairs of the form (myUserID, Movie ID) and that does not contain any movies that you have rated. This transformation will yield an RDD of the form: `[(0, 1), (0, 2), (0, 3), (0, 4)]`. Note that you can do this step with one RDD transformation

How can one do in one transformation ?. Any hints would be great. thanx Hi I am stuck how do we return list as part of lambda reduceByKey operation

-I am doing map and using lambda to return (MovieID,Rating)
-Now I am using reduceByKey and my lambda function returns list of ratings 

I am getting the following error please guide thanks in advance

TypeError: 'float' object is not iterable

y4JJavaError                             Traceback (most recent call last)
<ipython-input-22-e47a6d735823> in <module>()
      9 movieIDsWithRatingsRDD = (ratingsRDD
     10                           .map(lambda (UserID,MovieID,Rating) : (MovieID,Rating))).mapValues(lambda x : list(x))
---> 11 print 'movieIDsWithRatingsRDD: %s\n' % movieIDsWithRatingsRDD.take(3)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 41.0 failed 1 times, most recent failure: Lost task 0.0 in stage 41.0 (TID 37, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1220, in takeUpToNumLeft
    yield next(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1806, in <lambda>
    map_values_fn = lambda (k, v): (k, f(v))
  File "<ipython-input-22-e47a6d735823>", line 10, in <lambda>
TypeError: 'float' object is not iterable

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) I have this in my notebook:


similaritiesBroadcast = (crossSmall .map(lambda x: computeSimilarity(x)) .cache())


it never used the function  def computeSimilarityBroadcast(record):

but used the previously defined function. Looks like a typo.


thanks I didn't intend to use the broadcast value yet. but got this error.

<SNIP HONOR CODE VIOLATION>

Error:
Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.

Where did it go wrong?

 It took ages to be uploaded and finally it threw this error:Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 302, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 734, in func
    initial = next(iterator)
  File "", line 15, in 
TypeError: 'float' object has no attribute '__getitem__'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
 I believe there is a typo in the directions of Lab 4, section 3f.

On the one hand it says to yield an RDD with tuples of the form (Predicted Rating, Movie Name, number of ratings). On the other hand, in the example, the number of rating does not appear in the tuple. [(7.983121900375243, u'Under Siege (1992)'), (7.9769201864261285, u'Fifth Element, The (1997)')]

This does not affect the score of your submission, but I thought I would let you know about this typo. Hi,

My PC has 8 CPU cores, but only 2 are working when the job is runing.

I think 2 is set by default.

My question: if I set a higher level of parallelism, will the grader be influenced ? Should I do that ?

Hao There is a Certified Spark Developer Certification through DataBricks. 

How well does this course prepare the students to be able to sit for that certification test? 

I feel like my Spark skills have grown after taking this course, but do not know if I am ready to register for the Certification exam. 

Any comments from the instructors, or Databricks? 

Thank you. 
 I am trying below code, but it throws exception, when i try to apply map first & then reduce, then it works fine but i though below code should have worked. Any suggestions why it's failing?

#Args:#vendorRDD (RDD of (recordId, tokenizedValue)): Pair tuple of record ID to tokenized output#Returns:#count: count of all tokensRDD of (recordId, tokenizedValue)): Pair tuple of record ID to tokenized output

def countTokens(vendorRDD):       return vendorRDD.reduce(lambda x, y : len(x[1])+len(y[1])) I try to run the cells in order but i wait and i wait and it looks like no response and I can't go on to next cell until the one before it runs so I just sit and wait staring at the first cell waiting for it to give me some sign that I can go on to the next cell and I can go no further. I am so frustrated with this.

Also LECTURES would be MUCH MORE HELPFUL if they actually discussed the Workbook that was to follow the lectures and actually tolds us what quirks we might expect in  the workbook  AND the lectures could actually go through a few code examples so you would have some idea of what was expected, It is  like the lectures are soo high level they have very little to do with the homework - might as well not even have them.

ALSO please clear up for me this next question - Every time you come into a notebook you have to run ALL the cells ALL over again in sequence in order to get to the cell you left off in??? I woud appreciate some fast feedback on this as I am now going to go back to the workbook to cell 1 of LAB 3 and continue to stare at cell 1 to hope it gives back something so I can go on. What an ENORMOUS WAST OF TIME ! thank you ! I'm troubleshooting my solution to (4f) where I had Test 2 failed. I've compared my weightsRDD to some others on here and found mine to be grossly wrong. I'd like to check if my idfs output is correct and was wondering if someone could provide a few of their tokenSumPairTuple counts?

I'm looking at the top 10 using: 
print tokenSumPairTuple.takeOrdered(10, lambda (k, v): -v)

My results are: 
[('software', 94.0), ('new', 58.0), ('features', 58.0), ('use', 57.0), ('complete', 55.0), ('easy', 52.0), ('create', 48.0), ('system', 48.0), ('cd', 48.0), ('1', 47.0)]

Then idfsSmall gives: 
[('software', 4.25531914893617)]
[('new', 6.896551724137931)]
[('features', 6.896551724137931)][('use', 7.017543859649122)]
[('complete', 7.2727272727272725)]
[('easy', 7.6923076923076925)]

 I started with 
return (keys, [token]) (in swap) reduceByKey with lambda x,y: x+ y
Then after 50 minutes I gave up.My current iteration (after increasing the memory of the virtual box from 2GB to 4GB):
return (keys, token) (in swap) map(lambda t:[t])reduceByKey with  lambda x,y:x+ y
It is still more than 30 min and I am stuck after task 15/16 (probably reduceByKey)The join took few minutes, but it is surely over.Any ideas how long 4e processing this should take? What can I optimize?Any changes in Spark configuration?Note: I have I7  on Windows 7 64 bit, 8 GB, where the Vagrant 32 bit has 4GB RAM. Does anybody know how to show line number in each ipython cell?

TIA The example tests for 2b show results that don't seem to add up, even calculating by hand..

http://d.pr/i/11aVb

Am I missing something? I do not know if this was the lab initially since it took way less than lab 3. If given the feedback for the previous labs you decided to make it shorter, can we have also the longer version? Not graded, but simply to make it challenging for people who are familiar with Spark and want to simply learn more. predicted
[((1, 1), 5), ((1, 2), 3), ((1, 3), 4), ((2, 1), 3), ((2, 2), 2), ((2, 3), 4)]
actual
[((1, 2), 3), ((1, 3), 5), ((2, 1), 5), ((2, 2), 1)]
joined
[((1, 3), (4, 5)), ((2, 2), (2, 1)), ((1, 2), (3, 3)), ((2, 1), (3, 5))]

squaredErrorsRDD
[((1, 3), 1), ((2, 2), 1), ((1, 2), 0), ((2, 1), 4)]
numRatings= 4
totalError= 6

Looks very straing forward.. but my eeror is not correct ... why tips?
Error for test dataset (should be 1.22474487139): 1.5 Hello, 
I stuck in 2c in lab 3. 
I calculate  correctly unique token counts which is 4772 
I  find  idfsSmall.takeOrdered(1, lambda s : s[1])[0]  as ('aided', 400.0)
and  tokenSmallestIdf[1] is 1

Here is my code 
1. N=corpus.count
2. corpus.flatMap(lambda x: x[1]).distinct()
3. uniqueTokens.map(lambda a: (a, 1))
4. tokenCountPairTuple.reduceByKey(lambda a,b: a + b)
return tokenSumPairTuple.map(lambda (a,b): (a,N/b))
I don't understand which part is wrong. Can anyone help me?
 
I was able to hack something together for this that passes all the tests.

However, I wasn't able to code this the way I think would be optimal; and need some help with a deeper understanding of scalability issues for this!
(EDIT 6-28:  I found some good posts on StackOverflow that were very enlightening; so I maybe good with this for now)

Q1) Unable to ACCUMULATE/CHAIN a LIST of Hostname strings with the following syntax (list stays empty):
<REDACTED> Posting solutions is a violation of the honor code

So, I hacked the following instead:
<REDACTED> Posting solutions is a violation of the honor code

This is suboptimal as I need to later "split" to get the List of host strings I wanted to store in the first place.
While I was able to code a reduce lambda to build a set; I later had to invoke sortByKey; and that won't work on a Set type without a 'non-hashable type' error.

Q2) Unable to use 'set' content later in Map or SortByKey operations; as it's considered a 'non-hashable' type, and throws an error on such usages:
<REDACTED> Posting solutions is a violation of the honor code


So, I hacked the following instead:
<REDACTED> Posting solutions is a violation of the honor code

This is suboptimal as a reduce operation on an incrementally building a set is more scalable than
a distinct() operation spanning multiple partitions -- right?

Q3) The dailyHosts variable name in this exercise should be modified to something like dayToNumHosts;
to make it clear that we don't need to save hostnames!

Q4)  How is data moving between Driver and across Partitions when the following operations are invoked:
       - distinct()
       - SortByKey()
       - takeOrdered()
       i.e. if data to collect() for these is distributed across multiple partitions; then will there be a performance degradation when large amounts of data are 
       collected from distributed partitions to be 'joined' on the Driver machine before being able to do these operations?

Q5) When is it most advisable to issue 'cache()'; and then does stale data get held within this cache even if the underlying base data gets modified in real-time?
      

 Hi,
function norm as defined in a Lab3 exercise 3a:

Returns: norm: a dictionary of tokens to its TF values

and it seems to be wrong, cause I think it should return a single value.
Can anyone provide definition of the value that should be returned?

Regards
Pawel
 can some one please help me out with this code

N = corpusRDD.count() uniqueTokens = corpus.flatMapValues(lambda x:x).distinct() tokenCountPairTuple = uniqueTokens.map(lambda x:(x,1.0)) tokenSumPairTuple = tokenCountPairTuple.reduceByKey(lambda (a,b):a+b) return (tokenSumPairTuple.map(lambda (token,count) : (token,N/count)))

i keep getting 
There are 15144 unique tokens in the small datasets. Hi guys,

I am wondering whether there are other people in NYC taking this class.  Is there any interest of getting  together once in a while to discuss the labs and whatever other topics related to the data science?

Cheers, Alex Receiving the correct value for avgSimDups, the nonDupsRDD appears to be correct:
  (' Ama Goo', float) for the first records using several different RDD join methods.
 
I believe the nonDupsCount should be 41,254 based on 41300 (combined sim and goldStandard) minus the 146  trueDups I found. Is this accurate?

Struggling with how to  either "join", map, or filter the RDDs to remove the records which appear in both .  Would appreciate suggestions on how to approach this, I can't seem to find something in the RDD API or the appropriate combination of mapping/filtering.

 Hi All,

        For 2c  I am getting the following error. Appreciate your inputs at the earliest to address this issue (as grace period is about to end) and proceed further to next questions. 

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-11-43848a14b106> in <module>()
     13     tokenSumPairTuple = tokenCountPairTuple.map(lambda x:(x[0], N/x[1]))
     14     return (tokenSumPairTuple.sortBy(lambda x:x[1]))
---> 15 idfsSmall = idfs(amazonRecToToken.union(googleRecToToken))
     16 uniqueTokenCount = idfsSmall.count()
     17 

NameError: name 'amazonRecToToken' is not defined Hi All,
 I got an error for following line:

similarities = crossSmall.map( lambda s: computeSimilarity(s)).cache()

The error message is
Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.

 
All my prior tasks(prior to 3c) tests passed.

Please help.
Thanks in advance
 The two variablemsat the end of the cell are:

testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }
nm = norm(testVec1)

What is expected value of nm?
Is it sum of square roots of sqrt(2^2),sqrt(3^2), sqrt(5^2)? Which is 10?

Pleaew help cause I've spend almost 5hours on that simple case!

Regards. Can you still get a certificate or some notice that you've finished the course successfully?  If you, like me, have gotten far too little sleep and are now working on lab 4, you may be confused for a moment when you see
Error for test dataset (should be 1.22474487139): 1.22474487139
Error for test dataset2 (should be 3.16227766017): 3.16227766017
Error for testActual dataset (should be 0.0): 0.0
and try to figure out why there's an error when your values are identical to what they should be...

Have no fear! These messages aren't telling you that you have an error, they're just telling you the value you should be getting when computing your "Error" and then printing the value you got so you can compare it using your own eyes...

Phew!
:-) any hint?  amazonSmall.map(lambda (x,y): x[0],x[1])

when I use function like this to create a RDD of id and just the content (I will tokenize later)
it throws me error
name 'x' is not defined
 Any explanation  is helpful. I must be doing something silly 

But its not apparent to me

Manish Since it actually does two works - transform (id, title) to (myUserId, id), and filter only unrated things, I cannot think how to solve it with just one RDD transformation.

Does it require addition function, or it can be done with one transformation with python-restricted lambda function? I've gotten through all the labs up to this point, but I can't seem to get 2c to work.  Here's where I am:

Calculate N - no problem.  Cast as a float, I get 400.0.
Unique tokens - I do a flat map of the distinct token values in the RDD
tokenCountPairTuple - I create an RDD where each unique token has a value of 1
tokenSumPairTuple - I do a reduceByKey of the tokenCountPairTuple
idfs return - for each token, I compute N/n(t)

When I run this, I get a SPARK-5063 error.  What am I doing wrong?
 Thank you Professor for an awesome course, thank you friends for your support @ the forum. 
Looking forward for CS190.  simpleTokenizer:
stx=string.replace('/',' ').replace('?',' ').replace('.',' ').replace('!','').lower().strip()  stx= re.sub('[^0-9a-zA-Z_\-*\)\(\;\'\: ] ', '', stx)
return re.split(split_regex,stx) if len(stx)>0 else []

tokenize:
return ' '.join (wd for wd in simpleTokenize(string) if wd not in stopwords).split(' ') if len(' '.join (wd for wd in simpleTokenize(string) if wd not in stopwords)) <> 0 else []

am i missing any other puctuation or things? Not trying to share answer here. but trying to understand why counts are not matching as it been 3 days I am breaking my head.. please help


 Is it possible to get a prediction larger than 5 in part 3(f)? Here's my output:
My highest rated movies as predicted (for movies with more than 75 reviews):
(6.157304972967603, u'Firelight (1997)', 5)
(5.990451692409798, u'Land and Freedom (Tierra y libertad) (1995)', 6)
(5.739376700162565, u'Bandits (1997)', 4)
(5.689585540626463, u'Red Firecracker, Green Firecracker (1994)', 16)
(5.679824043290785, u'Dorado, El (1967)', 25)
(5.61131469836078, u'Steal Big, Steal Little (1995)', 7)
(5.604896815659058, u'Endurance (1998)', 12)
(5.542287652871513, u'Goodbye, Lover (1999)', 10)
(5.539886991803599, u'Best Man, The (Il Testimone dello sposo) (1997)', 2)
(5.5231449659228415, u'Lucie Aubrac (1997)', 2)
(5.518821320251409, u'Bandit Queen (1994)', 17)
(5.507815940742775, u'Destiny Turns on the Radio (1995)', 7)
(5.387678805884172, u'Cure, The (1995)', 7)
(5.35109795780477, u'On Any Sunday (1971)', 4)
(5.324308967483047, u'Sound of Music, The (1965)', 541)
(5.3151010355547665, u'Inherit the Wind (1960)', 133)
(5.297264941938113, u'Go West (1925)', 19)
(5.2848489929798, u'Trouble in Paradise (1932)', 16)
(5.269436208570292, u'Love Is the Devil (1998)', 5)
(5.268423004388489, u'Damsel in Distress, A (1937)', 10) This lab is driving me mad. I’ve spent about 4 hours trying to work out why my record counts are wrong for 4d.My amazonWeightsRDD.count() is 1363 and googleWeightsRDD.count(), 3226, both passing the tests.

The invertedPair test is passing.

I'm doing:
amazonInvPairsRDD = (amazonWeightsRDD.flatMap(invert).cache())
However, I get “There are 4918 Amazon inverted pairs and 12334 Google inverted pairs.”The structure looks correct to me:amazonInvPairsRDD.take(5)[('9', 'b000jz4hqo'), ('0', 'b000jz4hqo'), ('5', 'b000jz4hqo'), ('r', 'b0006zf55o'), ('e', 'b0006zf55o')]googleInvPairsRDD.take(5)[('c', 'http://www.google.com/base/feeds/snippets/11125907881740407428'), ('b', 'http://www.google.com/base/feeds/snippets/11125907881740407428'), ('k', 'http://www.google.com/base/feeds/snippets/11125907881740407428'), ('o', 'http://www.google.com/base/feeds/snippets/11125907881740407428'), ('q', 'http://www.google.com/base/feeds/snippets/11125907881740407428')]I’m thousands of records short. Anyone got any suggestions why?Thanks
 I am unable to get any counts whatsoever from Lab3 1c. My code always fails as soon as I call my tokenize function. However, the tokenize function works fine for the tests provided.

My only idea as to what is going wrong is that perhaps by converting strings to RDDs in my tokenize function I have set up a situation where Spark has issues.

I am not a particularly skilled Python programmer, being far, far more comfortable with R. So I used what was provided in earlier lectures, and did multiple table joins on RDDs to filter out stopwords.

I now wonder if this was perhaps not the method intended.

Anyone else have these issues? Should I completely rework my tokenize script to not use RDDs?
 i am getting the format as below. do i need to convert it to [(a,(b1,b2...))....] from [(a,[b1,b2...])....]

movieIDsWithRatingsRDD: [(2, [1.0, 5.0, 4.0, 5.0, 4.0, 4.0, 2.0, 4.0, 4.0, 5.0, 3.0, 1.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 4.0, 2.0, 4.0, 4.0, 2.0, 2.0, 4.0, 3.0, 1.0, 4.0, 4.0, 3.0, 4.0, 2.0, 2.0, 2.0, 4.0, 3.0, 4.0, 3.0, 4.0, 4.0, 1.0, 1.0, 4.0, 3.0, 3.0, 5.0, 2.0, 3.0, 3.0, 5.0, 4.0, 3.0, 5.0, 1.0, 3.0, 3.0, 4.0, 2.0, 4.0, 4.0, 4.0, 3.0, 5.0, 3.0, 3.0, 2.0, 3.0, 4.0, 1.0, 3.0, 3.0, 4.0, 2.0, 4.0, 3.0, 3.0, 3.0, 4.0, 2.0, 2.0, 1.0, 4.0, 5.0, 3.0, 4.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 3.0, 3.0, 3.0, 4.0, 3.0, 4.0, 4.0, 3.0, 3.0, 3.0, 4.0, 2.0, 4.0, 2.0, 3.0, 2.0, 4.0, 3.0, 4.0, 4.0, 3.0, 2.0, 3.0, 3.0, 2.0, 5.0, 4.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 1.0, 4.0, 3.0, 3.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 3.0, 2.0, 4.0, 3.0, 2.0, 3.0, 4.0, 2.0, 2.0, 3.0, 4.0, 4.0, 3.0, 4.0, 2.0, 3.0, 3.0, 3.0, 4.0, 3.0, 5.0, 3.0, 2.0, 2.0, 3.0, 5.0, 3.0, 5.0, 3.0, 3.0, 3.0, 3.0, 3.0, 5.0, 3.0, 3.0, 3.0, 4.0, 2.0, 2.0, 3.0, 2.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 5.0, 3.0, 4.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0, 1.0, 4.0, 1.0, 3.0, 4.0, 3.0, 3.0, 4.0, 1.0, 2.0, 4.0, 2.0, 1.0, 4.0, 3.0, 3.0, 2.0, 3.0, 4.0, 1.0, 4.0, 1.0, 3.0, 2.0, 4.0, 2.0, 3.0, 4.0, 1.0, 4.0, 1.0, 2.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 4.0, 4.0, 3.0, 4.0, 3.0, 4.0, 4.0, 3.0, 4.0, 3.0, 4.0, 4.0, 3.0, 2.0, 5.0, 3.0, 3.0, 3.0, 2.0, 3.0, 3.0, 2.0, 2.0, 3.0, 4.0, 4.0, 3.0, 3.0, 3.0, 4.0, 3.0, 4.0, 4.0, 3.0, 3.0, 5.0, 4.0, 2.0, 4.0, 3.0, 3.0, 1.0, 3.0, 4.0, 3.0, 3.0, 3.0, 2.0, 5.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 4.0, 4.0, 2.0, 3.0, 4.0, 4.0, 2.0, 5.0, 3.0, 4.0, 3.0, 2.0, 2.0, 4.0, 4.0, 1.0, 3.0, 5.0, 3.0, 4.0, 1.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0]), (4, [4.0, 1.0, 3.0, 2.0, 1.0, 5.0, 4.0, 2.0, 3.0, 1.0, 3.0, 3.0, 4.0, 4.0, 3.0, 3.0, 3.0, 3.0, 4.0, 2.0, 3.0, 1.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 4.0, 3.0, 1.0, 3.0, 4.0, 1.0, 2.0, 2.0, 4.0, 2.0, 3.0, 3.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 4.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 5.0, 3.0, 3.0, 3.0, 3.0, 5.0, 2.0, 1.0, 3.0, 2.0, 2.0, 1.0, 2.0, 2.0]), (6, [4.0, 4.0, 5.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 1.0, 4.0, 4.0, 5.0, 5.0, 5.0, 4.0, 5.0, 2.0, 4.0, 4.0, 2.0, 5.0, 4.0, 4.0, 4.0, 2.0, 5.0, 3.0, 4.0, 4.0, 5.0, 4.0, 4.0, 5.0, 5.0, 2.0, 4.0, 5.0, 5.0, 4.0, 4.0, 4.0, 5.0, 2.0, 5.0, 4.0, 4.0, 4.0, 3.0, 5.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 4.0, 4.0, 5.0, 3.0, 4.0, 4.0, 4.0, 5.0, 4.0, 4.0, 3.0, 4.0, 2.0, 4.0, 4.0, 3.0, 3.0, 4.0, 3.0, 3.0, 5.0, 4.0, 4.0, 3.0, 5.0, 4.0, 4.0, 2.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 2.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 3.0, 2.0, 3.0, 4.0, 5.0, 4.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 3.0, 5.0, 1.0, 5.0, 5.0, 3.0, 4.0, 3.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 2.0, 3.0, 3.0, 5.0, 4.0, 4.0, 3.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 3.0, 4.0, 2.0, 2.0, 5.0, 3.0, 1.0, 3.0, 5.0, 4.0, 4.0, 3.0, 4.0, 4.0, 3.0, 4.0, 5.0, 5.0, 3.0, 3.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 5.0, 3.0, 3.0, 1.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 2.0, 3.0, 4.0, 2.0, 3.0, 4.0, 4.0, 3.0, 4.0, 5.0, 5.0, 4.0, 3.0, 3.0, 3.0, 4.0, 2.0, 4.0, 5.0, 3.0, 2.0, 4.0, 3.0, 5.0, 4.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 2.0, 5.0, 4.0, 4.0, 4.0, 3.0, 3.0, 4.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 2.0, 3.0, 4.0, 4.0, 5.0, 1.0, 5.0, 2.0, 3.0, 4.0, 2.0, 1.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 5.0, 3.0, 4.0, 4.0, 4.0, 5.0, 2.0, 3.0, 4.0, 4.0, 3.0, 5.0, 3.0, 4.0, 4.0, 4.0, 5.0, 4.0, 3.0, 4.0, 2.0, 4.0, 5.0, 5.0, 4.0, 3.0, 5.0, 4.0, 4.0, 3.0, 2.0, 3.0, 3.0, 2.0, 3.0, 5.0, 2.0, 3.0, 5.0, 4.0, 5.0, 4.0, 4.0, 4.0, 5.0, 2.0, 3.0, 4.0, 4.0, 5.0, 5.0, 4.0, 1.0, 3.0, 4.0, 3.0, 5.0, 4.0, 4.0, 4.0, 4.0, 3.0, 5.0, 5.0, 4.0, 3.0, 2.0, 3.0, 4.0, 4.0, 4.0, 5.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 4.0, 3.0, 5.0, 4.0, 3.0, 3.0, 4.0, 4.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 5.0, 4.0, 3.0, 4.0, 4.0, 4.0, 4.0, 2.0, 3.0, 4.0, 3.0, 3.0, 5.0, 2.0, 4.0, 5.0, 2.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 1.0, 4.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 2.0, 4.0, 5.0, 2.0, 5.0, 2.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 3.0, 1.0, 5.0, 2.0, 4.0, 5.0, 3.0, 5.0, 5.0, 4.0, 5.0, 4.0, 4.0, 5.0, 5.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 2.0, 3.0, 4.0, 5.0, 3.0, 4.0, 4.0, 3.0, 3.0, 3.0, 4.0, 3.0, 5.0, 2.0, 5.0, 4.0, 3.0, 3.0, 3.0, 5.0, 3.0, 4.0, 4.0, 3.0, 5.0, 4.0, 4.0, 5.0, 5.0, 4.0, 5.0, 4.0, 4.0, 5.0, 5.0, 4.0, 3.0])]
 Hello,
I cannot figure out 3c. I am not understanding the directions.

What am i supposed to do with - 
similarities = (crossSmall .<FILL IN> .cache())

Thanks The movies I rated are:
myRatedMovies = [(0, 1088, 5), (0, 1171, 4), (0, 1047,5), (0,1195,5),  (0, 831, 4), (0, 503,5), (0, 651, 1), (0, 1447,5), (0, 1110, 4), (0, 700, 4),  (0, 527,1), (0, 776, 5), (0, 993,3), (0, 603, 3)]

which include all action movies like raiders of lost arc and all such. I am disappointed with the predictions though. I dont think any of the movies below would be to my liking. I wonder what is going. I have passed all the auto grader tests. 

My highest rated movies as predicted (for movies with more than 75 reviews):
(4.781697241990978, u'House of Yes, The (1997)', 87)
(4.5996828124844065, u'Matilda (1996)', 89)
(4.580804303120535, u"Breakfast at Tiffany's (1961)", 326)
(4.4837874999665095, u'I Know What You Did Last Summer (1997)', 242)
(4.468206621603829, u'Mirror Has Two Faces, The (1996)', 97)
(4.407878993147007, u'Girlfight (2000)', 96)
(4.347478068413505, u'Sword in the Stone, The (1963)', 161)
(4.3297754586167265, u'Corrina, Corrina (1994)', 117)
(4.276939404047734, u'What Ever Happened to Baby Jane? (1962)', 108)
(4.2534630527033865, u'Parent Trap, The (1961)', 125)
(4.250370854277559, u'Sound of Music, The (1965)', 541)
(4.240360940659877, u'Exotica (1994)', 107)
(4.237089066074079, u'Affair to Remember, An (1957)', 91)
(4.214547436515616, u'Bell, Book and Candle (1958)', 106)
(4.207353137068191, u'Trekkies (1997)', 123)
(4.168430231633249, u'Saving Grace (2000)', 114)
(4.117819865488036, u'Steel Magnolias (1989)', 148)
(4.084712427528029, u'Heavenly Creatures (1994)', 261)
(4.0821016348082, u'Nell (1994)', 88)
(4.064123755123063, u'Madonna: Truth or Dare (1991)', 86) 1. flatMap to calculate N  - I think this is similar to the count calculated in (1c) Tokenizing simall datasets. If I am not correct, please let me know.

2. map -> lambda - 'set' function for getting unique tokens from list - flatMap to flatten the list

3. map , reduceByKey to convert the tokens to tokenCountPairTuple

4. map to extract (k, float(N)/float(count))

I am getting correct count for Uniquetoken count.

But idfsSmall.takeOrdered(1, lambda s: s[1])[0] returns following

('software', 239.5744680851064) But IDF weight is wrong.

Sample 'print' output to show first few elements in the RDD looks fine.

I am not sure where I am making a mistake.

As this function gets called in other cells, more steps are failing.

I appreciate your help in resolving this issue. I had to struggle quite a bit to solve this exercise. The explanation that accompanied the exercise was not so clear to me. In case other students are struggling with this as well, I'd like to give some tips that helped me:

  - Use collectAsMap instead of collect to create broadcast dictionaries.
  - In function `fastCosineSimilarity`, given a ((ID, URL), tokens) tuple, for each token `t` in `tokens`:
    - look up the weight of `t` in the amazon (broadcast) dictionary that corresponds to key `ID`
    - look up the weight of `t` in the google (broadcast) dictionary that corresponds to key `URL`
    - multiply the two weights, and add them to `s`
  - Remember that the norms for google and amazon records where stored in `googleNormsBroadcast` and `amazonNormsBroadcast`

Note that when looking up weights you need two dictionary lookups: the first one to retrieve the dictionary that corresponds to an ID or URL, and the second one to retrieve the weight that corresponds to the token.

I hope this may help students stuck in this exercise.

 anybody has the experiences for Amazon AWS + spark?

A decent payment is acceptable. After four hours figuring out how to count how many documents content a token the solution i found was a for loop that test if the token is in the document and an autoincremental counter.

Really poor when we deal with map, filter, RDD and such advanced stuff

fortunately we worked with small dataset, cause i collected all documents to solve the task.

I am afraid that later in this lab driver have not resources enough as to eat collect() result.

Please, once deadline for submit this lab finish, show me proper way of do it. In question 2c of lab3, it says that IDF(t) = N/n(t) but isn't it the other way around? I mean, the number of docs that contain t OVER the total number of docs?

If my observation is correct, I think this should be pinned to alert other users.

Thank you testForAvgRDD = testForRMSERDD = testAvgRMSE = computeError(testForRMSERDD, testForAvgRDD)

The second of the above 3 lines in 3e could be confusing. I merely did testForRMSERDD = testRDD as I think that is what is needed. This extra variable was confusing me.  Hello everyone,

For Q4e, i first used groupByKey then used join-> map with swap -> reduceByKey -> map. But one of the alternative solutions in the forum is to use union. Was anyone able to get that to work? if yes, can you provide tips on how to be able to solve it using union?
 30 years ago I left high school without finishing it. So I when I see:

You will end up with an RDD with entries of the form (xi−yi)2 You might want to check out Python's math module to see how to compute these values.

Using an RDD action (but not collect()), compute the total squared error: SE=∑ni=1(xi−yi)2

I get totally lost, I know what x and y are supposed to be, I know how to minus and square things, but what is i, n, the drunk E? Then I get to the next one and I have 0 clue.

Could someone kindly explain this in simple English?

 Hi all, 

For tfidf, I have the result: 

Amazon record "b000hkgj8k" has tokens and weights:
{'autocad': 33.33333333333333, 'autodesk': 8.333333333333332, 'courseware': 66.66666666666666, 'psg': 33.33333333333333, '2007': 3.5, 'customizing': 16.666666666666664, 'interface': 3.0}
While the expected result is : 
{'autocad': 33.33333333333333, 'autodesk': 8.333333333333332, 'courseware': 66.66666666666666, 'psg': 33.33333333333333, '2007': 3.5087719298245617, 'customizing': 16.666666666666664, 'interface': 3.0303030303030303}, 'incorrect rec_b000hkgj8k_weights')

You can see only the values for keys '2007' and 'interface' have shorter decimals. Due to this small difference, the test is always failed. I tried to add "float" and without "float", the result does not change. What is the possible problem? Thank you very much!
 I was able to finish until 4c in lab 3 but did not have any success after that. I am running into series of 'Py4JJavaError' errors. Can I submit my work to autograder in this form? Would those errors cause any issue with autograder? Would I get partial credit until 4c? 
Am I allowed to comment any asserts that are causing the errors?
Thanks. I passed all the tests included in the python notebook, but after I uploaded my py file to autograder, I only got 63% of the points. Does autograder have extra tests that are not included in ipynb? Examples of autograder messages I got are:
Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
PicklingError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
Can't pickle builtin 

All tests passed
Removing stopwords (1b)
------------------------------------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect smallest IDF value

Implement a TF-IDF function (2f)
-------------------------------- I am kind of confused by "Create two collections, one for each of the full Amazon and Google datasets, where IDs/URLs map to the norm of the associated TF-IDF weighted token vectors." 
Can someone print out the some elements of such collection? The 2 collections here are actually dictionaries?  This is a minor thing, but in Lab 4 #3e, it says

Use the Python list `myRatedMovies` to transform the `moviesRDD` into an RDD with entries that are pairs of the form (myUserID, Movie ID) and that does not contain any movies that you have rated. This transformation will yield an RDD of the form: `[(0, 1), (0, 2), (0, 3), (0, 4)]`. Note that you can do this step with one RDD transformation.
I needed to use two transforms and I'm curious to how it can be done in one.  I searched through the PySpark RDD docs for a combination transform that both maps and filters at the same time (it would be cool to have this in the API).  Did I miss it? I'm getting a very small rounding error for one of the tokens ('2007') in 2f - I've implemented suggestions from @2402 and @2270 but hasn't helped. All the others match the test:

My output:
{'autocad': 33.33333333333333, 'autodesk': 8.333333333333332, 'courseware': 66.66666666666666, 'psg': 33.33333333333333, '2007': 3.5087719298245608, 'customizing': 16.666666666666664, 'interface': 3.0303030303030303}

Expected output:
{'autocad': 33.33333333333333, 'autodesk': 8.333333333333332,'courseware': 66.66666666666666, 'psg': 33.33333333333333,'2007': 3.5087719298245617, 'customizing': 16.666666666666664,'interface': 3.0303030303030303}

any suggestions would be greatly appreciated.
 I'm accustomed to transforming real values to integer values by scaling by desired precision and then coercing, something like the following if want precision to the thousands:

  int(round(5.555, 3) * 10**3)
Not having a lot of big data experience, I'm curious to here if this is a common transformation or if this is not really needed these days.  Thanks! Hi All,

I'm having trouble to understand Part 2 specifications.

I haven't started part 2 yet, but I want to understand what is happening.


It seems that there is something to do with matrix multiplication. Meaning for a missing rating, we would do a matrix multiplication on User's Rating row, and Feature's Column.

So Are we supposed to figure out feature column? Then when does the f[i] = equation comes in? What is r_ij?

Confused...

Thanks All I tried to get the trueDupsRDD by:
trueDupsRDD = (sims .join(goldStandard)) but this miserably fails. Please help.

I generated sims like this: 
sims = similaritiesBroadcast.map(lambda (a,b,c):(b,a,c))

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-43-73459a40bdcf> in <module>()
      5 trueDupsRDD = (sims
      6                .join(goldStandard))
----> 7 print trueDupsRDD.take(2)
      8 #trueDupsCount = trueDupsRDD.count()
      9 #avgSimDups = <FILL IN>

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 95.0 failed 1 times, most recent failure: Lost task 0.0 in stage 95.0 (TID 288, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1806, in <lambda>
    map_values_fn = lambda (k, v): (k, f(v))
ValueError: too many values to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
</span></span></span></span></span></span></span></span></span></span></span>
<p><p><span class="ansicyan"><span class="ansigreen"><span class="ansiyellow"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"> </span></span></span></span></span></span></span></span></span></span></span></p></p> Can the nice folks who have already completed or go on to complete Lab 4 please let us know here, what is the expected time for Lab 4 completion.My timings so far were:Lab 1: 2-3 hours, Lab 2: 6 hours, Lab 3: 13 hours!You see where this is headed! A 24 hour lab assignment will really throw a wrench in my schedules. I might as well give up. Hi all,

I'm quite unclear what's supposed to be in the nonDupRDDs. According to the instruction:

Create a new nonDupsRDD RDD that has the just the cosine similarity scores for 
those "AmazonID GoogleURL" pairs from the similaritiesBroadcast RDD that do not appear in both the sims RDD and gold standard RDD.

whereas the set of pairs in sims are actually the same as in similaritiesBroadcast 

What I think is that it should be the pairs in sims RDD that are not in gold standard RDD?

Could anyone help to clarify if my understanding is correct? So far, it returns me incorrect value when I implemented this way.

Thank you very much.
Han Hello,

While doing the following I am getting the error.All I am trying to do is to get the values eg: once after the join we get (k,(V1,V2)) and from this I only want (V1,V2).

sims=similaritiesBroadcast.map(lambda (a,b,c):(b,a,c))trueDupsRDD=sims.join(goldStandard).map(lambda (a,(b,c)):(b,c))
trueDupsRDD.take(10)

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-240-aac4dc4d948b> in <module>()
----> 1 trueDupsRDD.take(10)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 198.0 failed 1 times, most recent failure: Lost task 0.0 in stage 198.0 (TID 696, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1806, in <lambda>
    map_values_fn = lambda (k, v): (k, f(v))
ValueError: too many values to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span>
<p></p> Whats the best way to take
(a, b)(a, c)(a, d)(a, e)
and turn it into
(a, (b, c, d, e))
is reducing with a whitespace and then splitting the only option? NameError                                 Traceback (most recent call last)
<ipython-input-2-2f05832215a9> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
----> 2 stopfile = os.path.join(baseDir, inputPath, STOPWORDS_PATH)
      3 stopwords = set(sc.textFile(stopfile).collect())
      4 print 'These are the stopwords: %s' % stopwords
      5 

NameError: name 'baseDir' is not definedThe code was properly initially, but all of a sudden this happened.plz hlp!!!! I'd like to express my appreciation to TAs and Professor providing excellent and clear instructions for the lab 4. It's tremendously helpful. The class itself is interesting, but the type and preparation of lab 4 makes the class more fun.

Best Regards  def cosineSimilarity(string1, string2, idfsDictionary): """ Compute cosine similarity between two strings Args: string1 (str): first string string2 (str): second string idfsDictionary (dictionary): a dictionary of IDF values Returns: cossim: cosine similarity value """ w1 = tfidf(tokenize(string1), idfsDictionary) w2 = tfidf(tokenize(string2), idfsDictionary) return cossim(w1, w2)
cossimAdobe = cosineSimilarity('Adobe Photoshop', 'Adobe Illustrator', idfsSmallWeights)
print cossimAdobe

Error
KeyError                                  Traceback (most recent call last)
<ipython-input-226-d9c3d83350ed> in <module>()
     15 cossimAdobe = cosineSimilarity('Adobe Photoshop',
     16                                'Adobe Illustrator',
---> 17                                idfsSmallWeights)
     18 
     19 print cossimAdobe

<ipython-input-226-d9c3d83350ed> in cosineSimilarity(string1, string2, idfsDictionary)
     11     w1 = tfidf(tokenize(string1), idfsDictionary)
     12     w2 = tfidf(tokenize(string2), idfsDictionary)
---> 13     return cossim(w1, w2)
     14 
     15 cossimAdobe = cosineSimilarity('Adobe Photoshop',

<ipython-input-223-5b94bab46ffc> in cossim(a, b)
     38                 then by the norm of the second dictionary
     39     """
---> 40     return (dotprod (a, b)) / norm(a) / norm(b)
     41 
     42 testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }

<ipython-input-223-5b94bab46ffc> in dotprod(a, b)
     12     x = 0
     13     for k in a:
---> 14         x  += a[k] * b[k]
     15     return x
     16 

KeyError: 'photoshop' i want upgrade my ID The provided code seems to provide movie names, not movie IDs. I can work backwards, but either the instructions should be changed to indicate that we have to join the data sets to get the IDs ourselves or the provided code snippet should be updated.Current instructions:

"To help you provide ratings for yourself, we have included the following code to list the names and movie IDs of the 50 highest-rated movies from movieLimitedAndSortedByRatingRDD which we created in part 1 the lab"

Current code:
print 'Most rated movies:'
print '(average rating, movie name, number of reviews)'
for ratingsTuple in movieLimitedAndSortedByRatingRDD.take(50):
    print ratingsTuple
 my invert(t) looks like this:
    for k,v in t[1].items():        return ((k, t[0]), (v,t[0]))

for some reason, my invert is returning only the last (k,v), in items().  Though the first test passed, but the amazonInvPairsRDD and googleInvPairsRDD counts are not coming out right.

Please help. when I use the statement "amazonInvPairsRDD = amazonWeightsRDD.flatMapValues(invert)" I get the following error even though my invert function works fine on testing with amazonWeightsRDD.take(1). The error is as follows:

Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063. This should be easy, but I guess I do not know python as well as I should...I cannot seem to get the mapping to work within python...

Essentially, I do a map and use a lambda function to map x to : (x[0], (len(x[1]), sum(x[1])/float(len(x[1]))  I keep getting an error


'int' object has no attribute '__getitem__'

I would appreciate any suggestions...
 How do you deal with this PythonRDD object? I can't print by similarities.collect() nor similarities.take(1)

This is what I have:

similarities = (crossSmall                .map(computeSimilarity)                .cache())print similarities

Later on, similar function didn't work with error as follows:

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 175.0 failed 1 times, most recent failure: Lost task 0.0 in stage 175.0 (TID 422, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last): Hi, I'm having difficulty with Q3c. I consistently keep getting a 'List out of range error'. This makes me think that the RDD I have defined in the top line isn't the correct one. What RDD should we define in the beginning? I've attached my code below in case something else is incorrect with it.

<SNIP MAJOR HONOR CODE VIOLATION> Please do not post your code.
 When i have taken 1 element from rdd and print it i got the all token of that element

line = vendorRDD.take(1)    print line
[['http', 'www', 'google', 'com', 'base', 'feeds', 'snippets', '11448761432933644608']]
Please tell me How toI get the count of all tokens in that rdd

 i did this in my fastcosinesimilarity function


amazonNorm = amazonNormsBroadcast.value[amazonRec] googleNorm = googleNormsBroadcast.value[googleRec] value = s/float(amazonNorm*googleNorm)
 
i got an error 
TypeError: unsupported operand type(s) for *: 'dict' and 'dict'
because amazonNorm and googleNorm are dict
how can i divide the norms from s Hi,

In our lab3, I worked on a text mining problem ER as Text Similarity. In this problem, we bag all of words and them we calculated the idfs. And using this idfs and documents tfs I implemented tf-idf weights for the documents and the calculated similarity.

Now my question is:
I have totally a new document and want to calculate the similarity with existing documents. In my new document I have few words which are not available in idf reference dictionary. Now I could not calculate tf-idf weight for my new document. What to do now. Do I need to refresh the idf dictionary which includes the new words from my new document?

If that is the case, How google is calculating idf dictionary with their huge number of documents and including my new document (new words which are not in googles documents) on real time.

Regards,
Sivaji my fastCosineSimilarity works ok on a single record.
running a map on entire commonTokens completes without an error but  then  similaritiesFullRDD.count() generates this message.   Any ideas?


---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-193-c8d310436eec> in <module>()
     25 
     26 
---> 27 similaritiesFullRDD.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 874.0 failed 1 times, most recent failure: Lost task 0.0 in stage 874.0 (TID 1274, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-193-c8d310436eec>", line 24, in <lambda>
  File "<ipython-input-193-c8d310436eec>", line 19, in fastCosineSimilarity
TypeError: 'NoneType' object has no attribute '__getitem__'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
<p></p> In Lab 4 we load the file using 
rawRatings = sc.textFile(ratingsFilename).repartition(2)

Why 2 ?
How do we decide how to partition the data?

Thanks Staff of the some moocs prefer Q&A at the end of the classes to reinforce the learning. Since there might be some questions by the students and some guidance by the staff, I wonder if there could be a similar Q&A here.   In (1c), the Test.assertEquals() call checks that Shawshank Redemption has movieID 1088, and Schindler's List has movieID 1171.

But when I check the movies.dat file in the data directory at vagrant@sparkvm, it lists the former as movieID 318, and the latter as movieID 527. Is there a mismatch in movieID's?



 I learnt a bit from the lectures and quite a bit from the programming assignments. However, the course was less intellectually challenging but quite time consuming for a number of reasons. 

1. The comments in the labs are inaccurate or misleading
2. Same with variable names
3. The test asserts, especially based on count, don't really guarantee that the code is correct. 
4. Autograder issues
5. Piazza forums while useful to teaching staff is quite unfriendly compared to Coursera forums which are student friendly. 
6. The concept of collaborative filtering is quite interesting and I believe the lectures should be changed to focus on some of the theory behind the lab work. For example, having a lecture or two on recommender systems and collaborative filtering and a lecture or two on entity resolution (detailed) followed by the labs would be tremendously valuable to students. Luckily I took Machine Learning in Coursera, so I know how the collaborative filtering works and the brief description in the lab doesn't do justice. 

In my view, since the purpose of these MOOCs is education, I would suggest changing the content of the lectures to explain the concepts of the labs in lot more detail and then have students do the labs. 

Having said that, I have found the course useful in learning spark, which may have been the intent of the course. 

Thanks! Hello,

I have a time timeout for the lab 3. What I don't understand is the fact tha My solution takes but 18 minutes on my macbook. Since Autograder timeout 1 hour, this mean, it takes more than 3 times the time I have on my macbook pro.  Can we know the resources allocated to a grader process?

Thanks a lot.  Hi, guys.

Actually, I spent lots of time to do lab stuff.

The biggest reason is I'm a newbie not only spark but also python.

But it is not easy to debug my code w/ jupyter.

For example, if I want to check my own function which is in above cells, I need to drag to go up and read function manually, it takes lots of time.

Are there any tip to debug my code not only using 'print' thing.. I need like eclipse debug tool for watching my internal function and dynamic variables..

Hope to get any suggestion.

Best regards,
Chirs Cho I was able to pass lab 4 2d.
However I would like to make sure that I got the right result.
Can anyone verify?

The model had a RMSE on the test set of 0.891048561304 I am absolutely frustrated with this lab. I don't understand the directions. I thought I did but my answer is not correct. I can pass the first test but my answer is obviously wrong. 

for the amazonInvPairsRDD.count()  my result is always 1363

I end up with a "list of pairs of token to ID" which I interpret as 

[("foo", 1), ("bar", 1), ("baz", 1)]

it's enough to pass the unit test but no more. I've spent hours on it.  I am currently getting

38.5925557215

as the result for avgSimDups which is clearly wrong. The first test pass, my count is 146 after the join. All my previous tests passed which made me doubt that my similarity function is wrong. Now I am lost on how to possibly debug this.

Does anyone have any idea on how to possible debug this? Hello,

I did the submission twice and both times I had the timeout:

1. Your submission token ID is 1214955-cb0e24535db18973559c04b51db3a6ad:ip-172-31-16-129

2. Your submission token ID is 1216214-33591ce80cb7a8ffdb0e2164b3341abf:ip-172-31-25-41

Much of the processing delay seems to come from Part 4 where broadcast variables are setup by collecting on the RDDs.  Any suggestion on how to deal with this?

Thanks


 Hello,
On my lab1 I received an 80% even though I turned it in on Sunday the 14th when a 3 day grace period would end on Monday. Can you review this issue and adjust my progress grade accordingly?
Thanks,
Daxx 
dgrobert Hi All
 
Some of you may know that Spark 1.4 was announced recently. It is one of the bigger releases that also supports SparkR..
 
Could the TAs here suggest how to update 'sparkvm' virtual machine to Spark 1.4?
 
Thanks in advance
Nishant Gupta In lab 4 preliminaries, cannot run the code. book version 1.0.1
NameError                                 Traceback (most recent call last)
<ipython-input-5-e0fbd92521f3> in <module>()
      1 numPartitions = 2
----> 2 rawRatings = sc.textFile(ratingsFilename).repartition(numPartitions)
      3 rawMovies = sc.textFile(moviesFilename)
      4 
      5 def get_ratings_tuple(entry):

NameError: name 'sc' is not defined
 Hello everybody,

I do a fullOuterJoin between sims and goldStandard in exercise 3e.

Result is:


averagnonDupsRDD1
[(None, 'gold'), (None, 'gold'), (None, 'gold'), (None, 'gold'), (None, 'gold')]


After I calculate 

totalCount = (averagnonDupsRDD1 .map(lambda(x,y): x) .reduce(add))

But this fails. The message is:


TypeError: unsupported operand type(s) for +: 'float' and 'NoneType'

How can I solve this problem with noneType?.

Could somebody help me?

Thanks in advace

Carlota Vina






 I'm not sure if my approach is correct: we create the broadcast variable idfsFullBroadcast, then in order for the tfidf to work we need to pass idfsFullBroadcast.value as second parameter. If I try to pass idfsFullBroadcast I get an error. 
Is this the intended usage? 
In the documentation I read this sentence:
After the broadcast variable is created, it should be used instead of the value v in any functions run on the cluster so that v is not shipped to the nodes more than once.

So what am I missing? Do we get a performance gain using idfsFullBroadcast.value?

 the dotprod() function should be able to deal with non-overlapping dictionary keys, i.e. both a and b should be allowed to have keys that do not exist in the other dictionary.
The test based on tesVec1 and testVec2 does not cover that use case, while it easily could, by adding a new key-value pair to 1 of them. Be aware thought that this changes the norm of that changed testVec, which might impact the 2nd assertion. Any clue to calculate movieNameWithAvgRatingsRDD Hi, I got the result as RDD 
[('brown', 0.16666666666666666), ('lazy', 0.16666666666666666), ('jumps', 0.16666666666666666), ('fox', 0.16666666666666666), ('dog', 0.16666666666666666), ('quick', 0.16666666666666666)]but test case required dictionary. Please help as I am already late in assignments.  . let s say I have 90% before Sunday night [submitted]
Then I finish 100% Monday morning  and resubmit (20% penalty applies)
will my grade be downgraded to 80?

 I have created my dictionary in he format : 
{'brown': 1, 'lazy': 1, 'jumps': 1, 'fox': 1, 'dog': 1, 'quick': 1}
This passes test one, but for test 2, I have to add the values of similar keys. I am new to python therefore I am unable to figure out the right operation for that. Please can someone help me out ? My code for the commonTokens part is this (i post the code because there are several other post that describe these exact step):commonTokens = (amazonInvPairsRDD .join(googleInvPairsRDD) .map(swap) .reduceByKey(lambda a, b: a + ' ' + b) .map(lambda x: (x[0], x[1].split(' '))) .cache() ) but i get the following error:
<span class="ansired">---------------------------------------------------------------------------</span>
<span class="ansired">Py4JJavaError</span>                             Traceback (most recent call last)
<span class="ansigreen"><ipython-input-88-4b4994d43401></span> in <span class="ansicyan"><module><span class="ansiblue">()</span>
<span class="ansigreen">     19</span>                )
<span class="ansigreen">     20</span> <span class="ansiyellow"></span>
<span class="ansigreen">---> 21<span class="ansiyellow"> <span class="ansigreen">print</span> <span class="ansiblue">'Found %d common tokens'</span> <span class="ansiyellow">%</span> commonTokens<span class="ansiyellow">.</span>count<span class="ansiyellow">(</span><span class="ansiyellow">)</span><span class="ansiyellow"></span>

<span class="ansigreen">/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py</span> in <span class="ansicyan">count<span class="ansiblue">(self)</span>
<span class="ansigreen">    930</span>         <span class="ansicyan">3</span><span class="ansiyellow"></span>
<span class="ansigreen">    931</span>         """
<span class="ansigreen">--> 932<span class="ansiyellow">         <span class="ansigreen">return</span> self<span class="ansiyellow">.</span>mapPartitions<span class="ansiyellow">(</span><span class="ansigreen">lambda</span> i<span class="ansiyellow">:</span> <span class="ansiyellow">[</span>sum<span class="ansiyellow">(</span><span class="ansicyan">1</span> <span class="ansigreen">for</span> _ <span class="ansigreen">in</span> i<span class="ansiyellow">)</span><span class="ansiyellow">]</span><span class="ansiyellow">)</span><span class="ansiyellow">.</span>sum<span class="ansiyellow">(</span><span class="ansiyellow">)</span><span class="ansiyellow"></span>
<span class="ansigreen">    933</span> <span class="ansiyellow"></span>
<span class="ansigreen">    934</span>     <span class="ansigreen">def</span> stats<span class="ansiyellow">(</span>self<span class="ansiyellow">)</span><span class="ansiyellow">:</span><span class="ansiyellow"></span>

<span class="ansigreen">/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py</span> in <span class="ansicyan">sum<span class="ansiblue">(self)</span>
<span class="ansigreen">    921</span>         <span class="ansicyan">6.0</span><span class="ansiyellow"></span>
<span class="ansigreen">    922</span>         """
<span class="ansigreen">--> 923<span class="ansiyellow">         <span class="ansigreen">return</span> self<span class="ansiyellow">.</span>mapPartitions<span class="ansiyellow">(</span><span class="ansigreen">lambda</span> x<span class="ansiyellow">:</span> <span class="ansiyellow">[</span>sum<span class="ansiyellow">(</span>x<span class="ansiyellow">)</span><span class="ansiyellow">]</span><span class="ansiyellow">)</span><span class="ansiyellow">.</span>reduce<span class="ansiyellow">(</span>operator<span class="ansiyellow">.</span>add<span class="ansiyellow">)</span><span class="ansiyellow"></span>
<span class="ansigreen">    924</span> <span class="ansiyellow"></span>
<span class="ansigreen">    925</span>     <span class="ansigreen">def</span> count<span class="ansiyellow">(</span>self<span class="ansiyellow">)</span><span class="ansiyellow">:</span><span class="ansiyellow"></span>

<span class="ansigreen">/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py</span> in <span class="ansicyan">reduce<span class="ansiblue">(self, f)</span>
<span class="ansigreen">    737</span>             <span class="ansigreen">yield</span> reduce<span class="ansiyellow">(</span>f<span class="ansiyellow">,</span> iterator<span class="ansiyellow">,</span> initial<span class="ansiyellow">)</span><span class="ansiyellow"></span>
<span class="ansigreen">    738</span> <span class="ansiyellow"></span>
<span class="ansigreen">--> 739<span class="ansiyellow">         </span>vals <span class="ansiyellow">=</span> self<span class="ansiyellow">.</span>mapPartitions<span class="ansiyellow">(</span>func<span class="ansiyellow">)</span><span class="ansiyellow">.</span>collect<span class="ansiyellow">(</span><span class="ansiyellow">)</span><span class="ansiyellow"></span>
<span class="ansigreen">    740</span>         <span class="ansigreen">if</span> vals<span class="ansiyellow">:</span><span class="ansiyellow"></span>
<span class="ansigreen">    741</span>             <span class="ansigreen">return</span> reduce<span class="ansiyellow">(</span>f<span class="ansiyellow">,</span> vals<span class="ansiyellow">)</span><span class="ansiyellow"></span>

<span class="ansigreen">/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py</span> in <span class="ansicyan">collect<span class="ansiblue">(self)</span>
<span class="ansigreen">    711</span>         """
<span class="ansigreen">    712</span>         <span class="ansigreen">with</span> SCCallSiteSync<span class="ansiyellow">(</span>self<span class="ansiyellow">.</span>context<span class="ansiyellow">)</span> <span class="ansigreen">as</span> css<span class="ansiyellow">:</span><span class="ansiyellow"></span>
<span class="ansigreen">--> 713<span class="ansiyellow">             </span>port <span class="ansiyellow">=</span> self<span class="ansiyellow">.</span>ctx<span class="ansiyellow">.</span>_jvm<span class="ansiyellow">.</span>PythonRDD<span class="ansiyellow">.</span>collectAndServe<span class="ansiyellow">(</span>self<span class="ansiyellow">.</span>_jrdd<span class="ansiyellow">.</span>rdd<span class="ansiyellow">(</span><span class="ansiyellow">)</span><span class="ansiyellow">)</span><span class="ansiyellow"></span>
<span class="ansigreen">    714</span>         <span class="ansigreen">return</span> list<span class="ansiyellow">(</span>_load_from_socket<span class="ansiyellow">(</span>port<span class="ansiyellow">,</span> self<span class="ansiyellow">.</span>_jrdd_deserializer<span class="ansiyellow">)</span><span class="ansiyellow">)</span><span class="ansiyellow"></span>
<span class="ansigreen">    715</span> <span class="ansiyellow"></span>

<span class="ansigreen">/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py</span> in <span class="ansicyan">__call__<span class="ansiblue">(self, *args)</span>
<span class="ansigreen">    536</span>         answer <span class="ansiyellow">=</span> self<span class="ansiyellow">.</span>gateway_client<span class="ansiyellow">.</span>send_command<span class="ansiyellow">(</span>command<span class="ansiyellow">)</span><span class="ansiyellow"></span>
<span class="ansigreen">    537</span>         return_value = get_return_value(answer, self.gateway_client,
<span class="ansigreen">--> 538<span class="ansiyellow">                 self.target_id, self.name)
</span><span class="ansigreen">    539</span> <span class="ansiyellow"></span>
<span class="ansigreen">    540</span>         <span class="ansigreen">for</span> temp_arg <span class="ansigreen">in</span> temp_args<span class="ansiyellow">:</span><span class="ansiyellow"></span>

<span class="ansigreen">/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py</span> in <span class="ansicyan">get_return_value<span class="ansiblue">(answer, gateway_client, target_id, name)</span>
<span class="ansigreen">    298</span>                 raise Py4JJavaError(
<span class="ansigreen">    299</span>                     <span class="ansiblue">'An error occurred while calling {0}{1}{2}.\n'</span><span class="ansiyellow">.</span><span class="ansiyellow"></span>
<span class="ansigreen">--> 300<span class="ansiyellow">                     format(target_id, '.', name), value)
</span><span class="ansigreen">    301</span>             <span class="ansigreen">else</span><span class="ansiyellow">:</span><span class="ansiyellow"></span>
<span class="ansigreen">    302</span>                 raise Py4JError(

<span class="ansired">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 193.0 failed 1 times, most recent failure: Lost task 0.0 in stage 193.0 (TID 888, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 253, in mergeValues
    d[k] = comb(d[k], v) if k in d else creator(v)
  File "<ipython-input-88-4b4994d43401>", line 16, in <lambda>
TypeError: can only concatenate tuple (not "str") to tuple

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48
</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
Can someone help me with this??? I have spent many hours and i can't complete the last part 4f if i don't do this.Thanks in andvance. when I wasn't pass all tests, I submit once, it shows 63% completed. but when I pass all tests in my notebook within 6 minutes, it shows timeout. And I only have the last chance to try it. A little frustrated! 
Your submission token ID is 1210921 I have some data files and I want to know how can I upload the data files in virtual box to tryout some more spark on my own ...any help will be highly appreciated pairRDD = sc.parallelize([('a', 1), ('a', 2), ('a', 3), ('b', 1)])
group it by key

method 1: 
print dict(pairRDD.groupByKey().mapValues(list).collect())
=>[('a', [1, 2, 3]), ('b', [1])]

method 2: by Donald Fung
print (pairRDD
    .reduceByKey(lambda x,y:x+[y] if type(x)==type([]) else [x,y])    .map(lambda (k,v): (k,v) if type(v)==type([]) else (k,[v]))    .collect())
=>[('a', [1, 2, 3]), ('b', [1])]

can someone compare method 1 and method 2 in speed, scalable, restriction. Hello all,

I am having a hard time understanding what I am expected to do in 1c of lab 3 can anyone shed some light on the thought process? I cannot seem to grasp what amazon/googlerectotoken even means. Thank you. I am not sure why the 3c Perform Entity Resolution script has been running for the last half hour.

I haven't used any Spark Context in the tf tfidf functions. Has anybody come across this issue?

Any help is appreciated. When trying "vagrant up"  in cmd window, I get the following message :

The guest machine entered an invalid state while waiting for itto boot. Valid states are 'starting, running'. The machine is in the'aborted' state. Please verify everything is configuredproperly and try again.
If the provider you're using has a GUI that comes with it,it is often helpful to open that and watch the machine, since theGUI often has more helpful error messages than Vagrant can retrieve.For example, if you're using VirtualBox, run `vagrant up` while theVirtualBox GUI is open.

Question: How to reset the state in a valid one ?

Do I have to reinstall Vagrant, and VirtualBox and recreate the Virtual machine ?


When using with virtualBox UI a get a prompt window with following message ( I translated from French) : "Instruction at 0xd6ff8342 is using memory address 0x00000021. The memory could not be in 'read' state" 

Found this ticket on a similar problem : 
https://www.virtualbox.org/ticket/13671

I have tried to rename the VM sparkvm to sparkvm-aborted on virtualBox, to have a chance to install everything from Vagrant. .... No chance, my vm was renamed back to sparkvm.

How can I reinitiate an install from the start ? We would like to replicate environment for our projects outside of the classroom. Can you please point to any resources, documentation on how we go about setting up a lab with Vagrant, Ubuntu vM in VirtualBox and Jupyter? Is there a script or instructions that class instructors followed to set this up? Would you mind sharing this information?

Thank you. Hi,
I  unable to comprehend what is happening in  the  5 1 
line 

fpCounts = sc.accumulator(zeros, VectorAccumulatorParam())

can someone help in understanding what does the above line do

Thanks and regards I have set my proxy and  vagrant up, then it reports the following error,"Proxy Connect Aborted"


Can anyone help?
 Hi to find movieNameWithAvgRatingsRDD I have joined and movieIDsWithAvgRatingsRDD and I get the following output
 
(1, (u'Toy Story (1995)', (993, 4.145015105740181)))
 
now to get the movie name, rating count, rating avg I have the following which seems weird with scary indexes is there any better way also getting exception with below code
 
movieNameWithAvgRatingsRDD = (moviesRDD.join(movieIDsWithAvgRatingsRDD)).map(lambda a : (a[0][1][1][1],a[0][1][0],[0][1][1][0]))
 
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-61-7faa74645970> in <module>()
     26 movieNameWithAvgRatingsRDD = (moviesRDD
     27                               .join(movieIDsWithAvgRatingsRDD)).map(lambda a : (a[0][1][1][1],a[0][1][0],[0][1][1][0]))
---> 28 print 'movieNameWithAvgRatingsRDD: %s\n' % movieNameWithAvgRatingsRDD.take(3)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_clien def dotprod(a, b):
    """ Compute dot product
    Args:
        a (dictionary): first dictionary of record to value
        b (dictionary): second dictionary of record to value
    Returns:
        dotProd: result of the dot product with the two input dictionaries
    """
    sum=0
    for k in a:
        if k in b:
            sum += a[k] * b[k]
    return  sum
             
def norm(a):
    """ Compute square root of the dot product
    Args:
        a (dictionary): a dictionary of record to value
    Returns:
        norm: a dictionary of tokens to its TF values
    """
    return math.sqrt(dotprod(a, b))

def cossim(a, b):
    """ Compute cosine similarity
    Args:
        a (dictionary): first dictionary of record to value
        b (dictionary): second dictionary of record to value
    Returns:
        cossim: dot product of two dictionaries divided by the norm of the first dictionary and
                then by the norm of the second dictionary
    """
    return (dotprod(a, b) / (norm(a) * norm(b)))
im getting the error below for this code of mine. Can some one please help me out

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-60-a524599c73a1> in <module>()
     39 testVec2 = {'foo': 1, 'bar': 0, 'baz': 20 }
     40 dp = dotprod(testVec1, testVec2)
---> 41 nm = norm(testVec1)
     42 print dp, nm

<ipython-input-60-a524599c73a1> in norm(a)
     23         norm: a dictionary of tokens to its TF values
     24     """
---> 25     return math.sqrt(dotprod(a, b))
     26 
     27 def cossim(a, b):

NameError: global name 'b' is not defined

  (4c) Compute Norms for the weights from the full datasets
I am not sure how this is working for others.  For the below test assertion

Test.assertTrue(isinstance(amazonNormsBroadcast, Broadcast)

The fix is straight forward as shown below:

Test.assertTrue(isinstance(amazonNormsBroadcast, pyspark.broadcast.Broadcast)
 just finished cs100.1x...thank you so much profs and fellow class mates this was an awesome experience.

I have some data files and want to know the following
 

1. how to create a virtual box with Ipython, jupyter, sample data and spark ...I found vagrant and virtual box interesting ...want to introduce the same @ work place
2. how to use  localhost4040/stages to improve the performance of executing spark any help will be highly appreciated.
3. given some twitter data (500 TB) how many machines with what configuration will I require to do similar analytics in amazon.
4. my final objective is have the data in s3 and cluster in amazon and run jupyter in local system - need help/pointers to configure the same...any help from fellow classmates is highly appreciated
 
This is a hobby project on twitter data and politics in India ...want to make this app real time with some machine learning features...let me know if anyone is interested Hi 

I am struggling to solve this simple problem and here it how it looks

[(x,(len(y),sum(y)/len(y))) for (x,y) in tuple]

but does not yield what it is suppose to be.. any clue? Hello, 

While finding the commonTokens, I have joined amazon and google inverse pairs, and swapped them when the output looks something like:

[(('b0007yepy6', 'http://www.google.com/base/feeds/snippets/17293379770636740081'), 'aided'), (('b000hlt5j4', 'http://www.google.com/base/feeds/snippets/17293379770636740081'), 'aided')]I use reduceByKey() here and it gives me an error:Py4JJavaError Traceback (most recent call last) <ipython-input-45-0ff92a35aad0> in <module>() 13 commonTokens = (amazonInvPairsRDD 14 .join(googleInvPairsRDD).map(lambda record: swap(record)).reduceByKey(lambda (a,b): a+b)) ---> 15 print commonTokens.take(10) 16 print 'Found %d common tokens' % commonTokens.count() ...Using groupByKey is slow but this also gives an output of the form: [(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), <pyspark.resultiterable.ResultIterable object at 0xafe93e2c>), (('b000bnb72g', 'http://www.google.com/base/feeds/snippets/6959766318020460134'), <pyspark.resultiterable.ResultIterable object at 0xafe93f6c>)]Please help me.
 I was wondering why every endpoint has u before them in RDDs and not simply 'URI'?
 My code works, but I am unsure if is is the most optimal way to get the result.

I am making use of a set and appending all hosts for a day to it, to get the unique hosts. I am curious, is there a better way to aggregate unique lists in spark?

@TAs I understand its not right to share code, but then how can we know if our solution is optimal. And if we don't know that, we will not have learned much.

Edit: I am appending to the set in spark, inside rdd's map method I successfully completed the lab, but the predicted rating for the top movie is greater than 5. Here is the result i got.
My highest rated movies as predicted (for movies with more than 75 reviews):
(5.002569044454422, u'Beyond the Mat (2000)', 99)
(4.663812019820682, u'It Happened One Night (1934)', 190)
(4.6283847364130155, u'Dracula (1931)', 102)
(4.620103310257488, u'Adventures of Robin Hood, The (1938)', 164)
(4.5995230067905855, u'White Christmas (1954)', 145)
(4.585224786908935, u'Victor/Victoria (1982)', 191)
(4.561518492215826, u'Gone with the Wind (1939)', 558)
(4.55022849652121, u'Wizard of Oz, The (1939)', 817)
(4.52512694435611, u'Thin Man, The (1934)', 152)
(4.510686559468041, u'Mis\ufffdrables, Les (1995)', 115)
(4.501007965005252, u'Inherit the Wind (1960)', 133)
(4.499272223217229, u'Meet Me in St. Louis (1944)', 92)
(4.491069875809268, u'Shadow of a Doubt (1943)', 127)
(4.480161664078221, u'Great Race, The (1965)', 83)
(4.451987070017613, u'Top Hat (1935)', 131)
(4.443866428592815, u'Escape to Witch Mountain (1975)', 135)
(4.4409736535866795, u'Doctor Zhivago (1965)', 261)
(4.439159560458053, u'No Escape (1994)', 82)
(4.419913340908806, u'Bringing Up Baby (1938)', 192)
(4.418632373692837, u'Sound of Music, The (1965)', 541)

So, my question is, Is it OK to to get rating >5 in prediction. Shouldn't it be in range 1 to 5? Or i did some mistake, which auto grader couldn't catch ?
 
I read in the notebook

Create a new nonDupsRDD RDD that has the just the cosine similarity scores for those "AmazonID GoogleURL" pairs from the similaritiesBroadcast RDD that do not appear in both the sims RDD and gold standard RDD

I understand

pairs from the similaritiesBroadcast RDD that not appear neither in the sims RDD nor in gold standard RDD

But this is not possible for me because all sims records come from similaritiesBroadcast RDD . fot me it is not possible to find records in similaritiesBroadcast  that are not in sims. Form me similaritiesBroadcast  and sims are the same data set but with key arranged in a diferente way.

FOr me has no sense the sentence that explain what it is expected from me to do I have the following prints from RDD's:

movieIDsWithRatingsRDD: [(2, <pyspark.resultiterable.ResultIterable object at 0xaff5b72c>), (4, <pyspark.resultiterable.ResultIterable object at 0xaff5bc4c>), (6, <pyspark.resultiterable.ResultIterable object at 0xb0e46dac>)]
movieIDsWithAvgRatingsRDD: [(2, (1.0, 5.0)), (4, (1.0, 1.0)), (6, (1.0, 4.0))]
MoviesRDD: [(1, u'Toy Story (1995)'), (2, u'Jumanji (1995)'), (3, u'Grumpier Old Men (1995)')]

MovieRDD second key value: 2
movieIDsWithAvgRatingsRDD first key value: 2

2 == 2 => False // What I'm doing wrong? And because of it the join gives me an error as:

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1077.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1077.0 (TID 774, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-217-ad541a6a6c81>", line 10, in <lambda>
  File "<ipython-input-155-960133d6547f>", line 12, in getCountsAndAverages
IndexError: list index out of range








 Hi Al,

I got some strange printings in 2b:

Error for test dataset (should be 1.22474487139): 1.22474487139
Error for test dataset2 (should be 3.16227766017): 3.16227766017
Error for testActual dataset (should be 0.0): 0.0
It looks OK to me, any ideas ? Hi,

First of all, thanks for the wonderful class. It's been a while since I felt challenged on a MOOC or spent morning to night cracking on solutions without crashing of boredoom :)

I wanted to ask if it would be possible to let us know how many students made it to the second to last and finally last week by the time the class concludes. I am more interested on the numbers because it is a good measure for us of the difficulty / perseverance to get to the end of the course. It's also a interesting statistic to have at hand given I went for a verified certificate.

A second question is if the course will remain available for later access, or if not when it would be closing. I had a couple frustrating experiences with MOOCs closing out of sudden and leaving no trace of material for me to review when needing later on..

Thank you again, Hi,

I'm reading the first part of the Lab. I see that a lot of effort is put into taking samples sorted so that the sorting procedure is deterministic.

I was wondering why not using the ``sample`` method with a fix random seed.

(My intuition tells me that there is something obvious that I'm missing, but I don't know what...)

Thanks. Well I managed to finish this course, even the Lab 3 that would of had me pulling my hair out, if I had any.

Have to thank those that offered tips and advice (students and TA's), I couldn't of completed the course without the tips, especially what a drunken E (∑) meant.

See you in  CS190.1X if you are coming! (Yes I must be a masochist)



 Mine are:

My highest rated movies as predicted (for movies with more than 75 reviews):
(4.9840622259996845, u'Land and Freedom (Tierra y libertad) (1995)', 6)
(4.835325103327832, u'Finding North (1999)', 3)
(4.782851502217937, u'Go West (1925)', 19)
(4.767528297804809, u'Trouble in Paradise (1932)', 16)
(4.692980347893533, u'Endurance (1998)', 12)
(4.687738059967418, u'Tigrero: A Film That Was Never Made (1994)', 3)
(4.512741373952203, u'Meet John Doe (1941)', 28)
(4.5075244826390115, u'Maya Lin: A Strong Clear Vision (1994)', 27)
(4.488404107315131, u'Man of the Century (1999)', 3)
(4.46029137821576, u'Red Firecracker, Green Firecracker (1994)', 16)
(4.443452064041265, u'Sarafina! (1992)', 8)
(4.433399227459365, u'Hearts and Minds (1996)', 9)
(4.418022662100731, u'Firelight (1997)', 5)
(4.377843856668289, u'Inherit the Wind (1960)', 133)
(4.368970781996008, u'Gambler, The (A J\ufffdt\ufffdkos) (1997)', 4)
(4.364180393185103, u'Arguing the World (1996)', 5)
(4.338070652840086, u'Ballad of Narayama, The (Narayama Bushiko) (1958)', 2)
(4.328934426709464, u'Grapes of Wrath, The (1940)', 242)
(4.3241375974669145, u'Bound for Glory (1976)', 19)
(4.3061002577177705, u"It's a Wonderful Life (1946)", 343) I have passed all tests in notebook locally but when I submit my .py file I receive this error

Number of Ratings and Average Ratings for a Movie (1a)
------------------------------------------------------
Traceback (most recent call last):
  File "", line 38, in 
  File "/ok/submission.py", line 236
SyntaxError: Non-ASCII character '\xe2' in file /ok/submission.py on line 237, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'getCountsAndAverages' is not defined

Submission token ID is 1225766-c8a146bbaec51b61bcdb555b9b6fda1f:ip-172-31-31-180 Hi,
I am stuck at solving 3c. All the previous tests before 3c are passed. But not sure why I am getting the following exception.
Appreciate any help on this.

My crossSmall.take(1) output looks like

[(('http://www.google.com/base/feeds/snippets/11448761432933644608', 'spanish vocabulary builder "expand your vocabulary! contains fun lessons that both teach and entertain you\'ll quickly find yourself mastering new terms. includes games and more!" '), ('b000jz4hqo', 'clickart 950 000 - premier image pack (dvd-rom)  "broderbund"'))]

and the error I am getting is

---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
 in ()
     36             .collect()[0][2])
     37 
---> 38 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     39 print 'Requested similarity is %s.' % similarityAmazonGoogle

 in similar(amazonID, googleURL)
     34     return (similarities
     35             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
---> 36             .collect()[0][2])
     37 
     38 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')

IndexError: list index out of range
------------------------------------------------------------------------

I have similarities as

 return (similarities
            .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
            .collect()[0][2]) I'm not quite sure how to do the reduce in order to have the tokens as a list afterwards :/

while running this simple print::print commonTokens.take(3)This is what I'd expect (in order to proceed in 4f with the looping of tokens)::[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120']), (('b000bnb72g', 'http://www.google.com/base/feeds/snippets/6959766318020460134'), ['software']), (('b00009apna', 'http://www.google.com/base/feeds/snippets/13986409474793202514'), ['business'])]This is what I get with a reduceByKey(lambda (a,b): a+b)[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), '120'), (('b000bnb72g', 'http://www.google.com/base/feeds/snippets/6959766318020460134'), 'software'), (('b00009apna', 'http://www.google.com/base/feeds/snippets/13986409474793202514'), 'business')]

This is what happens with the groupByKey[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), <pyspark.resultiterable.ResultIterable object at 0xb06bd28c>), (('b000bnb72g', 'http://www.google.com/base/feeds/snippets/6959766318020460134'), <pyspark.resultiterable.ResultIterable object at 0xb06bd3ec>), (('b00009apna', 'http://www.google.com/base/feeds/snippets/13986409474793202514'), <pyspark.resultiterable.ResultIterable object at 0xb06bd44c>)]

It would seem to me that there should be an easy way to do it with another lambda function in reduceByKey - but I can't seem to get it right.
Maybe the same if going for combineByKey - if reduceByKey doesn't cover it.

I feel I've done 4f correct - but can't get the right input (commonTokens) to it :( Hi,

Consider the following snippet:

xsRDD = sc.parallelize([(1, 1), (2, 2), (1, 3), (2, 4), (1, 5), (1, 6), (1, 7)])
xsRDD.foldByKey([], lambda xs, x: xs + [x]).collect()

If I run this, I get the expected result:

[(1, [1, 3, 5, 6, 7]), (2, [2, 4])]

However, when running this in big data sets, I obtain tuples with nested lists, which is not what I would expect.

Am I understanding something in the semantics of foldByKey? I was not able to submit lab 1 assignment due to some errors. After That I could never see lab 2 , 3 and 4 problem. There were no .ipynb files listed related to lab 2 , 3 and so on at http://localhost:8001/tree. Does it mean that you get to see next lab exercise problem only if you complete the previous one?  What should the norm() function return in 3a?
4c expects a dictionary when norm() is called. I was thinking how to work with nice schema over tuples, wrote a small code ...I am getting pickling error 
when I want to use my parse function with map ...the last print statement when you uncomment it...anyone knows what could be the problem

PicklingError: Can't pickle __main__.Person: attribute lookup __main__.Person failed

RDD looks like 
[u'Orlando\tM\t40\tPython', u'Lina\tF\t39\tC#', u'John\tM\t30\tPython', u'Jane\tF\t32\tPython', u'Michelle\tF\t18\tPython', u'Daniel\tM\t20\tC#']
It is about name,gender,age,favorite_language

class Person:
 def parse(self,line):
 fields = line.split('\t')
 self.name = unicode(fields[0])
 self.gender = unicode(fields[1])
 self.age= int(unicode(fields[2]))
 self.favorite_language = unicode(fields[3])
 return self
 def __repr__(self):
 return "Person(%s,gender=%s, %d years old,likes %s)" % (self.name,self.gender,self.age,self.favorite_language)
people = sc.textFile(fileName)
print people
print people.collect()
print Person().parse(u'Orlando\tM\t40\tPython')
#print people.map(Person().parse).collect()
 Please I need help with this exercise because I don`t know why it crashes when it says that "File "", line 19, in fastCosineSimilarity TypeError: list indices must be integers, not str"

This is part of my code:

amazonWeightsBroadcast = sc.broadcast(amazonWeightsRDD.collectAsMap())
googleWeightsBroadcast = sc.broadcast(googleWeightsRDD.collectAsMap())
print commonTokens.take(3)
print amazonWeightsBroadcast.value
def fastCosineSimilarity(record):
    """ Compute Cosine Similarity using Broadcast variables
    Args:
        record: ((ID, URL), token)
    Returns:
        pair: ((ID, URL), cosine similarity value)
    """
    amazonRec = record[0][0]
    googleRec = record[0][1]
    tokens = record[1]
    s = 0
    for t in tokens:
        s += amazonWeightsBroadcast.value[amazonRec][t] * googleWeightsBroadcast.value[googleRec][t]     ...    return (key, value)similaritiesFullRDD = (commonTokens
                      .map(fastCosineSimilarity)
                       )And the output looks like that:print commonTokens.take(3)  [(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120']), (('b000bnb72g', 'http://www.google.com/base/feeds/snippets/6959766318020460134'), ['software']), (('b00009apna', 'http://www.google.com/base/feeds/snippets/13986409474793202514'), ['business'])]
print amazonWeightsBroadcast.value{'b0009mg80a': {'control': 0.780796279491833, 'right': 0.276979719942057, 'simple': 0.3281607551487414, 'text': 0.4609280835676978, 'pro': 0.4364933417882054, 'share': 0.4252223869532987, 'one': 0.13977217348927876, 'interface': 0.3144873903508772, 'computer': 0.16497699165947657, 'distant': 15.095394736842104, 'chat': 2.87531328320802, 'keyboard': 1.1611842105263157, 'home': 0.17603958876783796, 'message': 2.5158991228070176, 'softwre': 20.12719298245614, 'mouse': 0.862593984962406, 'best': 0.24951065680730752, 'use': 0.10464745051536987, 'timbuktu': 34.503759398496236, 'provide': 0.5805921052631579, 'support': 0.21798403952118564, 'send': 1.2847144456886899, 'macintosh': 0.31947925368978003, '1': 0.07652925088386364, 'email': 0.794494459833795, 'files': 0.21719992427110943, 'collaborate': 2.415263157894737, 'multiple': 0.30040586540979314, 'business': 0.16634043787153835, 'sitting': 12.076315789473682, 'intercom': 30.19078947368421, 'screen': 0.9738964346349744, 'communicate': 1.3126430205949655, 'choice': 0.6940411373260738, 'mac': 0.22171938413966372, 'user': 0.13358756404285047, 'tying': 30.19078947368421, 'front': 3.551857585139319, 'remote': 3.920881749829118, 'teach': 1.1611842105263157, 'standard': 0.18521956732321598, 'classroom': 2.415263157894737, 'instant': 0.47173108552631576, 'intuitive': 0.5391212406015037, 'license': 0.2278550148957299, 'technical': 1.05932594644506, 'many': 0.38706140350877194, 'setup': 1.472721437740693, 'computers': 1.1956748306409586, 'friendly': 1.1392750744786495, 'large': 1.1392750744786495, 'netopia': 8.625939849624059, 'without': 0.29891870766023965, 'small': 0.4052454962910632, 'security': 0.3490264679038637, 'operate': 5.031798245614035, 'os': 0.46686787845903416, 'options': 0.42823814856289655, 'view': 0.37272579597141}, 'b000g017kg': {'case': 6.454289732770746, 'publishing': 3.6420634920634924, ...And finally the exception:Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 569, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in  return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum() File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in  return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum() File "", line 19, in fastCosineSimilarity TypeError: list indices must be integers, not str at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135) at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61) at org.apache.spark.scheduler.Task.run(Task.scala:64) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) ...
  I have spent half my weekend on this one part of one problem, and it seems like my answer *should* work... but it just doesn't.

I don't have much more time to spend on this, and I'd really like to move on, but subsequent problems need tokenize to work.

Can someone please find the error in my code? It runs, but returns Null for tokenize(quickbrownfox). For some reason, the comparison doesn't work with quickbrownfox, but when I give it sample text to debug, it works fine.

Thanks in advance.

def tokenize(string):    """ An implementation of input string tokenization that excludes stopwords    Args:        string (str): input string    Returns:        list: a list of tokens without stopwords    """        stopwords2 = []    for sym in stopwords:        stopwords2.append(str(sym))    #    holding = sc.parallelize(simpleTokenize(string),4).zipWithIndex()#    stopwords3 = sc.parallelize(stopwords2, 4)#    stopwords4 = stopwords3.map(lambda x: (x,1))#    holding2 = holding.leftOuterJoin(stopwords4).filter(lambda (x,y): y[1] != 1)#    holding3 = sc.parallelize(holding2.takeOrdered(50000, lambda (x,y): y[0]),4)#    holding4 = holding3.map(lambda (x,y): x)    #    return holding4.collect()    tokens = list(simpleTokenize(string))    holding = []    for sym in tokens:        if sym in stopwords2:            continue            holding.append(sym)                return holdingprint tokenize(quickbrownfox) # Should give ['quick', 'brown', ... ] I used spark join transformation as  -->>    finalRDD = RDD1.join(RDD2)the joined is performed but

i get  Error  when I want to see the results of  -->>    finalRDD.take(1) 
or any other spark action like count()Can some body explain why so ?I want to see joined RDD so that I can perform correct .map transformation for next phase .
I also get the RDD as
<bound method PipelinedRDD.collect of PythonRDD[488] at RDD at PythonRDD.scala:43>
 Hi All,
 Please excuse me if this is already answered. I tried to search for this but could not find relevant matches in first 10 posts I found.
I do not have time to search thru 100s of posts.

Is there any FAQ for this course?

I want to load my own data files in Spark framework and do my data analysis.

Please point to me any instructions to do this.

Thanks in advance.
 Hello,

I have completed till lab3. I would like to download all the data this course is using to try some other stuffs and keep learning once the course is over. I have saved my codes. It only makes sense if I can also save data.

Please let me know what is the best way to download all the data.

Thanks. Is it possible to have those for each lab? Thanks to professor, instructors, teaching assistants, volunteers, and classmates. It was very good class and I learned significantly. I completed the course today. Thank you for making the lab 4 shorter and simpler. It will help us to take some break and start fresh in the  CS 190.1X.   How would i make a python iterable, would it be something like using it in a map as 
iter(Ratings) or something else. Could anyone guide me to a stackoverflow or some where that can assist me with this. Thanks.
 
Well I have the answer and it is not iter(Rating), but still like to understand it better why groupByKey, does the trick, thank you for your time. Hi Guys,

I have got an issue on the above, this is my output
Combined Ratings : [((1, 3), (4, 5)), ((2, 2), (2, 1)), ((1, 2), (3, 3)), ((2, 1), (3, 5))]
SquaredErrors : [4, 0, 1, 1]
Sum Total Error : 6
Num Ratings : 4
1.22474487139
Error for test dataset (should be 1.22474487139): 1.22474487139
Combined Ratings : [((2, 2), (5, 1)), ((1, 2), (5, 3))]
SquaredErrors : [0, 1]
Sum Total Error : 1
Num Ratings : 2
0.707106781187for square errors rdd i have joined the actual and predicted rdds and did a map and the lambda is (UserID - MovieID) ** 2is this the right way to do this ? Any hints ? 
fullCorpusRDD = <FILL IN> #here a similar union to the one used in 2B
idfsFull = idfs(fullCorpusRDD)
idfsFullCount = idfsFull.count()
print 'There are %s unique tokens in the full datasets.' % idfsFullCount



I get this error. I do not understand why. Any suggestion?
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-35-81992fe71b9e> in <module>()
      3 print fullCorpusRDD.take(3)
      4 idfsFull = idfs(fullCorpusRDD)
----> 5 idfsFullCount = idfsFull.count()
      6 print 'There are %s unique tokens in the full datasets.' % idfsFullCount
      7 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 90.0 failed 1 times, most recent failure: Lost task 0.0 in stage 90.0 (TID 425, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 90, in main
    command = pickleSer.loads(command.value)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/broadcast.py", line 106, in value
    self._value = self.load(self._path)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/broadcast.py", line 87, in load
    with open(path, 'rb', 1 << 20) as f:
IOError: [Errno 2] No such file or directory: '/tmp/spark-1a07ad3d-1ce8-4a85-a5a5-b10d8e3d1e4f/pyspark-623c9492-9425-4dac-86ae-95cc1d2164cf/tmpx6frCf'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


 This is more of a question for instructors. I have been very much been able to keep up the pace half way up to Lab 3. Unfortunately, I am unable to give enough time needed to complete them due to professional commitments.

I will not like to miss this opportunity of learning the course provides. It is my kind request to instructors to share/ allow to share the completed solutions once course lab submission deadline is over. This will allow me to make better use of my time and learn things I am missing at the moment as and when time permits.
 Hi!
I've stuck on 2c idfs(corpus) design and have now idea how to resolve calculations after extracting unique tokens out from small dataset. Sure, I got 4772 unique tokens and how to imply occurrences check across the corpus?

Could you suggest me the right approach, please? Can't understand, what should we do in lab4 3c. 
Why there are 2 predictAll calls? 

3f: 
Isn't there missing one operation? 
We have [(myId, 'name', 5), (myId, 'name', 5), (myId, 'name3', 5), (myId, 'name2', 5)] array.
And moviesRDD has structure(and doesn't contain myId in there) : 
[(1, u'Toy Story (1995)'), (2, u'Jumanji (1995)'), (3, u'Grumpier Old Men (1995)')] I have tried the following code in the invert()
pairs = []            for row in record[0][0]:                for key in record[0][1].keys():                        pairs.append(key, row[0])

but I get errors "str has no attribute keys".

I have also tried this one:
pairs = []            for row in record:                for key in record[1].keys():                        pairs.append(key, row[0])

but I get the same error.
Please help me understand the format of the record. I can't get the values out of the record. Can we just rewrite the invert as invert(id, dictionary)???
Please help me solve the issue with this invert() function. 
 Hi,

I am getting the following data for squared errors rdd. Am I heading int the right direction?

(I performed join on predictedReformattedRDD and actualReformattedRDD and then val1^2 - val2^2)




[((1, 3), -9), ((2, 2), 3), ((1, 2), 0), ((2, 1), -16)]


 Looking at the following link, I see that there are is a "weights" parameter.

https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit

But, what does it mean to be "weights for splits"? Hi, I'm trying to submit my file with .py extension and all test passed but autograder throws the next exception. What can I do?:

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 190
SyntaxError: Non-ASCII character '\xc2' in file /ok/submission.py on line 191, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'simpleTokenize' is not defined

Removing stopwords (1b)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tokenize' is not defined

Tokenizing the small datasets (1c)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'totalTokens' is not defined

Amazon record with the most tokens (1d)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'biggestRecordAmazon' is not defined

Implement a TF function (2a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tf' is not defined

Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'corpusRDD' is not defined

Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueTokenCount' is not defined

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'rec_b000hkgj8k_weights' is not defined

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dp' is not defined

Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsSmallBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonFullRecToToken' is not defined

Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsFullCount' is not defined

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonNormsBroadcast' is not defined

Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'invert' is not defined

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'commonTokens' is not defined

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similaritiesFullRDD' is not defined

-- 0 cases passed (0.0%) --


Your submission token ID is 1230013-6473715dd796fdb8ecbbd2d816428970:ip-172-31-24-8
Please include this submission token ID when you need support for your code submission.
Your anonymous student ID is 1ccfa956fac21c5b1e09214f3b2c7e71. Do not post this ID on Piazza.
 I completed the course this morning.

It was a great experience and I must admit that I learnt a lot given that I had some previous exposure to Spark, reading articles, books and Spark coding examples.

All lectures were very well presented - thank you so much Anthony !!!
However, working through the labs made the biggest difference deepening the learning experience. Without the labs I would have liked the course but with the labs I just loved it!!!

You also did a fantastic job, setting up the VM in conjunction with the iPython programming environment. This made it really enjoyable to work with Spark.
At one of my customers, we are using the Hortonworks Hadoop distribution (HDP 2.2.4) and let me tell you, it's a painful user experience working with Spark in that environment as of now.

A final comment concerning the labs. I can think of only one thing that would improve the lab experience. Next to actual lab files, I would have love a summary definition of all used RDDs and their structure. Especially on the later parts of the lab 2,3 and 4 I found myself jumping backward and forward trying to remember the structure of a previously defined RDD that was used in the current exercise.

Also a final comment on the choice of Python. I had little exposure to Python prior to this course, but I am fluent in quite a few programming languages.
From my personal experience, Python is the best choice for this course!!!
The Java API is too verbose and the learning curve for Scala is much steeper than that of Python.

All in all, I had a fantastic learning experience. Thank you all who helped to define, prepare, setup, execute and support this MOOC :-))


 For predictedWithCountsRDD, we need to find tuples of the form (Movie ID, (Predicted Rating, number of ratings)) from predictedRDD and movieCountsRDD.

But predictedRDD has multiple ratings for different users. So I am not sure what needs to go as predicted rating in predictedWithCountsRDD.

Can someone clarify? 1. For 5a (In [63]), I'm getting "TypeError: can't convert complex to int".
2. For 5c (In  [61]), I'm getting "NameError: name 'falsepos' is not defined".

Stack traces below.

Stack trace for 5a
-----------------------------------------------------------
Py4JJavaError Traceback (most recent call last)<ipython-input-63-41bfc54b7aeb> in <module>() 36 fpCounts += set_bit(b, 1, BINS) 37 ---> 38 simsFullValuesRDD.foreach(add_element) 39  40 # Remove true positives from FP counts
/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in foreach(self, f) 687 f(x) 688 return iter([])--> 689 self.mapPartitions(processPartition).count() # Force evaluation 690  691 def foreachPartition(self, f):
/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self) 930 3 931 """--> 932 return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum() 933  934 def stats(self):
/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self) 921 6.0 922 """--> 923 return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add) 924  925 def count(self):
/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f) 737 yield reduce(f, iterator, initial) 738 --> 739 vals = self.mapPartitions(func).collect() 740 if vals: 741 return reduce(f, vals)
/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self) 711 """ 712 with SCCallSiteSync(self.context) as css:--> 713 port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd()) 714 return list(_load_from_socket(port, self._jrdd_deserializer)) 715
/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args) 536 answer = self.gateway_client.send_command(command) 537 return_value = get_return_value(answer, self.gateway_client,--> 538 self.target_id, self.name) 539  540 for temp_arg in temp_args:
/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 298 raise Py4JJavaError( 299 'An error occurred while calling {0}{1}{2}.\n'.--> 300 format(target_id, '.', name), value) 301 else: 302 raise Py4JError(
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 187.0 failed 1 times, most recent failure: Lost task 0.0 in stage 187.0 (TID 791, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last): File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main process() File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process serializer.dump_stream(func(split_index, iterator), outfile) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func return func(split, prev_func(split, iterator)) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func return func(split, prev_func(split, iterator)) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func return func(split, prev_func(split, iterator)) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func return f(iterator) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 687, in processPartition f(x) File "<ipython-input-63-41bfc54b7aeb>", line 35, in add_element File "<ipython-input-63-41bfc54b7aeb>", line 27, in binTypeError: can't convert complex to int
at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
 at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61) at org.apache.spark.scheduler.Task.run(Task.scala:64) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)
Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
 at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
 at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
 at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
 at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
 at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
 at scala.Option.foreach(Option.scala:236)
 at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
 at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
 at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
 at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

-----------------------------------------------------------------------------------------------------------------------------------------------------

Stack trace for 5c

NameError Traceback (most recent call last)<ipython-input-61-6cb7b4e378d1> in <module>() 1 thresholds = [float(n) / nthresholds for n in range(0, nthresholds)]----> 2 falseposDict = dict([(t, falsepos(t)) for t in thresholds]) 3 falsenegDict = dict([(t, falseneg(t)) for t in thresholds]) 4 trueposDict = dict([(t, truepos(t)) for t in thresholds]) 5
NameError: name 'falsepos' is not defined


 Im' stuck with this. 

googleRec = 'http://www.google.com/base/feeds/snippets/18376072611700638452'

googleWeightsBroadcast.value[googleRec][120]  crash for '120' any idea? all the other test already passed

seems weird. Hello, I am trying to transform predictedRatingsRDD using map, but its giving me the error after following the steps as below:


Step 1
myUnratedMoviesRDD.take(5) 






[(0, u'Toy Story (1995)'), (0, u'Jumanji (1995)'), (0, u'Grumpier Old Men (1995)'), (0, u'Waiting to Exhale (1995)'), (0, u'Father of the Bride Part II (1995)')]
Step 2
predictedRatingsRDD uses the input RDD, `myUnratedMoviesRDD`, with myRatingsModel.predictAll()






Step 3
movieCountsRDD.take(2)


[(2, 332), (4, 71)]

Step 4:
transforming predictedRDD using map gives the following error.

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-279-47607f68d9d6> in <module>()
      2 
      3 movieCountsRDD = movieIDsWithAvgRatingsRDD.map(lambda x : (x[0], x[1][0]))
----> 4 print predictedRatingsRDD.map(lambda x : (x)).take(1)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2204.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2204.0 (TID 1057, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/recommendation.py", line 107, in <lambda>
    user_product = user_product.map(lambda (u, p): (int(u), int(p)))
ValueError: invalid literal for int() with base 10: 'Toy Story (1995)'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

 
Any insights will be awesome :)
 Hello all!
 This is a question about section 2b of lab 4.

Say I have two RDD's.

RDD1 is in the format of ( (userID,movieID), rating).
RDD2 is in the format of ( (userID,movieID), predictedRating).

I want to create an RDD of items where I get the square of (rating - predictedRating) for each key [ (userID,movieID)].

I use cogroup, so I have
rdd1.cogroup(rdd2)
Thus, I have something in the format of
[(key1, ([rating1], [rating2])), (key2 ([rating1], [rating2]))]
However, the values for each tuple are in the format of ( [rating1],[rating2] ), which are pyspark resultiterable objects.

Printing out the results after the cogroup gives me this:

[((1, 1), (<pyspark.resultiterable.ResultIterable object at 0xb0d71f4c>, <pyspark.resultiterable.ResultIterable object at 0xb0d710ec>)), ((1, 2), (<pyspark.resultiterable.ResultIterable object at 0xb0d71e8c>, <pyspark.resultiterable.ResultIterable object at 0xb0d71fcc>)), ((2, 3), (<pyspark.resultiterable.ResultIterable object at 0xb0d71dcc>, <pyspark.resultiterable.ResultIterable object at 0xb0d7134c>)), ((1, 3), (<pyspark.resultiterable.ResultIterable object at 0xb0d717cc>, <pyspark.resultiterable.ResultIterable object at 0xb0d7180c>)), ((2, 2), (<pyspark.resultiterable.ResultIterable object at 0xb0d712ac>, <pyspark.resultiterable.ResultIterable object at 0xb0d7126c>)), ((2, 1), (<pyspark.resultiterable.ResultIterable object at 0xb0d71e4c>, <pyspark.resultiterable.ResultIterable object at 0xb0d7182c>))]
[((1, 2), (<pyspark.resultiterable.ResultIterable object at 0xb1ff8c0c>, <pyspark.resultiterable.ResultIterable object at 0xb1ff8bec>)), ((1, 3), (<pyspark.resultiterable.ResultIterable object at 0xb1ff86ec>, <pyspark.resultiterable.ResultIterable object at 0xb1ff8b0c>)), ((2, 2), (<pyspark.resultiterable.ResultIterable object at 0xb1ff852c>, <pyspark.resultiterable.ResultIterable object at 0xb1ff8b4c>)), ((2, 1), (<pyspark.resultiterable.ResultIterable object at 0xb1ff864c>, <pyspark.resultiterable.ResultIterable object at 0xb0ded22c>))]
[((1, 2), (<pyspark.resultiterable.ResultIterable object at 0xb0d2430c>, <pyspark.resultiterable.ResultIterable object at 0xb0d241cc>)), ((1, 3), (<pyspark.resultiterable.ResultIterable object at 0xb0d2424c>, <pyspark.resultiterable.ResultIterable object at 0xb0d242cc>)), ((2, 2), (<pyspark.resultiterable.ResultIterable object at 0xb0d2426c>, <pyspark.resultiterable.ResultIterable object at 0xb0d242ec>)), ((2, 1), (<pyspark.resultiterable.ResultIterable object at 0xb0d242ac>, <pyspark.resultiterable.ResultIterable object at 0xb0d2434c>))]

I tried to use indexes like this (in a map transformation after the cogroup, but pyspark resultiterable objects do not support them.

Here's what I was doing:
.map(lambda (key,value) : (pow(value[0][0] - value[1][0],2))))

The corresponding error was:
TypeError: 'ResultIterable' object does not support indexing

So my question is:

how can I access the values of the two ratings after cogrouping the two rdd's without using indexing?

Thanks so much in advance!!! Dear sir,
this was such a fast paced course!... too many things to grasp. I am using python since 3 years , for me it was quite challenging. I am currently an employee so unable to give enough time to the course but it will be good if you take things little slowly . The labs were pretty interesting but cannot devote time. So I request please extend the time. i enrolled for verified certificate, if i can't complete the course it will be pretty bad. Decrease the minimum score or please increase the time .
The lab 3 alone need another week for me because I need to adjust my worklife and studies. Today is sunday and I am studying without taking rest. Please do either one of the above I get the cosinesimilarty for the following tokens as .05772.
How do I interpret this number?  DOes this mean the two string are similar or not similar.
How do I use this number.?

Thanks





['adobe', 'photoshop']
['adobe', 'illustrator']
{'photoshop': 22.22222222222222, 'adobe': 8.333333333333334}
{'adobe': 8.333333333333334, 'illustrator': 50.0}
0.0577243382163



 These four labs really took me a lot of time,coz I work as software engineer in a company now,and I had to work overtime almost every work day.I could only do these lab during lunchtime and at night.

I am not familiar with python as well,so sometimes it was not the process of coming up with a idea bothered me but the syntax of python did it.
However I  still got 100% and acquired much knowledge about Spark and big data processing  through hard work and with the help of this forum,thank u all.

I am interested in Big Data and Cloud Computing(I also took the course from IEEE,which is elementary level I guess),and I decided to study abroad next year,So I am preparing for the application,I hope this course would help and worth the $100 I paid.

By the way,I graduated from NanJing University of China ,I majored in Information management and Information system,hope to see you guys in US or other country around the word. I'm trying to perfom a join on 2 rdds :

rdd1 = sc.parallelize([((1, 1), 5), ((1, 2), 3), ((1, 3), 4), ((2, 1), 3), ((2, 2), 2), ((2, 3), 4)])
rdd2 = sc.parallelize([((1, 2), 3), ((1, 3), 5), ((2, 1), 5), ((2, 2), 1)])
rdd1.join(rdd2)
I'm getting the following error which I can't actually correct.

line 1806, in <lambda>
    map_values_fn = lambda (k, v): (k, f(v))
ValueError: too many values to unpack
Any suggestions for that?
 enrolled myself for scalable ml, wondering if 8 can do both in parallel to avail zeroes cert and skills Hi, 
I have been stuck with first part of the 4b while I am able to pass the 2nd and 3rd test for the exercise.
I've reviewed several posts but haven't seen anyone with this issue, any help is greatly appreciated.
I believe, I am doing something very straightforward but not sure where the extra 7 unique tokens may be coming from.
As I mentioned that all earlier and next set tests have passed successfully
# TODO: Replace <FILL IN> with appropriate code
#Only posting this code as I have seen many folks posts codes similar to below postedfullCorpusRDD = amazonFullRecToToken.union(googleFullRecToToken)idfsFull = idfs(fullCorpusRDD)idfsFullCount = idfsFull.count()print 'There are %s unique tokens in the full datasets.' % idfsFullCount

#tokenize and idfs passed their respective tests as well


There are 17084 unique tokens in the full datasets.
There are 1363 Amazon weights and 3226 Google weights. I have successfully completed the course with 100% marks. :)
When should I expect my verified certificate ?
#pin I've been stuck with this problem for hours and I haven't found any help on the forum. Could someone tell me what is wrong with this line
<REDACTED>

or give a hint what amazonWeightsRDD should look like. After trying for hours, I have no idea what structure the mapping between IDs/URLs and TF-IDF weighted token vectors should have.

Thanks. Hello,

Im struggeling with the right count for this question. 

Im getting a count of 22545 instead of 22520 and now I dont know which tokens i should parse or not.
For example, should the phrase 3-5 lead to '3','5' or '3-5' ?Or the phrase 'win 98 me 2000 xp' leads to 'win','98','2000','xp'. 'me' is missing because its' is a stop word. 

At the moment I filtering out "!?." at the end of a line and "!?," within the line.
Furthermore the phrase "...".

Could someone give me some advice?

Thank in advance!


 def computeSimilarity(record): googleRec = record[0] googleURL = googleRec[0] googleValue = googleRec[1]
similarities = (crossSmall.map(lambda p: computeSimilarity(p)).cache())  

This is basically running for last 3 hours and no response and no error either. Any ideas. All my previous tests are OK, including dot product (both sides check)
Thanks. Hi all,

at one step for this question, I have to concatenate all unique tokens
for each document id:

below what I have when I extract my 2 first token list doing:
print uniqueTokens.take(2) 

so these data are into an RDD
Do you know a transformation to obtain only one list containing all of the words ?

[['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund'], ['ca', 'international', 'arcserve', 'lap', 'desktop', 'oem', '30pk', 'backup', 'v11', '1', 'win', '30u', 'laptops', 'desktops', 'computer', 'associates']] I've accidentally deleted my code for lab 1 and lab 2, is there a way to retrieve them again? I've successfully submitted them to the autograder. I have tried all sorts of different things but can't seem to get past the creation of movieIDsWithAvgRatingsRDD.  I think it may be because I am not writting the lambda function properly.  Currently I have
   (lambda (k, v): (k[0], ([getCountsAndAverages(x) for x in v[0]]))

and although this appears to run OK the RDD I get after running it seems to be unuseable since any instruction I run against it fails except for type which gives
  
<class 'pyspark.rdd.PipelinedRDD'>My output from the first step i.e. movieIDsWithRatingsRDD has the form movieIDsWithRatingsRDD: [(2, <pyspark.resultiterable.ResultIterable object at 0xb0e8dbcc>), (4, <pyspark.resultiterable.ResultIterable object at 0xb0efbf6c>), (6, <pyspark.resultiterable.ResultIterable object at 0xb0efb32c>)]Just like it is supposed to haveThe problem is how do you use the resultiterable in a lambda expression - in other words how do you call getCountsAndAverages using the resultiterable for each of the results from movieIDsWithRatingsRDDAny help would be appreciated. sims = similaritiesBroadcast.map(lambda x: (x[1]+" "+x[0],x[2]))


trueDupsRDD = (sims
               .join(goldStandard))
trueDupsCount = trueDupsRDD.count()
avgSimDups = trueDupsRDD.map(lambda (ID,(cosScore,goldStr)):(ID,cosScore)).reduceByKey(lambda x,y : x+y).map(lambda (ID,score):score).sum()/float(trueDupsCount)


nonDupsRDD = (sims
               .map(lambda (amazonID,googleID,cosineScore):('%s %s' % (removeQuotes(googleID),removeQuotes(amazonID)),cosineScore))
               .subtractByKey(goldStandard))
avgSimNon = nonDupsRDD.map(lambda x:x[1]).sum(lambda x,y:x+y)/float(nonDupsCount)
im getting this error below for the above code. 

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-101-c597cc89ee4f> in <module>()
     12                .map(lambda (amazonID,googleID,cosineScore):('%s %s' % (removeQuotes(googleID),removeQuotes(amazonID)),cosineScore))
     13                .subtractByKey(goldStandard))
---> 14 avgSimNon = nonDupsRDD.map(lambda x:x[1]).sum(lambda x,y:x+y)/float(nonDupsCount)
     15 
     16 print 'There are %s true duplicates.' % trueDupsCount

TypeError: sum() takes exactly 1 argument (2 given)

Please someone help me out. im new to python and the deadline is nearby! Why is my code printing the entire list ? Can someone please explain to me what exactly does tokenCountPairTuple and tokenSumPairTuple should be ? I am very much confused. I've spend many hours on this and now I've almost reached my saturation point. can someone please help  ?I think i finally have solved this now. I get the output for running the first part as : 
There are 400 unique tokens in the small datasets.

But when I run the next piece of  code which is :
# TEST Implement an IDFs function (2c)Test.assertEquals(uniqueTokenCount, 4772, 'incorrect uniqueTokenCount')tokenSmallestIdf = idfsSmall.takeOrdered(1, lambda s: s[1])[0]Test.assertEquals(tokenSmallestIdf[0], 'software', 'incorrect smallest IDF token')Test.assertTrue(abs(tokenSmallestIdf[1] - 4.25531914894) < 0.0000000001,                'incorrect smallest IDF value')I do not see any output. Can anyone help. Please ? I am getting this: inside the invert function. When I execute the statement in python it works fine.
I am doing this [(t, record[0][0]) for t in record[1][1].keys()] in the invert function.

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-108-fdd983d2c901> in <module>()
     17 
     18 #print amazonInvPairsRDD.take(2)
---> 19 print '%s' % amazonInvPairsRDD.count()
     20 #print amazonWeightsRDD.take(2)
     21 #googleInvPairsRDD = (googleWeightsRDD

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 128.0 failed 1 times, most recent failure: Lost task 0.0 in stage 128.0 (TID 469, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-108-fdd983d2c901>", line 11, in invert
KeyError: 1

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> map(lambda (x,y,z): (x,y,z) if z>500)

Thanks. Hi,

I have been stuck with first part of the 4b. 

I applied:
fullCorpusRDD => join between amazonFullRecToToken and googleFullRecToToken
idfsFull = idfs(fullCorpusRDD)

But I have the output:
There are 71 unique tokens in the full datasets.
There are 1363 Amazon weights and 3226 Google weights.

And the test output:
1 test failed. incorrect idfsFullCount
1 test passed.
1 test passed.

I have in idtf function the test passed.

Where is my issue? I hope that anyone can help.

Thx!


 Hi All,
I already searched in Piazza, but could not find much help on this.

 my invert function works fine. I tested by following:

myp = invert(amazonWeightsRDD.first() )
print myp

Above gives me:
[['rom', 'b000jz4hqo'], ['clickart', 'b000jz4hqo'], ['950', 'b000jz4hqo'], ['image', 'b000jz4hqo'], ['premier', 'b000jz4hqo'], ['000', 'b000jz4hqo'], ['dvd', 'b000jz4hqo'], ['broderbund', 'b000jz4hqo'], ['pack', 'b000jz4hqo']]

But when I use this function in the following, I get only 1363 records in RDD, but expected count is 111387

amazonInvPairsRDD = amazonWeightsRDD.map(lambda s: invert(s))

Please help with any pointers.

Thanks in advance

 Hi,
I don't understand the 1d exercise :(

When I print the
vendorRDD

I have this
['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund', 'ca', 'international', 'arcserve', 'lap', 'desktop', 'oem', '30pk', 'oem', 'arcserve', 'backup', 'v11', '1', 'win', '30u', 'laptops', 'desktops', 'computer', 'associates'
....etc...	

First of all, is it correct?

After this I'll apply a map with lambda function to add 1 to every element and reduceByKey and takeOrdered for example the first 10 elements using

lambda s: -1 * s[1]

But the return is

[('adobe', 239), ('software', 128), ('cs3', 125), ('design', 86), ('new', 85), ('pro', 84), ('tools', 78), ('web', 76), ('professional', 70), ('3', 70)]

So... I suppose there is an error starting to take the input data. But I don't understand where and why!

Any suggestions is appreciate,
Best Regard.

ps. the 1a), 1b) and 1c) are ok


 Hi all,

I'm stucked in 4e exercice. I think the implementation of this part is fine but I'm suspecting about any other part in the Lab.

My overall implementation is like this

commonTokens = (RDD1
                              .<join op>(RDD2)
                              .<swap op>
                              .<reduce op> 
                              .cache())

Trying with an small data set I get:
[((ID1, URL1), [tokens]), ((ID2, URL2), [tokens])]

But when ecuting the full exercise I get the following error what I don't know how to interpret, any idea?

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-583-704bf279c4f0> in <module>()
     13 commonTokens = (googleInvPairsRDD.join(amazonInvPairsRDD).map(lambda r: swap(r)).reduceByKey(lambda a, b: a + b).cache())
     14 
---> 15 print 'Found %d common tokens' % commonTokens.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 811.0 failed 1 times, most recent failure: Lost task 0.0 in stage 811.0 (TID 4222, localhost): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:172)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:108)
	... 14 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

<p></p> Thank you to the instructor and all the TA's.. 

Has been a very handson learning both fo rpython and Spark.. Looking forward to use this more i my day-day projects.

Thanks again for the time and energy to make this course !! I am using the following code and gives the error:

sc.parallelize(vendorRDD).sortBy(lambda x: x[1]).collect()


Error




---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-71-753bf28003e2> in <module>()
      1 # TEST Amazon record with the most tokens (1d)
----> 2 Test.assertEquals(biggestRecordAmazon[0][0], 'b000o24l3q', 'incorrect biggestRecordAmazon')
      3 Test.assertEquals(len(biggestRecordAmazon[0][1]), 1547, 'incorrect len for biggestRecordAmazon')

TypeError: 'NoneType' object has no attribute '__getitem__'



 Hi guys, please post some hints. Thanks. Hello, I have accidentally submitted my lab2.py to the grader of lab 1. When I clicked Check, the grader told me that I submitted the wrong file, but it reset my progress score for that lab to 0%(my score before that was 100%).
My question is if there is a way to get the score back or should I resubmit my lab1.py and take the 20% penalty? This was a great introductory course to Apache Spark. Personally, I don't intend to get more into the user focussed side of Spark i.e. I'm not super interested in ML. But I do want to understand the systems side of Apache Spark to be able to better answer the question: "Why does this particular query take so long?".

What are resources I could use? Are there any other edX courses that continue from CS100.1x and dig deeper into the systems side?

Thanks!
 Here is the part I am doing
I am using 
sims=sims=similaritiesBroadcast.map(lambda (a,b,c):(b,a,c))

But I am struggling in combing those two keys in to one or merging (AmazonID,googleUrl,value)=(AmazonID GoogleUrl,value)

Any help in this would be grealty appreciated as I am new to python .

print goldStandard.take(1)
print sims.take(1)
[(u'b000jz4hqo http://www.google.com/base/feeds/snippets/18441480711193821750', 'gold')][('b000jz4hqo', 'http://www.google.com/base/feeds/snippets/11448761432933644608', 0.0)] While calculating trainingAvgRating when I directly try to use the reduce() action as trainingAvgRating = trainingRDD.reduce(lambda x,y: x[2]+y[2])/trainingRDD.count(), I get the following error. But trainingAvgRating = trainingRDD.map(lambda x: x[2]).reduce(lambda x,y: x+y)/trainingRDD.count() seems to work fine. What am I doing wrong?

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-81-ab86bcda69e0> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
----> 2 trainingAvgRating = trainingRDD.reduce(lambda x,y: x[2]+y[2])/trainingRDD.count()
      3 print 'The average rating for movies in the training set is %s' % trainingAvgRating
      4 
      5 testForAvgRDD = testRDD.map(lambda entry: (entry[0],entry[1],trainingAvgRating))

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1445.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1445.0 (TID 604, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 737, in func
    yield reduce(f, iterator, initial)
  File "<ipython-input-81-ab86bcda69e0>", line 2, in <lambda>
TypeError: 'float' object has no attribute '__getitem__'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

 I can't figure out how to calculate unique tokens. Using list(set(..)) doesn't work for me.

Example:
[(quick, fox, quick), (quick, wolf)] => [(quick, fox), (quick, wolf)]

uniqueTokens = corpus.map(lambda (id, tokens): tokens.???)

Any hints?
Thanks Whats the passing marks for the course ? 45 % ? In the collaborative filtering : 

For movie recommendations, we start with a matrix whose entries are movie ratings by users (shown in red in the diagram below). Each column represents a user (shown in green) and each row represents a particular movie (shown in blue).


I think the rows are the users and the columns are the ratings.

Thanks I am not able to convert the input-data to right format. I need glue to progess ...

Regards  A neat feature of Python is that you can use tuples at both sides of the assignment operator ('=').
(a,b)=(1,2)
to swap a and b:
(b,a)=(a,b)
This also works with nested tuples:
((a,b),c)=((1,2),3)print a #1print b #2print c #3

You can use this to assign an RDD tuple to your function parameters in 1 go, like:
((googleURL,amazonID),cs)=record
This is easier to read and shorter than what I see a lot in the questions:
googleURL=record[0][0]amazonID=record[0][1]cs=record[1]

more info: http://www.greenteapress.com/thinkpython/thinkCSpy/html/chap09.html


 Hi,

when I use this line in my cosine function, I keep having the same error : 
TypeError: unhashable type: 'list'

value = amazonNormsBroadcast.value[amazonRec]

(I simplified the line)

Thanks in advance for your help the value i got is 
(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), 7.24992115129287e-06)

the test is abs(similarityTest[0][1] - 4.286548414e-06) < 0.000000000001.

I don't know what went wrong. The code i have for fastCosineSimilarity is :- 

    amazonRec = record[0][0]
    googleRec = record[0][1]
    tokens = record[1]
    
    a = { k: v for k, v in amazonWeightsBroadcast.value.get(amazonRec).iteritems() if k in tokens}
    b = {k: v for k, v in googleWeightsBroadcast.value.get(googleRec).iteritems() if k in tokens}
   
    s = dotprod(a,b)  
    value = s/(googleNormsBroadcast.value.get(googleRec)*amazonNormsBroadcast.value.get(amazonRec))   
    key = (amazonRec, googleRec)
    return (key, value)
their common token is:
[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), ['data', 'complete', 'includes', 'software'])] Problem:Given two datasets, with records formatted as  (key,text), find the records in each dataset that refer to the same entity: the two keys refer to the same entity, so the information should be merged.The problem in important when merging two datasets into a single one.
High-level solutionDefine a similarity metric between two stringsIn: ([string1, string2])Out: scalar
Compute the similarity of all combinationsAnalyze the classification results (Accuracy, false-pos/neg, F-Score)Using a reference datasetFor multiple similarity threshold valuesDecide a similarity value thresholdMerge information for records more similar than this thresholdKeep the rest as distinct entities


 
For the similarity metric:Idea: Parse the text of each record into a set of tokens, and calculate a weight for each token, in each record that describes its importance. Then calculate the similarity between two records as a function of the weight vectors of their tokens.Processing steps (pythonic notation):For each dataset
Initial: [(key, text)]-> Tokenized: [(key, [token])]-> (TF-IDF) weighted: [key,{token: weight}]
For both datasets (1st, inefficient solution)
Two tokenized, weighted datasets: [key1,{token: weight}], [key2,{token: weight}]All record combinations: [ (key1,{token: weight}), (key2,{token: weight}) ]Similarities: [(key1,key2), similarity)]

Parse the text into a list of tokens.In: (key, text)Out: (key, [tokens])Tokens can be words, word roots, etcStopwords (common words), like [a, the, for,…] should be removed
Calculate weights using tf-idfTF(n)= term frequency= frequency of token n in a given (single) documentIDF(n)= inverse document frequency= 1 / <frequency of documents containing n>TF-IDF=frequency of token n in a given (single) document$$TF\_IDF(token,doc,corpus)=\frac{frequency\ of\ token\ in\ doc }{ frequency\ of\ documents\ in\ corpus\ containing\ token}$$
Calculate vector similarity using cosine similarity$$similarity = \cos \theta = \frac{a \cdot b}{\|a\| \|b\|} = \frac{\sum a_i b_i}{\sqrt{\sum a_i^2} \sqrt{\sum b_i^2}}$$The norms are the $$\|a\|=\sqrt{\sum a_i^2}$$ and can be precomputed

Speeding up the computationsComputing naively the similarities between records is quadratic.Most records will share only a few tokens, or none. We can find the records with common tokens, and their common tokens, to speed up things.Process:Compute and broadcast the dictionary {token: IT-IDF} for each record for both datasetsCompute and broadcast the norm for each record, for both datasetsFind the common tokens for each record couple:Build two RDDs for both datasets: [(token, DS1_recID)], [(token, DS2_recID)]Join by token: {token: (DS1_recID, DS2_recID)}Here is the main efficiency difference. We join only by common token, ending with ~1/2 of the record couples (after the aggregation below) than the Cartesian product of the datasets.If the shared tokens where fewer, the performance gains would be greater.Swap keys and aggregate to token list: {(DS1_recID, DS2_recID): [tokens]}Compute the similarity using the broadcasted dictionaries: {(DS1_recID, DS2_recID): similarity}

  I modified the test to this in 1d:

Test.assertEquals(biggestRecordAmazon[0], 'b000o24l3q', 'incorrect biggestRecordAmazon')
Test.assertEquals(len(biggestRecordAmazon[1]), 1547, 'incorrect len for biggestRecordAmazon')

I just don't understand why I would want or need two indexes in this case. Will this cause problems? can anyone give me a reasonable need for it? Am I misunderstanding what is being asked...?

My solution is correct provided the test has a single index on the biggest record object... Error message

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-96-7d83f4f9e22c> in <module>()
     36             .collect()[0][2])
     37 
---> 38 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     39 print 'Requested similarity is %s.' % similarityAmazonGoogle

<ipython-input-96-7d83f4f9e22c> in similar(amazonID, googleURL)
     33     """
     34     return (similarities
---> 35             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     36             .collect()[0][2])
     37 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 280.0 failed 1 times, most recent failure: Lost task 1.0 in stage 280.0 (TID 449, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-96-7d83f4f9e22c>", line 19, in computeSimilarity
  File "<ipython-input-47-d9c3d83350ed>", line 11, in cosineSimilarity
  File "<ipython-input-25-ccaf8dfc6953>", line 14, in tfidf
KeyError: 'google'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
</span></span></span></span></span></span></span></span></span></span></span>
<p><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansiyellow"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"> </span></span></span></span></span></span></span></span></span></span></span></p> When I try to download as Python (.py) file from VM it is downloading as .txt file, I renamed .txt as .py file and submitted in auto grader, but it is erroring out even the test I passed in the VM, need help Hi All,Even though I passed all test when I submitted my lab3 solutions I got the following problem:Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
PicklingError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION Can't pickle builtin My regular expression is working with the unit tests and with other strings I tried.
Submission details are:Your submission token ID is 1234713-c87c09ecfddc7fc9cdf33ca075cd466d:ip-172-31-20-67TIA, I wonder if using namedtuple's instead of simple tuples would be worth the effort.  It shouldn't impact performance noticeably.  It should be easier for students to understand.  Referencing fields by name instead of position should help make the steps more robust as things change.  Anyway, just food for thought. Hi, I have submitted my file .py for Lab 3 which passes all tests but the autograder throws all times the same exception.

I have reinstalled the environment but when I upload the output file .py to the autograder, the next exception is generated. Can I submit my solutions to any email address? What can i do? Thanks!!

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 115
SyntaxError: Non-ASCII character '\xc2' in file /ok/submission.py on line 116, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'simpleTokenize' is not defined

Removing stopwords (1b)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tokenize' is not defined

Tokenizing the small datasets (1c)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'totalTokens' is not defined

Amazon record with the most tokens (1d)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'biggestRecordAmazon' is not defined

Implement a TF function (2a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tf' is not defined

Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'corpusRDD' is not defined

Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueTokenCount' is not defined

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'rec_b000hkgj8k_weights' is not defined

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dp' is not defined

Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsSmallBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonFullRecToToken' is not defined

Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsFullCount' is not defined

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonNormsBroadcast' is not defined

Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'invert' is not defined

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'commonTokens' is not defined

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similaritiesFullRDD' is not defined

-- 0 cases passed (0.0%) --


Your submission token ID is 1239422-94215053a39b0f75929ba9d46c41353d:ip-172-31-24-4
Please include this submission token ID when you need support for your code submission.
Your anonymous student ID is 1ccfa956fac21c5b1e09214f3b2c7e71. Do not post this ID on Piazza.
 ---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-77-1095dbd1bf65> in <module>()
      1 # TEST Compute Norms for the weights from the full datasets (4c)
      2 Test.assertTrue(isinstance(amazonNormsBroadcast, Broadcast), 'incorrect amazonNormsBroadcast')
----> 3 Test.assertEquals(len(amazonNormsBroadcast.value), 1363, 'incorrect amazonNormsBroadcast.value')
      4 Test.assertTrue(isinstance(googleNormsBroadcast, Broadcast), 'incorrect googleNormsBroadcast')
      5 Test.assertEquals(len(googleNormsBroadcast.value), 3226, 'incorrect googleNormsBroadcast.value')

AttributeError: 'dict' object has no attribute 'value' When I do this: to compute the sum:


amazonRec = record[0][0] googleRec = record[0][1] tokens = record[1] s = dotprod(amazonWeightsBroadcast.value[amazonRec], googleWeightsBroadcast.value[googleRec])

where  record = commonTokens.take(2)[0]

I get the following error. Please help:

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-49-221aaad49c2d> in <module>()
     24 r1 = commonTokens.take(2)
     25 print r1[0]
---> 26 r2 = fastCosineSimilarity(r1[0])
     27 print r2
     28 

<ipython-input-49-221aaad49c2d> in fastCosineSimilarity(record)
     13     googleRec = record[0][1]
     14     tokens = record[1]
---> 15     s = dotprod(amazonWeightsBroadcast.value[amazonRec], googleWeightsBroadcast.value[googleRec]) #sum([amazonWeightsBroadcast.value[amazonRec][t]
     16          #        * googleWeightsBroadcast.value[googleRec][t]] for t in tokens)
     17     value = 3 #s/amazonNormsBroadcast.value.get(googleRec)/googleNormsBroadcast.value.get(amazonRec)

TypeError: list indices must be integers, not str Hi all.

So for the last part of lab4, I have the following for rated movies:
# TODO: Replace <FILL IN> with appropriate code
myUserID = 0

# Note that the movie IDs are the *last* number on each line. A common error was to use the number of ratings as the movie ID.
myRatedMovies = [
     (myUserID,u'Star Wars: Episode V - The Empire Strikes Back (1980)',5),
     (myUserID,u'Toy Story (1995)',4),
     (myUserID,u'Young Frankenstein (1974)',5),
     (myUserID,u'Star Wars: Episode IV - A New Hope (1977)',5),
     (myUserID,u'Raiders of the Lost Ark (1981)',5),
     (myUserID,u'Amadeus (1984)',5),
     (myUserID,u"One Flew Over the Cuckoo's Nest (1975)",5),
     (myUserID,u'Casablanca (1942)',5),
     (myUserID,u'Wizard of Oz, The (1939)',3),
     (myUserID,u'Indiana Jones and the Last Crusade (1989)',5)
     # The format of each line is (myUserID, movie ID, your rating)
     # For example, to give the movie "Star Wars: Episode IV - A New Hope (1977)" a five rating, you would add the following line:
     #   (myUserID, 260, 5),
    ]
myRatingsRDD = sc.parallelize(myRatedMovies)
print 'My movie ratings: %s' % myRatingsRDD.take(10)
but whenever I am trying to train my model with my ratings added with the following code:
myRatingsModel = ALS.train(trainingWithMyRatingsRDD, bestRank, seed=seed,iterations=iterations,lambda_=regularizationParameter)
I get the following error:
--------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-82-1fc5c55268ab> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
----> 2 myRatingsModel = ALS.train(trainingWithMyRatingsRDD, bestRank, seed=seed,iterations=iterations,lambda_=regularizationParameter)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/recommendation.py in train(cls, ratings, rank, iterations, lambda_, blocks, nonnegative, seed)
    138               seed=None):
    139         model = callMLlibFunc("trainALSModel", cls._prepare(ratings), rank, iterations,
--> 140                               lambda_, blocks, nonnegative, seed)
    141         return MatrixFactorizationModel(model)
    142 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/common.py in callMLlibFunc(name, *args)
    118     sc = SparkContext._active_spark_context
    119     api = getattr(sc._jvm.PythonMLLibAPI(), name)
--> 120     return callJavaFunc(sc, api, *args)
    121 
    122 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/common.py in callJavaFunc(sc, func, *args)
    111     """ Call Java Function """
    112     args = [_py2java(sc, a) for a in args]
--> 113     return _java2py(sc, func(*args))
    114 
    115 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o4229.trainALSModel.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1336.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1336.0 (TID 959, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 240, in dump_stream
    bytes = self.serializer.dumps(vs)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 402, in dumps
    return cPickle.dumps(obj, 2)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/recommendation.py", line 40, in __reduce__
    return Rating, (int(self.user), int(self.product), float(self.rating))
ValueError: invalid literal for int() with base 10: 'Star Wars: Episode V - The Empire Strikes Back (1980)'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


I believe that the computer is taking issue with the formatting of the RDD, but I fail to see why.

Please help, and thanks!! 
I cannot understand where the code splits csv into lines to prepare it for processing? I gather it should be done in loadData(path) method.
Could anyone clarify please?



And another short question on the data preparation.
Below I cannot fully comprehend how ''.join(filtering iteration) works. Could you help me confirm or negate my guess?

 Lab3 3c: the notebook run already over one hour, always show it is still running. The system is too busy at the moment? I'm getting this error in 4f

TypeError: 'Broadcast' object has no attribute '__getitem__'
 The text says :

Use the Python list myRatedMovies to transform the moviesRDD into an RDD with entries that are pairs of the form (myUserID, Movie ID) and that does not contain any movies that you have rated. This transformation will yield an RDD of the form: [(0, 1), (0, 2), (0, 3), (0, 4)]. Note that you can do this step with one RDD transformation.
Can it really be done in one RDD transformation? I used a filter and a map.
 Right now for commonTokens I'm doing RDD1 join RDD2 -> map with swap -> reduce -> cacheAnd it is just stuck in the evaluation mode. Anyone have an idea of what's going on? I was running 4e and kernel aborted and restarted. Then I restarted the VM and tried to do Kernal -> Restart and Cell -> Run all.
But now i am getting below mentioned error right in Part 0 itself. Please help.


---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-34-0ed3d95ab431> in <module>()
      1 import sys
      2 import os
----> 3 from test_helper import Testamazon
      4 
      5 baseDir = os.path.join('data')

ImportError: cannot import name Testamazon

 
 print predictedTestMyRatingsRDD.take(1)
gives this:

[Rating(user=1377, product=384, rating=1.9881307629486313)]
What does this output format mean, especially with the 'Rating' prefix to the record? Hello all,

For this exercise I'm doing the below:

def tf(tokens):    myDict = {}        myKeys = array('f', [])        for i in range(len(tokens)):        myKeys.append(1/len(tokens))          myDict = dict(zip(tokens, myKeys))    print myDict
but when I print the result is:

{'brown': 0.0, 'lazy': 0.0, 'jumps': 0.0, 'fox': 0.0, 'dog': 0.0, 'quick': 0.0}
How do I do to get the correct floating type on myKeys var?

Thanks and regards, The second test in lab4 1c indicates a failure, but when I compare the literals given in the test statement to the list given by movieLimitedAndSortedByRatingRDD.take(20) they are identical:

l1 = movieLimitedAndSortedByRatingRDD.take(20)
l2 = [(4.5349264705882355, u'Shawshank Redemption, The (1994)', 1088),
               (4.515798462852263, u"Schindler's List (1993)", 1171),
               (4.512893982808023, u'Godfather, The (1972)', 1047),
               (4.510460251046025, u'Raiders of the Lost Ark (1981)', 1195),
               (4.505415162454874, u'Usual Suspects, The (1995)', 831),
               (4.457256461232604, u'Rear Window (1954)', 503),
               (4.45468509984639, u'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)', 651),
               (4.43953006219765, u'Star Wars: Episode IV - A New Hope (1977)', 1447),
               (4.4, u'Sixth Sense, The (1999)', 1110), (4.394285714285714, u'North by Northwest (1959)', 700),
               (4.379506641366224, u'Citizen Kane (1941)', 527), (4.375, u'Casablanca (1942)', 776),
               (4.363975155279503, u'Godfather: Part II, The (1974)', 805),
               (4.358816276202219, u"One Flew Over the Cuckoo's Nest (1975)", 811),
               (4.358173076923077, u'Silence of the Lambs, The (1991)', 1248),
               (4.335826477187734, u'Saving Private Ryan (1998)', 1337),
               (4.326241134751773, u'Chinatown (1974)', 564),
               (4.325383304940375, u'Life Is Beautiful (La Vita \ufffd bella) (1997)', 587),
               (4.324110671936759, u'Monty Python and the Holy Grail (1974)', 759),
               (4.3096, u'Matrix, The (1999)', 1250)]
print set(l1) - set(l2)

The output is

set([])

Was this something that got corrected in version 1.0.1? Update:
I figured out the issue. I need to modify the way I was deriving myUnratedMoviesRDD.

---
Original question:
Anybody encountering this error?

predictedRatingsRDD = myRatingsModel.predictAll(myUnratedMoviesRDD)
print predictedRatingsRDD.take(1)
ValueError: invalid literal for int() with base 10: 'Toy Story (1995)'
 How do I see quizzes? I couldn't find them. Sorry for the last-minute cry for help, but I've exhausted my ideas and the board.
I would love some hints or other help.

My commonTokens seems identical the unit test info and the format one would expect.
The four broadcast variables were created using collectAsMap().
In my numerous variations of the fastCosineSimilarity function, values of broadcast variables are "called" with, for example, amazonWeightsBroadcast.value[t] for t in tokens.  I have tried comprehensions and loops to generate s and value.

One error trace follows:

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-196-4d1e912ff6ef> in <module>()
     21 #                       .cache())
     22 
---> 23 print similaritiesFullRDD.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 514.0 failed 1 times, most recent failure: Lost task 0.0 in stage 514.0 (TID 2719, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <genexpr>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "<ipython-input-196-4d1e912ff6ef>", line 19, in <lambda>
  File "<ipython-input-196-4d1e912ff6ef>", line 14, in fastCosineSimilarity
KeyError: '120'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) Is there a RDD method to do this crossSmall.  THis can be done i python easy. Looking for a way to do it using RDD functions and lambda.
THe procedure would be:

for each tuple in googleSmall :
      append all the tuples from amazonSmall
so (a,b) + (c,d) should give ((a,b),(c,d))

do I need a function to do this or can this be done using lambda and RDD function
Thanks for help
 I was able to pass Lab 4 (3e)  submit and get 100 points.
Later I discovered it was incorrect since I had not filtered for movies with more than 75 ratings.
There might be additional errors since the max rating is > 5

I therefore would like to suggest more tests for the exercise and/or use a given movie dataset instead of myRatedMovies. It would be nice to know if the assignment is solved correctly.
 I am getting 
0.500277597875
which is not correct 
That comes from 3a  in which I defined cossim as the result of the division of dotprod (having a and b as arguments) and the product of norm of a and norm of b. This product is converted to a float.

Has anybody defined cossim in another way and is getting a right answer in 3b?

Thanks



 when I tried to re-run lab 3 today, I got the following error:

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-16-c37f8beea13d> in <module>()
     51     return valid
     52 
---> 53 googleSmall = loadData(GOOGLE_SMALL_PATH)
     54 google = loadData(GOOGLE_PATH)
     55 amazonSmall = loadData(AMAZON_SMALL_PATH)

<ipython-input-16-c37f8beea13d> in loadData(path)
     33     """
     34     filename = os.path.join(baseDir, inputPath, path)
---> 35     raw = parseData(filename).cache()
     36     failed = (raw
     37               .filter(lambda s: s[1] == -1)

<ipython-input-16-c37f8beea13d> in parseData(filename)
     20         RDD: a RDD of parsed lines
     21     """
---> 22     return (sc
     23             .textFile(filename, 4, 0)
     24             .map(parseDatafileLine)

NameError: global name 'sc' is not defined

any idea how it happened??? Thank you so much:)
 

 I tried to convert the value to float but still getting the same error in Lab4 1c. Hi all,
I'm experiencing difficulties with exercice 3e.
I got the right value for avgSimDups and i'm very close to avgSimNon but it's not exactly equivalent.
My result is  0.00123025616144 while the test expects 0.00123476304656
I dont really understand because i use the same computation for avgSimNon and avgSimDups.Moreover I find 143 elements in the first set and 39854 elements for the second.Did i miss something ?Thx for your help
Bertrand
 I have already restarted the kernel several times and i am still getting this error.
 simpletokenize runs fine
tokenize runs fine
but I cannot get 22520 counts. My latest run produced 22221.


Here are results of simpletokenize:
print simpleTokenize('cd_rom !!!!123A/456_B/789C.123A dvd-rom win 98 me 2000 xp me studio dvd ? "cd" (hallmark edition) v.11 - end?.')print simpleTokenize('sierrahome hse hallmark card studio special edition win 98 me 2000 xp "hallmark card studio special edition (win 98 me 2000 xp)" "sierrahome"')print simpleTokenize(quickbrownfox)
['cd_rom', '123a', '456_b', '789c', '123a', 'dvd-rom', 'win', '98', 'me', '2000', 'xp', 'me', 'studio', 'dvd', 'cd', 'hallmark', 'edition', 'v', '11', 'end']['sierrahome', 'hse', 'hallmark', 'card', 'studio', 'special', 'edition', 'win', '98', 'me', '2000', 'xp', 'hallmark', 'card', 'studio', 'special', 'edition', 'win', '98', 'me', '2000', 'xp', 'sierrahome']['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']===================tokenize:print tokenize('sierrahome hse hallmark card (studio special edition) win 98 me 2000 xp "hallmark card studio special edition (win 98 me 2000 xp)" "sierrahome"')            
print tokenize(quickbrownfox) # Should give ['quick', 'brown', ... ]
print tokenize("Being why they're I'll v1.1 15 me rooms 1 year dvd-rom cd_rom at why a the_?.")
print tokenize("Why a the?")['sierrahome', 'hse', 'hallmark', 'card', 'studio', 'special', 'edition', 'win', '98', '2000', 'xp', 'hallmark', 'card', 'studio', 'special', 'edition', 'win', '98', '2000', 'xp', 'sierrahome']
['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']
["they're", "i'll", 'v1', '1', '15', 'rooms', '1', 'year', 'dvd-rom', 'cd_rom', 'the_']
[] Apologizes for pasting code in here but i cannot seem to figure out what the issue is:

print amazonWeightsRDD.take(1)

returns: 

[('b000jz4hqo', {'rom': 2.4051362683438153, 'clickart': 56.65432098765432, '950': 254.94444444444443, 'image': 3.6948470209339774, 'premier': 9.27070707070707, '000': 6.218157181571815, 'dvd': 1.287598204264871, 'broderbund': 22.169082125603865, 'pack': 2.98180636777128})]

My invert function is as follows:

def invert(record):
    """ Invert (ID, tokens) to a list of (token, ID)
    Args:
        record: a pair, (ID, token vector)
    Returns:
        pairs: a list of pairs of token to ID
    """
    pairs = []
    URL = record[0][0]
    tokens = record[0][1]
    
    for token in tokens:
        pairs.append((token, URL))
    return (pairs)

My amazonInvPairsRDD looks like this:

amazonInvPairsRDD = (amazonWeightsRDD.flatMap(invert).cache())

Printing out the first element from amazonInvPairsRDD provides this:

[('0', 'b')]
I cannot seem to figure out why the amazonInvPairsRDD has the wrong pair since if i print the pairs list from the invert function for a single RDD element i get the correct structure:
print invert(amazonWeightsRDD.take(1))[('rom', 'b000jz4hqo'), ('clickart', 'b000jz4hqo'), ('950', 'b000jz4hqo'), ('image', 'b000jz4hqo'), ('premier', 'b000jz4hqo'), ('000', 'b000jz4hqo'), ('dvd', 'b000jz4hqo'), ('broderbund', 'b000jz4hqo'), ('pack', 'b000jz4hqo')]

So because amazonInvPairsRDD has the wrong list pairs my counts are off:

There are 1363 Amazon inverted pairs and 3226 Google inverted pairs.

What am I doing wrong here? Can someone please explain what it means when tasks are 'skipped' and how to diagnose and fix?

 I've had a few stabs at this now and it's getting a bit deadliney ....

Can someone quickly state what 'shape' each of the RDDs along the way should be ?

To start with, is uniqueTokens supposed to be an RDD of tokens, or lists of unique tokens
(i.e. would take(1) give me a string or a list)? 

I'm assuming it's the latter, but 
I don't know of an operation I can apply to an RDD of lists that will emit a 'bigger' RDD
- that is, I can't tell how to emit multiple new keys for a given key. I just cant move forward. Any help is highly appreciated:

What am i missing:

similarities = (crossSmall .map(lambda record: computeSimilarity(record)) .cache())

Error:
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-50-747d7cfec924> in <module>()
     42             .collect()[0][2])
     43 
---> 44 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     45 print 'Requested similarity is %s.' % similarityAmazonGoogle

<ipython-input-50-747d7cfec924> in similar(amazonID, googleURL)
     39     """
     40     return (similarities
---> 41             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     42             .collect()[0][2])
     43 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 93.0 failed 1 times, most recent failure: Lost task 0.0 in stage 93.0 (TID 337, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-50-747d7cfec924>", line 29, in <lambda>
  File "<ipython-input-50-747d7cfec924>", line 25, in computeSimilarity
  File "<ipython-input-46-d9c3d83350ed>", line 13, in cosineSimilarity
  File "<ipython-input-38-98ab8604e2b0>", line 42, in cossim
ZeroDivisionError: float division by zero

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span> Hi All,
I am trying to simulate fastCosineSimilarity function by just taking one record first.
All my tests prior to 4f passed. I alredy spent 50+ hours on this lab so far.

 I have following code snippet:
t1 = amazonWeightsRDD.collectAsMap()
Some initial values from above are:
{'b000ea9u2a': {'10': 0.7298027989821882, 'finance': 23.901041666666668, 'liquid': 47.802083333333336, 'personal': 3.9022108843537415, 'modeless': 95.60416666666667, 'ledger': 57.3625, 'mac': 0.7021113831089352, '2': 0.4740702479338843, 'csdc': 10.622685185185185, 'higher': 1.5758928571428572, 'software': 0.15453259698275862}, 'b0009mg80a': {'control': 0.780796279491833, 'right': 0.276979719942057, 'simple': 0.3281607551487414, 'text': 0.4609280835676978, 'pro': 0.4364933417882054, 'share': 0.4252223869532987, 'one': 0.13977217348927876, 'front': 3.551857585139319, 'computer': 0.16497699165947657, 'distant': 15.095394736842104, 'chat': 2.87531328320802, 'keyboard': 1.1611842105263157, 'home': 0.17603958876783796, 'message': 2.5158991228070176, 'mouse': 0.862593984962406, 'best': 0.24951065680730752, 'use': 0.10464745051536987, 

testrec = \(commonTokens.filter( lambda x: x[0][0] == 'b00005lzly' and x[0][1] == 'http://www.google.com/base/feeds/snippets/13823221823254120257') .collect())

this gives me
[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), 'datacompleteincludessoftware')]

amazonRec = testrec[0][0][0]print amazonRec
Above gives me
b00005lzly

googleRec = testrec[0][0][1]print googleRec
This gives me
http://www.google.com/base/feeds/snippets/13823221823254120257

Then finally following 
tokens = testrec[0][1]print tokens
datacompleteincludessoftware

Then I issue
amazonwt = t1[ amazonRec ][ tokens ]

I get the error:

KeyError: 'datacompleteincludessoftware'

t1 is a nested dictionary of [amazonid][tokens]

Please help me with some pointers.

Thanks in advance



 I am creating the RDD for counting as follows:

amazonRecToToken = amazonSmall.flatMap(lambda x: tokenize(x[1]))googleRecToToken = googleSmall.flatMap(lambda x: tokenize(x[1]))

when i count i get 





There are 27424 tokens in the combined datasets

am i missing something?




 Hello Instructors, The autograder is taking too long to provide feedback of the submission of Lab-3 . I checked my notebook, there are no extra code blocks or print statements and the code passes all the  internal tests. No response yet from the autograder not even the "Your submission timed out " statement. Is there any alternate way I can submit. Hello ,

I am getting the below error and not sure how to fix ..Any help would be greatly appreciated.

amazonRec = record[0][0] googleRec = record[0][1] tokens = record[1] s = sum([amazonWeightsBroadcast.value[amazonRec][token] * googleWeightsBroadcast.value[googleRec][token]  for token in tokens])   value = s/(amazonNormsBroadcast.value.get(amazonRec) * googleNormsBroadcast.value.get(googleRec)) key = (amazonRec, googleRec) return (key, value)
similaritiesFullRDD = (commonTokens .map(fastCosineSimilarity) .cache())

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-380-049fcc923ff3> in <module>()
     24                        .cache())
     25 
---> 26 print similaritiesFullRDD.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 437.0 failed 1 times, most recent failure: Lost task 0.0 in stage 437.0 (TID 2285, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-380-049fcc923ff3>", line 16, in fastCosineSimilarity
KeyError: '1'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
<p></p>
<p></p> Curious - the notebooks, even the .py downloads, are using things not in the stdlib and not explicitly imported (e.g., pyspark).  How is this being done - is there a global import file being run on the VM somewhere? Just curious as to how you'd set this up.

Thanks,

Shayne I finally submitted the last assignment today. Lab 4 turned out to be a pleasant surprise in terms of time required!

A very sincere thanks to Prof. A. Joseph, TAs and all curriculum developers for developing and delivering this course. The labs were nicely structured - there was a fine balance between providing the steps to be taken, and letting us do the exploration/debugging around filling in the details.

(Lab 3 was the longest and required a good amount of effort, but it did provide a lot of value and learning.)

And many thanks to fellow student colleagues for the active Piazza forums - the discussions were truly very valuable. Good luck to all for wrapping up submissions. [edit] : It completed. Thanks. Hi,

In getCountsAndAverages(IDandRatingsTuple) function the IDandRatingsTuple already has single input tuple with "(MovieID,(Rating1,Ratng2,Rating3,..))", Then we need to write the code to return the tuple with "(MovieID,(nmebrOfRatings,averageRatings)", Which 

numberOfRatings =  IDandRatingsTuple.<Tranformer>
averageRatings =  numberOfRatings.<Tranformer>

P Lease let me know if i misunderstanding?! Though, I am not a good programmer, but I completed Lab-3 finally with an effort of 18 hours. I feel it is 5-6 hours extra due to my 4GB RAM on Core 2 Duo old laptop. My course progress page is not getting refreshed. Any problem with that ? Will I get my verified certificate or not ? I wish to get this with good grades. I request the instructors to check my progress report and intimate.
I enjoyed Part -1 and part -2. Struggled a lot to complete them. It was a nice experience to get my hands dirty in Python programming. All of the course mates are highly responsive and helping in nature. Thank you, mates. Part -3 was more on mathematics and algorithm. Learnt something new.Part- 4 was use of all the previous sections + extra. because of one [], I spent couple of hours effectively.
Part- 5 : It was good. Because, I understand stats.------------------------Suggestions :------------------------1. It would have been better, if this would have been explained a bit with 5/6 small video lectures in the mooc. I agree, this is the first time and hard to manage 100% .2. Please try to make it 200% better, than this time with 200% more level of difficulty and 200% better documentation in the ipynb, I will join that course also. I am enjoying this. def invert(record): """ Invert (ID, tokens) to a list of (token, ID) Args: record: a pair, (ID, token vector) Returns: pairs: a list of pairs of token to ID """ pairs= [(token, ID) for ID in record[0] for token in flatMap(record[1]) ] return pairs
amazonInvPairsRDD =amazonWeightsRDD.map(lambda s: invert(s)).cache()googleInvPairsRDD =googleWeightsRDD.map(lambda s: invert(s)).cache() Hi Staff, I enrolled late (last weekend), but I was still able to finish the course today. May I still pay $50 and get credit for the course?
How do I do this? Thanks much, mh I would love to connect with all of you

https://in.linkedin.com/in/krishnakalyan3
email krishnakalyan3@gmail.com and wanted to say thank you to the instructors.
I really liked the structure and especially the labs. With the unit tests given to see if you have reached the expected result i found it not to hard to get through.
Also it fits very well in the test first red to green coding routine I try to do when ever I can.

So I' looking forward to the machine learning course.

As a devops I would also like to hear more about setting up a spark cluster and administrating it, as this topic is  a bit hidden by providng the great vagrant box. Maybe this could be bart of an other small course.

So Thank you for this course
- Cornelius I'm not sure if it's just me, but is seems like the formulation for the math behind the Alternating Least Squares approach is extremely sparse in the Lab notebook and lacking any real detail. Moreover, the link for "Alternating Least Squares" in the book links to a Wiki page that doesn't really describe Alternating Least Squares anywhere?

While I am very familiar with regular least squares approaches, it would be nice if the notebook at least explained each term in the equation so it's clear what is being referenced.

For example, what is the matrix w? What is f(i)? Is lambda a regularization term? What is r_ij supposed to be? The rightmost graphic makes things quite confusing and it's hard to understand what is what without any accompanying explanation in the text.

In its current form, that entire section is rather unsatisfying. While I understand the logic behinf the Alt Lst Sq approach, I would like to understand the actual math/equations being presented. In its current form, the equations and math in the notebook seems like it was just thrown in there for the sake of it without any real explanation. Was this culled down to simplify the lab when Lab 4 was revised?

I'd appreciate it if someone could point me towards a reference where these terms are explained or at least a walkthrough what these terms are supposed to be so I can try to make sense of the equations.

Appreciate it and I don't mean to sound ungrateful or petty. I just found that section very lacking and atypical given the quality of all the labs so far. I am running the but there is not ouput.  All mwy programs were working and I was gettting results instantly from Jupyter. But the system  is not responding anymore.  SOmetimes it says 'Busy' but not output after the busy indicator is gone.
Is he systems in maintenance? It seems I'm  passing the wrong parameters. I read that predictAll expects one parameter 
user_product "user_product should be RDD of (user, product)"
So I'm passing it 
validationRDD.map(lambda(uid, mid, r): (uid, mid))
which is 
[(1, 1287), (1, 594), (1, 1270)]
But I get error
106         assert len(first) == 2, "user_product should be RDD of (user, product)"
What is wrong here? It seems like I cannot submit. I have no errors in my code as my code passed all the test, and locally it finishes quite fast. circa 10 - 15 mins.
But autograder does not work. I don't even have timeout. And now deadline is passed....
[All fine] What is meant by "create an RDD consisting of pairs mapping (ID, URL) to all the tokens the pair shares in common"?
Can someone provide some elements of such RDD? I have finished the lab3 and it worked no problem in the notebook. All tests passed. But after exporting it into a .py file and uploading it to the evaluation platform I got the error showed bellow,  has anybody any idea what is going on? and how to solve it?

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 1172
SyntaxError: Non-ASCII character '\xc2' in file /ok/submission.py on line 1173, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'simpleTokenize' is not defined

Removing stopwords (1b)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tokenize' is not defined

Tokenizing the small datasets (1c)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'totalTokens' is not defined

Amazon record with the most tokens (1d)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'biggestRecordAmazon' is not defined

Implement a TF function (2a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tf' is not defined

Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'corpusRDD' is not defined

Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueTokenCount' is not defined

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'rec_b000hkgj8k_weights' is not defined

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dp' is not defined

Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsSmallBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonFullRecToToken' is not defined

Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsFullCount' is not defined

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonNormsBroadcast' is not defined

Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'invert' is not defined

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'commonTokens' is not defined

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similaritiesFullRDD' is not defined

-- 0 cases passed (0.0%) --



I am still waiting for the autograder to finish, but I think I have found (at http://stackoverflow.com/questions/3001177/how-do-i-grep-for-all-non-ascii-characters-in-unix) a quick way of finding the offending character (at least for Linux and Mac), the following command will give the line number and highlight non-ascii chars in redgrep --color='auto' -P -n "[\x80-\xFF]" lab3_text_analysis_and_entity_resolution_student.pyThis will give you the line number, and will highlight non-ascii chars in red After a rather laborious lab 3 - completing lab 4 in a fraction of time felt a bit anti-climatic :)
I am looking forward to CS 190 - the journey shall continue there.

Sincere thanks to the Instructor, TAs, and fellow (more enlightened) students who were generous in sharing their insights throughout the course.

Another big win for MOOC! In Question 2 b, we need 2 datasets i.e predicted and actual. Just wondering if we create randomly or do we have actual set and other randomly. Can someone clarify it for me. I am passing all tests up to this point but am getting an incorrect cosine similarity result 
0.000805008733835. instead of 0.000303171940451My cosine similarity function seems to be correct based of previous test.I am using the google and amazon record tuple values directly for cosine similarity.  Do I need to parse these first to just compare the description component ?  The number of tokens was correct in previous tests so I assumed I did not need to do this. Hi, If somebody could help me out to find where is my error I will appreciate, I have an error in this line:

s = sum([amazonWeightsBroadcast.value[amazonRec][token] * googleWeightsBroadcast.value[googleRec][token] for token in tokens]) 

I dont know wich is the correct syntax to get a broadcast value, Im almost sure that my problem is in that line, the output error is 
TypeError: 'dict' object is not callable

thanks! smallCross = (googleSmall.cartesian(amazonSmall).cache())smallCross.take(1)

What is wrong with this. Why is this taking for ever? Please Help.. Lab 3 took this course from "This is interesting stuff that maybe I'll use" to "OH! I get way more than just the syntax, and this changes the way I think of dealing with data forever!" That's quite a change, so let me explain.

I use a lot of languages nearly daily (Ruby, Python, Java, PHP, Javascript, R, Octave/Matlab), building web applications at work and actively interested in image recognition, machine learning and, more recently, natural language processing at home.  With that said, this damn lab took a LOT of time (not sure how many hours total, but most of yesterday and some of today)!  Until Lab 3, I've never thought about how to answer real questions in those data processing areas that are big enough to care about, just the usual contrived toy projects mixed with the sort of brute force that any fluent developer can apply as needed.

Things that hurt a little but ultimately made the lab useful:

It introduced an entirely new topic.  I don't remember if the course description said anything about teaching us a very effective way of using natural language processing for a real application but, wow, how cool is that.  I'm taking a Coursera class on the same topic, and I understood more here than from that dedicated class.  Learning an entirely new topic in order to apply some seemingly simple Spark concepts seems crazy until you do it.  Then you realize that those concepts are not so simple when applied to a real problem, and the #1 failing of most programming language / tool introductions is that we all end up saying "that was good, but the class applications were too simplistic to get me far on a real question."

The basics were repeated often enough to become natural. Map, reduce, group ... wait, don't try to use an RDD in your worker! (not that this was mentioned in the lab notes, but you learned it from your error messages too I hope) ... rinse, lather, repeat.  These are the bread and butter of distributed processing, and they were repeated just enough to make them a little closer to natural for those of us who are used to having our processors in the same room.

Some explanations were a little lean. This was frustrating but, again, the explanations were one hell of a lot better than a typical "real" situation, where we're presented with a vague problem that we must solve.  The explanations presented a tight framework but didn't always say things like "you might want to look up the cartesian function." They didn't leave us with no guidance, but didn't spoon feed everything.

The presentation style forces you to keep track of things. I feel like there's no way to do this lab well and take away a good understanding of how to plan a real Spark project without keeping a journal of RDDs created, their structure, what variables have been broadcast, etc.  That is not done for you, and if it had been, I would have used it as a crutch rather than getting my head around how one has to map (no pun intended) things out on a Spark project in order to be effective.

We build a subtly broken implementation, then fixed it. I'm referring to figuring out how to use broadcast variables after not using them, and making use of an inverted index took the assignment from "toy project" to "real example." I prefer the approach of doing it the wrong way first to illustrate why an obvious path is flawed over just dictating the perfect path from the start, which makes it too easy to take the subtleties for granted.

The forums are good. Lots of good comments, and lots of good staff replies make for a good forum, and I had to make use of it in section 4.  A little struggle to figure something out combined with a means to eventually illuminate the answer really cements knowledge.

So, in summary: Thanks for exercises that make it very likely that I'll actually use the material again!

- Sam
 When I try to broadcast amazonNorms
amazonNormsBroadcast = sc.broadcast(amazonNorms)

I'm getting the following error.  Does anyone know why it's happening?

-
Exception                                 Traceback (most recent call last)
<ipython-input-241-2a70b63c96d4> in <module>()
      2 amazonNorms = amazonWeightsRDD.map(lambda rec:(rec[0], norm(rec[1])))
      3 print amazonNorms.first()
----> 4 amazonNormsBroadcast = sc.broadcast(amazonNorms)
      5 
      6 #googleNorms = googleWeightsRDD.map(lambda rec:(rec[0], norm(rec[1])))

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in broadcast(self, value)
    642         be sent to each cluster only once.
    643         """
--> 644         return Broadcast(self, value, self._pickled_broadcast_vars)
    645 
    646     def accumulator(self, value, accum_param=None):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/broadcast.py in __init__(self, sc, value, pickle_registry, path)
     63         if sc is not None:
     64             f = NamedTemporaryFile(delete=False, dir=sc._temp_dir)
---> 65             self._path = self.dump(value, f)
     66             self._jbroadcast = sc._jvm.PythonRDD.readBroadcastFromFile(sc._jsc, self._path)
     67             self._pickle_registry = pickle_registry

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/broadcast.py in dump(self, value, f)
     80         else:
     81             f.write('P')
---> 82             cPickle.dump(value, f, 2)
     83         f.close()
     84         return f.name

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in __getnewargs__(self)
    169         # This method is called when attempting to pickle an RDD, which is always an error:
    170         raise Exception(
--> 171             "It appears that you are attempting to broadcast an RDD or reference an RDD from an "
    172             "action or transformation. RDD transformations and actions can only be invoked by the "
    173             "driver, not inside of other transformations; for example, "

Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.

  Hello,

I enjoyed lab 3 which was very work-intensive but also valuable in terms of learning more about Spark.

Unfortunately I had an error with submission of lab 3. I submitted before the deadline, but it took a long time to grade and finally finished a couple of minutes after the deadline so I got subtracted 20%. Is there any way to be graded on the time of submission and not on the time of the grader's completion?

My submission id follows:

1253927-2be80dca48fc53f8a59483cf11ea04ea:ip-172-31-8-27 I've been able to get the myUnratedMoviesRDD to work so that it contains all the movie pairs that I didn't rate in the form
[(0, 1), (0, 2)...etc].  I checked that the movie ID's I had rated were the only values missing from myUnratedMoviesRDD. When I run the following code I get that the predictedRatingsRDD contains [] and is of size 0.

predictedRatingsRDD = myRatingsModel.predictAll(myUnratedMoviesRDD)print predictedRatingsRDD.take(10)print predictedRatingsRDD.count()

[]
0

Any ideas about what could be happening would definitely help, I've been looking back through the sections of the lab and I haven't had anything else unexpected happen, but I'm also assuming that predictedRatingsRDD should look like predictedTestMyRatingsRDD which contained:
[Rating(user=1377, product=384, rating=2.55232308981843), Rating(user=2909, product=384, rating=3.319101450315248), Rating(user=1947, product=1084, rating=3.990610447815195), Rating(user=2507, product=1084, rating=4.310672330450055)] As for late submissions after 3 day grace period of the due date, you lose 20 points.
 Your final score is 80

Your submission token ID is 1254136-2bd70108e59ed3630d9f032305532f02:ip-172-31-18-133
Please include this submission token ID when you need support for your code submission.
Hi, I submitted 10 minutes before the submission deadline, but it took quite a while for the autograder to complete. Shouldn't the penalty be based upon your submission time instead of autograder completion time?
 I am getting the following error after running 3c.
Please help me locate the error.
Thanks in advance !
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-58-4701de73f74d> in <module>()
     35             .collect()[0][2])
     36 
---> 37 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     38 print 'Requested similarity is %s.' % similarityAmazonGoogle

<ipython-input-58-4701de73f74d> in similar(amazonID, googleURL)
     32     """
     33     return (similarities
---> 34             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     35             .collect()[0][2])
     36 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 62.0 failed 1 times, most recent failure: Lost task 0.0 in stage 62.0 (TID 229, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-58-4701de73f74d>", line 22, in <lambda>
  File "<ipython-input-58-4701de73f74d>", line 18, in computeSimilarity
  File "<ipython-input-37-d9c3d83350ed>", line 12, in cosineSimilarity
  File "<ipython-input-20-3f79211065e3>", line 11, in tfidf
  File "<ipython-input-20-3f79211065e3>", line 11, in <dictcomp>
KeyError: 'heavily'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
 Hi,
Thanks all for the class, It was fascinating!
Now I'am willing to setup spark on a cloud that is based on openstack.
Does anybody now how to setup spark on cloud servers. I mean, the image creation and node configuration.
Many thanks From the pyspark documentation, it appears the function you can add only works on the key of the tuple?
>>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()
[('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]

The function def sortFunction(tuple): is applied to the entire tuple not just the key.

From the lab:

print oneRDD.sortBy(sortFunction, True).collect()

Does my question make sense? Hi TA,

Is there a place, where I can check the system time of my lab3 submission ?

I submitted mine before 00:00 UTC (to the best of my knowledge), and it took a while for it to run the auto grader. I finally got a score with 20 points less because of last submission. 

my submission token ID is 1254298-8b5aa645f11d0b4e6849a0d56ef010a1:ip-172-31-23-247 Hi,

I am using the concept of calculating word count to compute average (using groupby and map)

Result is (2, 3.232804232804233) but it does not help in getting the end result.

Any pointers on how to calculate average ?

 I couldn't understand what's the meaning of: 
You will end up with an RDD with entries of the form (xi−yi)²
Is it? ((1,1),6) ((1,1),3) => (6-3)² => (3)² => 9 I got crossSmall.using cartesian product. It has the google and amazon record lije
[((google rec),(amazonr rec))...]
He computeSImilairti function works fine
I am applying the map function to work on all the crossSmall as follows:

  map (lambda a: computeSimilarity(a) )

Does not work.

Help please

Thanks I couldn't understand why I'm getting this output from autograder because all my tests pass locally.
I've read about no-ascii, but I don't believe it's the case because I've checked the .py file and there aren't
no-ascii characters.

This is the output from autograder:

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
PicklingError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
Can't pickle builtin 

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
All tests passed
Implement an IDFs function (2c)
-------------------------------
All tests passed
Implement a TF-IDF function (2f)
--------------------------------
All tests passed
Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
All tests passed
Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogleBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
All tests passed
Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 923, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 739, in reduce
    vals = self.mapPartitions(func).collect()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 713, in collect
    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2288, in _jrdd
    pickled_cmd, bvars, env, includes = _prepare_for_python_RDD(self.ctx, command, self)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2206, in _prepare_for_python_RDD
    pickled_command = ser.dumps(command)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 411, in dumps
    return cloudpickle.dumps(obj, 2)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 816, in dumps
    cp.dump(obj)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 133, in dump
    return pickle.Pickler.dump(self, obj)
  File "/usr/lib/python2.7/pickle.py", line 224, in dump
    self.save(obj)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python2.7/pickle.py", line 562, in save_tuple
    save(element)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 254, in save_function
    self.save_function_tuple(obj, [themodule])
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 304, in save_function_tuple
    save((code, closure, base_globals))
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python2.7/pickle.py", line 548, in save_tuple
    save(element)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python2.7/pickle.py", line 600, in save_list
    self._batch_appends(iter(obj))
  File "/usr/lib/python2.7/pickle.py", line 633, in _batch_appends
    save(x)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 254, in save_function
    self.save_function_tuple(obj, [themodule])
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 304, in save_function_tuple
    save((code, closure, base_globals))
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python2.7/pickle.py", line 548, in save_tuple
    save(element)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python2.7/pickle.py", line 600, in save_list
    self._batch_appends(iter(obj))
  File "/usr/lib/python2.7/pickle.py", line 633, in _batch_appends
    save(x)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 254, in save_function
    self.save_function_tuple(obj, [themodule])
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 304, in save_function_tuple
    save((code, closure, base_globals))
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python2.7/pickle.py", line 548, in save_tuple
    save(element)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python2.7/pickle.py", line 600, in save_list
    self._batch_appends(iter(obj))
  File "/usr/lib/python2.7/pickle.py", line 633, in _batch_appends
    save(x)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 254, in save_function
    self.save_function_tuple(obj, [themodule])
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 304, in save_function_tuple
    save((code, closure, base_globals))
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python2.7/pickle.py", line 548, in save_tuple
    save(element)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python2.7/pickle.py", line 600, in save_list
    self._batch_appends(iter(obj))
  File "/usr/lib/python2.7/pickle.py", line 633, in _batch_appends
    save(x)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 254, in save_function
    self.save_function_tuple(obj, [themodule])
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 304, in save_function_tuple
    save((code, closure, base_globals))
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python2.7/pickle.py", line 548, in save_tuple
    save(element)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python2.7/pickle.py", line 600, in save_list
    self._batch_appends(iter(obj))
  File "/usr/lib/python2.7/pickle.py", line 636, in _batch_appends
    save(tmp[0])
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 249, in save_function
    self.save_function_tuple(obj, modList)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 309, in save_function_tuple
    save(f_globals)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 174, in save_dict
    pickle.Pickler.save_dict(self, obj)
  File "/usr/lib/python2.7/pickle.py", line 649, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/usr/lib/python2.7/pickle.py", line 681, in _batch_setitems
    save(v)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 249, in save_function
    self.save_function_tuple(obj, modList)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 309, in save_function_tuple
    save(f_globals)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 174, in save_dict
    pickle.Pickler.save_dict(self, obj)
  File "/usr/lib/python2.7/pickle.py", line 649, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/usr/lib/python2.7/pickle.py", line 686, in _batch_setitems
    save(v)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 249, in save_function
    self.save_function_tuple(obj, modList)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 309, in save_function_tuple
    save(f_globals)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 174, in save_dict
    pickle.Pickler.save_dict(self, obj)
  File "/usr/lib/python2.7/pickle.py", line 649, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/usr/lib/python2.7/pickle.py", line 686, in _batch_setitems
    save(v)
  File "/usr/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py", line 442, in save_global
    raise pickle.PicklingError("Can't pickle builtin %s" % obj)
PicklingError: Can't pickle builtin 

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonNormsBroadcast' is not defined

Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonInvPairsRDD' is not defined

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'commonTokens' is not defined

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similaritiesFullRDD' is not defined

Your submission token ID is 1255204-089101bc07ce5d863c0fad29e0c76b9c:ip-172-31-26-110

Regards Instructors - I rarely do this and am not that concerned about my grade on the lab as I got what I needed from the exercise, but if there is any chance i might not have 20 points deducted, I would appreciate that.  It was all my fault - thinking I still had 25 minutes left until midnight UTC.  I understand what precedent this would establish, but thought I would ask.

It would be a nice feature to show what time UTC it is on the courseware as well.

Thank you for considering this. Is there something wrong with test code here? This is a simple python problem and my python code works. But when I run the test I get an error?

NameError                                 Traceback (most recent call last)
<ipython-input-43-f93e43d197a1> in <module>()
      1 # TEST Number of Ratings and Average Ratings for a Movie (1a)
      2 
----> 3 Test.assertEquals(getCountsAndAverages((1, (1, 2, 3, 4))), (1, (4, 2.5)),
      4                             'incorrect getCountsAndAverages() with integer list')
      5 Test.assertEquals(getCountsAndAverages((100, (10.0, 20.0, 30.0))), (100, (3, 20.0)),

NameError: name 'Test' is not defined I wasn't able to finish Lab 3 on time. Just wondering, should I resubmit if I'm able to finish it?

Also, will there be any answers provided after the course is complete for all 4 Labs? Guys,

any idea why I'm getting 0.1666666716337204 instead of 0.1666666666666666 while dividing float(1)/float(6)?

Thanks
 
 In question 2c, what you guys understand as a document?

Thanks Hi this is my code for IDFS count

<REMOVED FOR HONOR CODE VIOLATION>
My output state as
"
There are 17079 unique tokens in the full datasets."


whereas the desired output is one less than my answer which is 17078

Please help me where am i doing the mistake.
all my previous tests have been passed Not that it makes a difference to me but I think there is a slight glitch in the autograder scoring.  I would assume that it would take the highest of all your submissions?  Ever the procrastinator I started Lab 3 this morning and it took an hour long than I expected.  Realizing I was going over I submitted what I had anyway with the last section still not working.  The autograder gave me 97%.  An hour later when I finished after time I submitted anyway to ensure there wasn't some issue and just to have my complete solution on file.  It gave me the 100% minus the 20% late penalty and recorded that as my score.  I feel like it should register whichever submission scored top based on adjusted scoring.

Like I said this didn't matter to me, but I'm guessing there could be someone who slips a grade or fails because of this.   Please help me understand how the lambda function in the following works:

smallIDFTokens = idfsSmall.takeOrdered(11, lambda s: s[1])

the value passed is like: 
('software', 4.25531914893617)
I understand that takeOrdered will print 11 elements based on the sort order of the lambda function. I didn't knew but but looking at answer I do understand that it takes the key value pair as one argument and sorts on the value. But what I do not understand is that where is the sort order defined in the lambda function or the lambda function just provides the value on which the sorting has to be done. Please correct my understanding if wrong and please provide some help guide to understand the lambda functions. well, i thought i'd get partial credit - all the way up thru 4e at least - currently blocked on that darned 4f problem.
anyway, auto grader failed completely. all i did was export it from the notebook at a .py file and upload it.

Your submission token ID is 1249248-7ed13ab0addda93ce90717e5cb3ece80:ip-172-31-19-209

 
----------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 443
SyntaxError: Non-ASCII character '\xe2' in file /ok/submission.py on line 444, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

clues?  ...and I have to say I thoroughly enjoyed this course. Lab 3 was quite difficult but I feel I learned a lot. I look forward to taking CS 190.1x.

Thanks for all the work that was put into this course. Could anyone give me a clue, why am i getting this:


---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-70-7769c145fdb3> in <module>()
     13 
     14 
---> 15 amazonWeightsBroadcast = sc.broadcast(amazonWeightsRDD.collectAsMap())
     16 googleWeightsBroadcast = sc.broadcast(googleWeightsRDD.collectAsMap())
     17 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collectAsMap(self)
   1443         4
   1444         """
-> 1445         return dict(self.collect())
   1446 
   1447     def keys(self):

ValueError: dictionary update sequence element #0 has length 9; 2 is required
 
print 'crossSmall = ',crossSmall.take(1)
gives me the proper result when I print it before the computeSimilairty function.
I use the same
print 'crossSmall = ',crossSmall.take(1) after the computeSImilarity() function I get blank.

It is not being modifed anywhere before the second call.

Is the cache doing something so I cannot access it a second time

thanks

 I used to install seaborn in anaconda. But within this virtual environment, how can I install seaborn for visualization? Please advise. Thank!  Now my code looks like this. Could anyone give me some hints to correct my code? Thanks!
# TODO: Replace <FILL IN> with appropriate code<REMOVED DUE TO HONOR CODE VIOLATION> im getting an error for this code below

<REDACTED>
i have initialised my sims and trueDupsRDD as below
sims = <REDACTED>

im getting this error below

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-102-c597cc89ee4f> in <module>()
     12                .map(lambda (amazonID,googleID,cosineScore):('%s %s' % (removeQuotes(googleID),removeQuotes(amazonID)),cosineScore))
     13                .subtractByKey(goldStandard))
---> 14 avgSimNon = nonDupsRDD.map(lambda x:x[1]).sum(lambda x,y:x+y)/float(nonDupsCount)
     15 
     16 print 'There are %s true duplicates.' % trueDupsCount

TypeError: sum() takes exactly 1 argument (2 given)

can someone please help me out!! "# Pre-compute TF-IDF weights.  Build mappings from record ID weight vector."
what's the exact ask? can someone show me an example? 

Does it ask for :
google_url1, { tfidf dict for the tokens for google url1}
 I am still working on Lab 3 and am struggling. I don't think I will be able to complete Lab 3 before tomorrow's deadline and Lab 4 before July 6th (taking the grace period into account)

I just want to know whats the last and final date to turn in lab 3 and lab 4 to receive a passing grade and certificate?

 I come to this class to learn how to use Python and Spark, to better understand things look really easy on power point.

The majority learning appears are doing the lab exercise. that's ok. but I can only achieve much with limited time.

I felt like getting most help from students attending MOOC, but not the instructors.

I really do not care if i am getting the certificate, but i need to make sure i do learn sth. after spending lots of time here.

Thanks,
 How to split the RDD to get movie name, rating etc, after I joined with movieRDD
movieNameWithAvgRatingsRDD: [(2049, (u'Happiest Millionaire, The (1967)', (22, 3.6818181818181817))), (3, (u'Grumpier Old Men (1995)', (299, 3.0468227424749164))), (2052, (u'Hocus Pocus (1993)', (94, 2.882978723404255)))] I have this recurring problem where I can't get any insight into what an RDD contains, for example, len(rdd), rdd.count(), rdd.take(1), rdd.collect() all throw various kinds of errors. I think this is due to some basic misunderstanding about RDDs in various states?? Here is an example from lab 3-3e where I can't debug my nonDupsRDD. Any help would be greatly appreciated.

nonDupsRDD = (sims.map...print sims.take(1)
print type(nonDupsRDD)
print nonDupsRDD
print nonDupsRDD.take(1)



[('b000jz4hqo http://www.google.com/base/feeds/snippets/11448761432933644608', 0.0)]<class 'pyspark.rdd.PipelinedRDD'>PythonRDD[1075] at RDD at PythonRDD.scala:43






---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-76-6c76ce311a7c> in <module>()
     11 print type(nonDupsRDD)
     12 print nonDupsRDD
---> 13 print nonDupsRDD.take(1)
     14 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 347.0 failed 1 times, most recent failure: Lost task 0.0 in stage 347.0 (TID 3104, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-76-6c76ce311a7c>", line 9, in <lambda>
ValueError: need more than 2 values to unpack


 I am not  python expert , Please advise how to solve below , some tutorial links will be helpful...
I am not asking for answers but some pointers to resolve this...

Args: IDandRatingsTuple: a single tuple of (MovieID, (Rating1, Rating2, Rating3, ...)) Returns: tuple: a tuple of (MovieID, (number of ratings, averageRating)) What is the point of declaring
 numPartitions = 2 
as the first line of code in Part 0?

It's immediately used to repartition ratingsFilename, but why is a repartition being done at all?
 I'm almost done with 4(f) except for this part where I get this error. Everything is how its supposed to be, but i still could not figure out what the problem is!

If i print the ratings with names RDD it looks like this

print ratingsWithNamesRDD.take(5)### (predictedRating, MovieName, numofRatings)

[(7.281529674890345, u'Hocus Pocus (1993)', 94), (5.6111024042235655, u'Dracula: Dead and Loving It (1995)', 90), (5.455886766182487, u'Roger & Me (1989)', 400), (5.199474112017169, u'Ace Ventura: Pet Detective (1994)', 354), (5.982642242551482, u'Powder (1995)', 309)]	

and executing the next line gives me the error

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-42-b5fcd016ca71> in <module>()
     17 print ratingsWithNamesRDD.take(5)
     18 
---> 19 predictedHighestRatedMovies = ratingsWithNamesRDD.takeOrdered(20, key=lambda x: -x[0])
     20 print ('My highest rated movies as predicted (for movies with more than 75 reviews):\n%s' %
     21         '\n'.join(map(str, predictedHighestRatedMovies)))

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in takeOrdered(self, num, key)
   1172             return heapq.nsmallest(num, a + b, key)
   1173 
-> 1174         return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
   1175 
   1176     def take(self, num):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2006.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2006.0 (TID 489, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1174, in <lambda>
    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
  File "/usr/lib/python2.7/heapq.py", line 432, in nsmallest
    result = _nsmallest(n, it)
  File "<ipython-input-42-b5fcd016ca71>", line 15, in <lambda>
TypeError: 'int' object has no attribute '__getitem__'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

 
I'm trying to figure out what this error could be. any help is appreciated!
Thanks! I'm reading all the posts, and I've re-read the question many times, but I can't seem to find a consistent answer as to what exactly the problem wants.  I'm sure if I keep playing around, I'll get the correct code, but not the meaning.  The title, "Identify common tokens from the full dataset" and the phrases in the problem definition, "common tokens" and "RDD that contains only tokens that appear in both datasets" seem to imply the intersection of the set of keys from amazonInvPairsRDD and googleInvPairsRDD.  But that intersection set would be much less than 2 million.  Again, I feel like I can manipulate the data until I pass the unit test, but I won't understand what exactly I'm trying to achieve. Hi there:

I got confused about the result of 2C, I got:

For rank 4 the RMSE is 0.892734779484
For rank 8 the RMSE is 0.890121292255
For rank 12 the RMSE is 0.890216118367
The best model was trained with rank 8

Is that correct?  Why rank 8 is the best (is that difference significant?)?  They are all so closed!  
And the RMSE of predictedTestRDD with that setting had a RMSE of 0.891048561304, is it valid?
Thanks for the hint. 

I have this problem with vagrant. I do not know how to fix it. If anyone knows how to fix this, please help. Thank y'all so much. Following are the sample prints of my amazonWeightsRDD and googleWeightsRDD in 4b (I have provided this just in case I was doing something wrong here?)
 

[('b000jz4hqo', {'rom': 1.8518518518518519, 'clickart': 22.22222222222222, '950': 44.44444444444444, 'image': 4.040404040404041, 'premier': 11.11111111111111, '000': 4.444444444444445, 'dvd': 1.7777777777777777, 'broderbund': 22.22222222222222, 'pack': 3.4188034188034186})]
[('http://www.google.com/base/feeds/snippets/11125907881740407428', {'quickbooks': 38.095238095238095, '2007': 6.015037593984962, 'learning': 6.015037593984962, 'intuit': 19.047619047619047})]

 
 
When I call the count function i.e. amazonWeightsRDD.count() or googleWeightsRDD.count(). I get the following error, what am I doing wrong here? Any tips or advice will help.
 

Error:
---------------------------------------------------------------------------Py4JJavaError                            Traceback (most recent call last)<ipython-input-562-79edac3e1bf8> in <module>()     16print amazonWeightsRDD.take(1)     17print googleWeightsRDD.take(1)---> 18 print 'There are %s Amazon weights and %s Google weights.' % (amazonWeightsRDD.count(),     19                                                               googleWeightsRDD.count())                              /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)   930       3   931         """--> 932       return self.mapPartitions(lambda i: [sum(1for _ in i)]).sum()   933   934   def stats(self): /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)   921       6.0   922         """--> 923       return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)   924   925   def count(self): /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)   737           yield reduce(f, iterator, initial)   738--> 739       vals = self.mapPartitions(func).collect()   740       if vals:   741           return reduce(f, vals) /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)   711        """   712       with SCCallSiteSync(self.context)as css:--> 713           port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())   714       return list(_load_from_socket(port, self._jrdd_deserializer))   715  /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)   536         answer = self.gateway_client.send_command(command)   537         return_value = get_return_value(answer, self.gateway_client,--> 538                self.target_id, self.name)   539    540       for temp_arg in temp_args: /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)   298                raise Py4JJavaError(   299                   'An error occurred while calling {0}{1}{2}.\n'.--> 300                     format(target_id, '.', name), value)   301           else:   302                 raise Py4JError( Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1027.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1027.0 (TID 4470, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last): File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main   process() File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process   serializer.dump_stream(func(split_index, iterator), outfile) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func   return func(split, prev_func(split, iterator)) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func   return func(split, prev_func(split, iterator)) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func   return func(split, prev_func(split, iterator)) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func   return f(iterator) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <lambda>   return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum() File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <genexpr>   return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum() File "<ipython-input-562-79edac3e1bf8>", line 14, in <lambda> File "<ipython-input-393-a7c2a0e6d9cc>", line 12, in tfidf File "<ipython-input-393-a7c2a0e6d9cc>", line 12, in <dictcomp>KeyError: 'pleased'         at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135) at org.apache.spark.api.python.PythonRDD<tt></tt>$$anon$1.<init>(PythonRDD.scala:176)        at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)        at org.apache.spark.scheduler.Task.run(Task.scala:64)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)        at java.lang.Thread.run(Thread.java:745) Driver stacktrace:        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204) at org.apache.spark.scheduler.DAGScheduler<tt></tt>$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192) at org.apache.spark.scheduler.DAGScheduler<tt></tt>$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354) at org.apache.spark.util.EventLoop<tt></tt>$$anon$1.run(EventLoop.scala:48) 

 Why did I loose 20 points if I submitted Lab 3 before Jun 29, 2015 at 00:00 UTC? Please advise. Thank you!

Tokenize a String (1a) ---------------------- Traceback (most recent call last): File "", line 28, in PicklingError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION Can't pickle builtin All tests passed Removing stopwords (1b) ----------------------- All tests passed Tokenizing the small datasets (1c) ---------------------------------- All tests passed Amazon record with the most tokens (1d) --------------------------------------- All tests passed Implement a TF function (2a) ---------------------------- All tests passed Create a corpus (2b) -------------------- All tests passed Implement an IDFs function (2c) ------------------------------- All tests passed Implement a TF-IDF function (2f) -------------------------------- All tests passed Implement the components of a cosineSimilarity function (3a) ------------------------------------------------------------ All tests passed Implement a cosineSimilarity function (3b) ------------------------------------------ All tests passed Perform Entity Resolution (3c) ------------------------------ All tests passed Perform Entity Resolution with Broadcast Variables (3d) ------------------------------------------------------- All tests passed Perform a Gold Standard evaluation (3e) --------------------------------------- All tests passed Tokenize the full dataset (4a) ------------------------------ All tests passed Compute IDFs and TF-IDFs for the full datasets (4b) --------------------------------------------------- All tests passed Compute Norms for the weights from the full datasets (4c) --------------------------------------------------------- All tests passed Create inverted indicies from the full datasets (4d) ---------------------------------------------------- All tests passed Identify common tokens from the full dataset (4e) ------------------------------------------------- All tests passed Identify common tokens from the full dataset (4f) ------------------------------------------------- All tests passed -- 19 cases passed (100.0%) -- As for late submissions after 3 day grace period of the due date, you lose 20 points. Your final score is 80 Your submission token ID is 1254132-1bb12447df97c904e13493fb58703ef3:ip-172-31-19-210 Please include this submission token ID when you need support for your code submission. Hi Guys. I've just submitted Lab 3, and got the following response:

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 453
SyntaxError: Non-ASCII character '\xc3' in file /ok/submission.py on line 454, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

any help would be highly appreciated! thanks! 

Pd: my code runs just fine as a ipynb. about idf. I have a query.I see other information.IDF is meaning contain word appear document number divide the total number of documents.Our lab3 assignment as if word appear number divide the total number of documents. Really a good course.  (And thanks, DataBricks for the web account; made it easier to do the problem sets at work on lunch hour and then keep working on them at home).

I'm very glad I took the Haskell MOOC (also on edX) before taking Apache Spark.  Even though I use Ruby at work (with a lot of enumerables/enumerators, maps and lambdas), practicing with Haskell really forces one to think in purely functional idioms (such as making RDD elements into tuples with a "1" in the "value" position and reducing- or folding, in Haskell)   And it removes some of the temptation to start "collect"-ing and operating on big Python lists.

It was also fortunate that this class coincided with the Spark Summit, to see the roadmap of near-term future developments.

Looking forward to the next class in the sequence.


 Hi, I am out of ideas and be stuck for hours figuring out how to get rid of this error, please help:

Exception                                 Traceback (most recent call last)
<ipython-input-143-0860c1d5e2f9> in <module>()
     20 
     21 similarities = (crossSmall
---> 22                 .map(lambda s: computeSimilarity(s))#.foreachPartition(computeSimilarity)
     23                 .cache())
     24 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in cache(self)
    189         """
    190         self.is_cached = True
--> 191         self.persist(StorageLevel.MEMORY_ONLY_SER)
    192         return self
    193 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in persist(self, storageLevel)
    205         self.is_cached = True
    206         javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)
--> 207         self._jrdd.persist(javaStorageLevel)
    208         return self
    209 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _jrdd(self)
   2286         command = (self.func, profiler, self._prev_jrdd_deserializer,
   2287                    self._jrdd_deserializer)
-> 2288         pickled_cmd, bvars, env, includes = _prepare_for_python_RDD(self.ctx, command, self)
   2289         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(),
   2290                                              bytearray(pickled_cmd),

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _prepare_for_python_RDD(sc, command, obj)
   2204     # the serialized command will be compressed by broadcast
   2205     ser = CloudPickleSerializer()
-> 2206     pickled_command = ser.dumps(command)
   2207     if len(pickled_command) > (1 << 20):  # 1M
   2208         broadcast = sc.broadcast(pickled_command)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py in dumps(self, obj)
    409 
    410     def dumps(self, obj):
--> 411         return cloudpickle.dumps(obj, 2)
    412 
    413 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dumps(obj, protocol)
    814 
    815     cp = CloudPickler(file,protocol)
--> 816     cp.dump(obj)
    817 
    818     #print 'cloud dumped', str(obj), str(cp.modules)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dump(self, obj)
    131         self.inject_addons()
    132         try:
--> 133             return pickle.Pickler.dump(self, obj)
    134         except RuntimeError, e:
    135             if 'recursion' in e.args[0]:

/usr/lib/python2.7/pickle.pyc in dump(self, obj)
    222         if self.proto >= 2:
    223             self.write(PROTO + chr(self.proto))
--> 224         self.save(obj)
    225         self.write(STOP)
    226 

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    560         write(MARK)
    561         for element in obj:
--> 562             save(element)
    563 
    564         if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    634                 write(APPENDS)
    635             elif n:
--> 636                 save(tmp[0])
    637                 write(APPEND)
    638             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    684                 k, v = tmp[0]
    685                 save(k)
--> 686                 save(v)
    687                 write(SETITEM)
    688             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    679                 for k, v in tmp:
    680                     save(k)
--> 681                     save(v)
    682                 write(SETITEMS)
    683             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    679                 for k, v in tmp:
    680                     save(k)
--> 681                     save(v)
    682                 write(SETITEMS)
    683             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    684                 k, v = tmp[0]
    685                 save(k)
--> 686                 save(v)
    687                 write(SETITEM)
    688             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    684                 k, v = tmp[0]
    685                 save(k)
--> 686                 save(v)
    687                 write(SETITEM)
    688             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    304             reduce = getattr(obj, "__reduce_ex__", None)
    305             if reduce:
--> 306                 rv = reduce(self.proto)
    307             else:
    308                 reduce = getattr(obj, "__reduce__", None)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in __getnewargs__(self)
    242         # This method is called when attempting to pickle SparkContext, which is always an error:
    243         raise Exception(
--> 244             "It appears that you are attempting to reference SparkContext from a broadcast "
    245             "variable, action, or transforamtion. SparkContext can only be used on the driver, "
    246             "not in code that it run on workers. For more information, see SPARK-5063."

Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063. Hi,

I submitted my homework prior to the deadline and received a 94%.  I felt like working more and decide to check my updated answers to see if everything is correct.  However, when I did it, the grader neglected my previous grade and gave me 100%-20% = 80% now.  Can you fix it?  My email is mytang82@gmail.com.  Thank you.

Ming I worked steadily through the Lab 3 notebook and was on track to finish by the deadline, but I worked through 4f and believe I have the code right, but my value for the cosine similarity is coming out as 

7.24992115129e-06
rather than 4.286548414e-06.

I don't want to post my code, but here are some values that I'm getting for the unit test record:

s: 594.731111392
dot-pairs: ['18.5040322581*18.5040322581', '7.72558922559*7.72558922559', '13.6577380952*13.6577380952', '2.47252155172*2.47252155172']
amazon norm: 11490.7640508
googleRec: http://www.google.com/base/feeds/snippets/13823221823254120257
google norm: 7139.01784366
amazonRec: b00005lzly
value: 7.24992115129e-06
tokens: ['data', 'complete', 'includes', 'software']
My token-weight values for both amazon and google were the same, which on its face seems weird to me, but I double checked their derivation and everything looks fine (e.g. not using the wrong variable somewhere), all unit tests to this point were passing.

I don't know if it is possible to get some additional time to complete the lab.  Part of my issue in running close to the deadline to begin with was due to poor instructions (misuse of 'document') on an earlier problem, which has been noted by several people.

Regards,

Steve
 Hi ,

I scored 73 in lab3 before due date and then i got a few improvement in lab3 after several hours of trying to get 78. But due to pass the due date, i got a penalty of 20 points from 78 to 58. Which i felt very unfair. I would expect the max of my scores, that is 73 rather than 78 or 58 as a fair way. I feel like my entire effort got wasted and it is disappointed to see my score 58.

Thanks
Nirmla I found the deadlines pretty challenging for a number of reasons :-

I've had to travel from Europe to USA for work and stay in hotelMy laptop brokeI had to use useless AT&T network bandwidth to download VMThe official deadlines fall on a friday, a monday would be far better, I ended up needing the 3 days extension every time

and a whole bunch of other stuff as got in the way, I've done my best to keep up, and normally I complete Edx and Coursera courses without issue, but this time has been really tough.

Is there any chance to add an extra day or change the submission deadline in any way from 29th June 00:00 UTC ? Hi everyone!

My code is:

amazonRec = record[0][0] googleRec = record[0][1] #tokens = record[1] tokens = record[1].split(' ') # Split the token with sep=' '  s = sum( [ amazonWeightsBroadcast.value.get(amazonRec).get(t) * googleWeightsBroadcast.value.get(googleRec).get(t) for t in tokens ] ) value = s / ( amazonNormsBroadcast.value.get(amazonRec) ) / ( googleNormsBroadcast.value.get(googleRec) ) key = (amazonRec, googleRec) return (key, value)
similaritiesFullRDD = (commonTokens .map(fastCosineSimilarity) .cache() )

In 4f i can't figure what is wrong and i get this error which i think tries to multiply None with float. But i am wondering how is possible to return None with .get() ??? The common tokens are both in amazon and in google datasets! Help please!

  serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-50-39e2ada622eb>", line 18, in fastCosineSimilarity
TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94) The code for the set_bit() function seems rather clumsy:
def set_bit(x, value, length):
    bits = []
    for y in xrange(length):
        if (x == y):
          bits.append(value)
        else:
          bits.append(0)
        return bits
when it could be simply
def set_bit(x, value, length):
    bits    = [0]*length
    bits[x] = value
    return bits
Moreover, because the first argument is always a bin number, the code below conveys a tad more information.
def set_bit(binNum, value, length):
    bits         = [0]*length
    bits[binNum] = value
    return bits My notebook runs correctly, however the autograder keeps complaining about exception error:

Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
PicklingError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
Can't pickle builtin 

 The doc for join[1] says:

Performs a hash join across the cluster.

What does that mean?

[1]https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.join   Stuck in 3c for more than 3 hours... 

Here is my steps:
1 Use cartesian to join two RDD
2 In computeSimilarity, I used "googleRec = record[0][0]" to get data

The error message suggested: 
 float division by zero
But I am really confused now. How can I overcome this problem? Thanks!

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-271-f883563aa81d> in <module>()
     36             .collect())
     37 
---> 38 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     39 print 'Requested similarity is %s.' % similarityAmazonGoogle
     40 

<ipython-input-271-f883563aa81d> in similar(amazonID, googleURL)
     33     """
     34     return (similarities
---> 35             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     36             .collect())
     37 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 594.0 failed 1 times, most recent failure: Lost task 0.0 in stage 594.0 (TID 2206, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-271-f883563aa81d>", line 23, in <lambda>
  File "<ipython-input-271-f883563aa81d>", line 19, in computeSimilarity
  File "<ipython-input-261-a96cdda05881>", line 15, in cosineSimilarity
  File "<ipython-input-259-f47e67c2e86a>", line 39, in cossim
ZeroDivisionError: float division by zero

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
</span></span></span></span></span></span></span></span></span></span></span>
<p><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansiyellow"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"> </span></span></span></span></span></span></span></span></span></span></span></p> movieIDsWithRatingsRDD = ratingsRDD.map(lambda (x,y,z):(y,z)).groupByKey()
movieIDsWithAvgRatingsRDD = movieIDsWithRatingsRDD.map(lambda a:getCountsAndAverages(a))
print 'movieIDsWithAvgRatingsRDD: %s\n' % movieIDsWithAvgRatingsRDD.take(3)



'ResultIterable' object does not support indexing

Please help I'm blocked !! Due to a very busy work schedule, I can only do labs on the weekend.  I was working through Lab 3 and had gotten a score of about 83% before the end of the 3 day grace period.  I continued working until I got to 94%, but because I submitted my work after the deadline I lost 20 points.  So I ended up with 75% instead of 83%.
It's not that big a deal, but wouldn't it make sense to use the score of the highest attempt? I've just enrolled in the introduction to spark course few days ago, I am already late but trying my best to catchup.

I have posted a question before about submitting the labs and assignments after the dude date and I understood that there is a penalty 20% reduction in grade but at the end I can get a "Pass" grade if I finished the whole course.

Now the problem that I am putting lots of time and effort on this course being overdue. I just found out that the due date for the verification is overdue.

My question: How and by any mean I can still get my certificate verified. And my feedback is that, if you keep the enrolment button open then you should provide some flexibility to such things.

Please advise. I don't want to finish this course without a verified certificate.

Thanks
Ramy im getting a huge error for this code

<SNIP MAJOR HONOR CODE VIOLATION>

error

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-158-a957b3cf6166> in <module>()
     12                .map(lambda (amazonID,googleID,cosineScore):('%s %s' % (removeQuotes(googleID),removeQuotes(amazonID)),cosineScore))
     13                .subtractByKey(goldStandard))
---> 14 avgSimNon = nonDupsRDD.map(lambda x:x[1]).sum()/float(nonDupsRDD.count())
     15 
     16 print 'There are %s true duplicates.' % trueDupsCount

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 311.0 failed 1 times, most recent failure: Lost task 0.0 in stage 311.0 (TID 2024, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-158-a957b3cf6166>", line 12, in <lambda>
ValueError: need more than 2 values to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
</span></span></span></span></span></span></span></span></span></span></span></span></span>
<p><p>Any ideas?? im new to python and this is the only error which i have in lab 3..please help me out..i have been sitting with this error for two days</p></p> Hi,

The grader times out. 
I have tried a few submissions, since last night.
I cleaned the extra code and such, but it still times-out.
Any help appreciated. Last submission id:
1265424-eed2d6097467b16ca70c0df40169a4e6:ip-172-31-18-134
 I get this error. Can someone help me resolve it, please ? 
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-36-1095dbd1bf65> in <module>()
      1 # TEST Compute Norms for the weights from the full datasets (4c)
----> 2 Test.assertTrue(isinstance(amazonNormsBroadcast, Broadcast), 'incorrect amazonNormsBroadcast')
      3 Test.assertEquals(len(amazonNormsBroadcast.value), 1363, 'incorrect amazonNormsBroadcast.value')
      4 Test.assertTrue(isinstance(googleNormsBroadcast, Broadcast), 'incorrect googleNormsBroadcast')
      5 Test.assertEquals(len(googleNormsBroadcast.value), 3226, 'incorrect googleNormsBroadcast.value')

NameError: name 'Broadcast' is not defined
 What is one to make of these two autograder error reports?

1. Tokenizing a string



when iPython shows nothing wrong?:


2. Autograder cannot find file


This report seems to say that the autograder cannot find a file of its own making. I submitted a first version of Lab 3 on Sunday (France) with score of 84% (I had no time to finish the last two questions).

I then decided to make a second submission this morning (Monday) with an additional answer. Because I was not any more in the grace period I received a message saying that I would only get a 75% score which is lower than the score I got with the first submission! It doesn't seem fair to me as the first submission was made in time and is at the end, better. Any chance to find a solution and ensure that the best score is the one that will be taken into account?

Best regards Hi folks,
I have one off topic question. I looking for web application for documenting business metadata in the data warehouse. I found this BI encyklopeadia solution from Billigence company http://www.billigence.com/en/clientspartners/semanta/. If it will be possible
 I would like to use the open source software. Can anyone recommend me a similar solution from the open source world? Thanks. 
Hi,

It seems the terms “rows” and “columns” are used incorrectly in several parts of the ipython notebook for lab 4 (lab4_machine_learning_student.ipynb, version 1.0.1). The use of terms might be correct, but in that case, there is still discrepancy between figures and texts.

Similar discrepancy is found in the videos from lecture Lecture 8.

Hope it helps ;)  This pre-populated code, which is supposed to fetch a list of token for the id 'recb000hkgj8k' from the amazonRecToToken results in an error and i cannot move forward due to this:

recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1]

This is the error: 

IndexError: list index out of range I'm trying to execute my Spark code on a EMR cluster to investigate a little bit and I found an issue with error logs. I manage to find where they're being stored inside hdfs or the S3 folder I declared when creating the cluster but I cannot seem to find an easier way to inspect those logs. Does anyone knows how to do so? Editing the log4j.properties file seems to be a good path but haven't been able to get anything out of it so far. Any help is much appreciated!!
Cheers. Just want to say thank you dear Professor Joseph and TA-Team for the great course!
I've learned quite interesting things about spark, closures and ml potential.
Even though I've finished with 100% while running throught the labs with not to much effort, I feel that I have to learn a lot about the
underlying mathematical foundations  So I'm gooing to skip the following course and take LA and Calculus instead. I've submitted my work for lab3 thursday 25th, and I had a 96% grade for it, because I made a mistake at the very last test, which I understand.

Today, I've fixed this very last test, and I submitted my lab3.py to the autograder.
All the automatic tests have been completed, a bright 100%. 
However, because I re-submitted my work after the 3-day grace period,
I got a 20 point penalty, ended down to 80% for my all work !

I cannot believe it, how is that possible !?  Can't you just take my higher grade in the end ?
Okay my last submission was after the grace period, but why penalize all the work, and no just the only part  I completed late ?

Regards,

submission token ID is 1271868-bf829114a3c1491a5ad06a0c41e8e89c
 import redef removePunctuation(text):
return re.sub(r'[ \W,_]+' , ' ', text.lower()).lstrip()print removePunctuation('Hi, you!')print removePunctuation(' No under_score!')


result show : 
hi you 
no under score  There is no lectures for week 5? Or is something wrong with my account?

Thanks Hi,

I have not  enrolled for Signature Track. Can I still get any course completion certificate ? In this part, we will use Spark to find the name, number of ratings, and the average rating of the 20 movies with the highest average rating and more than 500 reviews. 

We want to filter our movies with high ratings but fewer than or equal to 500 reviews


The first sentence adds to the understanding.  The second sentence ... just adds confusion. I am submitting my lab 3 assignment now and auto grader has marked it late when i'm still in the grace period.
or did i lost the deadline? i can submit it till Monday night right ? after joining predicted and actual I am trying to get the pairs that is in both RDDS

I get:

actual RDD format [((1, 2), 3), ((1, 3), 5), ((2, 1), 5), ((2, 2), 1)]
predicted RDD format [((1, 1), 5), ((1, 2), 3), ((1, 3), 4), ((2, 1), 3), ((2, 2), 2), ((2, 3), 4)]
joined = [((1, 3), (4, 5)), ((2, 2), (2, 1)), ((1, 2), (3, 3)), ((2, 1), (3, 5))]
totalError = [1, 1, 0, 4] <-- should this be the sum of these 4
NumRatings = 4

But I believe there should be at most 4 since we have only 4 actual. I then try to apply a map to the join, this is where I am confused. I am trying to say if (a,b) in both actual and predicted then return this:
[((1, 3), (4, 5)), ((2, 2), (2, 1)), ((1, 2), (3, 3)), ((2, 1), (3, 5))]

of course I believe it to be less. Am I right in my thinking?  This is a confused and not working version of what I am trying to do after join. Any clarifications would be most appreciated:
map(lambda ((a,b),c):(c for k in a if k in b)) I got the following long stack trace on trueDupsRDD.count().  Please help?
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-39-36f021826c7d> in <module>()
      4 trueDupsRDD = trueDupsRDD = (sims.join(goldStandard)).map(lambda x: similarBroadcast(split(x[0])))
      5 # print 'trueDupsRDD %s' % trueDupsRDD.take(2)
----> 6 trueDupsCount = trueDupsRDD.count()
      7 # avgSimDups = (trueDupsRDD.map(lambda x: x).reduce(sum))/float(trueDupsCount)
      8 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _jrdd(self)
   2286         command = (self.func, profiler, self._prev_jrdd_deserializer,
   2287                    self._jrdd_deserializer)
-> 2288         pickled_cmd, bvars, env, includes = _prepare_for_python_RDD(self.ctx, command, self)
   2289         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(),
   2290                                              bytearray(pickled_cmd),

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _prepare_for_python_RDD(sc, command, obj)
   2204     # the serialized command will be compressed by broadcast
   2205     ser = CloudPickleSerializer()
-> 2206     pickled_command = ser.dumps(command)
   2207     if len(pickled_command) > (1 << 20):  # 1M
   2208         broadcast = sc.broadcast(pickled_command)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py in dumps(self, obj)
    409 
    410     def dumps(self, obj):
--> 411         return cloudpickle.dumps(obj, 2)
    412 
    413 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dumps(obj, protocol)
    814 
    815     cp = CloudPickler(file,protocol)
--> 816     cp.dump(obj)
    817 
    818     #print 'cloud dumped', str(obj), str(cp.modules)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dump(self, obj)
    131         self.inject_addons()
    132         try:
--> 133             return pickle.Pickler.dump(self, obj)
    134         except RuntimeError, e:
    135             if 'recursion' in e.args[0]:

/usr/lib/python2.7/pickle.pyc in dump(self, obj)
    222         if self.proto >= 2:
    223             self.write(PROTO + chr(self.proto))
--> 224         self.save(obj)
    225         self.write(STOP)
    226 

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    560         write(MARK)
    561         for element in obj:
--> 562             save(element)
    563 
    564         if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    631                 write(MARK)
    632                 for x in tmp:
--> 633                     save(x)
    634                 write(APPENDS)
    635             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    631                 write(MARK)
    632                 for x in tmp:
--> 633                     save(x)
    634                 write(APPENDS)
    635             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    631                 write(MARK)
    632                 for x in tmp:
--> 633                     save(x)
    634                 write(APPENDS)
    635             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    631                 write(MARK)
    632                 for x in tmp:
--> 633                     save(x)
    634                 write(APPENDS)
    635             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    634                 write(APPENDS)
    635             elif n:
--> 636                 save(tmp[0])
    637                 write(APPEND)
    638             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    684                 k, v = tmp[0]
    685                 save(k)
--> 686                 save(v)
    687                 write(SETITEM)
    688             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    684                 k, v = tmp[0]
    685                 save(k)
--> 686                 save(v)
    687                 write(SETITEM)
    688             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    304             reduce = getattr(obj, "__reduce_ex__", None)
    305             if reduce:
--> 306                 rv = reduce(self.proto)
    307             else:
    308                 reduce = getattr(obj, "__reduce__", None)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in __getnewargs__(self)
    169         # This method is called when attempting to pickle an RDD, which is always an error:
    170         raise Exception(
--> 171             "It appears that you are attempting to broadcast an RDD or reference an RDD from an "
    172             "action or transformation. RDD transformations and actions can only be invoked by the "
    173             "driver, not inside of other transformations; for example, "

Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.
Isn't trueDupsRDD supposed to be a RDD of the just the cosine similarity score? Please help with this error.  I am getting this error on Lab 4, 3e when I do 

"predictedRatingsRDD = myRatingsModel.predictAll(myUnratedMoviesRDD)"

predictedRatingsRDD.take(2)

I also did
print type(myRatedMovies)print 'moviesRDD', moviesRDD.take(20)print myUnratedMoviesRDD.take(10)

and I get
<type 'list'>
moviesRDD [(1, u'Toy Story (1995)'), (2, u'Jumanji (1995)'), (3, u'Grumpier Old Men (1995)'), (4, u'Waiting to Exhale (1995)'), (5, u'Father of the Bride Part II (1995)'), (6, u'Heat (1995)'), (7, u'Sabrina (1995)'), (8, u'Tom and Huck (1995)'), (9, u'Sudden Death (1995)'), (10, u'GoldenEye (1995)'), (11, u'American President, The (1995)'), (12, u'Dracula: Dead and Loving It (1995)'), (13, u'Balto (1995)'), (14, u'Nixon (1995)'), (15, u'Cutthroat Island (1995)'), (16, u'Casino (1995)'), (17, u'Sense and Sensibility (1995)'), (18, u'Four Rooms (1995)'), (19, u'Ace Ventura: When Nature Calls (1995)'), (20, u'Money Train (1995)')]
[(1, u'Toy Story (1995)'), (2, u'Jumanji (1995)'), (3, u'Grumpier Old Men (1995)'), (4, u'Waiting to Exhale (1995)'), (5, u'Father of the Bride Part II (1995)'), (6, u'Heat (1995)'), (7, u'Sabrina (1995)'), (8, u'Tom and Huck (1995)'), (9, u'Sudden Death (1995)'), (10, u'GoldenEye (1995)')]

********************* Error *****
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-26-69c2fd7596ab> in <module>()
     12 # Use the input RDD, myUnratedMoviesRDD, with myRatingsModel.predictAll() to predict your ratings for the movies
     13 predictedRatingsRDD = myRatingsModel.predictAll(myUnratedMoviesRDD)
---> 14 print '\nhg', predictedRatingsRDD.take(5)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1103.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1103.0 (TID 339, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/recommendation.py", line 107, in <lambda>
    user_product = user_product.map(lambda (u, p): (int(u), int(p)))
ValueError: invalid literal for int() with base 10: 'Toy Story (1995)'
********************* I have submitted lab3 twice since yesterday. I got autograder error both times. Below is the message I got after the second submission. When I run lab 3 in the note book it works fine. "I use the Run All" command.

Appreciate some help.
Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token ID is 1262551-aac38e207fc2ab4cc42167eaf16a1508:ip-172-31-26-111
Please include this submission token ID when you need support for your code submission.
 can anyone please help me with this?
the union of amazonRecToToken and googleRecToToken is giving a count of 22520
after applying distinct() to it the count is not 400. How can it pass the test?Please guide me where I'm going wrong? Hi, I'm trying to run part 3c using ALS.train() with arguments trainingWithMyRatingsRDD, rank=bestRank, seed=seed, iterations=iterations, lambda_=regularizationParameter. However, I keep getting the same error message and can't seem to tell why. What parameter am I missing?

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-27-eb3252d46127> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
----> 2 myRatingsModel = ALS.train(trainingWithMyRatingsRDD, rank=bestRank, seed=seed, iterations=iterations, lambda_=regularizationParameter)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/recommendation.py in train(cls, ratings, rank, iterations, lambda_, blocks, nonnegative, seed)
    138               seed=None):
    139         model = callMLlibFunc("trainALSModel", cls._prepare(ratings), rank, iterations,
--> 140                               lambda_, blocks, nonnegative, seed)
    141         return MatrixFactorizationModel(model)
    142 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/common.py in callMLlibFunc(name, *args)
    118     sc = SparkContext._active_spark_context
    119     api = getattr(sc._jvm.PythonMLLibAPI(), name)
--> 120     return callJavaFunc(sc, api, *args)
    121 
    122 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/common.py in callJavaFunc(sc, func, *args)
    111     """ Call Java Function """
    112     args = [_py2java(sc, a) for a in args]
--> 113     return _java2py(sc, func(*args))
    114 
    115 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o1458.trainALSModel.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1099.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1099.0 (TID 350, localhost): java.lang.NegativeArraySizeException
	at org.apache.spark.ml.recommendation.ALS$NormalEquation.<init>(ALS.scala:440)
	at org.apache.spark.ml.recommendation.ALS$$anonfun$org$apache$spark$ml$recommendation$ALS$$computeFactors$1.apply(ALS.scala:1100)
	at org.apache.spark.ml.recommendation.ALS$$anonfun$org$apache$spark$ml$recommendation$ALS$$computeFactors$1.apply(ALS.scala:1092)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$15.apply(PairRDDFunctions.scala:674)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$15.apply(PairRDDFunctions.scala:674)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:126)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span></span></span></span> Instead of a list, calling groupByKey() in PySpark will return a pyspark.resultiterable.Resultiterable object.  I've noticed that functions like len() and sum() behave as expected when passed a pyspark.resultiterable.Resultiterable object rather than a list, but I haven't tested things like list comprehension or pushing/popping.  In general, can the pyspark resultiterable data type always be treated like a list? 5a & b passed.
Lab3/5c fails with the following stack trace:

---------------------------------------------------------------------------
ZeroDivisionError                         Traceback (most recent call last)
<ipython-input-49-6cb7b4e378d1> in <module>()
      4 trueposDict = dict([(t, truepos(t)) for t in thresholds])
      5 
----> 6 precisions = [precision(t) for t in thresholds]
      7 recalls = [recall(t) for t in thresholds]
      8 fmeasures = [fmeasure(t) for t in thresholds]

<ipython-input-48-1473fca4fbda> in precision(threshold)
      5 def precision(threshold):
      6     tp = trueposDict[threshold]
----> 7     return float(tp) / (tp + falseposDict[threshold])
      8 
      9 def recall(threshold):

ZeroDivisionError: float division by zero

 In lab3 4b,

I am unable to calculate amazonWeightsRDD and googleWeightsRDD.I used the following:

amazonWeightsRDD = tfidf(amazonFullRecToToken.map(lambda (k,v):k),idfsFullBroadcast.values())

and similarly for googleWeightsRDD

but i get an error like the below:



There are 71 unique tokens in the full datasets.






---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-332-7839d56ee7eb> in <module>()
     11 
     12 # Pre-compute TF-IDF weights.  Build mappings from record ID weight vector.
---> 13 amazonWeightsRDD = tfidf(amazonFullRecToToken.map(lambda (k,v):k),idfsFullBroadcast.values())
     14 googleWeightsRDD = tfidf(googleFullRecToToken.map(lambda (k,v):k),idfsFullBroadcast.values())
     15 print 'There are %s Amazon weights and %s Google weights.' % (amazonWeightsRDD.count(),

<ipython-input-309-c607a534fb39> in tfidf(tokens, idfs)
      8         dictionary: a dictionary of records to TF-IDF values
      9     """
---> 10     tfs = tf(tokens)
     11     tfIdfDict={}
     12     for (k,v) in tfs.items():

<ipython-input-301-c89bb6b6309c> in tf(tokens)
      9     """
     10     dict={}
---> 11     tot=len(tokens)
     12     for key in tokens:
     13         if(dict.has_key(key)):

TypeError: object of type 'PipelinedRDD' has no len()



so please help how do I remove this error.My tf function defined earlier is working fine as tested in through previous cases. While joining RDDs, how to remove keys that do not appear in both RDDs predictedRDD and actualRDD This course is very badly structured.
I would recommend that you would like to introduce the following changes in the future versions of the course:

Along with the videos on theoretical concepts of big data you should have explained the practical implementation of Spark in the videos using some cases. There were no examples/ videos explaining the use of Spark's transformations, actions, Python's concepts (such as Regular Expressions).
 
I liked the course The Analytics Edge by MITx in which they teach R.
Please see how they explained several techniques as well as implementation in R using different cases and then provide similar cases in homework assignments. I just try to check the lab3  file.  I erase print help functions and helper cells, and follow the suggestions @3109 . I don't finish the 4e and 4f, the others passed al test.  the output message from autograder is very strange:

Can someone help me? Thanks in advance.


Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 1116
SyntaxError: Non-ASCII character '\xc3' in file /ok/submission.py on line 1117, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'simpleTokenize' is not defined

Removing stopwords (1b)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tokenize' is not defined

Tokenizing the small datasets (1c)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'totalTokens' is not defined

Amazon record with the most tokens (1d)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'biggestRecordAmazon' is not defined

Implement a TF function (2a)
----------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tf' is not defined

Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'corpusRDD' is not defined

Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueTokenCount' is not defined

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'rec_b000hkgj8k_weights' is not defined

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'dp' is not defined

Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsSmallBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonFullRecToToken' is not defined

Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsFullCount' is not defined

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonNormsBroadcast' is not defined

Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'invert' is not defined

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'commonTokens' is not defined

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similaritiesFullRDD' is not defined

-- 0 cases passed (0.0%) --


As for late submissions after 3 day grace period of the due date, you lose 20 points.
 Your final score is 0

 Is it possible to get ratings above five? The last step of my Lab 4 output this:

My highest rated movies as predicted (for movies with more than 75 reviews):
(5.226982173988938, u'Paths of Glory (1957)', 105)
(5.175138842240664, u'American History X (1998)', 341)
(5.158769322964465, u'Fight Club (1999)', 693)
(5.13157334310306, u'Life Is Beautiful (La Vita \ufffd bella) (1997)', 587)
(5.117625687538389, u'Usual Suspects, The (1995)', 831) Hi

I uploaded my third lab like 6 hours ago (June 29th, 8:00 am UTC aprox.) and the 3 day grace period ends June 29th 00:00 UTC but I received a 20 points penalty. I think is unfair according to the rules.
Please help me

edX username: dhsegura The code section at the beginning of 3a is listing out the 50 top rated movies on average. However, the last column is the number of ratings and not a movie ID, as the text hints. This is evident from the header column printed out with the following statement:

print '(average rating, movie name, number of reviews)'

Therefore, the list is not quite useful. You need to find the corresponding movie ID from the RDD created upon the movie.dat file.
 In Lab 4 - 3(e) we can find this instructions:

Use the Python list myRatedMovies to transform the moviesRDD into an RDD with entries that are pairs of the form (myUserID, Movie ID) and that does not contain any movies that you have rated. This transformation will yield an RDD of the form: [(0, 1), (0, 2), (0, 3), (0, 4)]. Note that you can do this step with one RDD transformation.

How can this possible?
My best effort lead to 1 map, 1 subtractByKey and another map transformation.
 Hi Please let me know how to convert a python dictionary into Tuple RDD.
I got dictionary as 

{'aided': 400.0, 'limited': 100.0, 'searchable': 400.0}and tried sc.parallelize(finaloutput) but this give me only key values in RDD not the values.Its bit urgent. Thanks
 Hi everybody,

In lab 3 3C I filled the code for compute Similariry, executed the similarities assignation and printed similarities. All looks good. However when I execute the rest of the code which I did not write, after several minutes i get "list index out of range in the .collect()[0][2])". I haver reviewed my code an can not find any anomaly. Any recommendation? An important aspect missing from the course is the installation and setup of Apache Spark itself.  The use of vagrant and pre-built image is helpful in programming with a standard base, but it hides all the aspects of installation and setup.  It will be very useful, if some details on how to setup some nodes to use Apache Spark are also included. Test.assertEquals(amazonInvPairsRDD.count(), 111387, 'incorrect amazonInvPairsRDD.count()')Test.assertEquals(googleInvPairsRDD.count(), 77678, 'incorrect googleInvPairsRDD.count()')


Test.assertEquals(commonTokens.count(), 2441100, 'incorrect commonTokens.count()')

How can commonTokens be 24,41,100 when total tokens (which may have duplicates) for amazon is 1,11,387 and for google is 77,678? 

Confused! any help what is expected for commonTokens implementation? I don't get it. 
 Just curious: why Piazza instead of the (default) edx Discussion forum (used for other edx courses) ? Thanks ...
 I followed every single recommendation on optimizing for autograder, and my Lab 3 is still timing out.

It seems to take a lot less time on my local Mac to run through all of the code than it does with the autograder.

Can I submit my *.py file to someone who has access to a better autograder EC2 instance? Hi,

I submitted Lab 2 with no problems and got 100%. However, when I was  submitting Lab3, I used the Lab2 autograder by accident. Now I have 0% for Lab2. Please help to reverse to the first submission. Thank you! I am not able to run any program since this morning.  Yesterday it was slow, today so far it is not running at all.

I don;t think I have any data intensive code, but nothing is working.

Anyone else having this issue?

Thanks After about 4hrs of going over this I'm finally close.  Here is what I have:

N = 400.0  (I cast as a float)
uniqueTokens = flatMap on the value field no distinct or anything output of take(5) = 
['clickart', '950', '000', 'premier', 'image']

tokenCountPairTuple = same code as Lab 2 3(a) except float(1).  take(5) = 
[('clickart', 1.0), ('950', 1.0), ('000', 1.0), ('premier', 1.0), ('image', 1.0)]
tokenSumPairTuple = exact same code as Lab 2 3(a) next line.  take(5) =
[('aided', 1.0), ('precise', 9.0), ('duplex', 1.0), ('dance', 1.0), ('breath', 2.0)]
return map(lambda (a,b): (a,N/b))

The value for 'software' is about 1/2 the expected answer

filtering on the steps for software I get:

tokenSumPairTuple = 185.0
return value =
[('software', 2.1621621621621623)]

which is 400/185.  The expected value of 4.2.... implies tokenSumPairTuple should be 1702.

All previous steps pass validation.  What am I missing here?
 Hi,
I implemented 1b, all steps validate except for the last one.
The output of my RDD is: 
movieNameWithAvgRatingsRDD: [(3.5671641791044775, u'Great Mouse Detective, The (1986)', 67), (3.763948497854077, u'Moonstruck (1987)', 466), (2.676056338028169, u'Waiting to Exhale (1995)', 71)]
Although the exercise doesn't make any mentioning of sorting, the output seems to be sorted alphabetically.
Was this missed in the topic or did I do something wrong ? I thought of using getCountsAndAverages() for trainingAvgRating. That should return an RDD, but we seep to print a list as:
print 'The average rating for movies in the training set is %s' % trainingAvgRating

Should I be writing my own average on this one and not use a previous function? Spark groupByKey and reduceByKey provide similar functionality. reduceByKey is strongly favored and recommended over groupByKey due to shuffling and performance reasons. It makes sense to always use reduceByKey. What are the reasons to use groupByKey at all? Are there situations where groupByKey is better? Are there situations where only groupByKey can be used?
 
Any insight into this will be greatly helpful to everyone.
 
Thanks in advance. Quick question: Did everyone receive a login to Databricks cloud ? I had submitting the request but don't recall getting a response. I suppose candidates were chosen randomly ? Just to confirm, even though the heading say number of reviews the last number is actually the movie ID correct?

Most rated movies:
(average rating, movie name, number of reviews)
(4.5349264705882355, u'Shawshank Redemption, The (1994)', 1088)
(4.515798462852263, u"Schindler's List (1993)", 1171)
(4.512893982808023, u'Godfather, The (1972)', 1047)
(4.510460251046025, u'Raiders of the Lost Ark (1981)', 1195)
(4.505415162454874, u'Usual Suspects, The (1995)', 831)
(4.457256461232604, u'Rear Window (1954)', 503)
(4.45468509984639, u'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)', 651)
(4.43953006219765, u'Star Wars: Episode IV - A New Hope (1977)', 1447) This is the form of movieRDD. There is no movieID info here?[(1, u'Toy Story (1995)'), (2, u'Jumanji (1995)'), (3, u'Grumpier Old Men (1995)')] Hi anyone faced this issue? where the idfsFullCount is not matching? Any tips\help on how you resolved it? Is is too late to submit lab 3. I need to make some changes, not worried about an high grade but better score for overall grade. So in lab 4, 2b, I tried this:

predictedReformattedRDD = testPredicted.map(lambda (x,y,z): ((x,y), z))
actualReformattedRDD = testActual.map(lambda (x,y,z): ((x,y),z))
totalRDD = predictedReformattedRDD.join(actualReformattedRDD)
print "printing results of join"
print totalRDD.take(1)
totalRDD2 = totalRDD.map(lambda (x,y): (x,(y[0]-y[1])**2))
print "printing mapping to squares" 
print totalRDD2.take(1)
totalError2 = totalRDD2.map(lambda ((x1,x2),y): y).reduce(add) 
print "printing total error2" 
print totalError2

I get the expected print outs at first: with input RDDs of ((1,3),4) and ((1,3),5), I get:

printing results of join
[((1, 3), (4, 5))]
printing mapping to squares
[((1, 3), 1)]
printing total error2
6

Why does reduce(add), acting on 1, yields a 6?   Hello Anthony D Joseph and TAs

I have certain suggestion for this mooc and I hope u will look into it

1.  MORE PROGRAMMING : Quizzes which come after the lectures should be more practical rather than theoretical. You should give more programming based practice questions in quizzes which are quite similar to the questions which come in labs. 

We study so less in lecture and then when I open the lab exercise , I am completely baffled by the questions because i have never seen or practiced those questions before and I eventually I end up wasting a lot of time on google and piazza  

2. Lectures should be more a little lengthier and should focus more on syntax and programming part. ( In week 2, there were 4 to 5 lectures which discussed abt big data, google's predictive modeling and other things . I think these slides are important but not necessary and instead of dedicating so many lectures on such theory, more should be given on apache spark syntax and how to use them in different scenario

3. Autograder should be rectified as I have lost atleast a thousand hair strands because of the autograder and the white space problem. 

4. More hints should be provided instead of just writing ' Fill in the code' . Hints such as whether only python could be used to solve the problems or we need to work with RDDs ( so that we know which one to use straight away)

5. I enrolled for this course as a ID verified student and I was so excited to get X-Series certificate but I am now tooooo exhausted to continue with CS190x ( coz of autograder and all) and therefore I have un-enrolled from that course. I hope, if the course is online next year then I will be able to complete cs190x and get X verified certification

6. I have already completed two MOOCs from MIT for python language and those were fantastic. Please look at those MOOCs. MIT Guys maintained a complete balance between thoery and coding part I don't understand what I am doing wrong.
I am applying simply tf() to the tokens and then by the keys of one dict create a dict that has the products for respective keys in its values.
But I am getting thrown. What am I doing wrong?
KeyError: 'aided' Thanks for running the course, it has been very informative and interesting. However I have a couple of suggestions.

Piazza is very messy. A traditional forum (with real folders over tags) I personally find much more readable.Some element of peer assessment. A few times I found a solution that "worked" but I don't think I've done it correctly. A chance to review others code would give me the chance to see better solutions.

Thanks,
    Michael Everytime I have to shutdown, start and run all the cells.  But it stops at 3C. The same code was working yesterday.
I also got the assert box say pass.

Today there is no response on any of the cells.

Any advise?

Thanks

Ram Is the correct version for Lab4 3F ? Thanks.



(3f) Predict Your Ratings
We have our predicted ratings. Now we can print out the 25 movies with the highest predicted ratings.
The steps you should perform are:
#### From Parts (1b) and (1c), we know that we should look at movies with a reasonable number of reviews (e.g., more than 75 reviews). You can experiment with a lower threshold, but fewer ratings for a movie may yield higher prediction errors. Transform movieIDsWithAvgRatingsRDD from Part (1b), which has the form (MovieID, (number of ratings, average rating)), into an RDD of the form (MovieID, number of ratings): [(2, 332), (4, 71), (6, 442)]#### We want to see movie names, instead of movie IDs. Transform predictedRatingsRDD into an RDD with entries that are pairs of the form (Movie ID, Predicted Rating): [(3456, -0.5501005376936687), (1080, 1.5885892024487962), (320, -3.7952255522487865)]#### Use RDD transformations with predictedRDD and movieCountsRDD to yield an RDD with tuples of the form (Movie ID, (Predicted Rating, number of ratings)): [(2050, (0.6694097486155939, 44)), (10, (5.29762541533513, 418)), (2060, (0.5055259373841172, 97))]#### Use RDD transformations with predictedWithCountsRDD and moviesRDD to yield an RDD with tuples of the form (Predicted Rating, Movie Name, number of ratings), for movies with more than 75 ratings. For example: [(7.983121900375243, u'Under Siege (1992)'), (7.9769201864261285, u'Fifth Element, The (1997)')]

 After printing my predictedRatingsRDD.take(1) that I got in part 3e, I get this:

[Rating(user=0, product=1084, rating=3.9657129364631465)]
How am I supposed to decipher this and then transform this into the predictedRDDin part 3f? I inadvertently doubled-clicked in a markdown area of Jupyter. The effect is to switch from a rendering to a code view.

How can I switch back to the rendering view?

I posted my message in the lab 4 forum because I found no real really matching category.

Thank you for your help. Hi, I am bit confused what needs to filled in following under computeSimilarity

googleRec = record[0] amazonRec = record[1] googleURL = <SNIP HONOR CODE VIOLATION> amazonID = <SNIP HONOR CODE VIOLATION> #googleValue = <FILL IN> #amazonValue = <FILL IN> I'm exhausted from this course but none the less would like continue torturing myself.
Does anyone know if i signup as an unverified student for cs190.1x how long do I have if I want to change to a verified student?  Thanks.

 I am getting an error when getting the records with more than 500 ratings:

input:[ (avgrating, title, num of rating), .....]<REDACTED>

But I am getting the following syntax error:
     SyntaxError: invalid syntax

 
Note: I haven't done the sortFunction( ) part yet.

Thanks for help

 This is a very good course since I using mooc. And the piazza disscussion give a lot 
help to me.

For my questioin:

I mean this is a begining course, some pepole may want to take a advance course. Such like spark inner or graphx or something other?

 Hi all, I think I have more or less the correct code for this exercise:

1. predictedRDD.<SNIP>
2. actualRDD<SNIP>
3.<SNIP
4. squaredErrorsRDD.<SNIP>
5. squaredErrorsRDD.<SNIP>
6. <SNIP>

but I keep getting errors regarding how I cast my variables.
TypeError: unsupported operand type(s) for /: 'PipelinedRDD' and 'float'

I am not sure when to use float(), can anyone help me out? thank you This is the first 3 elements from predictedRatingsRDD.What is this, how do I access elements of this?[Rating(user=0, product=1084, rating=4.0229758323146045), Rating(user=0, product=3586, rating=3.8288299639899956), Rating(user=0, product=3702, rating=3.2346158513299423)] Hello all!

I just completed this class with a 100% average on verified track.

When and how will I get my verified certificate?

Thanks! Can this deadline of last assignment submission can be extended for 1-2
 week ?
Regards
Abdullah
 Well, it is said to calculate the average of ratings. The exercise expect us to make:

sum(avg_film_reviews)/count(avg_film_reviews).

In reality, as we have the number of reviews on all films, the formula should be:

sum(number_film_reviews*avg_film_reviews) / sum(avg_film_reviews)

Why? Basically, if you have a film with 10 reviews and got 5 stars in average, and there is another movie with 1k revies and average of 3, you are setting both avg_reviews as the same importance.

Proof:
   not correct: (5+3)/2 = 4
   correct: ( (5*10) + (1000*3) )/(1010) = 3,01980

Am I wrong in my thought? 

 I am not able to find the correct avgSimNon.

Please verify my approach.
I took union of sims and goldstandard and then used subtractByKey on the result and the trueDupsRDD.

Help appreciated. Thanks. I have no idea of what is going in background here how jobs are executing, do they perform optimal?
Without Spark internals this training is more like a Python API training. Hi All,

Can you please let me know if it is possible to upgrade to verified track now? I do not see the upgrade button on the homepage.. Thank you. lab  - 4(d) -  problem with invoking flatMap(invert) on amazonWeightsRDD

Is there anyone who can help? I didn't yet figure out what the issue is...
 
Here are some additional details/questions:
 
1) For the invert() function, isn't the input and return parameters both tuples?
2) If so, aren't the elements of the amazonInvPairsRDD already a tuple, and can I not do a map and pass the RDD elements to the invert() function (see below)?
3) Or do I need to convert it into a list before calling invert()? I am not clear why that is the case...
4) what is the easiest way to convert the elements of an RDD into a list? I was thinking of the map function, ex: rdd.map(lambda x: list(x)).  Is there a better way in this scenario?
5) I did try the below code to convert to a list and then call invert on the elements of the RDD, but it was giving an error saying str object has no attribute keys... Couldn't yet figure out where the issue is.
 
amazonInvPairsRDD = (amazonWeightsRDD                     .map(lambda x: list(x))                     .map(invert)                     .cache())
 File "<ipython-input-189-f9401be45df6>", line 15, in invert
AttributeError: 'str' object has no attribute 'keys'


This is the error I am getting. Not clear what exactly is the problem and how to fix it. I spent several hours on this, but couldn't identify the problem. Please help!


AttributeError: 'str' object has no attribute 'keys'

My invert function is returning the Pairs RDD - hopefully correctly. But when I invoke the invert function on  amazonWeightsRDD, I am getting a syntax error. This is happening whether I do amazonWeightsRDD.flatMap(invert) or amazonWeightsRDD.map(invert)
amazonInvPairsRDD = (amazonWeightsRDD                    .flatMap(invert)                    .cache())

Input record::: RDD..take(2) ===>

[('b000jz4hqo', {'rom': 2.4051362683438153, 'clickart': 56.65432098765432, '950': 254.94444444444443, 'image': 3.6948470209339774, 'premier': 9.27070707070707, '000': 6.218157181571815, 'dvd': 1.287598204264871, 'broderbund': 22.169082125603865, 'pack': 2.98180636777128}), ('b0006zf55o', {'laptops': 11.588383838383837, 'desktops': 12.74722222222222, 'backup': 2.8015873015873014, 'win': 0.501859142607174, 'ca': 9.10515873015873, 'v11': 50.98888888888888, '30u': 84.98148148148148, '30pk': 254.94444444444443, 'desktop': 2.23635477582846, '1': 0.3231235037318687, 'arcserve': 24.28042328042328, 'computer': 0.6965695203400122, 'lap': 127.47222222222221, 'oem': 46.35353535353535, 'international': 9.44238683127572, 'associates': 7.284126984126985})]

def invert (record):
    pairs = []    for rec in record:        for token in rec[1].keys():            pairs.append((token, rec[0]))    return (pairs)
This is what I get back when invoke invert(amazonWeightsRDD.take(2)) 

PAIRS.... If I Pass in only 1 record, the function will be correctly returning a pair. But I wonder if this is correct if I pass two records at the same time. However, since I will be invoking with xxxRDD.flatMap()  or xxxRDD.map(), I believe the function will only be passed one RDD at a time, so I think the function should work correctly in that case.
[('rom', 'b000jz4hqo'), ('clickart', 'b000jz4hqo'), ('950', 'b000jz4hqo'), ('image', 'b000jz4hqo'), ('premier', 'b000jz4hqo'), ('000', 'b000jz4hqo'), ('dvd', 'b000jz4hqo'), ('broderbund', 'b000jz4hqo'), ('pack', 'b000jz4hqo'), ('laptops', 'b0006zf55o'), ('desktops', 'b0006zf55o'), ('backup', 'b0006zf55o'), ('win', 'b0006zf55o'), ('ca', 'b0006zf55o'), ('v11', 'b0006zf55o'), ('30u', 'b0006zf55o'), ('30pk', 'b0006zf55o'), ('desktop', 'b0006zf55o'), ('1', 'b0006zf55o'), ('arcserve', 'b0006zf55o'), ('computer', 'b0006zf55o'), ('lap', 'b0006zf55o'), ('oem', 'b0006zf55o'), ('international', 'b0006zf55o'), ('associates', 'b0006zf55o')]************************

Py4JJavaError  - TRACE....

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-123-c0069e3e180d> in <module>()
     39 # print googleInvPairsRDD.take(2) # TODELETE
     40 
---> 41 print 'There are %s Amazon inverted pairs and %s Google inverted pairs.' % (amazonInvPairsRDD.count(),
     42                                                                             googleInvPairsRDD.count())

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 295.0 failed 1 times, most recent failure: Lost task 0.0 in stage 295.0 (TID 952, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-123-c0069e3e180d>", line 14, in invert
AttributeError: 'str' object has no attribute 'keys'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
<p><p></p> <p></p></p> 
ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server
Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 425, in start
    self.socket.connect((self.address, self.port))
  File "/usr/lib/python2.7/socket.py", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
  hi,
i need to ask what is the way out to integrate apache spark with any frontend frameworks to create webapp.
eg-if i want to take the ratings from the user in for recommender system ,i will need UI,how will i integrate pyspark with UI like movielens My script works fine locally. I pass both the tests yet the autograder fails it. I tried reloading vagrant and ran all the cells; then I saved the new notebook and uploaded it to the autograder, but it still fails. What could be wrong?

My output
# TEST Movies with Highest Average Ratings and more than 500 Reviews (1c)
​
Test.assertEquals(movieLimitedAndSortedByRatingRDD.count(), 194,
                'incorrect movieLimitedAndSortedByRatingRDD.count()')
Test.assertEquals(movieLimitedAndSortedByRatingRDD.take(20),
              [(4.5349264705882355, u'Shawshank Redemption, The (1994)', 1088),
               (4.515798462852263, u"Schindler's List (1993)", 1171),
               (4.512893982808023, u'Godfather, The (1972)', 1047),
               (4.510460251046025, u'Raiders of the Lost Ark (1981)', 1195),
               (4.505415162454874, u'Usual Suspects, The (1995)', 831),
               (4.457256461232604, u'Rear Window (1954)', 503),
               (4.45468509984639, u'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)', 651),
               (4.43953006219765, u'Star Wars: Episode IV - A New Hope (1977)', 1447),
               (4.4, u'Sixth Sense, The (1999)', 1110), (4.394285714285714, u'North by Northwest (1959)', 700),
               (4.379506641366224, u'Citizen Kane (1941)', 527), (4.375, u'Casablanca (1942)', 776),
               (4.363975155279503, u'Godfather: Part II, The (1974)', 805),
               (4.358816276202219, u"One Flew Over the Cuckoo's Nest (1975)", 811),
               (4.358173076923077, u'Silence of the Lambs, The (1991)', 1248),
               (4.335826477187734, u'Saving Private Ryan (1998)', 1337),
               (4.326241134751773, u'Chinatown (1974)', 564),
               (4.325383304940375, u'Life Is Beautiful (La Vita \ufffd bella) (1997)', 587),
               (4.324110671936759, u'Monty Python and the Holy Grail (1974)', 759),
               (4.3096, u'Matrix, The (1999)', 1250)], 'incorrect sortedByRatingRDD.take(20)')
1 test passed.
1 test passed.
grader feedback
Movies with Highest Average Ratings and at Least 500 Reviews (1c)
-----------------------------------------------------------------
Traceback (most recent call last):
  File "", line 19, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect sortedByRatingRDD.take(20) I think I know the idea that transformation create another RDD without immediately begin the computation. An action will set off the chain of transformation defined.

My question I don't seen this label being used much in Spark documentation. Neither Python nor scala doc mention this. Is it consider self evident which class of action an API belongs to?

It is obvious to me collect() and take() are action. It is still not very intuitive to me to think of reduce as action. I missed the starting of the course and would like to catch up on all the assignments before course ends, so that I can pass with 80%. When is the end of course before which I need to submit all of them? Hello, I know I'm out of time for this lab, but I'm getting the wrong word count: I got 
There are 22571 tokens in the combined datasets
vs 22520 in the test. My tokenize function converted all letters to lowercase, replaced all \W characters with whitespace, and I also tried eliminating strings with length 0 but it didn't make any difference and I was wondering if it missed something, as it still passed all tests.
My map function for the amazonSmall and googleSmall works like this (first line is take(1) from amazonSmall, second is take(1) from amazonRecToToken):
[('b000jz4hqo', 'clickart 950 000 - premier image pack (dvd-rom)  "broderbund"')]
[('b000jz4hqo', ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund'])]

any ideas on what could have gone wrong? Adittion of War stories(or made up stories :P ). Examples of code which were really slow and were not obvious. How to correct them / achieve speedup , in lectures in future versions.

Assignments were almost excellent and description needs to be improved to make clear the goal. They were engaging and I do understand it takes lot of effort to make assignments which reduces unnecessary work for students.  Again assignments were excellent. 

A supplementary video at the end telling how can one can make their own cluster for spark and what cluster management frameworks are there and how to set them up. How to configure spark. I have tested the return values fro computeSimilarity function and it returns ok
Now I run the line similarityAmazonGoogle and the print statement after that.  Now it takes a very long time to respond or not at all. I got passed once, but I removed the print statement and tried several time but nothing came back in 10-15 minutes

However without the similarityAmazonGoogle, it responds fairly quickly

Is this because of the collect statement..

Any reason why?  My laptop is pretty good :) Hello,
Lab3 submit is completed.
So can you tip me about 4d - 4e - 4f.
I submitted my lab without them. Because i couldn't get 4d? Hi I am trying to solve lab 4 -2d I followed 2c exercise and I have the following line of code which seems to compile fine but test case is failing I dont know why please guide.

myModel = ALS.train(testRDD, 8, 5) //rank followed by iteration

code seems to compile but my test case is failing

The model had a RMSE on the test set of 0.623803940044

1 test failed. incorrect testRMSE I get this error after execute the second cell, dont know why, I didnt touch any line, and also restart the VM to be sure everything was fine:

NameError                                 Traceback (most recent call last)
<ipython-input-2-e0fbd92521f3> in <module>()
      1 numPartitions = 2
----> 2 rawRatings = sc.textFile(ratingsFilename).repartition(numPartitions)
      3 rawMovies = sc.textFile(moviesFilename)
      4 
      5 def get_ratings_tuple(entry):

NameError: name 'ratingsFilename' is not defined I can't figure out how to go from an RDD of lists [(word1, word2), (word2, word3)] (the unique words in each document) to an RDD of entries [word1, word2, word2, word3] (all the unique words of each document put together) - could someone please help?

(my next step will be to count the occurrences of each word - reduceByKey?)

Many thanks I am getting the following error while uploading my lab3 file.
Tokenize a String (1a)----------------------Traceback (most recent call last):  File "", line 28, in PicklingError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTIONCan't pickle builtin 


But all my test are passed %100, but I only got 80%. I am not sure what this error?
Could someone plain the problem to me.




 sims = similaritiesBroadcast.map(computeSimilarityBroadcast).take(5)
i get the following error when i try to print that :
ZeroDivisionError: float division by zero.

please help, thanks in advance
 Thanks everyone for making this course on edX! May someone help me with lab 4 2c please. I passed the following and it fails saying that it cannot reduce() empy RDD. In the test section some of them fail but some of them pass.

validationForPredictRDD = validationRDD.<Honor Code>


1 test passed.
1 test passed.
1 test passed.
1 test failed. incorrect errors[0]
1 test failed. incorrect errors[1]
1 test failed. incorrect errors[2]



ValueError                                Traceback (most recent call last)
<ipython-input-94-2bd5924c8941> in <module>()
     19                       lambda_=regularizationParameter)
     20     predictedRatingsRDD = model.predictAll(validationForPredictRDD)
---> 21     error = computeError(predictedRatingsRDD, validationRDD)
     22     errors[err] = error
     23     err += 1

<ipython-input-85-f014a08788a6> in computeError(predictedRDD, actualRDD)
     23 
     24     # Compute the total squared error - do not use collect()
---> 25     totalError = squaredErrorsRDD.reduce(lambda a,b:a+b)
     26 
     27     # Count the number of entries for which you computed the total squared error

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    740         if vals:
    741             return reduce(f, vals)
--> 742         raise ValueError("Can not reduce() empty RDD")
    743 
    744     def treeReduce(self, f, depth=2):

ValueError: Can not reduce() empty RDD Hi I am trying to solve lab4-2e and confused how to come up testForAvgRDD using the following instructions

Use the average rating that you just determined and the testRDD to create an RDD with entries of the form (userID, movieID, average rating).

When we do take on testRDD it is already in the form (userID, movieID, average rating) as shown in below second line how do we use 3.57409571052 and stuff into testForAvgRDD please guide.


The average rating for movies in the training set is 3.57409571052
[(1, 1193, 5.0)]
The RMSE on the average set is 0.0
 Timeout error happened during grading..help I'm very confused by this part of the lab.

For the first question, I computed a simple average: 
trainingAvgRating = trainingRDD.map(lambda x: x[2]).mean()
This appears to be right... Then the directions say: 
Use the average rating that you just determined and the testRDD to create an RDD with entries of the form (userID, movieID, average rating).
Average rating of what? testRDD contains predictions while trainingRDD contains actual values. Each userID and movieID only has one value: either a prediction (from testRDD) or the actual (from trainingRDD). No idea what we are supposed to be averaging.  

Can someone please explain what they are asking for? Thanks! I'm having problems getting myUnratedMoviesRDD

I'm not exactly too sure how to filter out the movies I have rated.  I've attempted to turn myRatedMovies into an RDD and then use the subtract function as well as trying to use filter to filter out movieIDs.

Nothing seems to be working.  Any help is appreciated.

Thanks. sfd Hello all,

I am surprisinlgy stuck on 3b for lab 3 that seems to be pretty straightforward but I guess it isnt as starightforward as I thought.

I made w1 and w2 variables that call the tfidf function with string1 and string2 respectively with idfsdictionary as the second args in the tfidf function and I got this error:
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-179-0cdd4b11271c> in <module>()
     16 cossimAdobe = cosineSimilarity('Adobe Photoshop',
     17                                'Adobe Illustrator',
---> 18                                idfsSmallWeights)
     19 
     20 print cossimAdobe

<ipython-input-179-0cdd4b11271c> in cosineSimilarity(string1, string2, idfsDictionary)
     10     """
     11 
---> 12     w1 = tfidf(string1, idfsDictionary)
     13     w2 = tfidf(string2, idfsDictionary)
     14     return cossim(w1, w2)

<ipython-input-174-c4d0b064b06f> in tfidf(tokens, idfs)
      9     """
     10     tfs = tf(tokens)
---> 11     tfIdfDict = {k: v*idfs[k] for k,v in tfs.items()}
     12     return tfIdfDict
     13 

<ipython-input-174-c4d0b064b06f> in <dictcomp>((k, v))
      9     """
     10     tfs = tf(tokens)
---> 11     tfIdfDict = {k: v*idfs[k] for k,v in tfs.items()}
     12     return tfIdfDict
     13 

KeyError: 'A'

Can I get help please? In any uncertain situation:
    .map(lambda ...)
 i dont understand how to join the two rdd 
fullCorpusRDD = amazonFullRecToToken.union(googleFullRecToToken)
i have been trying the above join.
any help shall be appreciated .
thanks in advance  Dear Instructors,
Thank you for a great course.  Exercises and labs were helpful in understanding the material.  I hope to revisit the python notebooks several times in the next year.

The effort was definitely lot more than 5-7 hours.  Lab 3 took more than 50 hours.  I think feedback from that was incorporated in lab 4.  I could see that the questions were more descriptive of what was needed.  I felt most of my time was spent doing "dimensional analysis", i.e., matching what is the input and what is the expected output.  This required a lot of scrolling.

An idea to counter this could be to vertically split the screen (along lines of what we do when we compare versions of a document) like scroll in window 1 scrolls in other.  In this setup, the left window will describe what is needed and right window will give ability to write code.

Thanks again for the course. We now have a queue page available for the autograder: http://52.27.38.42/ 

Using your edX anonymous student ID, you can watch your submission as it is processed by the autograder. Note that we've set the timeout for grading a submission to an hour, so the autograder may take up to an hour to return you feedback on your submission.

Your student ID is included in the feedback from the autograder. Hopefully I may be hallucinating after working through sleepless nights on this lab 3. The symptom is Autograder is giving idfsFullCount not defined error although it did pass the test successfully  during my internal tests.  Looking at the code sequence in the Python Book, the autograder error makes sense.

My core problem seems to be in the python notebook code the test code comes first before the actual computation itself:
My test code
# TEST Compute IDFs and TF-IDFs for the full datasets (4b)Test.assertEquals(idfsFullCount, 17078, 'incorrect idfsFullCount')Test.assertEquals(amazonWeightsRDD.count(), 1363, 'incorrect amazonWeightsRDD.count()')Test.assertEquals(googleWeightsRDD.count(), 3226, 'incorrect googleWeightsRDD.count()')
comes first in sequence  
where as the actual code for computing idfsFullCount comes later in my notebook!
i ( there is  one more code box in between):
# TODO: Replace <FILL IN> with appropriate codefullCorpusRDD = < Whatever code>idfsFull =< Whatever code>idfsFullCount =< Whatever code>
I tried to restart and reload the environment but it seems like it will read from the saved file any way.  It is quite possible that my mouse must have jumped around and did some unintended command short cuts. Is there a way to revert back to the original sequence of code for this 4(b) alone? I managed to complete Lab4 quite quickly, passing the autograder with no issues. However, my prediction results made no sense at all. They included movies that I thought I had given ratings to. The issue is with your instructions and tests at the very beginning.

1B

You instruct us to build movieNameWithAvgRatingsRDD:

(average rating, movie name, number of ratings)

As you can see, we map it without MovieID.

3A

You now want to use movieLimitedAndSortedByRatingsRDD to help us pick our movies and rate them.

This is merely a filtered and sorted version of movieNameWithAvgRatingsRDD, and thus only has these same fields:

(average rating, movie name, number of ratings)

As you can see, the MovieID is not included in this RDD, so how can we use it to pick and rate them?

In the second cell of this part, you even comment:

# Note that the movie IDs are the *last* number on each line. A common error was to use the number of ratings as the movie ID.

I'm thus assuming that the printed out RDD items should be in this form:

(average rating, movie name, number of ratings, MovieID)

Conclusion:

Unfortunately, going back to 1B to add the MovieID to movieNameWithAvgRatingsRDD will lead the last test to fail, so it is not an option if you want to pass the autograder.

You leave students with no real way to successfully pass this lab with results that make sense.

Personal comment:

It's a real shame that most of your labs were of poor quality in terms of instructions, containing logic mistakes over and over. Other than that, the course was really interesting, impressive and challenging. Losing hours trying to find what mistakes you left in the notebooks is a rather frustrating experience. Hello
I need some clearance about Lab 2f. 
As I understood we calculate each record occurence in tokens . The output might be like that:

{'autocad': 2, 'autodesk': 1, 'courseware': 2, 'psg': 1, '2007': 2, 'customizing': 2, 'interface': 2}

then we find the total occurence in each token of this tokens occurence in idfs.
For exaple
{'autocad': xxxx, 'autodesk': yyyyy....} 
After that we divide first of token total (which is 2+2+1+2+2=9) to each token total in idfs

like 
xxxx/ 9  yyyy/9  
am I right?
Thanks I submitted my lab3 a couple of hours ago but I got the following message. Am I right in understanding that the grace period ends today (based on UTC time we still have a couple of more hours to go). 


As for late submissions after 3 day grace period of the due date, you lose 20 points.
 Your final score is 80 I was kind of stupid and sent my full lab 3 results today (I though the grace period was until  June 29th 23:59). I completed them 100%, but due to my 'late submission' I received only 80%.
Nevertheless, I had already completed the lab 90% before the end of the grace period. So by sending the completed lab, I now lost 10%. Is there a way to remove the latest submission, so I can get my grade of 90% back?

Thx Hello, 

I've made a partial submission of the lab 3, with a total score of 89, currently I'm finishing the lab but it says late submissions will be penalized 20 points.
Does it means if I successfully finish the lab 3 and submit it after deadline my score will be 80? Here are some printed output for 2b when running testError = computeError(testPredicted, testActual)

I do not understand why the answer is wrong. 

print predictedReformattedRDD.collect()[((1, 1), 5), ((1, 2), 3), ((1, 3), 4), ((2, 1), 3), ((2, 2), 2), ((2, 3), 4)]print actualReformattedRDD.collect()[((1, 2), 3), ((1, 3), 5), ((2, 1), 5), ((2, 2), 1)]print matchingRDD (User ID, Movie ID)[((1, 3), (4, 5)), ((2, 2), (2, 1)), ((1, 2), (3, 3)), ((2, 1), (3, 5))]print squaredErrorsRDD.collect()[1, 1, 0, 4]print totalError6.0print numRatings4.0print mse 1.5 I found the formulation of part 3a a bit confusing. I understand that I have to create my own list of ratings with the movie IDs. This means that if I want to rate Godfather with a 5, I have to inster this line:
(myUserID, 858, 5),
Am I right?

And where do I find these IDs? I took them manually from the movies.dat file. Is that the right way to do?

Finally I did not understand the sentence:
# Note that the movie IDs are the *last* number on each line. A common error was to use the number of ratings as the movie ID.

What does the word last mean? The movie ID is the first number...

Thank you for your help! There are two ways to map a tuple (UserID, MovieID, Rating) to a tuple (UserID, MovieID))

a)
bRDD = aRDD.map(lambda (UserID, MovieID, Rating) : (UserID, MovieID))
b) 
cRDD = aRDD.map(lambda (x) : (x[0], x[1]))

I find the version a) much more readable. Does anyone know if there is a drawback in performance? Hi,
I've just submitted an assigment for Lab 3.
I hoped I managed before grace period ends (Lab 3 is June 26, 2015 at 00:00 UTC) not to loose 20 points.
Current Server Time is 2015-06-29 22:20:24 UTC (My local timne is 2015-06-30 00:20:24)

I thought grace period ends at midnight Monday->Tuesday but I received 80 points only.

Is it correct?

Regards
Pawel
 As much as I enjoyed the steep learning curve in lab1 and lab2, lab3 is killing me. I am 2 days late already and still stuck in 2c. I am a paying student and a full-time working software project manager and cannot cope with the workload of this course. Whoever did device this clearly did not have the working class in mind. So far- very frustrating, even though I enjoy the extremely well-laid-out labs and technology behind it and can appreciate the efford the course team put into this. However, this is no course for people who need to work for a living, and want to learn Data Science on such a high level. Please note that I do actively code with Python for 2-3 years. In lab 4 3e, first step: it is noted "that you can do this step with one RDD transformation." I managed to do this with 2 transformations, a filter and a map, but I wonder what method can be used to do it in one single transformation.
Anyone has a hint? As the course name suggests this is a 100.1x
Is there a series of course 100.2x, etc...? 

If yes, when are these planned?

Thanks What should be content of amazonWeightsRDD should be?

Can someone post amazonWeightsRDD.take(2) 
Hi,

  When I define the cosineSimilarity() method I get this output.

0.826297021229
  All my tests have passed up to this point.
  
I've made sure that I 'tokenize(string1)' and string2 before calling the tfidf() method,

               w1 = tfidf(tokenize(string1), idfsDictionary)
               w2 ...... etc

I've also made sure that my dotprod() method produces the sum of the product of values
*only* for tokens that appear in both dictionaries, as other people have pointed out.

I don't know what's wrong.
I'm late submitting this lab.
I've spent days working on it.
The instructions are not always clear, and I'm fed up with it.
Any help would be appreciated.

 This was a great course and a great experience. Challenging but achievable and well worth the time and effort spent.

I agreed with another commenter who noted the significant difference in effort required between Lab 3 and 4, but this was a welcome change when I was hoping I would get enough time to complete the last lab. Hey everybodyI'm facing this problem with question 4c (lab3) for the last 4 hours, so I decided to ask for help.My amazonNorms has:(['b000jz4hqo'], 262.3974984324429), (['b0006zf55o'], 307.15035476158107), (['b00004tkvy'], 297.61466829291277), (['b000g80lqo'], 46.18178962503383), (['b0006se5bq'], 138.37001973890693), (['b000ehpzv8'], 333.2922094728262), (['b00021xhzw'], 41.904883268027554), ...I have followed the steps below:1) amazonWeightsRDD -> map + collectAsMap()
2) amazonNormsBroadcast-> broadcast amazonNorms 

But I still get the error: 
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-611-ccf0bfaffcb7> in <module>()
      1 # TODO: Replace <FILL IN> with appropriate code
      2 
----> 3 amazonNorms = amazonWeightsRDD.map(lambda x: (x[0], norm(x[1]))).collectAsMap()
      4 amazonNormsBroadcast = sc.broadcast(amazonNorms)
      5 #googleNorms = googleWeightsRDD.map(lambda x: (x[0], norm(x[1]))).collectAsMap()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collectAsMap(self)
   1443         4
   1444         """
-> 1445         return dict(self.collect())
   1446 
   1447     def keys(self):

TypeError: unhashable type: 'list'


Just don't know what else to try.
Thanks. anyone has experience of building a computer cluster? For example, build a cluster with raspberry pi. I am thinking about building a cluster for parallel computing, but don't know how to go about it. Can someone give some insights and recommendations? Is this a document?

('b000jz4hqo', ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund']
Or is the entirety of corpusRDD a document?

[('b000jz4hqo', ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund']), 
('b0006zf55o', ['ca', 'international', 'arcserve', 'lap', 'desktop', 'oem', '30pk', 'oem', 'arcserve', 'backup', 'v11', '1', 'win', '30u', 'laptops', 'desktops', 'computer', 'associates']), 
('b00004tkvy', ['noah', 'ark', 'activity', 'center', 'jewel', 'case', 'ages', '3', '8', 'victory', 'multimedia']), 
('b000g80lqo', ['peachtree', 'sage', 'premium', 'accounting', 'nonprofits', '2007', 'peachtree', 'premium', 'accounting', 'nonprofits', '2007', 'affordable', 'easy', 'use', 'accounting', 'solution', 'provides', 'donor', 'grantor', 'management', 're', 'like', 'nonprofit', 'organizations', 're', 'constantly', 'striving', 'maximize', 'every', 'dollar', 'annual', 'operating', 'budget', 'financial', 'reporting', 'programs', 'funds', 'advanced', 'operational', 'reporting', 'rock', 'solid', 'core', 'accounting', 'features', 'made', 'peachtree', 'choice', 'hundreds', 'thousands', 'small', 'businesses', 'result', 'accounting', 'solution', 'tailor', 'made', 'challenges', 'operating', 'nonprofit', 'organization', 'keep', 'audit', 'trail', 'record', 'report', 'changes', 'made', 'transactions', 'improve', 'data', 'integrity', 'prior', 'period', 'locking', 'archive', 'organization', 'data', 'snap', 'shots', 'data', 'closed', 'year', 'set', 'individual', 'user', 'profiles', 'password', 'protection', 'peachtree', 'restore', 'wizard', 'restores', 'backed', 'data', 'files', 'plus', 'web', 'transactions', 'customized', 'forms', 'includes', 'standard', 'accounting', 'features', 'general', 'ledger', 'accounts', 'receivable', 'accounts', 'payable', 'payroll', 'solutions', 'time', 'billing', 'job', 'costing', 'fixed', 'assets', 'analysis', 'reporting', 'customization', 'easily', 'convert', 'quickbooks', 'sage', 'software']), 

...


('b0006se5bq', ['singing', 'coach', 'unlimited', 'singing', 'coach', 'unlimited', 'electronic', 'learning', 'products', 'win', 'nt', '2000', 'xp', 'carry', 'tune', 'technologies'])] Hi,

I am using Suse box to install virtualbox and i got the following error while bring up vagrant.

####
spark:~/myvagrant # vagrant init sparkmooc/base; vagrant up --provider virtualbox`Vagrantfile` already exists in this directory. Remove it beforerunning `vagrant init`.Bringing machine 'sparkvm' up with 'virtualbox' provider...==> sparkvm: Box 'sparkmooc/base' could not be found. Attempting to find and install...    sparkvm: Box Provider: virtualbox    sparkvm: Box Version: >= 0The box 'sparkmooc/base' could not be found orcould not be accessed in the remote catalog. If this is a privatebox on HashiCorp's Atlas, please verify you're logged in via`vagrant login`. Also, please double-check the name. The expandedURL and error message are shown below:URL: ["https://atlas.hashicorp.com/sparkmooc/base"]Error: Couldn't resolve host 'atlas.hashicorp.com'spark:~/myvagrant # vagrant box listThere are no installed boxes! Use `vagrant box add` to add some.spark:~/myvagrant #
##############

Any help appreciated..

Kumar I was not able to complete Lab 3 until today (Monday 29 June at about 5:20 Eastern time).  If there is supposed to be a 3 day grace period and the assignment was due Friday before midnight then why did I get the 80% for my submission?  Shouldn't I be within the 3 day grace period?  After I finished Lab 3, I wanted to reorganize the code for personal use. As I walked through the code, I found these questions. 

1. print statement in parseDatafileLine function never worked. Spark passed this statement, and ran the statement after it. 

def parseDatafileLine(datafileLine):
    """ Parse a line of the data file using the specified regular expression pattern
    Args:
        datafileLine (str): input string that is a line from the data file
    Returns:
        str: a string parsed using the given regular expression and without the quote characters
    """
    match = re.search(DATAFILE_PATTERN, datafileLine)
    if match is None:
        print 'Invalid datafile line: %s' % datafileLine
        return (datafileLine, -1)
    elif match.group(1) == '"id"':
        print 'Header datafile line: %s' % datafileLine
        return (datafileLine, 0)
    else:
        product = '%s %s %s' % (match.group(2), match.group(3), match.group(4))
        return ((removeQuotes(match.group(1)), product), 1)

2. When Spark is caching the RDD, does it cache the same RDD twice? I know the memory will only cache the same value once until memory runs out.

testRDD.cache()testRDD.cache()
As I was doing part (4e), I ran this block for many times for testing codes. I found out that my computer was unable to produce the same result with original speed after it ran this block for many times; however, if I restarted the Kernel and ran all blocks above and including this block, it will produce the output will original speed. Since RDD was cached, running the same block for many times should not take more and more time.

If you have any advice, please help me out with theses questions.

Thank you! i am able to get movieIDsWithRatingsRDD as below:

movieIDsWithRatingsRDD: [(2, <pyspark.resultiterable.ResultIterable object at 0xb0de704c>), (4, <pyspark.resultiterable.ResultIterable object at 0xb0de724c>), (6, <pyspark.resultiterable.ResultIterable object at 0xb0de726c>)]

For the following i am getting an error:
#movieIDsWithAvgRatingsRDD = <REDACTED>

---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-33-b3cb825a5b22> in <module>()
     11 #movieIDsWithAvgRatingsRDD = movieIDsWithRatingsRDD.map(getCountsAndAverages())
     12 movieIDsWithAvgRatingsRDD = movieIDsWithRatingsRDD.map(lambda a:getCountsAndAverages(a))
---> 13 print 'movieIDsWithAvgRatingsRDD: %s\n' % movieIDsWithAvgRatingsRDD.take(3)
     14 
     15 # To `movieIDsWithAvgRatingsRDD`, apply RDD transformations that use `moviesRDD` to get the movie

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1192         """
   1193         items = []
-> 1194         totalParts = self._jrdd.partitions().size()
   1195         partsScanned = 0
   1196 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _jrdd(self)
   2286         command = (self.func, profiler, self._prev_jrdd_deserializer,
   2287                    self._jrdd_deserializer)
-> 2288         pickled_cmd, bvars, env, includes = _prepare_for_python_RDD(self.ctx, command, self)
   2289         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(),
   2290                                              bytearray(pickled_cmd),

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _prepare_for_python_RDD(sc, command, obj)
   2204     # the serialized command will be compressed by broadcast
   2205     ser = CloudPickleSerializer()
-> 2206     pickled_command = ser.dumps(command)
   2207     if len(pickled_command) > (1 << 20):  # 1M
   2208         broadcast = sc.broadcast(pickled_command)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py in dumps(self, obj)
    409 
    410     def dumps(self, obj):
--> 411         return cloudpickle.dumps(obj, 2)
    412 
    413 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dumps(obj, protocol)
    814 
    815     cp = CloudPickler(file,protocol)
--> 816     cp.dump(obj)
    817 
    818     #print 'cloud dumped', str(obj), str(cp.modules)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dump(self, obj)
    131         self.inject_addons()
    132         try:
--> 133             return pickle.Pickler.dump(self, obj)
    134         except RuntimeError, e:
    135             if 'recursion' in e.args[0]:

/usr/lib/python2.7/pickle.pyc in dump(self, obj)
    222         if self.proto >= 2:
    223             self.write(PROTO + chr(self.proto))
--> 224         self.save(obj)
    225         self.write(STOP)
    226 

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    560         write(MARK)
    561         for element in obj:
--> 562             save(element)
    563 
    564         if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    631                 write(MARK)
    632                 for x in tmp:
--> 633                     save(x)
    634                 write(APPENDS)
    635             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    634                 write(APPENDS)
    635             elif n:
--> 636                 save(tmp[0])
    637                 write(APPEND)
    638             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    684                 k, v = tmp[0]
    685                 save(k)
--> 686                 save(v)
    687                 write(SETITEM)
    688             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    684                 k, v = tmp[0]
    685                 save(k)
--> 686                 save(v)
    687                 write(SETITEM)
    688             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    304             reduce = getattr(obj, "__reduce_ex__", None)
    305             if reduce:
--> 306                 rv = reduce(self.proto)
    307             else:
    308                 reduce = getattr(obj, "__reduce__", None)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in __getnewargs__(self)
    242         # This method is called when attempting to pickle SparkContext, which is always an error:
    243         raise Exception(
--> 244             "It appears that you are attempting to reference SparkContext from a broadcast "
    245             "variable, action, or transforamtion. SparkContext can only be used on the driver, "
    246             "not in code that it run on workers. For more information, see SPARK-5063."

Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.


 Hi,

I'm using filter((x,y,x):(y,z>500)) and test failed with the output:

Movies with highest ratings: [(5.0, u"Those Who Love Me Can Take the Train (Ceux qui m'aiment prendront le train) (1998)", 1), (5.0, u'Schlafes Bruder (Brother of Sleep) (1995)', 1), (5.0, u'Mamma Roma (1962)', 1), (5.0, u'Hour of the Pig, The (1993)', 1), (5.0, u'Gate of Heavenly Peace, The (1995)', 3), (5.0, u'Follow the Bitch (1998)', 1), (5.0, u'Dingo (1992)', 1), (5.0, u'Criminal Lovers (Les Amants Criminels) (1999)', 1), (5.0, u'Born American (1986)', 1), (5.0, u'Bittersweet Motel (2000)', 1), (5.0, u'Ballad of Narayama, The (Narayama Bushiko) (1958)', 2), (5.0, u'Baby, The (1973)', 1), (5.0, u'24 7: Twenty Four Seven (1997)', 1), (4.666666666666667, u'West Beirut (West Beyrouth) (1998)', 6), (4.666666666666667, u'Seven Chances (1925)', 3), (4.666666666666667, u'I Am Cuba (Soy Cuba/Ya Kuba) (1964)', 3), (4.666666666666667, u"Ed's Next Move (1996)", 3), (4.609756097560975, u'Sanjuro (1962)', 41), (4.6, u'Arguing the World (1996)', 5), (4.586330935251799, u'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)', 278)]

What the output should be, please help?! Hi All,
I was able to solve 4d without much issues given the amazing community had posted help and notes to assist with the solution with active engagement from the instructor, one thing I struggled with understanding why I couldn't use .keys() in my loop within the invert() function.
I had the right function as I was working on the example to unit test my code but when I will call it using flaMap, it won't work.
Kindly help me understand why when the below functions produce similar output, one works with the inverting the list and other one doesn't.
I am missing something fairly basic and may be this will allow me to better understand some concepts that I may have ignored to graps correctly.

Taking a small example below:
record= ((1, {'foo': 2,'new':3}),(2,{'bar':55}))recordRDD= sc.parallelize(((1, {'foo': 2,'new':3}),(2,{'bar':55})))recordlist= recordRDD.flatMap(lambda x:x).collect()for i in recordlist[1].keys(): print i

produces the same output as 
for i in recordlist[1]:print i I have done following for 2b...can someone help with this step

1.map for generating ((UserID, MovieID), Rating)
[((1, 1), 5), ((1, 2), 3), ((1, 3), 4)]

2.map for actualRDD generating ((UserID, MovieID), Rating)
[((1, 2), 3), ((1, 3), 5), ((2, 1), 5)]
3. power(predictedRDD joined with actual,2)

gives me error ..

TypeError: a float is required

Any clue or help will help me to progress this step

 I've written my code and am geting correct results, but the tests are failing (as shown in this screenshot).
 Any ideas why?

 Hi,

I am doing the following to get nonDupsRDD but it is giving me blank.

trueDupsRDD = (sims.join(goldStandard))
nonDupsRDD = (sims .leftOuterJoin(trueDupsRDD).filter(lambda x:x[1][1] == "None").map(lambda x:x[1][0]))

Please help Problem Solved. Heartfelt thanks to instructor to make the course available for us.

I'm taking on Scalable ML, following this course.

Bravo... and keep the great work Hello, 
I found the solution in 2 f. But I have a difficulty in format it. How can I do it. 
My result look like this
and I'm using decimal.Decimal(value).quantize(decimal.Decimal('.00000000000001'), rounding=decimal.ROUND_DOWN)
can anyone suggest me the correct format?
{'autocad': Decimal('33.33333333333333'), 'autodesk': Decimal('8.33333333333333'), 'courseware': Decimal('66.66666666666667'), 'psg': Decimal('33.33333333333333'), '2007': Decimal('3.50877192982456'), 'customizing': Decimal('16.66666666666666'), 'interface': Decimal('3.03030303030303')} This is what I am getting as the final similarity between the given googleURL and the amazonID: 






[('http://www.google.com/base/feeds/snippets/17242822440574356561', 'b000o24l3q', 0.00036282314410049604)]

I can't figure out why!




 Hi,

I am trying to print the following.

print myRatingsRDD.filter(lambda (x,y,z): y==1088).collect()
print moviesRDD.filter(lambda (x,y): x==1088).collect()
[(0, 1088, 5)]
[(1088, u'Dirty Dancing (1987)')]

if you look at the description in Part 3(a). we are seeing the following. So Movie Id 1088 is for "Shawshank Redemption, The 1994". But moviesRDD has the value "Dirty Dancing (1987)".  Am I doing something wrong here?

Most rated movies:
(number of ratings, (movie name, movie ID))
(4.5349264705882355, u'Shawshank Redemption, The (1994)', 1088) Please help check why it fails compilation.
lab4 (3c):

# TODO: Replace <FILL IN> with appropriate code
seed1 = 5Literations1 = 5regularizationParameter1 = 0.1bestRank1 = 8tolerance = 0.02myRatingsModel = ALS.train(trainingWithMyRatingsRDD, bestRank1, seed=seed1, iterations=iterations1,                           lambda_=regularizationParameter1)                                #myModel = ALS.train(trainingRDD, bestRank, seed=seed, iterations=iterations, #                    lambda_=regularizationParameter)

----------------------------
---------------------------------------------------------------------------Py4JJavaError                             Traceback (most recent call last)<ipython-input-38-fd0d56ff43db> in <module>()
7 tolerance = 0.02
8 myRatingsModel = ALS.train(trainingWithMyRatingsRDD, bestRank1, seed=seed1, iterations=iterations1, ---->
9 lambda_=regularizationParameter1)
10
11 #myModel = ALS.train(trainingRDD, bestRank, seed=seed, iterations=iterations, /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/recommendation.py in train(cls, ratings, rank, iterations, lambda_, blocks, nonnegative, seed)
138 seed=None):
139 model = callMLlibFunc("trainALSModel", cls._prepare(ratings), rank, iterations, -->
140 lambda_, blocks, nonnegative, seed)
141 return MatrixFactorizationModel(model)
142 /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/common.py in callMLlibFunc(name, *args)

118 sc = SparkContext._active_spark_context
119 api = getattr(sc._jvm.PythonMLLibAPI(), name) -->
120 return callJavaFunc(sc, api, *args)
121
122 /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/common.py in callJavaFunc(sc, func, *args)

111 """ Call Java Function """
112 args = [_py2java(sc, a) for a in args] -->
113 return _java2py(sc, func(*args))
114
115 /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args) 536 answer = self.gateway_client.send_command(command) 537 return_value = get_return_value(answer, self.gateway_client, --> 538 self.target_id, self.name) 539 540 for temp_arg in temp_args: /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 298 raise Py4JJavaError( 299 'An error occurred while calling {0}{1}{2}.\n'. --> 300 format(target_id, '.', name), value) 301 else: 302 raise Py4JError( Py4JJavaError: An error occurred while calling o2788.trainALSModel. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1397.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1397.0 (TID 563, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last): File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main process() File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process serializer.dump_stream(func(split_index, iterator), outfile) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 240, in dump_stream bytes = self.serializer.dumps(vs) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 402, in dumps return cPickle.dumps(obj, 2) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/recommendation.py", line 40, in __reduce__ return Rating, (int(self.user), int(self.product), float(self.rating)) ValueError: invalid literal for int() with base 10: 'Shawshank Redemption, The (1994)' at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135) at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:64) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) Hi,

I referred to all the posts regarding to this question.

But  I am still struggling with finding the N that is number of documents.

Can somebody share the correct direction to find N?

Thanks,
Prasad My codes were running well in Jupyter and passed every test cases. But whenever I submitted it to Autograder, it reported the following error massage. I don't have any idea where is the wrong?

Using ALS.train (2c)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect errors[0]

 I think I've narrowed it down to a division by zero argument. For this part all my previous parts run successfully. 

I use an RDD union,
<SNIP HONOR CODE VIOLATOIN>

My next line of code is applying the computeSimilarity to every record but then I get this error:

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-300-28eb93ab3b57> in <module>()
     18     return (googleURL, amazonID, cs)
     19 
---> 20 similarities = crossSmall.map(lambda rec: computeSimilarity(rec)).cache().collect()
     21 
     22 def similar(amazonID, googleURL):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 441.0 failed 1 times, most recent failure: Lost task 0.0 in stage 441.0 (TID 1546, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-300-28eb93ab3b57>", line 20, in <lambda>
  File "<ipython-input-300-28eb93ab3b57>", line 17, in computeSimilarity
  File "<ipython-input-296-80ba49727d8a>", line 13, in cosineSimilarity
  File "<ipython-input-294-a7f27788306b>", line 45, in cossim
ZeroDivisionError: float division by zero

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

 

Would love some help troubleshooting this problem.

Thank you,

Ray Xiao

 What is work of top function ?
[u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds']
why all word  start with z character come?
In output in start why "u" word see ?
 when will the course be available again ? due to work load in june I struggle to match up with the pace and hope i will do it religiously next time  . if i try to submit all my pending when will the last day to submit all the lab and assignments and how long discussion support be available .one more thing can i do the Scalable machine learning if i would skip this introductory course . please guide .  At point 1a i solved the challenge with this code : 

# manage the empty string  if(string == ' '):  return [] else: listSpace = re.split(split_regex, string.lower())  # remove the space if in it  # if '' in listSpace: listSpace.remove('') return listSpace

but i received the error in the grade process : 

submission token ID is 1318977-8a68a9d4a746e167aba9d51b30b3277d:ip-172-31-26-106Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 73.0 failed 1 times, most recent failure: Lost task 0.0 in stage 73.0 (TID 365, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 923, in 
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
TypeError: 'float' object is not iterable

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) I solved pretty all the exercises but i get some problem in 4a, 4c problabli raised from tokenizer function, so i post my function and i hope in a little hint to solve the problem ! My tokenize function pass all the test but maybe is wrong on some details !! 

A = [] strList = re.split(r'\W+', string.lower()) for word in strList: if word not in stopwords: A.append(word)  if '' in A: A.remove('') return A

thanks in advance !!  Hi!
1a looks like easy but I can't iterate or use the tuple. as an ex t = (1,3,4) print len(t) nothing comes or iterate over tuple by for loop using iteritems nothing. Could you help me using tuple.

Thanks If i round my average result to 1 decimal in order to pass 1a tests, 1b will not work. When i don't round my average, 1a test will fail, but also some test in 1b because number precision is not the same. What i am doin wrong?

My results:

[(3.6818181818181803, u'Happiest Millionaire, The (1967)', 22), (3.046822742474927, u'Grumpier Old Men (1995)', 299), (2.8829787234042543, u'Hocus Pocus (1993)', 94)]


Expected results

[(3.6818181818181817, u'Happiest Millionaire, The (1967)', 22), (3.0468227424749164, u'Grumpier Old Men (1995)', 299), (2.882978723404255, u'Hocus Pocus (1993)', 94)]

 What is wrong with the below code. Getting the error: 
TypeError: 'int' object has no attribute '__getitem__'

# TODO: Replace <FILL IN> with appropriate code
amazonRecToToken = amazonSmall.map(lambda s: (s[0], tokenize(s[1])))
googleRecToToken = googleSmall.map(lambda s: (s[0], tokenize(s[1])))
def countTokens(vendorRDD):
 """ Count and return the number of tokens
 Args:
 vendorRDD (RDD of (recordId, tokenizedValue)): Pair tuple of record ID to tokenized output
 Returns:
 count: count of all tokens
 """
<SNIP HONOR CODE VIOLAION>
totalTokens = countTokens(amazonRecToToken) + countTokens(googleRecToToken)
print totalTokens
However, similar code runs fine, shown below:
temp = sc.parallelize([("a", ['spanish','vocabulary','builder']), 
                       ("b", ['spanish','vocabulary','builder'])])
temp2 = temp.map(lambda x: (x[0], len(x[1])))
temp2.collect()
count = temp2.reduce(lambda x,y: x[1]+y[1])
print count
 I try to build the movieIDsWithRatingsRDD, but i can't get the hint -> 

This transformation will yield an RDD of the form: [(1, <pyspark.resultiterable.ResultIterable object at 0x7f16d50e7c90>), (2, <pyspark.resultiterable.ResultIterable object at 0x7f16d50e79d0>), (3, <pyspark.resultiterable.ResultIterable object at 0x7f16d50e7610>)]

how can get a iterable object from the function getCountsAndAverages()  ?? 

My try was (fail) : 

<REDACTED> I have a correct solution for lab 4f which I tested on a small version of commonTokens. However, when I run it on the full data set it takes too long (over half an hour now and still not complete).

How long did it take to finish for you guys? I got this exception.


It seems it's in the calculations of avgSimNon. Something in this module

What I am doing is
for nonDupRDD->filter for those RDD not in trueDupsRDD->then get the second elements
for avgSimNon->calculate the sum and divide by the # of elements in nonDupRDD

From "-->" I am seeing that the problem is in nonDupRDD. Is such filtering I have performed allowed? Why does it contradict with broadcasting?
 ... hello all,
I got the expected outcomes for all the variables in 1b. However, when I ran the test, 5 passed but one returned:
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-41-04a890f7bcc2> in <module>()
     21                 'incorrect movieIDsWithAvgRatingsRDD.takeOrdered(3)')
     22 
---> 23 Test.assertEquals(movieNameWithAvgRatingsRDD.count(), 3615,
     24                 'incorrect movieNameWithAvgRatingsRDD.count() (expected 3615)')
     25 Test.assertEquals(movieNameWithAvgRatingsRDD.takeOrdered(3),

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 113.0 failed 1 times, most recent failure: Lost task 0.0 in stage 113.0 (TID 88, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <genexpr>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "<ipython-input-39-226060d37801>", line 17, in <lambda>
TypeError: 'NoneType' object is not iterable

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

what could be the problem? Hi,

 I wonder why I've got wrong results in lab 4 2a. Any hint ? please!

Error for test dataset (should be 1.22474487139): 1.5
Error for test dataset2 (should be 3.16227766017): 10.0
Error for testActual dataset (should be 0.0): 0.0 After progressing well on this lab I get stuck on Question 2B and at the very beginning, I did not even managed to transform RRDs in the correct format: 'list' object has no attribute 'map'
How come?
I assume it is quite trivial but feel so stuck that I would really appreciate some hint.
Thanks.



aRDD=[((1, 2), 3), ((1, 3), 5), ((2, 1), 5), ((2, 2), 1)]
bRDD=[((1, 1), 5), ((1, 2), 3), ((1, 3), 4), ((2, 1), 3), ((2, 2), 2), ((2, 3), 4)]

def computeError(predictedRDD, actualRDD):
 # Transform predictedRDD into the tuples of the form ((UserID, MovieID), Rating)
 	predictedReformattedRDD = predictedRDD.map(lambda (UserID, MovieID, Rating): ((UserID, MovieID), Rating)) 
	return predictedReformattedRDD.take(3)

print computeError(aRDD,bRDD)
AttributeError                            Traceback (most recent call last)
<ipython-input-18-e27ee8333aa5> in <module>()
     26 
     27 
---> 28 print computeError(aRDD,bRDD)
     29 
     30 """# Compute the squared error for each matching entry (i.e., the same (User ID, Movie ID) in each

<ipython-input-18-e27ee8333aa5> in computeError(predictedRDD, actualRDD)
     15     """
     16     # Transform predictedRDD into the tuples of the form ((UserID, MovieID), Rating)
---> 17     predictedReformattedRDD = predictedRDD.map(lambda (UserID, MovieID, Rating): ((UserID, MovieID), Rating))
     18 
     19 

AttributeError: 'list' object has no attribute 'map'
Thanks for answer below. Now I understand what was wrong. -and do not feel so proud of myself- 

But sims RDD is a similaritiesBroadcast simply transformed.


Every transformation of the form: S\S\G=Null, where S and G are sets.

Could instructors please clarify this task? I am totally lost as to what is needed here. Hi.

groupByKey and reduceByKey work by different way (groupByKey does not produce preliminary combined result, but reduceByKey does and has bettter performance).
Does anybode know about combineByKey and aggregateByKey? What difference except first parameter in these functions (combineByKey takes zeroFunction, aggregateByKey - zeroValue)? Or aggregationByKey and reduceByKey are just implementation of combineByKey?
I used .reduceByKey(lambda x,y: x+" "+y) and map with split after that into lab3/4e but not sure that it's best way for doing it.

Collecting into list with combineByKey and aggregateByKey:
a = sc.parallelize([(1,1),(1,2),(1,3),(2,1),(2,5),(3,6),(3,7)], 5)
print (a .glom() .collect() )
print (a .combineByKey(lambda value: [value], lambda x,y: x+[y], lambda x,y: x+y) .collect() )
print (a .aggregateByKey([], lambda x,y: x+[y], lambda x,y: x+y) .collect() )
[[(1, 1)], [(1, 2)], [(1, 3), (2, 1)], [(2, 5)], [(3, 6), (3, 7)]][(1, [1, 2, 3]), (2, [1, 5]), (3, [6, 7])][(1, [1, 2, 3]), (2, [1, 5]), (3, [6, 7])] Great course, really useful to learn Spark.

As suggested by others, the instructions for each lab could be improved (I have noticed that lab 4 already goes in that direction).

I would probably suggest to develop shorter labs and more of them, this would allow students to become more confident along the learning process.

Looking forward to tackle the machine learning one :)

all the best to everyone:
David L.
 In the last output of 3f (i.e. top 20 highest rated movies), I also see the movies I rated (i.e; with ID I put in myRatedMovies), is it supposed to be the case ?

I checked myRatedMovies which has the form:

[(0, 1171, 5), ...]
Then in 3e I have the following step :

myUnratedMoviesRDD = (moviesRDD		     .filter(lambda (movieID, movieTitle): movieID not in [x[1] for x in myRatedMovies]		     ...)predictedRatingsRDD = myRatingsModel.predictAll(myUnratedMoviesRDD)
which normally should get rid of the movies I rated and only make predictions for the movie I have not seen.
Any though? This was possibly the most enriching online course I've ever taken. By far. Berkeley, hats off to you. I'm looking through the Quantum Mechanics/Computation archived course now, and I've signed up for the Scalable Machine Learning course.

1. The labs were brutal. I just started programming with Python barely 3 days before this course started, and I have loved it every step of the way.

2. I have programmed using the Spark transformations so much, I now feel awkward or outright limited if I cannot use those transformations. I'm going to download Spark and install it on Windows, not for the speed, but simply for the ease of use with large data rows.

3. The use of an external forum (one that my college uses, nonetheless) was a good idea. High levels of activity, huge amounts of information going around, honor code being stuck to. Great to see, great to be a part of.

4. The lectures were effective, succinct, and to the point. I liked them. "Reading off the slides" is not a bad thing. It lets people relax, knowing that the slides have all the information. Also helps people with limitations in Internet speed (welcome to India, where 70 kB/s is _fast_).

Overall, I loved the course, and I have already recommended it to quite a few friends of mine. It helped me learn Python and big data analysis in one fell swoop. I am still a newbie, but at least I now know how to map things. And that is a very, very empowering feeling. Thank you for the course. It was a pleasure. Will definitely be looking out for more courses from you guys. My laptop is taking a very long time to run cells in Lab 3. It got hung processing 4e. So I rebooted my machine, restarted vagrant and ran all the cells again. 3c took a long time to process (over 1.5 hours). I basically left my computer running overnight. In the morning it was done. I am now running 3d and it is taking a long time as well. 

3c and 3d are cells that I completed a couple of days ago. The code and the answers are correct. In fact, I've done all the coding until 4f where I'm actually stuck. But I need to run the cells before 4f. This exercise is taking days primarily because cells are taking so long to run. I have restarted my machine several times suspecting a hung process only to find out that nothing was wrong with the code. The cell was just taking long to process.

Any idea how I can speed up my computer? Or allocate more memory to running this faster?

It is a one year old computer. Lenovo Yogi 2. 8GB Ram. Intel i7. Windows 8.1 64bit. There are no program running other than Chrome.

Thanks. Instructors, this was an excellent course, thank you for what clearly has been much effort on your parts. Unfortunately I cannot currently commit the time to take CS 190, will this course be available for certification once it gets archived? Thanks. Any help is hightly appreciated

print "\nInput RDD"print (amazonWeightsRDD.take(1))
print "\nO/p of Invert"print invert(amazonWeightsRDD.take(1))
amazonInvPairsRDD = (amazonWeightsRDD .map(lambda x: invert(list(x))) .cache())print amazonInvPairsRDD.first()


Error / Output:


Invert func:

Input RDD
[('b000jz4hqo', {'rom': 2.4051362683438153, 'clickart': 56.65432098765432, '950': 254.94444444444443, 'image': 3.6948470209339774, 'premier': 9.27070707070707, '000': 6.218157181571815, 'dvd': 1.287598204264871, 'broderbund': 22.169082125603865, 'pack': 2.98180636777128})]

O/p of Invert
Type of record : <type 'list'>
[('rom', 'b000jz4hqo'), ('clickart', 'b000jz4hqo'), ('950', 'b000jz4hqo'), ('image', 'b000jz4hqo'), ('premier', 'b000jz4hqo'), ('000', 'b000jz4hqo'), ('dvd', 'b000jz4hqo'), ('broderbund', 'b000jz4hqo'), ('pack', 'b000jz4hqo')]






---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-122-a337ebb946b9> in <module>()
     19                     .map(lambda x: invert(list(x)))
     20                     .cache())
---> 21 print amazonInvPairsRDD.first()
     22 
     23 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in first(self)
   1240         ValueError: RDD is empty
   1241         """
-> 1242         rs = self.take(1)
   1243         if rs:
   1244             return rs[0]

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 703.0 failed 1 times, most recent failure: Lost task 0.0 in stage 703.0 (TID 1971, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-122-a337ebb946b9>", line 19, in <lambda>
  File "<ipython-input-122-a337ebb946b9>", line 7, in invert
AttributeError: 'str' object has no attribute 'keys'

 


 
I've calculated SE as an array of (x-y)^2 and now I have to sum them 

print squaredErrorsRDD.take(14)
[('aux', 1), ('aux', 1), ('aux', 0), ('aux', 4)]

I try to avoid using collect(), but if I use

totalError = squaredErrorsRDD.reduceByKey(sum)print totalError.take(2)

I get the following error, I cannot see the error, what I am doing wrong?

TypeError: 'int' object is not iterable Hello everyone,
    I joined very lately. I am interested in completing the assignment programs. Any one please give me proper meanings of dayToHostPairTuple, dayGroupedHosts ,dayHostCount ,dailyHosts. I am really getting confused. As this course comes to conclusion, I should like to thank all the course staff involved in producing this excellent course on Apache Spark using the Python language. And Offering it for free on EdX is a noble endeavor. May all your tribes increase!

I find this course quite interesting and fun - the labs made it challenging as well.

Apache Spark, I came to realized after learning its architecture, will be the technology of the future for cloud computing. Unless, of course, somebody comes out with a better technology later on.  However, its open source nature will make this quite difficult to happen. Perhaps, what remains for Apache Spark is to develop more API's (and perhaps libraries) that will cater to a much larger user needs and cover other areas of research as well.

This course also gave me the opportunity to sharpen my Python skills on lambda expressions and using function map. Prior to this course, I  rarely meet problems requiring use of these two functions. Finally, I learned how to apply machine learning techniques on data, a field that I have always been too curious to study.

In summary, my overall rating for this course is:

This is a course par excellence!!!

Again, thank you very much :) <REMOVED FOR VIOLATING PIAZZA TERMS OF USE AND EDX HONOR CODE> I just submitted last lab and quiz for BerkeleyX - CS100.1x. Grade is 99%. How can I get Honor code certificate?
Thanks! All,
 Is there a better way to represent the tuples in the lambda map or reduce operation, rather than doing rec[1][0][1]. It doesnt seem readable, unless I am missing something here.

Its not specific to a lab, but in almost all the labs, I used this indexes to get my output.

Thanks in advance # TODO: Replace <FILL IN> with appropriate codefullCorpusRDD = amazon.union(google)idfsFull = idfs(fullCorpusRDD)idfsFullCount = idfsFull.count()print 'There are %s unique tokens in the full datasets.' % idfsFullCount
# Recompute IDFs for full datasetidfsFullWeights = idfsFull.collectAsMap()idfsFullBroadcast = sc.broadcast(idfsFullWeights)
# Pre-compute TF-IDF weights. Build mappings from record ID weight vector.amazonWeightsRDD = amazonFullRecToToken.map(lambda x: (x[0],tfidf(x[1], idfsFullBroadcast.value())))googleWeightsRDD = googleFullRecToToken.map(lambda x: (x[0],tfidf(x[1], idfsFullBroadcast.value())))print 'There are %s Amazon weights and %s Google weights.' % (amazonWeightsRDD.count(), googleWeightsRDD.count())

The above code gives me issue 
TypeError: 'dict' object is not callablePlease help me where i am wrong Hi. I am trying to compute the squared difference in 2b by mapping elements of my squaredErrorsRDD to a pow function that makes use of x[0][1][0] and x[0][1][1]. I am getting an error that states
TypeError: 'int' object has no attribute '__getitem__'

I am missing something simple here and it has not dawned on me yet . Design's two steps for the first line of code:

1. Use map() to do a simple 2-tuple to 3-tuple transformation.
2. Use foldByKey() to create a list:

foldByKey([], lambda a,b: [a] + [b])
But for some reason the print statement after the first line of code fails on a lambda having wrong number of elements.

In simple python it works fine to add lists together like this: [1,2,3] + [9] which returns [1,2,3,9].

My question is, can anyone demonstrate that there is in fact some way to use foldByKey to build a list from integers?

Thanks for showing your demo if you are able.(Speculations are not useful) Hello all,

This looks like a fantastic course.  Unfortunately I just came across it today and I haven't found mention of a future offering.  Is this course expected to be offered again immediately upon completion of this instance or is that information not known?  Thanks in advance! Comparing your model:

hi all, from what i understand trainingAvgRating should be a number. i map trainingRDD, which has the form [(x, y, z)], to sum(z) / float(len(z)) but this does not print correctly:

The average rating for movies in the training set is PythonRDD[2647] at RDD at PythonRDD.scala:43

what am i doing wrong? thanks in advance


 Hello,

For the part:"Use the Python list myRatedMovies to transform the moviesRDD into an RDD with entries that are pairs of the form (myUserID, Movie ID) and that does not contain any movies that you have rated. This transformation will yield an RDD of the form: [(0, 1), (0, 2), (0, 3), (0, 4)]. Note that you can do this step with one RDD transformation"

I am not sure what needs to be done....I saw some help provided in another post but I was not able to figure it out especially when it is possible to do it in one RDD operation. Would really appreciate the help.  This is happening again.  The responses were fairly quick until 11:00 AM EST.  Now there is no response to programs starting 3C.
It appears like the cartesian product is slowing the system considerably.

Tried restarting kernel and vagrant a couple of times but did not help.

Is there a way to avoid the cartesian product so I can proceed with the the rest?

The problem is I am unable to test anything after 3C before I submit to autograder. 

Anyway just happy to be learning :)

 Hi all,

I like to thank all people who creates this course. I looking forward to the next one.

Kind Regard,

Ruud The lab unit tests need to be written to be runnable exactly ONE test at a time.  Currently, MANY tests are in each runnable block of unit test code.  The problem with the way the labs are written is that commenting some code out is needed to run exactly one unit test.  Commenting out unit tests is bad form.

The good news: This problem can be easily fixed with better unit test code design.

It's important to be able to run exactly one unit test at a time when developing software. Please consider improving your unit test code in these labs.

Additionally, in doing as I recommend, you will be providing good examples of correct unit test design, for the nascent software engineers at your university.

Thanks!

ps. Current unit test code in all labs is like this:
------------------
x = 
y =
------------------
test(x)
test(y)
------------------

It is better to be like this:
-----------------
x = 
test(x)
------------------
y=
test(y)
-----------------

Now you can work on one thing at a time. You don't need to modify (comment out) code that you did not work on yet.  There is less clutter and more clarity in the output of your current unit under test this way.

Hope this helps. I am totally lost. I do understand that I have to do
['key',{'key1':val1, 'key2':val2, 'key3':val3}] --> [('key1','key'), ('key2','key'), ('key3','key')]
But honesty, I dunno how to approach this thing, especially concisely. 
Instructions suggest it should be one line of code, but I am totally lost in dictionary and list comprehensions, how to iterate through record[1][0], but keep record[0] as the value for all of them.
 for i in record[1].keys():
        (i, record[0])
But how do I make a list out of this?

EDIT. Thank you instructors for misleading with
return (pairs) 
:( Made me lose like 3 hours with this piece of cake part.
 Hi all,

I have written a lambda function for "movieIDsWithAvgRatingsRDD" and the test fails.
here is my output:
[(2, 332, 3.174698795180723), (4, 71, 2.676056338028169), (6, 442, 3.7918552036199094)]
and this is the expected output:
[(1, (993, 4.145015105740181)), (2, (332, 3.174698795180723)), (3, (299, 3.0468227424749164))],
I am using a map and lambda function to that takes a tuple and return another tuple with the same key and another tuple for the value " (len(y), sum(y)/len(y))

I am also struggling to define "movieNameWithAvgRatingsRDD"
I am trying to use the "join" but it is giving me a nasty error. my plan is to:  moviesRDD.join(movieIDsWithAvgRatingsRDD)
and then use a lambda function but the join is giving me the error.

Any help greatly appreciated
Thanks
S Hi,

Can anyone recommend a good resource for how to set up a Hadoop - Spark environment on Amazon, for playing and learning?

Thanks Here is some lab code, as provided in the lab:

Test.assertEquals(movieNameWithAvgRatingsRDD.takeOrdered(3), [(1.0, u'Autopsy (Macchie Solari) (1975)', 1), (1.0, u'Better Living (1998)', 1), (1.0, u'Big Squeeze, The (1996)', 3)], 'incorrect movieNameWithAvgRatingsRDD.takeOrdered(3)')


I don't see how the lab can reliably expect these three movies to come out in the order shown. 1.0 is the same value for all 3 tuples.  It's still ordered correctly if Big Squeeze is zeroth.

Can you please explain how Autopsy can reliably be emitted first, even though all 3 movies here have the same 1.0 score? hi,

i am stuck on this question. i try to filter out values from moviesRDD that are not in myRatedMovies as such:

(x, y) for (x, y, z) not in myRatedMovies

but i get an error that says 

 File "<ipython-input-198-504bacba87e9>", line 6
    .filter((x, y) for (x, y, z) not in myRatedMovies))#<FILL IN>)
                                   ^
SyntaxError: invalid syntax

what is the problem? thanks #More legible tuple printingdef tp(atuple):  for t in atuple: print t

It is much easier to read a tuple with the above function, than the standard print function appearing all throughout the labs.

(Tip:  Are you new to python?  Fix the indentation if the function does not seem to work for you.)

Remember to comment out your extra prints for the autograder to be happy however!

Dear staff:  Maybe next time the course is offered, the lab author could use a formatted tuple printer function like mine, instead of making student leave in these hard to read prints which the autograder might require for grading.

Thanks!

Edit: There seemed to be some confusion about what printing code would be improved. In this case it is the code appearing in the lab template, not the code the student writes.  As template code, the printing code in the lab would need to be generic, so it would still work with any number of tuples in the result.  It would not operate correctly if the course instructors were to hard-code a tuple index into a generic print function in the template code that is intended to handle a variable number of tuples.  

Hope this helps clarify any confusion. Below code is not working. However, similar code works.
Not working:
def idfs(corpus): """ Compute IDF Args: corpus (RDD): input corpus Returns: RDD: a RDD of (token, IDF value) """    N = corpus.count()
    uniqueTokens = corpus.reduce(lambda a,b: set(a[1]) | set(b[1]))
    return uniqueTokensidfsSmall = idfs(amazonRecToToken.union(googleRecToToken))
uniqueTokenCount = len(idfsSmall)#WORKING CODEtemp = sc.parallelize([("a", ['spanish','vocabulary','builder']), 
                       ("b", ['spanish','vocabulary','builder'])])
count = temp.reduce(lambda x,y: set(x[1])|set(y[1]))
print count Lab 4 , 1A

My function passes for float but fails for int and the xrange(20) test cases.
Any ideas?

I got it. Thanks. :) Hi,

at Lab4 1b i gor the first 6 Test as passed but a exception on:

    movieNameWithAvgRatingsRDD.count()

Suggestions ? ( i did some magic with list() on:

pyspark.resultiterable.ResultIterable object's

could be the couse of all this?)

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1161.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1161.0 (TID 739, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <genexpr>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "<ipython-input-84-65acd04f29a0>", line 26, in <lambda>
IndexError: list index out of range
 My notebook passed all tests locally... but grader is not accepting this case.I wonder why?Please adviceTraceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect errors[0]

Testing Your Model (2d) I just noticed main language Spark is developed in is Scala, is it possible to redo all exercises in Scala instead of python? I accidentally zoomed out all cells of the lab4 notebook. How to zoom back in?  In the countTokens function I have mapped the vendorRDD using the length function on the tokenized description. That should give the number of tokens in the description of the url. So, I am lost as to how I am supposed to extract just the number of tokens from the RDD, any hints? this one should be easy but need some help
I can't seem to evaluate either of the two sets of lines in my jupyter
set 1
T = len(1,2)

set 2
mytuple = (1,2)len(mytuple)

in both cases I get



---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-59-880e2a9c6bcf> in <module>()
     17 
     18 mytuple = (1,2)
---> 19 len(mytuple)
     20 
     21 #sum_values= sum(T)


TypeError: 'int' object is not callable
 I am getting the following error when I test. 
NameError: name 'Broadcast' is not defined
The results are correct, but some place Broadcast needs to be defined. Not sure where.
Can I cast it to a Broadcast variable? If so how/
The counts are correct.

Thanks

Ram

Test.assertTrue(isinstance(amazonNormsBroadcast, Broadcast), 'incorrect amazonNormsBroadcast') My list is:

myRatedMovies = [     (0, 1088, 5),     (0, 603, 5),     (0, 789, 5),     (0, 1300, 5),     (0, 1129, 5),     (0, 1218, 5),     (0, 1039, 5),     (0, 1250, 5),     (0, 1248, 5),     (0, 1039, 5),     (0, 1370, 4),     (0, 1013, 3),     (0, 560, 5),     (0, 693, 5),     (0, 705, 5),     (0, 658, 5)    ]
Why predictAll returns me rating bigger than 5?

My highest rated movies as predicted (for movies with more than 75 reviews):
(5.436774945185716, u'Usual Suspects, The (1995)', 50)
(5.313797267340874, u'Paths of Glory (1957)', 1178)
(5.262157275240231, u'Requiem for a Dream (2000)', 3949)
(5.261882238258949, u'Life Is Beautiful (La Vita \ufffd bella) (1997)', 2324)
(5.231087240176752, u'Shawshank Redemption, The (1994)', 318)
(5.2251753833412895, u'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)', 2019)
(5.224378947449326, u'Killer, The (Die xue shuang xiong) (1989)', 1218)
(5.223423855237172, u"One Flew Over the Cuckoo's Nest (1975)", 1193)
(5.202642916711039, u'Wrong Trousers, The (1993)', 1148)
(5.185017489282609, u'Gaslight (1944)', 906)
(5.1770194759290975, u'Christmas Story, A (1983)', 2804)
(5.170729082604328, u'Pulp Fiction (1994)', 296)
(5.170403943418809, u'Godfather, The (1972)', 858)
(5.168505477717094, u'Casablanca (1942)', 912) Hello everybody.

I did a cogroup and I have a RDD:

firstRDD
[(2049, (<pyspark.resultiterable.ResultIterable object at 0xb1f35fac>, <pyspark.resultiterable.ResultIterable object at 0xb1f35b6c>)), (3, (<pyspark.resultiterable.ResultIterable object at 0xb1f35ccc>, <pyspark.resultiterable.ResultIterable object at 0xb1f357cc>)), (2052, (<pyspark.resultiterable.ResultIterable object at 0xb1f352ec>, <pyspark.resultiterable.ResultIterable object at 0xb1f3542c>))]

How can I transform the object 

pyspark.resultiterable.ResultIterable object

in the right value.

Thanks in advance

Carlota Vina




















 I've submitted the Labs and get a full grade, but I keep wondering if my solution is the same as the instructor solution. I think it would be a good idea to post the instructor solutions to the Labs, maybe at the end of the course. I think this will add more to the learning process. Being new to Python and Spark, all of this looks like hieroglyphs to me and it tested my pattern recognition skills.

Many thanks, I took Coursera's Mining Massive Datasets MOOC last year and I highly recommend it if it's offered again. The textbook is very well written and freely available. This MOOC is very complementary with this course: "Mining Massive Datasets" is more about the various big data algorithms from a conceptual perspective whereas this course focuses more on the practical implementation of big data pipelines. I am kind of stuck in  lab3 here, can somebody help me with the solution please?

# TODO: Replace <FILL IN> with appropriate code
def idfs(corpus):
    """ Compute IDF
    Args:
        corpus (RDD): input corpus
    Returns:
        RDD: a RDD of (token, IDF value)
    """
    N = corpusRDD.count()
    print corpus.take(3) 
    uniqueTokens = corpus.map(lambda x: x[1].split(" "))
    #.filter(lambda x: x != " ").map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)
    print uniqueTokens.collect()
    #tokenCountPairTuple = uniqueTokens.<FILL IN>
    #tokenSumPairTuple = tokenCountPairTuple.<FILL IN>
    #return (tokenSumPairTuple.<FILL IN>)

idfsSmall = idfs(amazonRecToToken.union(googleRecToToken))
uniqueTokenCount = idfsSmall.count()

print 'There are %s unique tokens in the small datasets.' % uniqueTokenCount
 Special Thanks! to the instructors and all of you on here!
I had no idea I could pass this course, especially as I had limited python knowledge and zero Spark knowledge.

But, I did quite well, actually, despite a 40% score for lab 3, I got an A.
Thanks again and Bravo!
 I have compiled a solution that looks (after hours of reading the forum) totally the same to what everyone claims to work.
But I am still getting this. Could you please give me a hint WHAT EXACTLY this error message says?


 

I got this that differs from question sample data. Any suggestion?

My highest rated movies as predicted (for movies with more than 75 reviews):
(4.365498488094911, u'Matilda (1996)', 89)
(4.282234688830012, u'True Romance (1993)', 308)
(4.227207673735606, u'Big Hit, The (1998)', 89)
(4.2129470666048405, u'Terminator 2: Judgment Day (1991)', 1370)
(4.166089902916706, u'Matrix, The (1999)', 1250)
(4.155143012604221, u'Once Were Warriors (1994)', 78)
(4.137784032904476, u'Tombstone (1993)', 242) Hi,

I just wanted to say that I've enjoyed the course and appreciate all of the hard work that went into developing and running the course. After spending too much time on Lab 3, I appreciate that you reworked and cut back on Lab 4. I also appreciate the Piazza contributors who had the time to debug issues in the Lab instructions and provide timely input to Piazza for the rest of us. 

One thing I would have liked to see is a little more attention given to memory consumption by the labs. During Lab 3 I had to start shutting down processes on my 16GB MacBook in order to free up memory when using the large data sets. So it would be nice if the lab instructions could give some guidance on expected memory consumption during the different steps of the lab since we are running everything on a single machine and are not taking advantage of distributing the data across multiple machines.
 It would be great to see some stats/breakdown once the course is over.

Great course. Thanks to all involved. hello,

I need support to submit correctly my Lab3
I made a mistake : I thought that we need'nt fill all the answers where <FILL IN> is specified.
And at that moment maybe my note was around 20 or 30%, I have used many submissions for a null results because I wanted to know what was my percentage of tests passed.
 
Now my situation is the following: I have had "all tests passed" with the notebook in Jupyter, when I ran the Part 1, 2 and 3 and I have begun the part 4 with the question 4a for which the test passed  also.
Now I have used 9 tries. I wouldn't like not to be evaluated. My actual grade must be around 75%. And I would prefer obtain an evaluation now.

Below the last errors I have had when I submitted my file "lab3_text_analysis_and_entity_resolution_student.py" with the autograder. I would really like to be evaluated whithout finish answering the last questions of Part4.


Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 16, in 
  File "/ok/submission.py", line 262
SyntaxError: Non-ASCII character '\xc3' in file /ok/submission.py on line 263, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'simpleTokenize' is not defined

Removing stopwords (1b)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'tokenize' is not definedEtc ............

I have carefully inspected my .py file so that not to leave any print debug statements.
And I have filled the last <FILL IN> sections of Part 4 whith some bad answers
Or do I have to absolutely fill all the last questions ?

Can you give me good advice not to lose all my work for this Lab, to obtain an evaluation and be able to quickly  pursue after with Lab4 ?

Thank you very much for your support

Below my submission token Id:

1368573-e0ae4f8c2e05d76470d7b886eb46a647:ip-172-31-18-130

 Hi,

Lecture 4 (RESILIENT DISTRIBUTED DATASETS (ABSTRACTION)) looks like it is supposed to have two different videos, but both of them are the same. Based on the transcripts, the top one appears to be wrong.

I just joined this class since I learned about it from cs190.1x. 
For movie recommendations, we start with a matrix whose entries
are movie ratings by users (shown in red in the diagram below).
Each column represents a user (shown in green) and each row 
represents a particular movie (shown in blue). 
I believe this description of the movie ratings matrix is wrong. Each matrix entry {uimj} represents the rating by user ui of movie mj. Each column j contains movie mj's ratings. Each row i contains user ui's ratings. The matrix rows and columns do not have colors. Hi All,
 I calculated this outside of the function for easy debugging.
I used following algorithm:
Starting point is testPredicted, testActual
Convert 2 RDDs into two RDDs in the format (
(UserID, MovieID), Rating) ( R1, R2)

Join R1 and R2 on keys. You get 4 elements in the resultant RDD
Above gives following
[((1, 3), (4, 5)), ((2, 2), (2, 1)), ((1, 2)[((1, 3), 1.0), ((2, 2), 1.0), ((1, 2), 0.0), ((2, 1), 4.0)], (3, 3)), ((2, 1), (3, 5))]

To above apply map lambda to calculate squared errors for each element. This gives:
[((1, 3), 1.0), ((2, 2), 1.0), ((1, 2), 0.0), ((2, 1), 4.0)]

Get values from above RDD using RDD built-in values()
This gives following:
[1.0, 1.0, 0.0, 4.0]

Apply sum() built-in, you get 6.0
There 4 elements common in the join. 6/4=1.5

The expected answer is 1.22474487139
Please tell me what is wrong in the above logic.

Thanks in advance

 Understand the formula given for Alternating Least Squares would
be helped if all if its terms were defined. In particular, what
are w, rij, Nbrs(i), and $$\lambda$$?
 Thanks to all who posted here.  I don't do much posting, but I read many.  Y'all are my heros for not just figuring out how, but leaving bread crumbs for the rest of us.  Staff, that you for an excellent adventure! 

This code works fine and I get the tuples as expected. However, when I go to run the tests, here is what I get:

1 test passed.1 test passed.1 test passed.1 test passed.1 test passed.1 test passed.So the first six tests pass, but the last two fail:

Test.assertEquals(movieNameWithAvgRatingsRDD.count(), 3615, 
'incorrect movieNameWithAvgRatingsRDD.count() (expected 3615)')Test.assertEquals(movieNameWithAvgRatingsRDD.takeOrdered(3), [(1.0, u'Autopsy (Macchie Solari) (1975)', 1), (1.0, u'Better Living (1998)', 1), (1.0, u'Big Squeeze, The (1996)', 3)], 'incorrect movieNameWithAvgRatingsRDD.takeOrdered(3)')

Why does movieNameWithAvgRatingsRDD.count() fail? As does movieNameWithAvgRatingsRDD.takeOrdered(3)?

Cheers
Ram
 I am trying to work through lab 2 and discovering it is near impossible because I am completely unable to print out the contents of an RDD at any stage. I get a syntax error which points to any line with a collect() or take(x) statement in it. It's very strange and since I don't know what I've gotten to, I am not sure what the next step should be. Any guidance on how to get around these syntax errors when simply trying to peer inside an RDD and see what's in there at a certain point? Thanks,
Jen

dayToHostPairTuple = access_logs.map(lambda log: ((log.datetime.day, log.host), 1)   )

dayGroupedHosts = dayToHostPairTuple.groupByKey()

dayHostCount = dayGroupedHosts.map(lambda (x,y): (set(x), y) )

print dayHostCount.take(5)==========ERROR BELOW==========
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-24-7db37eadc159> in <module>()
      7 dayHostCount = dayGroupedHosts.map(lambda (x,y): (set(x), y) )
      8 
----> 9 print dayHostCount.take(5)
     10 
     11 #dailyHosts = (dayHostCount

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
 Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
NameError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
global name 'parseDatafileLine' is not defined

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'totalTokens' is not defined

Amazon record with the most tokens (1d)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'biggestRecordAmazon' is not defined

Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'corpusRDD' is not defined

Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'uniqueTokenCount' is not defined

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'rec_b000hkgj8k_weights' is not defined

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsSmallBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonFullRecToToken' is not defined

Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'idfsFullCount' is not defined

Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonNormsBroadcast' is not defined

Create inverted indicies from the full datasets (4d)
----------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'amazonInvPairsRDD' is not defined

Identify common tokens from the full dataset (4e)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'commonTokens' is not defined

Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similaritiesFullRDD' is not defined

I don't understand how it is telling me that those variables are not defined. I looked into my python code and see that all of them are defined.My submission id is :
1380414-be96b5d98451e9efb593552158fc5ffd:ip-172-31-31-103
  Hi.

How to calculate RMSE,
How to calculate squaredErrorsRDD and what the ouptput should be?!
Please need help.
Thanks.
 Thanks for putting up this class, it was very helpful.  The main motivation for me to finish the class is because I paid the 50 bucks for the certificate. :)  The class material is adequate, I can sense the instructor and his assistance have spent countless hours to preparing the materials for the class, especially the labs.  Although lab 3 is very hard, I was able to complete it after a few days and gathering of some useful information in Piazza.  To whose who can finish every lab under 2 hours, and in Ivy League school. You will have a bright future. I would like to get some clarification on :

Using the two inverted indicies (RDDs where each element is a pair of a token and an ID or URL that contains that token), create a new RDD that contains only tokens that appear in both datasets. This will yield an RDD of pairs of (token, iterable(ID, URL)).
if a token(t1) belongs to two Google url(g1,g2), two amazon IDs( a1, a2).

what is the expected result?

So it would be a 4 records set:
g1, a1, t1
g1, a2, t1
g2, a1, t1
g2, a2, t1

 def computeSimilarity(record): """ Compute similarity on a combination record Args: record: a pair, (google record, amazon record) Returns: pair: a pair, (google URL, amazon ID, cosine similarity value) """

I may be picky,
but (google URL, amazon ID, cosine similarity value) does not look like a  pair,
I guess it meant  ( (google URL, amazon ID), cosine similarity value) 

or just change 'pair' to 'tuple' I just wanted to understand what do you mean ranks? In one accidental click on something a cell became formatted into a huge font (first several words colored blue), then just like a text.
Double click makes all cell typed in huge font.
Double click returns it back to this strange state.
Either way, the text in the cell is same as it was, but it would not execute by ctrl-Enter

Any advice? Thanks Hi,

This is the output of 1b:
movieIDsWithRatingsRDD: [(2, <pyspark.resultiterable.ResultIterable object at 0xb0f0a68c>), (4, <pyspark.resultiterable.ResultIterable object at 0xb0eb636c>), (6, <pyspark.resultiterable.ResultIterable object at 0xb0eb6b2c>)]
movieIDsWithAvgRatingsRDD: [(2, (332, 3.174698795180723)), (4, (71, 2.676056338028169)), (6, (442, 3.7918552036199094))]
movieNameWithAvgRatingsRDD: [(2049, u'Happiest Millionaire, The (1967)', 22), (3, u'Grumpier Old Men (1995)', 299), (2052, u'Hocus Pocus (1993)', 94)]

But the test show failing at the final step:
1 test passed.1 test passed.1 test passed.1 test passed.1 test passed.1 test passed.1 test passed.1 test failed. incorrect movieNameWithAvgRatingsRDD.takeOrdered(3)

I have print:
print movieNameWithAvgRatingsRDD.takeOrdered(3)
it gives: [(1, u'Toy Story (1995)', 993), (2, u'Jumanji (1995)', 332), (3, u'Grumpier Old Men (1995)', 299)]

I have used join with moviesRDD and movieIDsWithAvgRatingsRDD, do I need to use different join such left or right ?

Could you please help me to solve the issue.

Thanks in advance. Hello,

I must have missed something. 
What's a "Python iterable of Ratings for MovieID" and does it look like this?

<pyspark.resultiterable.ResultIterable object at 0x7f16d50e7c90>

I get this result:

movieIDsWithRatingsRDD: [(2, <pyspark.resultiterable.ResultIterable object at 0xb0c5df0c>), (4, <pyspark.resultiterable.ResultIterable object at 0xb0c5df6c>), (6, <pyspark.resultiterable.ResultIterable object at 0xb0dd92ec>)]

movieIDsWithAvgRatingsRDD: [(2, (332, 3.174698795180723)), (4, (71, 2.676056338028169)), (6, (442, 3.7918552036199094))]

movieNameWithAvgRatingsRDD: [(2049, (u'Happiest Millionaire, The (1967)', <pyspark.resultiterable.ResultIterable object at 0xb0dba1cc>)), (3, (u'Grumpier Old Men (1995)', <pyspark.resultiterable.ResultIterable object at 0xb0dba30c>)), (2052, (u'Hocus Pocus (1993)', <pyspark.resultiterable.ResultIterable object at 0xb0dba24c>))]


and the tests all good except for the last one:



1 test passed.
1 test passed.
1 test passed.
1 test passed.
1 test passed.
1 test passed.
1 test passed.
1 test failed. incorrect movieNameWithAvgRatingsRDD.takeOrdered(3)


Thanks Hi,

Could you please upload the consolidated video files for all the lectures of this course.
It would be very much helpful to download a single zip file with all videos.

For future refererence it will be helpful.!!

Thanks. Hi all,
i am using the following logic for testForAvgRDD :  testRDD.map(lambda (userID, movieID, rating) : (userID,movieID,float(sum(rating))/count(rating)))

I am getting the error : 'float' object is not iterable

I have tried using the mean function too, It was returning an error.
Any help here would be great.  Hi,
 
I paid for CS 100.1X verified certificate on 25th June but the id verification was completed yesterday after several retries. Will I still get the verified certificate because the edx dashboard says honor code as of now?
 
Thanks,
Akul I think in the instructions to 3e:

"Use the Python list myRatedMovies to transform the moviesRDD into an RDD with entries that are pairs of the form (myUserID, Movie ID) and that does not contain any movies that you have rated."

"moviesRDD" should be replaced with "ratingsRDD"

so, according to me it should read:

"Use the Python list myRatedMovies to transform the ratingsRDD into an RDD with entries that are pairs of the form (myUserID, Movie ID) and that does not contain any movies that you have rated. Thanks for a great course. It was tough going but well worth the effort. Thanks again. 



 Hi,

I want to re-use sparkVM for my features practice. as per the instructions, currently I am using ipython note book to edit to completed the labs. Please let me know how to re use this VM. Here are my question:

1. How to access sparkVM from my local windows machine(like putty etc..).
2. How to connect sparkVM and run programs that I have wrote in anconda spyder(Python IDE) which is install in my local windows machine.
3. Any other ways/applications need to use for accessing to rum my programs on sparkVM from local machine.

Regards,
Sivaji The instructions for Lab 4 was so much better documented to the previous lab that I literally waste no time at all.  

It was a huge improvement, at least for me. In the previous labs, I've encounter a few ambiguities whereas I passed the not so stringent test and due to the lack of better wordings, I had to back track a few steps to resolve the problem ultimately.

 Kudos to the team for the huge improvement in Lab 4.  Thanks again for the great learning experience. Hello. 
1399861-487e35ed1478712c57e1f763cc84d0ff:ip-172-31-31-99
It raises this.
Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
PicklingError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
Can't pickle builtin 

The rest is flawless. I had no exceptions in my notebook. I have passed with 100%-20 points=80. What could have caused this Traceback? I am not the most expert on Math. But interesting question would be: Can Apache Spark take advantage over traditional WLS to perform state estimation? Thanks. All my tests are passing. But when i do autograder it fails for 2c. I tried resubmitting couple of times but with same error. I am pasting my submission tokenID.

Your submission token ID is 1404390-009281225878b704ba45adcd2d69ea20:ip-172-31-31-96

Using ALS.train (2c)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect errors[0]
  I'm happy to say I passed lab 4 with 100%, but I have a lingering question - the RMSE results for each ranking are in limits but they're quite different to the target values in the Test.assert statements
I get:
For rank 4 the RMSE is 0.892734779484
For rank 8 the RMSE is 0.890121292255
For rank 12 the RMSE is 0.890216118367
The best model was trained with rank 8

Is this normal? The test case results for computeError() were spot on. Hello there, I've been stuck on this lab 4 ages. Even after accepting that I will bear the 20% deduction, I seem not to be shifting. The autograder times out.
Going through the lab for like the 100th time, I realise that my Issues began with Qn 3C on the similar() function where I use a collect().

(Sorry for putting code here but at this rate I have to.)
We've been told to refrain from using collect() but what's the best alternative?
Plus if we are to work with huge amounts of data in future, should we simply do away with the collect() method??
I thought it was there to be utilized.Thanx Hi, I am confused by the definition of rank in the text of the notebook. Isn't rank k the number of columns in user's matrix which is = to the number of rows in the movies (horizontal vector)? so basically we get: uxk  X kxm = uxm 
The sentences in the text of part 2c suggest otherwise!! I have completed the course and I have already verified my Id.... So, I would like to know when and how I will obtain a certificate for this course.

Thanks in advance. How do I upgrade / pay for a certificate for this course? I did not "upgrade to Verified Track" at first because I did not think I would complete this course, but now I have completed the course and have achieved a high grade, so I would like to pay for the certificate. Can anyone help me? thank you it works (I pass the test) but I find it weird since that function is supposed to take as argument a tuple with 2 entries. 
However we provide it with a 3-entries tuple.  What does it mean, "Remember to use `float` for calculation"
I am getting a weird error : 
TypeError: 'float' object has no attribute '__getitem__' Hi there,

For lab4 2b, just the first test/result is wrong the others are ok,any idea ?

I find 1.0 where I should find 1.22474487139

# TEST Root Mean Square Error (2b)Error for test dataset (should be 1.22474487139): 1.0Error for test dataset2 (should be 3.16227766017): 3.16227766017Error for testActual dataset (should be 0.0): 0.0 it's exactely the same mistake as https://piazza.com/class/i9esrcg0gpf8k?cid=4054#


Any help is welcome,regards, whats wrong with this code??

MovieID=<REDACTED>
return (MovieID,(Numberofratings, averageRating))

out of three tests, 2nd one is passed, 1st and 3rd are failed,
help required!! Hello,

I have completed all exervices in Lab 4. I execute them and all test pass. I have even test to re-start the kernel and it is the same, I achieve to pass all the test. But when I upload the file to the autograder, I got an error in the 2C test. Below is the log. I woudl like to obtain the 100% points here. Any hint about how to solve this? Thanks in advance

Number of Ratings and Average Ratings for a Movie (1a)
------------------------------------------------------
All tests passed
Movies with Highest Average Ratings (1b)
----------------------------------------
All tests passed
Movies with Highest Average Ratings and more than 500 Reviews (1c)
------------------------------------------------------------------
All tests passed
Root Mean Square Error (2b)
---------------------------
All tests passed
Using ALS.train (2c)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect errors[0]

Testing Your Model (2d)
-----------------------
All tests passed
Comparing Your Model (2e)
-------------------------
All tests passed
-- 6 cases passed (85.0%) --


Your submission token ID is 1417489-5278a9499d98b5358c038001d27d12ac:ip-172-31-22-183
Please include this submission token ID when you need support for your code submission. I have finished all the labs and the quizzes. I was not able fully completed  Lab1 and Lab3. Can I go back and finish it? I didn't see another post about this.  my computeError() passed fine, but now in 2d when calculating testRMSE I get the following dump.  I don't think anything should be getting passed to computeError that had zero reviews should it?  if so I need to add that condition to computeError but how do we handle that condition, return 0?

ZeroDivisionError                         Traceback (most recent call last)
<ipython-input-21-374531294e78> in <module>()
      5 predictedTestRDD = myModel.predictAll(validationForPredictRDD)
      6 
----> 7 testRMSE = computeError(testRDD, predictedTestRDD)
      8 
      9 #print 'The model had a RMSE on the test set of %s' % testRMSE

<ipython-input-13-c43496cb59d2> in computeError(predictedRDD, actualRDD)
     36 
     37     # Using the total squared error and the number of entries, compute the RSME
---> 38     return math.sqrt(totalError/numRatings)
     39 
     40 # sc.parallelize turns a Python list into a Spark RDD.

ZeroDivisionError: float division by zero Hi,

How can this and Scalable machine learning courses help us in the preparation of Spark certification.

Any links, materials and Preparation guidance would be of great help! I have submitted all lab exercises and a score around 84%. Will i get certificate for completion and how verified certificate works.

Please advise to proceed further.. Sorry for pasting code
ratedlist = [x[1] for x in myRatedMovies ]             myUnratedMoviesRDD = moviesRDD.map(lambda x: (0,x[0]) if (x[0] not in ratedlist) )

I think what am doing above is correct. I get syntax eerror below

 File "<ipython-input-51-e9e806a3eb48>", line 6
    myUnratedMoviesRDD = moviesRDD.map(lambda x: (0,x[0]) if (x[0] not in ratedlist) )
                                                                                     ^
SyntaxError: invalid syntax I think there is a small error in 3a, we see a note that - 
"... we have included the following code to list the names and movie IDs of the 50 highest-rated movies"

However, the code below uses movieLimitedAndSortedByRatingRDD, 
which is a the form - (average rating, movie name, number of reviews)

This caused me to use number of reviews as MovieId and obviously messes 
up the results.  4.358816276202219, u"One Flew Over the Cuckoo's Nest (1975)", 811)

(4.282367447595561, u'GoodFellas (1990)', 811)

Why we have one ID 811 for two different movies? This question is not related to the grading or the algorithm. However, I am just curious why in this set of data, we have this record. I guess the IDs should be unique for movies, right?

Hope to get some idea from the one who design this lab. Since we may learn something from this which may be related to data cleaning or other problems when we deal with other data science or data mining problems. 

I am reposting this as I still have no help and can not resolve this. I have looked in my cossim function and I've put print statements in my code but this error pops up before anything even prints out. 


I think I've narrowed it down to a division by zero argument. For this part all my previous parts run successfully. 
 
I use an RDD union,

 
My next line of code is applying the computeSimilarity to every record but then I get this error:
 
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-300-28eb93ab3b57> in <module>()
     18     return (googleURL, amazonID, cs)
     19 
---> 20 similarities = crossSmall.map(lambda rec: computeSimilarity(rec)).cache().collect()
     21 
     22 def similar(amazonID, googleURL):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 441.0 failed 1 times, most recent failure: Lost task 0.0 in stage 441.0 (TID 1546, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-300-28eb93ab3b57>", line 20, in <lambda>
  File "<ipython-input-300-28eb93ab3b57>", line 17, in computeSimilarity
  File "<ipython-input-296-80ba49727d8a>", line 13, in cosineSimilarity
  File "<ipython-input-294-a7f27788306b>", line 45, in cossim
ZeroDivisionError: float division by zero

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
 
 
Would love some help troubleshooting this problem.
 
Thank you,
 
Ray Xiao
 solved How long after the lectures have ended, do we have to complete the lab assignments?

I am asking because I started late and am currently working on lab2 and hoping to move to the rest by the weekend. Hi,

Since course is ending, I just wanted to find ways by which I can make use of the knowledge I learned here,
So I am looking for some real life problems, If TA's has suggestion please help me out.

Is there any course on how make use of Spark Streaming ? hints please why I am getting movieIDsWithRatingsRDD count as 2999? What did go wrong? Would you be posting solutions to labs? Is it possible to turn in labs after the end of the course (Monday 7/6?) and still get the verified certificate? Or will the courseworks simply calculate your progress % on Monday and give you the certificate if you are above the requisite percentage? I started the course late with the verified certificate and was intending to just take the 20% hit on the lab grades. movieNameWithAvgRatingsRDD = (<REDACTED>)

Error says int object has no attirbute __get item__

Please suggest me on how to fix this error! Hello, unfortunately I'm a little bit late with lab 3 submission.I can't loag file to submission. I have something like this while if I want to load file from lab 4 I have 

As you can see I can't download file in lab3. The circle next to the text "Choose file" suggest that the computer is waiting for something, it doesn't stop. I already used 6 submissions trying to load file but nothing :( I don't know what should I do? 

I tried on different systems and browser.

EDIT: My id (username) is tyskaj. I tried one more time today but still the same problem... Hi,
What is the method for creating an iterable RDD?  It is not just a list?

Ram  Thanks for providing nice course on Apahce Spark.

Finally Completed the course. Feeling happy. :)

Looking forward to complete Scalable Machine Learning. :P I submitted my solution to lab 3 several times and also got more and more things done with each commit.
However the last commit timed out and I decided not to re-commit. After commission of lab 4 today I have to
see that lab 3 is counted as 0% today. How can this be? The task description is:
Create a new `nonDupsRDD` RDD that has the just the cosine similarity scores for those "AmazonID GoogleURL" pairs from the `similaritiesBroadcast` RDD that **do not** appear in both the *sims* RDD and gold standard RDD.

But sims is a simple transformation of similaritiesBroadcast. nonDrupsRDD must be an empty RDD,  isn't it?

sims = similaritiesBroadcast.map(lambda (k1, k2, s): ... )

What am I missing? Is there any chance of getting the full, before-it-was-trimmed-down version of lab 4, for those of us who'd like to complete the work on our own?

I understand that we wouldn't be able to rely on any help from the instructional staff, that there'd be no mechanism for submitting it to an autograder and that it wouldn't count toward coursework for this class... but it would still be a great exercise for those of us who enjoy a challenge and want to extend our learning. Please have a look with the last code: 
movieNameWithAvgRatingsRDD = <REDACTED>

I really think there is not problem with that but it does not work. Thank you!  Just got done with the lab-4. I too struggled with lab-3 (not because of confusing instructions or anything - my own mistakes and oversights) - this course has now become one where I have invested the most number of hours to date! I can definitely sense the difference between the hours needed for lab-3 and lab-4. Given the MOOCs are for such a diverse set of people, the more options there are for people to decide the level of difficulty of the problems they want to solve for themselves (with minimum being set by instructors), the better the course will be.

An option could be to set points for each problem (based on level of difficulty, time needed, etc.) and have a candidate score a minimum number of points in each lab and a minimum number of points over all. People who have the time and skills to score more can certainly do so.
On similar lines, one should be able to choose what level of helpful hints one gets to see - some may not want to see the "number of transformation" needed to accomplish something (as mentioned in lab-4 - I have to admit that I found that helpful myself because it was there!!) and may want to figure it out themselves.

Wanted to say my thanks to all the instructors who must have put a huge number of hours creating this course and also to all the folks who are answering questions in the forum.

This is US specific - have a Happy 4th !! Hello - I'm trying to access the data to download it my laptop, but it doesn't show up on clicking the metric-learning link. Can anyone please post a working link? Hi All,
 I am using a temporary variable called temp in worker function getCountsAndAverages

Autograder error says 
name 'temp' is not defined

Does that mean temporary variables are not allowed in worker functions?
Or 'temp' is reserved name in pySpark?

Thanks
 Got got done and it was indeed a delightful one!
Thanks to the instructors and classmates.

Cheers

Oh - see you at Scalable Machine Learning :) Hi All,
 I followed following algorithm:
1. Extract movie-id, rating from trainingRDD
2. group them together by key
3. reduce
4. sum all values
5. divide #4 above by number of elements in trainingRDD. assign this value to trainingAvgRating

Autograder is complaining
name 'trainingAvgRating' is not defined 
All the unit tests passed for lab 4.

Please let me know what I am doing wrong
Thanks
 Hi,
Is there a way to use an RDD as a broadcast variable?

For example:
s= [(1,'a'), (2,'b'), (3, 'c')]rdd = sc.parallelize(s)
# then to use above RDD as a lookup dictionary can I do something like this?lookup = sc.broadcast(rdd)# which can be used like thisanswer = previouslyCreatedRDD.map(lambda x: (x[0], lookup.value[x[1]]))

if this can not be done, the way I think it can be done is to
collectedLookup = rdd.collectAsMap()lookup = sc.broadcast(collectedLookup)

Any suggestions? Dear Teacher

I studied about to hours part 5. I carefully translated and really understood all steps and data structures you created. 

How is it possible to get a Gold Standard file in real life?

For part 5, gold Standard is your joker card. Without it you have not idea of which records are true Duplicates. With out GOld standard no way to calculate false positives or negatives.

Please give some hint about how it is possible to obtain a Gold standard, let's say, to measure the quality of an ER algorithm to identify equal Health services offered in different web pages. 

Yours All my tests are passing. But when i do autograder it fails for 2b. Could you help me?

And the curious thing is that part 3 hasn't been run ... :-(

Number of Ratings and Average Ratings for a Movie (1a)------------------------------------------------------All tests passedMovies with Highest Average Ratings (1b)----------------------------------------All tests passedMovies with Highest Average Ratings and more than 500 Reviews (1c)------------------------------------------------------------------All tests passedRoot Mean Square Error (2b)---------------------------Traceback (most recent call last): File "", line 2, in  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue raise TestFailure(msg)TestFailure: incorrect testError (expected 1.22474487139)
Using ALS.train (2c)--------------------All tests passedTesting Your Model (2d)-----------------------All tests passedComparing Your Model (2e)-------------------------All tests passed-- 6 cases passed (85.0%) --
Your submission token ID is 1444861-75a74b1fb2a7ce896ef3d568898798db:ip-172-31-22-76

Many thanks

Fabrice S. lab 4 - 1(b) Getting an error in the last step of exercise 1(b).- int object not iterable.

 File "<ipython-input-38-57129e5e3da2>", line 19, in <lambda>
TypeError: 'int' object is not iterable

I am joining moviesRDD with movieIDsWithAvgRatingsRDD, and doing a map transformation on the intermediate RDD as follows to get the rating (d), move name (b), and movie ID (a). But then when I do a take(3) on the resulting RDD, I am getting an error. Looks like the problem is with the map. Please help!

(moviesRDD.join(movieIDsWithAvgRatingsRDD)                              .map(lambda ((a,b),(c,d)): (d,b,a)))

*** Edit:  ==> I changed the output of the lambda function to (d,b,c), since the 3rd field should be the number of ratings.  But I am still getting the same error about 'int' object is not an iterable..

(moviesRDD                              .join(movieIDsWithAvgRatingsRDD)                              .map(lambda ((a,b),(c,d)): (d,b,c)))


<class 'pyspark.rdd.PipelinedRDD'>
Did not complain on the join, but it is failing on take(3)



---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-38-57129e5e3da2> in <module>()
     20 print type(movieNameWithAvgRatingsRDD)
     21 print 'Did not complain on the join, but it is failing on take(3)'
---> 22 print 'movieNameWithAvgRatingsRDD: %s\n' % movieNameWithAvgRatingsRDD.take(3)
     23 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 111.0 failed 1 times, most recent failure: Lost task 0.0 in stage 111.0 (TID 92, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1220, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-38-57129e5e3da2>", line 19, in <lambda>
TypeError: 'int' object is not iterable

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)




 1)
I'd like to convert a broadcast variable into a Python dictionary object so it's easier to search by key. So that dataset will occupy double amount of memory - how can I release memory held bythe broadcast variable?
2)
Is this a good practice/design? My understanding broadcast variables we use in this labs when used as broadcastvar.value returns object of type list, so I have to convert it to a dictionary object.

3)
In Perl I could easily cast an array into a hash and search by key, but in Python my understanding this is not available, so I have to create another object. Is this right? As with others,all tests pass locally, but the autograder fails on two sections:

....
Movies with Highest Average Ratings (1b)----------------------------------------Traceback (most recent call last):  File "", line 2, in TypeError: 'list' object is not callable
....
Using ALS.train (2c)--------------------Traceback (most recent call last):  File "", line 1, in   File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue    raise TestFailure(msg)TestFailure: incorrect errors[0]
Your submission token ID is 1402999-affa3e4b5b679f18dd1636ce3bed88ee:ip-172-31-22-183

I've seen a posting about 2c.  Do I have to copy all of my work into a newer version of the notebook for the fix to work? 
Thanks. I am finding it hard to decipher on what question of 3a.. what is being asked and what need to be done. i think there need to be some join to be done on moviesRDD and RatingsRDD to get the movieID and ratings or movie and then create new list with myuserid after that what need to be done? I'm having a bear of a time resolving the similarity values for lab 3, 3c. The output from defining computeSimilarity when i run the cell is :
Requested similarity is 0.0.
I tried troubleshooting, beginning with altering tfidf and dotprod - in every case I pass all tests piror to 3c. To troubleshoot further, I wrote some test functions with declared values based on filtering from idfsSmallWeights per similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561'):
print idfsSmallWeights['using']
print idfsSmallWeights['smart']
print idfsSmallWeights['best']
print idfsSmallWeights['software']

print cosineSimilarity(gVal,aVal,idfsSmallWeights) #output matches test, error must be in call
and the output looks like:
13.3333333333
50.0
22.2222222222
4.25531914894
0.000303171940451 #matches test assertion
So, I'm stuck. Any help would be appreciated. Hi everyone,

I just want to know when this course is given again.

Thanks for the information,

 Can any one do this step after finishing the lab 3?
print similaritiesFullRDD.takeOrdered(2, lambda x: -x[1])
I turned in my lab 3, and got full credit of it. However, above line will give this output
[(('b000v9yxj4', 'http://www.google.com/base/feeds/snippets/17521446718236049500'), 1.0000000000000002), (('b0009dt87e', 'http://www.google.com/base/feeds/snippets/13017887935047670097'), 1.0)]
How can a cosine similarity score be greater than 1?

And why using inverted index method is way slower than using the first method? (I changed amazonSmall to amazon RDD) Hey guys! hope you're learning as much as i am.
Im trying to compute "similarities" but i cant pass the "computeSimilarity" function to the map transformation:

----------------------------------------------------------------------------------------------------
crossSmall = (googleSmall.<REDACTED>

def computeSimilarity(record):...#(here i assign each tuple and value to its respective var)
... return (googleURL, amazonID, cs)
similarities = crossSmall.map(computeSimilarity).cache() #computeSimilarity 
----------------------------------------------------------------------------------------------------------------------
Every time i use the "computeSimilarity" func as argument, i get this error:
Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
I dont really know what else to try. I already changed cosineSimilarity and all the functions that are invoked directly or indirectly by the "computeSimilarity" func to much simpler versions, without spark transformations but it doesn't seem to work. Looks like the problem is in the computeSimilarity func itself. I realized this when i changed the function inside the map transformation to a much simpler one (lambda x: 1) and all my values were indeed changed to 1. I really appreciate any kind help! How do I know if the course is completed successfully? All labs (till 4 are done with score around  88%)
when the certificates would be issued?
How do I get verified certificate? currently no link i could find in my course page..

Please suggest A great thanks to everybody involved in making such a nice course. Great experience. Learned a lot and looking forward to scalable machine learning.
Regards
Pranav. 
testR = sc.parallelize([(('A1', 'G1'), 'C1'), (('A1', 'G1'), 'C2'), (('A1', 'G1'), 'C3'), (('A2', 'G2'), 'C1'), (('A2', 'G2'), 'C2')])
print testR.count()
test2 = testR.reduceByKey(lambda x,y : list(x) + list(y))
print test2.take(5)
Prints:
5
[(('A1', 'G1'), ['C', '1', 'C', '2','C', '3']), (('A2', 'G2'), ['C', '1', 'C', '2'])]# C1, C2, C3 are supposed to be separate tokens. Did I miss the certificate deadline!? I was going over the updates and it appears that I have missed the deadline for receiving a certificate for my hard work.

Is there anything that you can do about this so I can apply for a certificate for this class?

Thanks,
Daxx  This course was classic. I would like this big data track to have a third course that deals with real-time streaming. This is useful for the nature of data that is generated in the real world.

Kindly provide notes for how to set up the Apache spark on our local machine. This is useful for people who want to deploy in production. Hello,

First of all I would like to thank you for wonderful courses you offered by edX, which made me more interested about Spark.
 
I am planning to continue my education with Spark, and I am planning to visit Spark summit Europe 2015.

It would be nice if we can get some discount codes, if possible.

Is it possible to get it from DataBricks and use it for registration?

Best regards Create a pair RDD called corpusRDD, consisting of a combination of the two small datasets, amazonRecToToken and googleRecToToken. Each element of the corpusRDD should be a pair consisting of a key from one of the small datasets (ID or URL) and the value is the associated value for that key from the small datasets

Can anyone explain what we should do here.Please Hi.

I submit the lab a first time, and it give me a grade and  2 error, so I fix them, and now is not possible to submit the lab because I get (this happens 3 or 4 times) :

"Timeout error happened during grading. Please review your code to be more efficient and submit the code again."

 And the old grade is gone.

 I'm using Databricks, and all the test are good.

the  token ID given by the grader is 1458760-d94af2b071f5dcc699bc13f8ddb8022e:ip-172-31-31-97

Thank. Hello everyone,
I was wondering what do we need to know about spark other than programming in spark(assignment type work). Something like, do we need to configure something? what do we need to know about driver/worker(master slave)? Suppose I am running an application on a stand-alone cluster with 3 worker nodes.
Given the following example code,

rDD = sc.textFile(hdfs_input_dir + "/input.txt")
	.repartition(6)  // since there are 3 worker nodes, 2 tasks will be assigned to each of worker nodes
	.map(line => line.reverse)
	.cache()

for (i <- 1 to 10) {
	rDD.saveAsTextFile(hdfs_output_dir + "/iter_" + i)
}

I have a question about how Spark internals handle cached partitions.
If my understanding is correct, in the first stage, "input.txt" will be divided into 6 partitions.
Then, in the second stage, the driver will assign "1 partition + map transformation" to a worker as a task
(The second stage consists of 6 tasks since the "input.txt" is divided into 6 partitions).
And the worker caches the partition into its memory before processing saveAsTextFile action function.

Let the 6 partitions be {p1, p2, p3, p4, p5, p6} and let the 3 woker nodes be {w1, w2, w3}.
If partitions are assigned to worker nodes like below,

w1 : p1, p4
w2 : p2, p5
w3 : p3, p6

the worker node, w1, cached p1 and p4 partitions in its memory, w2 cached p2 and p5 partitions in its memory, and so on.

Now here is my question. In the example code above, I am calling saveAsTextFile action function 10 times.
Will there be any chance that the worker node, w1, be assigned with partitions  that it did not cache, for example, p2?
If this is true, in the worst case, every worker node will have to cache all 6 partitions of rDD in its JVM heap.

I am asking this question because when I check the size of Spark memory fraction used by cache function, the size is much greater than the actual size of the "input.txt" file. And I am curious if there is redundant caching over the worker nodes in the cluster as well as the serialized/deserialized caching difference.


Thank you in advance. Looks like I'm missing something with section 3a of Lab4. It says <<...the following code to list the names and movie IDs of the 50 highest-rated movies from movieLimitedAndSortedByRatingRDD...>> but instead the code display the tuple :
(average rating, movie name, number of reviews)
Where is the movie Id ?
The next section has a note that says <<the movie IDs are the *last* number on each line. A common error was to use the number of ratings as the movie ID.>>
This is really confusing as I do not see any movie Id in previous output, and the last number is of course the number of rating, so for sure if we consider that number we will end up with the common error that this note is highlighting.
Are we supposed to find out by ourselves the movie IDs corresponding to our selection ? Or is it a mistake in the exercise ?...or maybe I've missed something :)/
 I found that I could not combine two RDDs together? Can anyone give me some hints about how to realize it? So I understand How MapReduce works to an extent, I know that the mapping is split across your cluster and then the reduce feature is applied on each of the nodes I.E. Wordcount. But where does the finally reduce happen when receiving all the mapreduces from the other nodes? Does it happen on the master node? Do you specify were it can happen? What is standard protocol  for this. What if your final reducing is still a huge amount of data? I get below for true dups and non-dups:

There are 146 true duplicates.
The average similarity of true duplicates is 0.248900918558.
And for non duplicates, it is 0.00118782909294.
# of true dups seems to be fine. Also all tests until this have passed. I am not sure where is the error. I am doing below in my dotprod:

for each key in vector a:
   check if key present in vector b, then add the key to a new a vector
for each key in vecotr b:
   icheck if key present in vector a, then add the key to a new b vector

then compute the dot product of new a vector and new b vector using numpy np.dot()

cossine similarity function also seems to be fine. I am not sure what is the issue. Could you please help?
          

 I have completed my course how do I pay and apply for the certificate? I am stuck with second question in 3f. I am doing this:

# Transform movieIDsWithAvgRatingsRDD from part (1b), which has the form (MovieID, (number of r
<REDACTED>
print predictedRatingsRDD.take(3)print predictedRDD.take(3)

I get the following error. How do I join the ratings with movieID??




[Rating(user=0, product=1084, rating=1.9137792832266527), Rating(user=0, product=3586, rating=1.85171623362745), Rating(user=0, product=3702, rating=1.9354752484043392)]






---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-67-0b403ea7ba3c> in <module>()
      7 predictedRDD = predictedRatingsRDD.join(movieCountsRDD)
      8 print predictedRatingsRDD.take(3)
----> 9 print predictedRDD.take(3)
     10 
     11 # Use RDD transformations with predictedRDD and movieCountsRDD to yield an RDD with tuples of the form (Movie ID, (Predicted Rating, number of ratings))

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1855.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1855.0 (TID 676, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1806, in <lambda>
    map_values_fn = lambda (k, v): (k, f(v))
ValueError: too many values to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span></span></span></span></span></span></span></span></span></span></span>
</div> </div></div>
</div> The range of topics covered were a perfect fit for what was advertised. I realize it was never meant to be an in-depth look at the topics - rather a survey of how data science can be used at scale using Apache Spark on some specific use cases. I found lab 3 on ER particularly interesting, even though finding what movie to watch was equally motivating.

I really appreciated how the labs were structured using Jupyter too. And more importantly how those labs really reinforced the points discussed in the lectures and sometimes went beyond.

Also, I dig this short 5 week format. CS190.1x is going to be a great follow up.
Finally it goes without saying how active the instructors and other students have been in the forums. Simply put, lab 3 would have been a lot worse if not for this active community!

so many thanks edX, BerkeleyX, databricks
     # Transform predictedRDD into the tuples of the form ((UserID, MovieID), Rating)    predictedReformattedRDD = predictedRDD.map(lambda (UserID, MovieID, Rating): ((UserID, MovieID), Rating))    print predictedRDD.take(3)
Why above code printed unnested tuple as below? I am confused

[(1, 1, 5), (1, 2, 3), (1, 3, 4)]
 The seed is a random seed, but I was wondering why is this '0L'? Shouldn't it be an integer?

 Hi all,

Sinking in sometime troubleshooting this so I figured I would come here and ask if anyone has been experiencing this as well. I am running on windows 8.1.

I had the time out issues with vagrant but after I disabled Hyper-V, that went away. Unfortunately I am still experiencing issues. When the autosave fails, I can no longer save the notebook or export it. The home page can be reloaded and still shows the IPython notebook running. After a while it comes back and it works.

I would appreciate any thoughts . Thank you.

Pedro Very well structured course in right time. I could definitely say that it saved us good money which we would have otherwise spent in some other online/class room training. Giving out such wonderful and valuable course for free is really a great step and sets a example of that education and knowledge sharing shouldn't money motivated. Really enjoyed each topic and labs. Liked the thought of cutting down lab 4 as there is another course on machine learning where it is extensively discussed.

Thanks to everyone who took part in building this course and provided it for free.  Hi,
  
  I have a question relative to the equivalence of a function in spark/scala that I foud here.

How is it possible to do this in spark/scala

print type(wordsRDD)
You have all the code below
wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']
wordsRDD = sc.parallelize(wordsList, 4)
# Print out the type of wordsRDD
print type(wordsRDD)
Thanks very much for your answer Hi,

I need help in concatenating two tupes as below:

INPUT in FILE : 

(('0000000105aaaaa',), ('qqqqqqqqqqqqqqqqqqq', '1', 'NULL', '1', 'NULL', 'NULL', 'N', '', 'NULL', 'NULL', '11111111', '2012-02-24 08:59:21.004000', '0'))



EXPECTED O/P: 

0000000105aaaaa, qqqqqqqqqqqqqqqqqqq, 1, NULL, 1, NULL, NULL, N, , NULL, NULL, 11111111, 2012-02-24 08:59:21.004000, 0


And i want to write the o/p to a file.

Please help on this.

Thanks.

 
I have downloaded lab 4 ipynb from
https://raw.githubusercontent.com/spark-mooc/mooc-setup/master/lab4_machine_learning_student.ipynb 

But when I tried uploading it in jupyter, I'm getting following error,
Cannot upload invalid Notebook


The error was: SyntaxError: Unexpected token <

I'm using vm from "scalable machine learning" class . I suppose vms are same for both classes.

Thanks 
 I am getting an error. Could you give me a hint what I doing wrong? It seems it cannot do the join, but why?
ValueError: too many values to unpack
EDIT: I have tried to collect() and take() for what came out of the join, but am getting the same error. Please, help. Hi , I have updated my lab 4 in auto grader and it is 71 percent complete. My Course progress stands at 81 percent. I don't think I will be able to complete the lab 4 100 %. Wanted to confirm that I will get completion certificate with this progress number?  For Lab 4, part 0, the last line checks the results of:
ratingsRDD.takeOrdered(1, key=lambda (user, movie, rating): movie)
The first line says,
numPartitions = 2


If I run it then, it works fine.  The line evaluates to:
[(1, 1, 5.0)]
If I change numPartitions to 40, however, I get the below result:
[(194, 1, 4.0)]

Why are my results changing based on the number of partitions I use? When trying to calculate MovienameWithAvgratingRDD, i got some issue.
1. After join the data is coming proper as given below
[(2049, (u'Happiest Millionaire, The (1967)', (22, 3.6818181818181817))), (3, (u'Grumpier Old Men (1995)', (299, 3.0468227424749164))), (2052, (u'Hocus Pocus (1993)', (94, 2.882978723404255)))]				
How to get the proper format. I have applied map(lambda (a,(b),(c,d)) : (d,b,c))), but here I am getting error.

Please let me know how to fix this isue Hi,

When I tried for print totalError.take(1) I am getting error.

What is the expected output of totalError.

For my print squaredErrorsRDD.take(1) , I get the following output

[((1, 3), 1)]
[((2, 2), 16)]


Please help me in fixing errors of the code. My code for lab 4 successfully passes local tests, but fails autograder's tests for 1b and 1c with this output:
Number of Ratings and Average Ratings for a Movie (1a)
------------------------------------------------------
All tests passed
Movies with Highest Average Ratings (1b)
----------------------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect movieNameWithAvgRatingsRDD.count() (expected 3615)

Movies with Highest Average Ratings and more than 500 Reviews (1c)
------------------------------------------------------------------
Traceback (most recent call last):
  File "", line 19, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect sortedByRatingRDD.take(20)

Root Mean Square Error (2b)
---------------------------
All tests passed
Using ALS.train (2c)
--------------------
All tests passed
Testing Your Model (2d)
-----------------------
All tests passed
Comparing Your Model (2e)
-------------------------
All tests passed
-- 5 cases passed (71.0%) --
Trouble is in movieNameWithAvgRatingsRDD, which length in my lab is 3615, and is not 3615 in autograder. My lab 4 version is 1.0.1. Is it possible that data for lab 4 on my vm differs from data on autograder? How can i fix this?

This issue was also discussed here and there was a response from Anthony D. Joseph:
We've rolled out some changes to the autograder to address 1b and 1c issues. Please try resubmitting.
I've tried, no luck. My submissions IDs:
1473057-97b11e376035d1857dbb88be93070192:ip-172-31-22-28
1478680-854fba3645469021b8f5c3b2f424b1d9:ip-172-31-26-20
1480508-d39ccee672a38fd4681e935ce91e53ca:ip-172-31-31-179
I don't know if this is an important notice, but I have relatively fresh vm as it was redownloaded on June 24.

I'm getting a bit nervous about this: I've already used 6/10 submissions and deadline for lab 4 is tomorrow and I want 100% for this course. Please, help ASAP


 I was not doing programming since I left engineer school seven years ago.
I enrolled this course because I had some free time to do some python tutorials before and was attracted by Big Data issues.
I actually really enjoyed programming.

I would like to continue to play with Python/Spark on my own laptop.
I have well understood the limited interest of using spark and RDDs on a single laptop like I am planning but why not.

So what I would like is explanations about how to create an environment with let's say a workbook like we used for the course where I could code and import/export data from CSV/text files. 
I assume I won't do so much more than I could do with excel but feel like investing some time in this.

My laptop is on linux mint but I am not so familiar with linux (just got pissed off by the idea to recover windows license after a HDD crash) and I would appreciate semi-detailled instructions or any link to a tutorial if by chance it exists.

Thanks a lot for your help.
And thanks for this very interesting MOOC.

François
 I'm stuck on lab 4 3e. I believe I have the code in the cell written correctly but the predictedRatingsRDD result is empty. I think it's because trainingRDD (used by myRatingsModel) has no entries with UserID 0. I'm passing all tests up to this cell. No idea what's wrong.

print myUnratedMoviesRDD.take(10)
[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10)]

predictedRatingsRDD = myRatingsModel.predictAll(myUnratedMoviesRDD)
print predictedRatingsRDD.count()
0
 trainingAvgRating,testForAvgRDD
Use the trainingRDD to compute the average rating across all movies in that training dataset.Use the average rating that you just determined and the testRDD to create an RDD with entries of the form (userID, movieID, average rating).Use the testRDD to create an RDD with entries of the form (userID, movieID, rating). My doubt is second point (userID,MovieID,average Rating). What does this mean? average rating for a given movieID by a particular user with userID. Ran it again: 
submission token ID is 1545376-4173ad68f48b3e5906a047f37786b193:ip-172-31-24-96
Trace below, all tests pass on local.
Tokenize a String (1a)
----------------------
Traceback (most recent call last):
  File "", line 28, in 
ZeroDivisionError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
float division by zero

All tests passed
Removing stopwords (1b)
-----------------------
All tests passed
Tokenizing the small datasets (1c)
----------------------------------
All tests passed
Amazon record with the most tokens (1d)
---------------------------------------
All tests passed
Implement a TF function (2a)
----------------------------
All tests passed
Create a corpus (2b)
--------------------
All tests passed
Implement an IDFs function (2c)
-------------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect smallest IDF value

Implement a TF-IDF function (2f)
--------------------------------
Traceback (most recent call last):
  File "", line 5, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 38, in assertEquals
    cls.assertTrue(var == val, msg)
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect rec_b000hkgj8k_weights

Implement the components of a cosineSimilarity function (3a)
------------------------------------------------------------
All tests passed
Implement a cosineSimilarity function (3b)
------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'cossimAdobe' is not defined

Perform Entity Resolution (3c)
------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogle' is not defined

Perform Entity Resolution with Broadcast Variables (3d)
-------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'similarityAmazonGoogleBroadcast' is not defined

Perform a Gold Standard evaluation (3e)
---------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trueDupsCount' is not defined

Tokenize the full dataset (4a)
------------------------------
All tests passed
Compute IDFs and TF-IDFs for the full datasets (4b)
---------------------------------------------------
All tests passed
Compute Norms for the weights from the full datasets (4c)
---------------------------------------------------------
All tests passed
Create inverted indicies from the full datasets (4d)
----------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4e)
-------------------------------------------------
All tests passed
Identify common tokens from the full dataset (4f)
-------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 713, in collect
    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/java_gateway.py", line 538, in __call__
    self.target_id, self.name)
  File "/spark-1.3.1-bin-hadoop2.6/python/build/py4j/protocol.py", line 300, in get_return_value
    format(target_id, '.', name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 96.0 failed 1 times, most recent failure: Lost task 0.0 in stage 96.0 (TID 316, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "", line 19, in fastCosineSimilarity
ZeroDivisionError: float division by zero

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


-- 12 cases passed (63.0%) --


As for late submissions after 3 day grace period of the due date, you lose 20 points.
 Your final score is 50

Your submission token ID is 1545376-4173ad68f48b3e5906a047f37786b193:ip-172-31-24-96<br /></init></string></module></string></module></string></module></string></module></string></module></string></module></string></module></string></module></string> I was able to redo all the Lab's, without Vagrant and VirtualBox on my Slackware linux laptop with only 2 GB RAM.
1. Install spark-1.3.1 (or better still spark-1.4.0)
    by downloading the source and compiling it.

2. Download python module "test_helper" from
    
I was able to redo all the Lab's, without Vagrant and VirtualBox on my Slackware linux laptop with only 2 GB RAM.
1. Install spark-1.3.1 (or better still spark-1.4.0)
    by downloading the source and compiling it.

2. Download python module "test_helper" from
     https://pypi.python.org/pypi/test_helper/0.2
    and install it as a python module.
3.  Add <Spark HOME>/bin to your PATH in
      $HOME/.bashrc (or equivalent)
4. Start notebook as:
    IPYTHON_OPTS="notebook" pyspark

5. The Notebook page will open in your browser.
6. Follow remaining steps as given in the    Course.

When the pyspark plus Notebook is running, only 1 GB of physical RAM is occupied on my machine and I could comfortably  redo all the labs.

This may be of interest to some of the students. After completing all the labs i have to thank the instructors for providing a set of exercises (freely) for all of us to play spark

My only negative impression is that some functions are a bit too pedantically <fill(ed).in>.
I believe an approach with more implementation freedom would let us think more how to translate the lab questions in spark/python solutions,
because i admit that sometimes i couldnt understand why i was doing something and i kept trying only to satisfy the assertions.

To put it in a juvenile plain way i felt like cheating somehow.

Of course this would more demanding for the grading process but it's just a suggestion::))

Thanks again until the next time - Hello everyone!

I am trying to apply my knowledge gained after finishing this course in my working life. 

I have a very specific use case, and still not sure whether spark can help me with this

So I've got a set of huge csv files which share common structure (in total about 25 GB) and I need to perform modifications on one single column

the simplified form is   ENTITY_ID, LAST_CHANGE_DATE(yyyyMMdd)...

entity_id is not unique and that is the time when transformation starts.
For each grouped (by id, date) entry date field should be transformed to the form of yyyyMMdd hour:minute:second. Time should start from noon, and each entry of a group should add 1 second to the timestamp.

As an example 

1,20150702
1,20150702
2,20150702

should become 
1,20150702 12:00:00
1,20150702 12:00:01
2,20150702 12:00:00

I was thinking of counting number of elements of each grouped(by id and date) entry as the first step
and during mapping of the whole RDD use these counts as lookup table in order to find out the total number of entries inside of group, but it wouldnt fit to my memory, additionally I don't have a clue how to enumerate than elements to calculate offset in seconds,

If anybody has any suggestion I would highly appreciate it. 

Thank you! Hello!
   I would like to know where in the course has been mentioned that the overall grading policy will take into the considerations the latest submitted code for the labs? 
The reason for asking this, is that for the laboratory 3 I got a higher percent on a previous submission than the latest submitted one. Is it possible to change this policy to take into consideration the best grade not the latest one. In my case, I assumed that this was the policy. therefore I refactored my code with a worst one and I omitted to submit the best one. Thus, I have some penalties regarding my percentage.
 Thank you very much for the great course and work.
 Regards,
  Florin It appears that for this problem, the solution accepted by the autograder requires summing the avgRating for the movies and dividing by the total tuple count.
Alternatively, one can do the sum product of the number of ratings and avgRating for each movie and then divide by the sum of number of ratings across all movies.
Both approaches give numbers within a reasonable tolerance but I'll like to think that the second approach is more reflective of the global average.
Any thoughts? i've been hiting my head against the wall and no answer.

what am i missing?

i can't think of a way to get the first word uf a line using the split() function... (i keep thinking it should let me use my_string[0])

more than a hint , i require a link or something to read about this to finally solve it

tanks Indeed this is a great honor for me to be the part of such an excellent course. I must acknowledge and must say that this course was a great success. I learned a lot during this course. I had no prior experience of working with Python and Apache Spark but the way the labs were set up, made me learn both tools. The labs were such a fun to perform and were set up in an excellent way.
I thank all the team of this course who provided us such a great course. I would also like to thanks all the TA's and students who supported me during the course when I had problems in doing the tasks in the labs.
Thank you all.
I look forward to next course and hope that I'll have the same pleasure at the end of that course that is "Scalable Machine Learning".     
I am having stack trace while sorting with sortFunction
Here is the debug log for my 1C
DBG: = Count Before sorting =  movieLimitedAndSortedByRatingRDD 194
DBG: = Before Sorting movieNameWithAvgRatingsRDD = <strong>[(2.594810379241517, u'Mission to Mars (2000)', 501),</strong> (2.9293893129770994, u'Mars Attacks! (1996)', 524), (2.966876971608833, u'Blair Witch Project, The (1999)', 634), (3.003992015968064, u'Honey, I Shrunk the Kids (1989)', 501)]

Update on my trace
Here is the trace error
<strong>UnicodeEncodeError: 'ascii' codec can't encode character u'\ufffd' in position 7: ordinal not in range(128)</strong>
It appears to me issue with unicode.

Thanks again appreciate your help.
I get traces while sorting movieNameWithAvgRatingsRDD with 
sortBy(sortFunction, True) option.

Please suggest some ideas or hints to resolve this issue.
Maybe I am sort function with avg rating (2.966876971608833) and Movie Name. Please ignore this post. I fixed it was my silly extra debug statement I had put in sortFunction. Hi,

For those of us who are behind on the assignments, in particular assignment 3 and 4, when is the last date on which we can submit an assignment. I understand the grace period for assignment 4 is until July 6, but in case we can't get it done by then, will we have some time to submit the assignments with 20% penalty after July 6 or is July 6 the last date to submit any assignments ?
 If I understand correctly, the course is ending on July 6. Considering 3 day grace period for lab 4, every one is force to complete the lab on time and complete the course by July 6. 

I was hoping that like other labs, there should be some time till when we can submit with penalty :)

If it can not be done, then also it is ok as this has been a great course, but one week extension or something like that might have helped a lot of people try multiple times even if it is at a loss of 20%

 Hi,
My solution for lab4 1b past all the tests but failed at the last one:
Test.assertEquals(movieNameWithAvgRatingsRDD.takeOrdered(3),                [(1.0, u'Autopsy (Macchie Solari) (1975)', 1), (1.0, u'Better Living (1998)', 1),                 (1.0, u'Big Squeeze, The (1996)', 3)],                 'incorrect movieNameWithAvgRatingsRDD.takeOrdered(3)')
If print out my rdd, the result is:
print movieNameWithAvgRatingsRDD.takeOrdered(3)
[(1, u'24 7: Twenty Four Seven (1997)', 5.0), (1, u'An Unforgettable Summer (1994)', 3.0), (1, u'Ashes of Time (1994)', 4.0)]
It seems the order is wrong. I tried combine the int and string to a new string and sort the rdd, but the result is not right either.
I was wondering how did you deal with the order.
Thanks!
 
Below is my result of the rdds

movieIDsWithRatingsRDD: [(2, [1.0, 5.0, 4.0, 5.0, 4.0, 4.0, 2.0, 4.0, 4.0, 5.0, 3.0, 1.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 4.0, 2.0, 4.0, 4.0, 2.0, 2.0, 4.0, 3.0, 1.0, 4.0, 4.0, 3.0, 4.0, 2.0, 2.0, 2.0, 4.0, 3.0, 4.0, 3.0, 4.0, 4.0, 1.0, 1.0, 4.0, 3.0, 3.0, 5.0, 2.0, 3.0, 3.0, 5.0, 4.0, 3.0, 5.0, 1.0, 3.0, 3.0, 4.0, 2.0, 4.0, 4.0, 4.0, 3.0, 5.0, 3.0, 3.0, 2.0, 3.0, 4.0, 1.0, 3.0, 3.0, 4.0, 2.0, 4.0, 3.0, 3.0, 3.0, 4.0, 2.0, 2.0, 1.0, 4.0, 5.0, 3.0, 4.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 3.0, 3.0, 3.0, 4.0, 3.0, 4.0, 4.0, 3.0, 3.0, 3.0, 4.0, 2.0, 4.0, 2.0, 3.0, 2.0, 4.0, 3.0, 4.0, 4.0, 3.0, 2.0, 3.0, 3.0, 2.0, 5.0, 4.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 1.0, 4.0, 3.0, 3.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 3.0, 2.0, 4.0, 3.0, 2.0, 3.0, 4.0, 2.0, 2.0, 3.0, 4.0, 4.0, 3.0, 4.0, 2.0, 3.0, 3.0, 3.0, 4.0, 3.0, 5.0, 3.0, 2.0, 2.0, 3.0, 5.0, 3.0, 5.0, 3.0, 3.0, 3.0, 3.0, 3.0, 5.0, 3.0, 3.0, 3.0, 4.0, 2.0, 2.0, 3.0, 2.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 5.0, 3.0, 4.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0, 1.0, 4.0, 1.0, 3.0, 4.0, 3.0, 3.0, 4.0, 1.0, 2.0, 4.0, 2.0, 1.0, 4.0, 3.0, 3.0, 2.0, 3.0, 4.0, 1.0, 4.0, 1.0, 3.0, 2.0, 4.0, 2.0, 3.0, 4.0, 1.0, 4.0, 1.0, 2.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 4.0, 4.0, 3.0, 4.0, 3.0, 4.0, 4.0, 3.0, 4.0, 3.0, 4.0, 4.0, 3.0, 2.0, 5.0, 3.0, 3.0, 3.0, 2.0, 3.0, 3.0, 2.0, 2.0, 3.0, 4.0, 4.0, 3.0, 3.0, 3.0, 4.0, 3.0, 4.0, 4.0, 3.0, 3.0, 5.0, 4.0, 2.0, 4.0, 3.0, 3.0, 1.0, 3.0, 4.0, 3.0, 3.0, 3.0, 2.0, 5.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 4.0, 4.0, 2.0, 3.0, 4.0, 4.0, 2.0, 5.0, 3.0, 4.0, 3.0, 2.0, 2.0, 4.0, 4.0, 1.0, 3.0, 5.0, 3.0, 4.0, 1.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0]), (4, [4.0, 1.0, 3.0, 2.0, 1.0, 5.0, 4.0, 2.0, 3.0, 1.0, 3.0, 3.0, 4.0, 4.0, 3.0, 3.0, 3.0, 3.0, 4.0, 2.0, 3.0, 1.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 4.0, 3.0, 1.0, 3.0, 4.0, 1.0, 2.0, 2.0, 4.0, 2.0, 3.0, 3.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 4.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 5.0, 3.0, 3.0, 3.0, 3.0, 5.0, 2.0, 1.0, 3.0, 2.0, 2.0, 1.0, 2.0, 2.0]), (6, [4.0, 4.0, 5.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 1.0, 4.0, 4.0, 5.0, 5.0, 5.0, 4.0, 5.0, 2.0, 4.0, 4.0, 2.0, 5.0, 4.0, 4.0, 4.0, 2.0, 5.0, 3.0, 4.0, 4.0, 5.0, 4.0, 4.0, 5.0, 5.0, 2.0, 4.0, 5.0, 5.0, 4.0, 4.0, 4.0, 5.0, 2.0, 5.0, 4.0, 4.0, 4.0, 3.0, 5.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 4.0, 4.0, 5.0, 3.0, 4.0, 4.0, 4.0, 5.0, 4.0, 4.0, 3.0, 4.0, 2.0, 4.0, 4.0, 3.0, 3.0, 4.0, 3.0, 3.0, 5.0, 4.0, 4.0, 3.0, 5.0, 4.0, 4.0, 2.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 2.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 3.0, 2.0, 3.0, 4.0, 5.0, 4.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 3.0, 5.0, 1.0, 5.0, 5.0, 3.0, 4.0, 3.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 2.0, 3.0, 3.0, 5.0, 4.0, 4.0, 3.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 3.0, 4.0, 2.0, 2.0, 5.0, 3.0, 1.0, 3.0, 5.0, 4.0, 4.0, 3.0, 4.0, 4.0, 3.0, 4.0, 5.0, 5.0, 3.0, 3.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 5.0, 3.0, 3.0, 1.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 2.0, 3.0, 4.0, 2.0, 3.0, 4.0, 4.0, 3.0, 4.0, 5.0, 5.0, 4.0, 3.0, 3.0, 3.0, 4.0, 2.0, 4.0, 5.0, 3.0, 2.0, 4.0, 3.0, 5.0, 4.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 2.0, 5.0, 4.0, 4.0, 4.0, 3.0, 3.0, 4.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 2.0, 3.0, 4.0, 4.0, 5.0, 1.0, 5.0, 2.0, 3.0, 4.0, 2.0, 1.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 5.0, 3.0, 4.0, 4.0, 4.0, 5.0, 2.0, 3.0, 4.0, 4.0, 3.0, 5.0, 3.0, 4.0, 4.0, 4.0, 5.0, 4.0, 3.0, 4.0, 2.0, 4.0, 5.0, 5.0, 4.0, 3.0, 5.0, 4.0, 4.0, 3.0, 2.0, 3.0, 3.0, 2.0, 3.0, 5.0, 2.0, 3.0, 5.0, 4.0, 5.0, 4.0, 4.0, 4.0, 5.0, 2.0, 3.0, 4.0, 4.0, 5.0, 5.0, 4.0, 1.0, 3.0, 4.0, 3.0, 5.0, 4.0, 4.0, 4.0, 4.0, 3.0, 5.0, 5.0, 4.0, 3.0, 2.0, 3.0, 4.0, 4.0, 4.0, 5.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 4.0, 3.0, 5.0, 4.0, 3.0, 3.0, 4.0, 4.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 5.0, 4.0, 3.0, 4.0, 4.0, 4.0, 4.0, 2.0, 3.0, 4.0, 3.0, 3.0, 5.0, 2.0, 4.0, 5.0, 2.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 1.0, 4.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 2.0, 4.0, 5.0, 2.0, 5.0, 2.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 3.0, 1.0, 5.0, 2.0, 4.0, 5.0, 3.0, 5.0, 5.0, 4.0, 5.0, 4.0, 4.0, 5.0, 5.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 2.0, 3.0, 4.0, 5.0, 3.0, 4.0, 4.0, 3.0, 3.0, 3.0, 4.0, 3.0, 5.0, 2.0, 5.0, 4.0, 3.0, 3.0, 3.0, 5.0, 3.0, 4.0, 4.0, 3.0, 5.0, 4.0, 4.0, 5.0, 5.0, 4.0, 5.0, 4.0, 4.0, 5.0, 5.0, 4.0, 3.0])]

movieIDsWithAvgRatingsRDD: [(2, (332, 3.174698795180723)), (4, (71, 2.676056338028169)), (6, (442, 3.7918552036199094))]

movieNameWithAvgRatingsRDD: [(22, u'Happiest Millionaire, The (1967)', 3.6818181818181817), (299, u'Grumpier Old Men (1995)', 3.0468227424749164), (94, u'Hocus Pocus (1993)', 2.882978723404255)]
 Thanks to UC Berkeley and staff for releasing such a high quality MOOC for free on the edx platform. The production value is incredibly high and courseware very polished. Much appreciated it.

If I may offer a bit of feedback, being taken a number of MOOC on data science:
- the voice over on slides: rather than reading the slides verbatim like Siri :-) would be nice to speak to the intuition behind, and/or talk to additional examples to help illustrate the concept. 
- python notebook assignment format: for someone from a tech background, I felt that there is too much hand holding in the assignments. Rather than spending time on the core problem to solve, I ended up trying to understand exactly what the next micro-step really want to achieve. I would prefer leaving the solution more open-ended, and allow for more flexibility in terms of how to structure the code.
- in similar vein, would be nice to (at least as an option) to include into the tutorial setting up the full environment (with the understanding that the student, if s/he chooses to, will have to pay for the AWS resources) Having actually done it from start-to-finish (as opposed to filling in code snippets just like in an exam...) will help reinforce learning. I have passed all tests but the autograder is showing following error mesage. PLease help me
-----------------------------------------------------
Traceback (most recent call last):
  File "", line 50, in 
  File "", line 1
    version 1.0.0
              ^
SyntaxError: MAKE SURE YOUR NOTEBOOK RUNS WITHOUT RAISING AN EXCEPTION
invalid syntax

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'getCountsAndAverages' is not defined

Movies with Highest Average Ratings (1b)
----------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'movieIDsWithRatingsRDD' is not defined

Movies with Highest Average Ratings and more than 500 Reviews (1c)
------------------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'movieLimitedAndSortedByRatingRDD' is not defined

Root Mean Square Error (2b)
---------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'testError' is not defined

Using ALS.train (2c)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trainingRDD' is not defined

Testing Your Model (2d)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'testRMSE' is not defined

Comparing Your Model (2e)
-------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trainingAvgRating' is not defined

-- 0 cases passed (0.0%) --


 See output below. My lab passes all the tests locally, plus error is in /ok/submissions.py line 506.  Tried it twice.  The .py file being uploaded has the code in it.  Any ideas?

Number of Ratings and Average Ratings for a Movie (1a)
------------------------------------------------------
Traceback (most recent call last):
  File "", line 38, in 
  File "/ok/submission.py", line 506
SyntaxError: Non-ASCII character '\xc2' in file /ok/submission.py on line 507, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'getCountsAndAverages' is not defined

Movies with Highest Average Ratings (1b)
----------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'movieIDsWithRatingsRDD' is not defined

Movies with Highest Average Ratings and more than 500 Reviews (1c)
------------------------------------------------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'movieLimitedAndSortedByRatingRDD' is not defined

Root Mean Square Error (2b)
---------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'testError' is not defined

Using ALS.train (2c)
--------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trainingRDD' is not defined

Testing Your Model (2d)
-----------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'testRMSE' is not defined

Comparing Your Model (2e)
-------------------------
Traceback (most recent call last):
  File "", line 1, in 
NameError: name 'trainingAvgRating' is not defined

-- 0 cases passed (0.0%) --


Your submission token ID is 1500385-fbc52d2c5b4112174c8c651a13199701:ip-172-31-24-197
Please include this submission token ID when you need support for your code submission.  Hi All,

I am a newbie here and not much aware of python or spark. I have found this course very interesting, so decided to hang on. Its been real fun so far.

Now, I am completely stuck on lab3, question 1. I have no idea what is the meaning of:
biggestRecordAmazon[0][0] or biggestRecordAmazon[1][1] or biggestRecordAmazon[0][1] etc.

Can somebody guide me to a documentation where this is explained? I am getting intimidated looking at two square brackets. One square bracket till lab2 was fine. .

Any help would be appreciated. 

Thanks in advance. I found the lectures and the optional reading material quite well organized and pretty useful. As for the labs, it was a steep learning for me. I was stuck with Python for a long time and Lab-3 was pretty intense. I thought Lab-4 was a lot more in tune with 4-8 hours per week.

Over all great leaning and many thanks to UCB teaching staff, TAs and the vibrant Piazza community.  There are some zip archive that includes all material course like lectures, videos, etc.? Hello,

Thanks for the instructors,TA and fellow students for making this class interesting and fun to learn. I want to know if I have to use Spark in future for my personal projects, how should I go about setting up on my laptop or AWS. 

Any guidelines in this regard will be really helpful.

Ankit Hi,

When i used mean for RDD to ratings it works !

When I applied sum and len to the rating it gives me an RDD as output.

What is the error I am doing here? Hello everybody,

I have two RDD.

First RDD is 

moviesIDName2
[(u'$1,000,000 Duck (1971)', 2031), (u"'Night Mother (1986)", 3112), (u"'Til There Was You (1997)", 779)]
Second is 

final3
[(u'2001: A Space Odyssey (1968)', 811, 4.07521578298397), (u'Abyss, The (1989)', 835, 3.7005988023952097), (u'Air Force One (1997)', 522, 3.5632183908045976)]

I want to do a union two RDD with every RDD value. I try 

final1 = final3.union(moviesIDName2)

But this is wrong. I have an error

   map_values_fn = lambda (k, v): (k, f(v))
ValueError: too many values to unpack

How can I do an union this RDD?

Thanks in advance

Carlota Vina










 I've did the lab 4 until the point 4d, I can't upgrade the lab because the running time always is out. I was thinking this issue is related with virtual machine configuration, I have a computer with 4 processors but the virtual machine has only one enabled. I was trying to change the configuration, but it is protected, so the question is should I reinstall the virtual machine in order to fix this issue? Or is there another way to do that, and I've don't figured out yet. The subject is something arcane for me, for that reason any hint will be appreciated, thanks in advance. Finished my final assignments recently and on the occasion I would like to thank Prof. Joseph for offering a great course on EdX.  I very much enjoyed the Spark introductory journey especially using PySpark.  It made lot easier using Python than using Java.

Furthermore, with new concepts, like RDD, having notebooks with code templates were very important as they provided the direction to solve those tough assignments, especially lab 3!  I learned a lot and thanks to the excellent and prompt support throughout by all (Prof, TAs and community here).

Thanks again and look forward to using Spark,
~Shiva

 Lab exercises are helping to get the hands on experience needed.

Especially it took lot of time to do Lab 3 which I submitted only partial before 3-day window after due date.

I will try to finish Lab 3 when I get a chance to try again. 
 Hello UCB teaching staff,

First of all, thank you very much for all your effort on making this an excellent course.

I am wondering if it is okay to host the completed labs (IPython notebook files) in a public location such as Github *AFTER* the course is completed (i.e. after July 6). That's would be a nice way to showcase the work we have done.

Thanks. Really having trouble understanding the question and instructions, been spinning my wheels for a while on it now.

Firstly, I don't understand what is meant in the instructions regarding the following:
Create an RDD that is a combination of the small Google and small Amazon datasets that has as elements all pairs of elements (a, b) where a is in self and b is in other. The result will be an RDD of the form: [ ((Google URL1, Google String1), (Amazon ID1, Amazon String1)), ((Google URL1, Google String1), (Amazon ID2, Amazon String2)), ((Google URL2, Google String2), (Amazon ID1, Amazon String1)), ... ]
I understand how to simply "union" the two datasets, but [ ( (Google URL1, Google String1), Amazon ID1, (Amazon String 1) ) ...] appears to suggest the size of both amazonSmall and googleSmall are the same size.Secondly, I don't see where the computeSimilarity function is used.  I see that it's defined, but unlike previous examples, I do not see it used anywhere.  Can you clarify where it is to be used?

Thanks! Hello, 

I signed up to the scalable machine learning follow-up course on the verified track, but had not noticed this one had started earlier. While I will most likely try to complete this course during the weekend, verification is no longer open. So my questions:

-Is there still a way to verify for the purpose of the XSeries?
-If not, is there a plan to redo this course (and when?) 
-If the course is reopened will it be possible to combine the verified certificate of the other course with the new version? 

Thanks in advance for your answers. I understand that it's necessary to place deadlines for the appropriate management of things. Though since the verification process is the same for both courses I'd half expect even one verification to be sufficient (as there is no verification for the exams or assignments themselves) and that the work performed should be the determining factor. Perhaps a late fee would be a good option?

Thanks,

Michael Sluydts im getting a huge error for this code. Can someone please help me out. Im new to python and spark.

movieLimitedAndSortedByRatingRDD = <POSTING SOLUTIONS IS A VIOLATION OF THE HONOR CODE>

my movieNameWithAvgRatingsRDD was initialised as below

movieNameWithAvgRatingsRDD = (moviesRDD
                              <REDACTED>
 Now I've completed this course I am starting to use spark on our cluster at work.  While I feel confident with the basics of pyspark (thank you teachers, TAs, and fellow students!), looking at https://spark.apache.org/docs/latest/api/python/pyspark.html I see there's a lot more to learn.  One thing is that a lot of functions have a numpartitions parameter. Can someone give guidelines how that should be set?  Say if my cluster has n nodes, each with m GB RAM, the data I am processing is b bytes, is there some function f(n, m, b) that I should use  to supply the numpartitions parameter? Thanks! If you're still working on Lab 3, we encourage you to take a break and try Lab 4. Based on your feedback from Lab 3, we added lots of guidance and example outputs to Lab 4 - it should be much more approachable than Lab 3.

We think you'll be able complete Lab 4 quickly and then you can return to working on on Lab 3.

Please remember that the course ends Sunday night. Please plan on submitting all of your work to the autograder no later than July 5 23:00 UTC to allow for grading to complete before the end of the course an hour later at July 6 00:00 UTC. Due to the limited course staff that we are sharing with CS 190.1x (started Monday), we cannot extend the end of the course.

#pin I like this skill 
Please helps me understand 
I'm in need basic steps I joined late to the course and only in my 2nd week now. I won't be able to complete all my assignments by the end of the course. Will the course material be available after the course ends? Will the videos be available? Will the autograder work?


 Really enjoyed the course.  Though I did spend a lot of time in some of the labs trying to figure out what the task at hand really was instead of how to do it efficiently in Spark (this is a Spark course, right?).   Maybe it might help to have example input and output for each of the steps.  That would help people visualize the results and help them infer what is described in the text if they're not quite getting it.

Thank you again, Dan

P.S. Any thought of doing a Spark admin course? That is, how to understand and diagnose the operating environment?  i.e. spark monitoring for  execution bottlenecks, memory usage patterns, multi-user / submit issues, plus things like best practices for  system config, etc.   Functionality is all good but doesn't buy you much if you can't deliver a performing system. I didn't quite understand what movie matrix and the user matrix are.

Could you explain what 
mij (ith row and jth column of the movie matrix) and 
uij (ith row and jth column of the user matrix) represent?

Thanks in advance. Hey guys
I have started this pretty late andjust browsed through week 2 lectures .Now I am planing to complete Lab 1 but cant seem to find the instruction or any note about the lab 1 .Is it that edx removed instructions as its already July
edit : found it :) Task says to use myUserID, movieID, rating, to create our one RDD of most favourite movies. But, where to find movieID? Movies are given by their title, not ID. 

Am I missing something?
 I would like to get some hints on lab 4 - 2d. I am getting RMSE on the test set as below, not been able to figure out the problem  :
0.680712681192

In ALS.train, I have passed testRDD and used the rank as 8.

And I passed (predictedTestRDD, testRDD) as parameters to computeError.

print testForPredictingRDD.take(3)
[(1, 1193), (1, 2398), (1, 1035)]
print predictedTestRDD.take(3)
[Rating(user=1377, product=384, rating=2.900582415819435), Rating(user=2909, product=384, rating=3.956881731000366), Rating(user=1947, product=1084, rating=4.132897513455408)]
print testRDD.take(3)
[(1, 1193, 5.0), (1, 2398, 4.0), (1, 1035, 5.0)]


 
 Ya, still need to work on Lab4. Lab3 took over my life last 8-10 days. Almost want to give up lab3, but the digging and other smart & generous people in the class through Piazza help me a lot.

Like most learner students, want to say thank you for the great class before the class shut down. Yes, next round will be better (easier for student to learn). And again, without Piazza discussion lots of us may be in trouble - for me, that's the REAL fun  & Value of MOOC (online class in general). 
Dear instructors,

  I finished lab4 early and wanted to give it a shot at completing lab3.  I've shutdown jupyter and bounced my virtual box via vagrant halt/up.

  I keep getting this error on the first test in part 0 of lab3 :(

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-2-f2254825ff32> in <module>()
      1 # TEST Perform a Gold Standard evaluation (3e)
----> 2 Test.assertEquals(trueDupsCount, 146, 'incorrect trueDupsCount')
      3 Test.assertTrue(abs(avgSimDups - 0.264332573435) < 0.0000001, 'incorrect avgSimDups')
      4 Test.assertTrue(abs(avgSimNon - 0.00123476304656) < 0.0000001, 'incorrect avgSimNon')

NameError: name 'Test' is not defined

 
Iany hints on how to fix it?

TIA I've completed all the assignments, even not with a full score due to the late submission of lab3. It's really a tough but very helpful course that helps me step into the word of Apache Spark. Many thanks to the course team.
As to the lab4 regarding the recommendation using collaborative filtering method, I found an interesting blog for your reference, if you would like to get more ideas about that.
http://dataaspirant.com/2015/05/25/collaborative-filtering-recommendation-engine-implementation-in-python/ If yes, how it will be delivered?

This is my first MOOC, so please excuse my ignorance.

 I know Python but did not know how or where to use it.  This course taught me a lot about Big Data applications using Python and Spark.

The support from the staff was great!

I could have done better if I had more time to invest from work, but I have thoroughly enjoyed the step by step guidance in each of the labs.

Thanks again Professor.  Looking forward to more such courses.

Ram Hello when I run lab 3 3c this error is throwing. I don't understand can anyone help me?
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-26-9806b3b079da> in <module>()
     32             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     33             .collect()[0][2])
---> 34 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     35 print 'Requested similarity is %s.' % similarityAmazonGoogle

<ipython-input-26-9806b3b079da> in similar(amazonID, googleURL)
     30     """
     31     return (similarities
---> 32             .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))
     33             .collect()[0][2])
     34 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 64.0 failed 1 times, most recent failure: Lost task 0.0 in stage 64.0 (TID 239, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-26-9806b3b079da>", line 21, in <lambda>
  File "<ipython-input-26-9806b3b079da>", line 18, in computeSimilarity
  File "<ipython-input-24-c6de5b524bbd>", line 14, in cosineSimilarity
  File "<ipython-input-20-90860b7d6da7>", line 25, in tfidf
KeyError: 'and'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
</span></span></span></span></span></span></span></span></span></span></span>
<p><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansiyellow"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"> </span></span></span></span></span></span></span></span></span></span></span></p> Spent quite some time already trying to figure out how to iterate through a list while in a lambda function within a filter. Trying something along the lines of:
 
ThisRDD = (ThatRDD.filter(lambda (x,y): (for s in List: x != s[1])))

Short of editing the List so that it contains only single elements before running the filter, I'm at a loss.

but get SyntaxError: invalid syntax. Any ideas would be greatly appreciated. Thanks in advance. I have struggled hours trying to get lab 3 3c to work.  If I could get crossSmall figured out, the test would be downhill. I have looked to the api and the only thing I can find is Cartesian, witch yields 10 times the record.  I really want to finish all this if nothing more for the learning of it. Good evening guys, I am new with Python and I am having problems mapping a function on a RDD
the problem is more or less: I have a python user defined function:

def myfunction(a):
         line1
         line2
         line3
         return(what i want to return)

newRDD=someexistingRDD.map(lambda (x,y):(x, myfunction)) or

newRDD=someexistingRDD.map(lambda (x,y):(x, myfunction(x,y))

which one is the correct syntaxis?

Thanks for the help

 It doesn't make sense that we take(50) on a list of movies that doesn't provide the movieID. This doesn't help us find the movieID, as the last number is the number of reviews. Even the explanation says the movieID is the last number on each line, but doesn't clarify which RDD that is in. Definitely not the cell above which shows the highest rated movies.  A huge thank you to the instructors and TAs involved in this course.
This was my first EDx course (done a few MOOCs in Coursera before) and an excellent experience.

I've learnt a lot and look forward to the ML course!

best regards
--balu Hi Prof. Anthony and other teaching staff
 Can you please suggest a road-map for students who want to enhance their Spark development skills?

This could include
1. reading references
2. Suggestions about doing pilot projects on publicly available data. The data is available, but the challenge is defining the problem and doing a meaningful data analysis.

Thanks I want to say thanks to the staff for the opportunity to create my own recommendation system.

I'm super fan of Akira Kurosawa and Toshiro Mifune and I have a nice suprise when the recommendation system that I build in this class recommend to me one of their films.

Really loves this course. I've have learned a lot.

Thanks again!


  Hi,

My laptop has 4 cores, so if I divide my dataset/collection in more than 4 parts i.e sc.parallelize(collection,n) where n > 4, how is it processed?

Thank You! This forum has interesting discussion and resources. Will this information be accessible forever or will it be removed after the course ends? Hello,

I'm confusing how to calculate RMSE , this is my output:

[((1, 1), 5), ((1, 2), 3), ((1, 3), 4), ((2, 1), 3)]  ---predictedReformattedRDD
[((1, 2), 3), ((1, 3), 5), ((2, 1), 5), ((2, 2), 1)]  ---  actualReformattedRDD
[((1, 3), 1), ((2, 2), 1), ((1, 2), 0), ((2, 1), 4)]   --- squaredErrorsRDD
((1, 3), 1, (2, 2), 1, (1, 2), 0, (2, 1), 4)          --- totalError
4                                                     ---  numRatings

I really appreciate if someone can give example how to caclulate RMSE,

Thanks, By using ALS I obtain RMSE that close to 0.9 and 
by using average prediction RMSE is close to 1.12.

I think that one of the points of this Lab is to conclude
that ALS performs much better than Average prediction method.

But with Average rating close to 3.57 as for me 1.12 is
at least somewhat comparable with 0.9 RMSE.

Please, can someone explain for me why given these
RMSE values ALS is much better than Average?

Thanks.  
k = sc.parallelize(IDandRatingsTuple)

count = len(IDandRatingsTuple[1]) avg = float(sum(IDandRatingsTuple[1])/count)
result = k.map(lambda (x, y) :  (x, count(y), avg(y))).groupByKey()

I got an error 
PythonRDD[101] at RDD at PythonRDD.scala:43 I am stuck on computing total squared error. Instead of 6, totalError is coming to 4. Code below:

SquaredErrors RDD is:  [((1, 3), 1), ((2, 2), 1), ((1, 2), 0), ((2, 1), 4)]

# Compute the total squared error - do not use collect() totalError = squaredErrorsRDD.reduceByKey(lambda ((a,b),c): sum(b)) print "totalError: ", totalError.count()

OUTPUT:
SquaredErrors: [((1, 3), 1), ((2, 2), 1), ((1, 2), 0), ((2, 1), 4)]totalError: 4

Can someone suggest what I am doing wrong?
 Hello, people,

I've done lab3, but my code runs takes almost an hour to run (because of 3c and a check in 3c each take 25 mins, everything else takes a very little time), but I pass all the checks locally and the grader gives me timeout (and I tried a couple of times)
My code is dead simple, and I can't seem to find where did I go wrong, I d not have additional print and collect statements. After reading the forum I think that the problem is in my basic functions (which are straightforward and so I don't get how can you optimize them to run 3c in 3 minutes not in 25!) so I hope I won't break any rules by posting some parts of them (the ones most critical as I think as they are used everywhere below).

POSTING SOLUTIONS IS A VIOLATION OF THE HONOR CODE

simpleTokenize:
split_regex = r'\W+'
<REDACTED>
tokenize:
return <REDACTED>
tf:
    for item in tokens:
        <REDACTED>
idfs:
<REDACTED>
ifidf:
   <REDACTED> 
I would be super grateful for any help, I spent lots of time and health on this lab and I dream to get it accepted :) I found from the internet that you stay in a cell and type ctrl-l.
When I do that the cursor goes to the URL box in the top. I am using google chrome to run ipython notebook.

Please help.
 Whenever I try to start my vm I get the following error:



help.What is this all of a sudden? Hi Prof. Anthony and teaching staff
 I have been doing Oracle development as my job for a long time. Oracle has been maintaining a website called asktom.com
Tom Kyte is a Oracle Guru. He and his staff answer user's questions for free. Those questions include questions about how to do something in SQL or PL/SQL.
Of course, they answer questions about Oracle specific implementation of SQL and general questions about data structures and algorithms.

Can Databricks do something like this?
Advantages for Databricks:
1. increasing user base. The more successful users you have, they will market Apache Spark themselves by word of mouth, so you get free marketing.

2. You can better compete with other commercial players like SAP Hana.

3. Many small to medium size businesses can not afford huge expenses for commercial products like SAP Hana. So if you operate a forum as suggested above, you would get a big chunk of small to medium size business market space.

This would cost you some staff salaries, but in my humble opinion your ROI could be huge.
You could have some highly skilled people leading in the US and the rest of the staff can be in a place like India where high tech labor is cheap.

Please see if you can commit resources for such a forum.

Thanks in advance
 I always had a problem with my short memory and a cheatsheet is more than welcome. 
Found Pyspark-Pictures by  Googling IMAGES with pyspark cheat sheat 
Another resource that I find useful to help in Choosing the right estimator (and critique) 





 In Lab 4 - 3c we are training a model with our ratings added. Assuming some production environment with a huge user-product space, do we really have to train a model every time new user added to the system? What other options do we have here?  Hello! Please answer me: what is  the python analogue of sc.parallize? Thanks I have fallen behind in the SPARK course CS 100.1X. Even though I signed up (and paid) to take it with a certificate I realize that between work responsibilities and this course– there isn’t sufficient time for me to stay on track with the assignments – and I plan to complete them at a slower pace. I was wondering a few things-
                When is the course completely done?                Until when can I submit material for creditwill I be  able to access the online course material once the course is over? (otherwise I will spend time to download everything in advance so I can continue to take it.Will I be able to continue to log into piazza to see course postings?Will autograder continue to work if I want to submit test just for my own purposes to see how I have done (or until when can I submit to the auto grader) 
While I enjoyed the course, there are a few things that I would suggest that I think would have been helpful
I found that in the assignments I spent considerable time just trying to decipher what was being asked for. Sample output of what each step in an assignment should look like would have been very helpfulThe videos were excellent as quick introduction to a topic – but if the videos followed through with a few examples using the functions would have been really helpful.For those of us who are balancing the course and assignments with work related deadlines (that will likely take priority over this) a version of the course that is at a slower pace would be a really good option (maybe self paced?)
 
 
Thanks,
 
Abie Hello I have doubt.
What is the exact function of similarities

similarities.filter(lambda record: (record[0] == googleURL and record[1] == amazonID)).collect()[0][2])

Another doubt is crossSmall is of type
[ ((Google URL1, Google String1), (Amazon ID1, Amazon String1)), ((Google URL1, Google String1), (Amazon ID2, Amazon String2)), ((Google URL2, Google String2), (Amazon ID1, Amazon String1)), ... ]
 and I have used similarities = (crossSmall.map(computeSimilarity).cache()). Is this correct?
 hi please help me how to get the python iterable format in the second half of the tuple. All i am trying do is

movieIDsWithRatingsRDD = (ratingsRDD
                                              .map(lambda (x,y,z) : (x,(z))))
for which the output is
movieIDsWithRatingsRDD: [(1, 5.0), (1, 3.0), (1, 5.0)]
Please help me i am unable to figure out how to get the iterable format good morning, I am definitely having problems passing the function
getCountsAndAverages to a RDD

i use someting lke someRDD.map(lambda (x,y):(x,getCountsAndAverages)) and give me errors
some advice or hint
thanks for your help I think my code is right but something needs to be inside a bracket or something

   .filter(lambda x: (0,x[0]) if x[0] not in mylist)   

I get invalid syntax. Thanks for the help. The second last bullet states:
For validation, use the testRDDand your computeError function to compute the RMSE between testRDD and the predictedTestRDD from the model.
Instead of testRDD we should be using testForPredictingRDD for validation ! Hi Prof. Anthony and other teaching staff,
You all have done a commendable job of starting to popularize Apache Spark by offering this MOOC.
I had been looking for a Hadoop MOOC, but never found one.

 I passed the lab 3 with 100%(80% for late submissions)

This took me 70+ hours to complete.
I was stuck in 4f for a long time because:
1. The requirements were not clear to me.
2. The template code was ambiguous and variable names would not convey what needs to be done.
3. My 4e was wrong because my 4c was wrong
4. My 4c was wrong because I did not have correct type for broadcast variables.
5. The unit test passed for 4c and 4e because the auto-grader did not check for types of objects created.

Here are some suggestions for future course sessions:
1. Improve auto-grader with checking of types of objects created.
2. If your goal is to popularize Apache Soark and market Databricks, you could do following:
    a. For each task specify requirements clearly. Specify what is the expected return type of objects created.
    b. In code templates, choose meaningful names for variables.
    c. When writing lab descriptions, use a skilled tech writer because in general, techies do not write correct, understandable English.

Here is one observation:
UCB gets the cream of students from all over the world in their under-grad and graduate programs.
If you design the MOOC for that kind of student population, then other students would get frustrated and leave the course and may not touch Apache Spark again thinking it is too hard.

If you put extra efforts now, two things could happen
1. Apache Spark will gain major market share as compared to SAP Hana etc.
2. Databricks would increase their business and if they bring out a successful IPO, all stake holders would become instant millionaires.

Thanks
 can anyone give me some detailed clue about how to get the iterable in 1b first part! 
Lab 4 - 3E and 3F - I am getting below Error when accessing predictedRatingsRDD in 3E and 3F.  Would appreciate some hint/help.

In lab 3-d the RMSE was computed as .891955339853, and
predictedTestMyRatingsRDD.take(3) is as follows:

[Rating(user=1377, product=384, rating=2.2374001604380482), Rating(user=2909, product=384, rating=3.506568199514622), Rating(user=1947, product=1084, rating=4.015931923693815)]

But then in 3-e, getting the below error when trying to print out predictedRatingsRDD.count() or predictedRatingsRDD.take(3)

  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/recommendation.py", line 107, in <lambda>
    user_product = user_product.map(lambda (u, p): (int(u), int(p)))
ValueError: invalid literal for int() with base 10: 'Jumanji (1995)'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)

predictedTestMyRatingsRDD  was computed using:

myRatingsModel.predictAll(testForPredictingRDD)

  rawRatings = sc.textFile(ratingsFilename).repartition(numPartitions)
When this is done, I believe the partition by default is the no of cores available. what will re-partition ensure? Please point me to a doc which talk about it in detail? I finished this wonderful course with a total score of 90%. Many compliments to Prof. Anthony Joseph and to the great University of California Berkley.
I learned a lot of usefull concepts about Big data world.
Ciao a tutti dall'Italia. Hello everbody,

I would like to ask a question about exercise 3e.

I have a RDD

movies2
[(0, 1), (0, 2), (0, 3)]


I executed 

myRatingsModel = ALS.train(trainingWithMyRatingsRDD, 4, seed=seed, iterations=5, lambda_=0.1)

When I apply predictAll

predictedRatingsRDD = myRatingsModel.predictAll(movies2)

print predictedRatingsRDD.collect()

[]


Why is the predictAll empty?

Thanks in advance

Carlota Vina









 I am sure there is plenty more to cover, specially when trying to trouble shoot and understand that stack output. More one spark SQL and Graph X, I do not know how much of it is covered in the ML course. I carried out most of the course through starbucks coffee shop, restarting the codes and remembering everything from the top was challenging as I did a few hours at a time that causes you lose your train of thoughts. The restarting of the cells was a killer.

Thanks for the great course. Actually the start of what you are doing for the ML course should be included in this one too, as basic understanding of how to look at the data and how to pass it along can reduce a lot of confusions. Hi,
I have passed this course with 99% and I should thank everyone who contributed to this great MOOC. I have also enrolled in CS190.1x but I would like to ask instructors (or even students) for any suggestions about additional learning or materials in Spark. 
Specifically I need to know how to learn and master Apache Spark with Scala. What are the best resources, materials, or classes for that to build on the knowledge we gained in this course? I have found it hard to find a similar MOOC or video training focused on Scala.
Thanks
S Anthony and Student TAs:

    Thanks for all the hardwork in putting this class together. I learned more than what I could have imagined in a little over 4 weeks. One thing I would suggest is that you have some supplementary material on Python. I thought I knew Python fairly well(I created a football game, a vocabulary quiz based on SAT words, coded up a ANN Deep Learning algorithm with hidden layers and feedback). Dictionaries, list comprehensions, etc. would be extremely helpful to learn. Potentially an optional lab with that material and more to help those with only basic programming skills to get up to speed on the necessaries. 

      In any case...thanks again!!

Jeff 
# TODO: Replace <FILL IN> with appropriate code
import math

def computeError(predictedRDD, actualRDD):
<REDACTED>

# sc.parallelize turns a Python list into a Spark RDD.
testPredicted = sc.parallelize([
    (1, 1, 5),
    (1, 2, 3),
    (1, 3, 4),
    (2, 1, 3),
    (2, 2, 2),
    (2, 3, 4)])
testActual = sc.parallelize([
     (1, 2, 3),
     (1, 3, 5),
     (2, 1, 5),
     (2, 2, 1)])
testPredicted2 = sc.parallelize([
     (2, 2, 5),
     (1, 2, 5)])
testError = computeError(testPredicted, testActual)
print 'Error for test dataset (should be 1.22474487139): %s' % testError

testError2 = computeError(testPredicted2, testActual)
print 'Error for test dataset2 (should be 3.16227766017): %s' % testError2

testError3 = computeError(testActual, testActual)
print 'Error for testActual dataset (should be 0.0): %s' % testError3
Im getting an error for this code above. I really don't know how this error is coming.

File "<ipython-input-66-09bc0ca53786>", line 37
    testPredicted = sc.parallelize([
                ^
SyntaxError: invalid syntax

  I got this, and failed the last test. What could be wrong ? 
movieNameWithAvgRatingsRDD: [(3.6818181818181817, u'Happiest Millionaire, The (1967)', 2049), (3.0468227424749164, u'Grumpier Old Men (1995)', 3), (2.882978723404255, u'Hocus Pocus (1993)', 2052)] If we have two RDD that we have sorted using "sortByKey" and we apply the join transformation, shouldnt we expect a RDD sorted by key?

My experiments show that it's not the case. If I want the resulting RDD to be sorted, I need to apply sortByKey to it.

Why? Hello 
I have problem with python. when I try to generate list from tuple like that

for key in IDandRatingsTuple:       temp=list(IDandRatingsTuple[key])
this error is throwing
I don't understand. How can I get tuple inside of tuple
---> 16         temp=list(IDandRatingsTuple[key])
     17         total=0.0
     18         itemTotal=0.0

TypeError: tuple indices must be integers, not tuple Currently 
I am doing it like this:
#Use RDD transformations with PredictedWithCountsRDD and moviesRDD to yield an RDD with tuples of the form (Predicted Rating, Movie Name, number of ratings), for movies with more than 75 ratings

getting IndexError: tuple index out of range

Got it it was obvious!!!

 Hey all I ran through my lab 4 and I don't see any errors it all ties out fine. However when I submit I get an error but I looked at the code and the assert it looks right so I'm not sure exactly what its failing on, I do have lab 4 1.0.2

Number of Ratings and Average Ratings for a Movie (1a)
------------------------------------------------------
All tests passed
Movies with Highest Average Ratings (1b)
----------------------------------------
All tests passed
Movies with Highest Average Ratings and more than 500 Reviews (1c)
------------------------------------------------------------------
All tests passed
Root Mean Square Error (2b)
---------------------------
Traceback (most recent call last):
  File "", line 2, in 
  File "/usr/local/lib/python2.7/dist-packages/test_helper/test_helper.py", line 34, in assertTrue
    raise TestFailure(msg)
TestFailure: incorrect testError (expected 1.22474487139)

Using ALS.train (2c)
--------------------
All tests passed
Testing Your Model (2d)
-----------------------
All tests passed
Comparing Your Model (2e)
-------------------------
All tests passed
-- 6 cases passed (85.0%) --

my Submission token is: 
1565321-bf82a9456ae414ecb20dbb1913fa7221:ip-172-31-28-149 In instances in more than one lab, I'm getting errors when trying to use count() and take() after transforming RDDs. Might there be something I'm doing wrong in general? Hello friends,I'm having a (almost permanent) problem in question 4f.My amazonWeightsRDD.take(1)[('b000jz4hqo', {'rom': 2.4051362683438153, 'clickart': 56.65432098765432, '950': 254.94444444444443, 'image': 3.6948470209339774, 'premier': 9.27070707070707, '000': 6.218157181571815, 'dvd': 1.287598204264871, 'broderbund': 22.169082125603865, 'pack': 2.98180636777128})]My googleWeightsRDD.take(1)[('http://www.google.com/base/feeds/snippets/11125907881740407428', {'quickbooks': 17.48190476190476, '2007': 4.985334057577403, 'learning': 5.932773109243698, 'intuit': 13.379008746355684})]My commonTokens[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/18376072611700638452'), ['120']), (('b000bnb72g', 'http://www.google.com/base/feeds/snippets/6959766318020460134'), ['software']), (('b00009apna', 'http://www.google.com/base/feeds/snippets/13986409474793202514'), ['business']), (('b00004ochi', 'http://www.google.com/base/feeds/snippets/7147648211015076337'), ['rom']), (('b000gcgqvy', 'http://www.google.com/base/feeds/snippets/6874875179525744781'), ['win', 'xp'])]And I'm always getting the error message:---------------------------------------------------------------------------Py4JJavaError Traceback (most recent call last) in () 32 .map(fastCosineSimilarity) 33 .cache())---> 34 print similaritiesFullRDD.count()/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self) 930 3 931 """--> 932 return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum() 933  934 def stats(self):/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self) 921 6.0 922 """--> 923 return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add) 924  925 def count(self):/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f) 737 yield reduce(f, iterator, initial) 738 --> 739 vals = self.mapPartitions(func).collect() 740 if vals: 741 return reduce(f, vals)/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self) 711 """ 712 with SCCallSiteSync(self.context) as css:--> 713 port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd()) 714 return list(_load_from_socket(port, self._jrdd_deserializer)) 715 /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args) 536 answer = self.gateway_client.send_command(command) 537 return_value = get_return_value(answer, self.gateway_client,--> 538 self.target_id, self.name) 539  540 for temp_arg in temp_args:/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 298 raise Py4JJavaError( 299 'An error occurred while calling {0}{1}{2}.\n'.--> 300 format(target_id, '.', name), value) 301 else: 302 raise Py4JError(Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 257.0 failed 1 times, most recent failure: Lost task 0.0 in stage 257.0 (TID 938, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last): File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main process() File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process serializer.dump_stream(func(split_index, iterator), outfile) File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream vs = list(itertools.islice(iterator, batch)) File "", line 24, in fastCosineSimilarity File "", line 24, in AttributeError: 'dict' object has no attribute 'value' at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
 at org.apache.spark.api.python.PythonRDD$$anon$1.(PythonRDD.scala:176) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70) at org.apache.spark.rdd.RDD.iterator(RDD.scala:242) at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
 at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
 at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
 at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
 at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
 at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)<br /><br />My function:<br />def fastCosineSimilarity(record):<br /><br /> """ Compute Cosine Similarity using Broadcast variables<br /> Args:<br /> record: ((ID, URL), token)<br /> Returns:<br /> pair: ((ID, URL), cosine similarity value)<br /> """<br /> amazonRec = record[0][0]<br /> googleRec = record[0][1]<br /><br /> tokens = [record[1]]<br /> <br /> s = sum((amazonWeightsBroadcast.value[amazonRec][token] * googleWeightsBroadcast.value[googleRec][token] for token in tokens))<br /> value = s/(amazonNormsBroadcast.value[amazonRec])/(googleNormsBroadcast.value[googleRec])<br /> key = (amazonRec, googleRec)<br /><br /> return (key, value)<br /><br /><br />Thanks in advance.</init></genexpr></ipython-input-98-ff5114e17f1d></ipython-input-98-ff5114e17f1d></module></ipython-input-98-ff5114e17f1d></p> On local IPython notebooks, I can execute %qtconsole in a cell and get a terminal to pop up in the same kernel - helpful for quickly scratching out code.  Is there any way to do something similar in this environment - it seems the best alternative is creating and deleting scratch cells.

Thanks. I was wondering if someone could double check my results in lab 4. I do pass the test, submission id 
 1566318-aaf2100d8e0d921798b1781d96a8e60c:ip-172-31-22-195my problem is that the final results in 3f do not make sense.my ratings give 5 to toy story and 1 to Jaws. similarly 5s to other children's movies and 1s to horror movies. But my top recommendations in 3f are all horror movies, Texas Chainsaw Massacre the top choice.Could someone with working code run thru my set of ratings and see what is the top recommendation that their code provides? My ratings are (0, 993, 5), (0, 941, 1), (0, 750, 1), (0, 603, 1), (0, 789, 5), (0, 562, 1), (0, 539, 1), (0, 1083, 5), (0, 817, 5), (0, 594, 1)thanks so much in advance! hi all,

i'm getting this error while trying to run "vagrant up --provider=virtualbox", any idea of why i'm getting this error?

Thanks. Are there major use cases where Hadoop MapReduce remains a far better choice than Spark? I could believe for simple, one-pass ETL-like jobs -- e.g., data transformation or data integration. But what if the data doesn't fit memory -- does Spark handle that well enough? Do you see Spark broadly superior to Hadoop MapReduce?
Great course; much appreciated! Hi,

I am enjoying doing lab 4. Question 2c seems too simple with just  line of change as below -

validationForPredictRDD = <REDACTED>

However, it doesn't gives me the output and keeps running until infinity. It would be great, if you
can please help me know where I am going wrong. I tried to restart my laptop even tried to stop/start
vagrant, but still I don't get output from it.

Please help! I really want to complete this course and take a deep dive into Spark once done, but I am still stuck at this point. Today, I submitted my lab 4 assignment and with that I completed the course.

Many kudos to the Instructor's (particularly Prof. Anthony) and fellow students for the wonderful tips and guidance.

Once again thanks,

Ram Could you provide examples of :


1) what you mean by "build a reliable transmission protocol using a relay server"

2) dependencies between data streams and processing steps

Thanks

TJ
 I am new in this course.

 I installed oracle vm and vagrant.

I opened the oracle vm and select a vm from left menu and log-in it with `vagrant` for user/and password successfully.

But now when i try http://127.0.0.1:8001/ browser can't open it.

What should i do?

(The 8001 port is not busy) Hello,

I got error when using predictAll, just follow the instruction on section 2d.
output is :


---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-38-f8f674d551aa> in <module>()
      3                       lambda_=regularizationParameter)
      4 testForPredictingRDD = testRDD.map(lambda x,y,z:(x,y))
----> 5 predictedTestRDD = myModel.predictAll()
      6 
      7 testRMSE = computeError(testRDD, predictedTestRDD)

TypeError: predictAll() takes exactly 2 arguments (1 given)

  for movieNameWithAvgRatingsRDD:

<REDACTED>
 got an error:

AttributeError: 'PipelinedRDD' object has no attribute 'Join'



 Thanks to Professor Joseph, the TAs and all classmates, without your help I think I can’t advance and gain my certified.

I’d like to do a reflection about my experience here. When I saw few months ago the course's promotion I didn't know anything about Spark, only that I read in some book that it would be the replace of Hadoop (and didn't nothing about Hadoop too, only it was used in big data) in the future, and this has been offered by one of the most important universities in the world, but nothing more.

So, I think, I could try, even with the constraints that I have: 1. Language (English isn’t my mother language as you can note in this reading) . 2. I ‘m not an expert programmer and my python knowledge is almost null, and 3. I don’t know anything about spark. Therefore, I think it will be good for me if I'm subscribe and watch the videos.

This class became a challenge for me, and beyond to earn a piece of paper, the mere fact of having gone as far, it's very valuable to me.

I think the teacher and the instructors made a great effort. I think I learned a lot of things too, and I consider the selfless contribution of all classmates was amazing, and that is an excellent environment for learning through collaboration with others. Forgive my English and thanks to everyone!
 I know the answer to this question is simple, but for some reason I am completely blocked. I used the takeOrdered method and I am reversing using a lambda function. I tried both of these with different results

key= lambda s: -1 * s[1] ==> Highest token is 9
key=lambda s: s[-1] ==> Highest has 19 tokens

I tried the sortBy() method too, but I get the highest tokens to be 61 the function I am passing in here is (lambda s: s[1], False).collect()

What am I doing wrong?

 let's say I have these commands

mydata1 = sc.parallelize([12, 23, 44, 12, 20])mysum1 = mydata1.reduce(lambda a,b:a+b)print mysum1
111

why I can not use reduce with tuples? I would like something like that to give the same result but i get an error
mydata2 = sc.parallelize([    ('a', 12),    ('b', 23),    ('c', 44),    ('d', 12),    ('e', 20)])
mysum2=mydata2.reduce(lambda a,b:a[1]+b[1] )


finally if I do this
mysum3=mydata2.reduce(lambda a,b:a+b)print mysum3
I get
('a', 12, 'b', 23, 'c', 44, 'd', 12, 'e', 20)
how does Spark handle a+b in this case? Please Change

"Using the model and validationForPredictRDD, we can predict rating values by calling model.predictAll() with the validationForPredictRDD dataset, where model is the model we generated with ALS.train(). predictAll accepts an RDD with each entry in the format (userID, movieID) and outputs an RDD with each entry in the format (userID, movieID, rating)."

"Using the model and validationForPredictRDD, we can predict rating values by calling model.predictAll() with the validationForPredictRDD dataset, where model is the model we generated with ALS.train(). Function model.predictAll accepts an RDD with each entry in the format (userID, movieID) and outputs an RDD with each entry in the format (userID, movieID, rating)." Hi,

Can i resubmit lab3 again, i just finish lab4 and now move on to finish lab3 which lab3 takes huge time for me.

Thanks Was all-in-all a very worthwhile way to spend my time and an effective way to learn the basics of Python as utilized in Spark. I look forward to delving deeper into data science.   Hi All,
 I do some hobby projects at my one of my home pc. I have installed Oracle and written some PL/SQL programs to do data analysis on publicly available data.

Is Python a procedural language for Spark?
My guess is yes and I can covert my PL/SQL code to Spark/Python using control statements like loop, if then else etc.

But that wouldn't be fun. Would it be?
My idea is to implement my procedural algorithm using map/reduce transforms(there could be several of them)

I would like to benchmark Spark against Oracle.
Has anyone in this forum converted Oracle PL/SQL code to Spark map-reduce?
Can you share your experiences doing it?

One more question: How to persist data in Spark? Please give me some pointers.

Thanks
 For my lab 3 I submitted on time and earned 94% grade (I had not finished the last problem). Then, today, in the after deadline period, I submitted lab 3 again, having finished the last problem. I passed every test, including the one I had missed before and earned 80% (this was expected). Ok, so I checked my progress to find that edX (unlike Coursera) didn't keep my best score of 94%! What!? Wait minute, Why should I be punished for just wanting to see that I was able to get all of the answers right--I've praised this course elsewhere in these comments and I have valued it and worked quite hard because I'm very invested in the technology. I'm here to learn. There is no logical reason to lower my score just because I submitted again. I don't deserve that. I know this is just because edX hasn't acheived that level of refinement yet (my mistake for trusting they had!) So my lab 3 lost 14% just because I was working hard to prove to myself I could finish the lab correctly. This is wrong. There was no warning that subsequent scores might be lowered on follow up submissions. So come on. I hate to be begging online here for fairness. And I'm sorry for being upset, but doesn't feel good to have worked as hard as I have to be burned like this. .foldByKey([], lambda v1,v2: v1+[v2] if type(v2) != list else v1+v2)

looks somewhat hard to read. Has anybody figured out a better way to run foldByKey to produce an array? Is it possible to apply collaborative filtering to demographics data as well as to their purchasing history?
Let's say we have a set of customers (users), and we know some of their demographics data (age, education, income etc) - D attributes;
and their purchasing history (categories of stuff they bought or sometimes even SKUs of items they bought) - P attributes.

So the question is if collaborative filtering algorithm would be applicable to both D and P attributes for our company?
Let's say we will treat (D,P) attributes as "movies" and "ratings" will be either 0 or 1: 0 for categorical D attributes that are not known about that user and 1 for known, then probably each categorical D value should be "exploaded" into separate attributes. 
P attributes "rating" will be higher depending on how much or how often that user has bought stuff of that SKU.

So the hope is that be adding demographics data, collaborative filtering would work more precise because we add also some relevant information about those people that can help build stronger prediction models. I was just curious about using collaborative filtering in a real world application with millions of users and lots of products. When we added our own ratings to the training RDD, we had to retrain the entire model over again. 

It seems like a lot of computation to re-train the entire model just because a new user signed up and/or a user rated a product for the first time. Is there a way to approximately update the model continuously without retraining the whole thing from scratch? Or do companies wait for enough new users and new ratings to pile up before retraining? Seems like you couldn't do that or a new user wouldn't get recommendations right away. 


The following description says movieID, but the code gives number of reviews:

To help you provide ratings for yourself, we have included the following code to list the names and movie IDs of the 50 highest-rated movies from movieLimitedAndSortedByRatingRDD which we created in part 1 the lab.¶





In [24]:


















print 'Most rated movies:'
print '(average rating, movie name, number of reviews)'
for ratingsTuple in movieLimitedAndSortedByRatingRDD.take(50):
    print ratingsTuple










 Hello I really don't understand the third part of lab 4 1b 
I use join but all of the data are joined. I cannot generate map because in map there is key value.
So what should I use in third part ? Can any one explain a little

Thanks I already finish this lab, but I'm wondering if what I did for 3e is optimal. What I did was:

1) I made a list called ratedMovies, that contains all the movieIDs of the movies I rated.
2) Applied a transformation to remove the movies that I had already rated from moviesRDD, which left me and RDD with the form (movieID, title), the same one as moviesRDD
3) Transform that to the form (myUserID, movieID), that is [(0,1),(0,2),...]

Basically,

ratedMovies = take movieIDs of the movies I rated in 3a
myUnratedMoviesRDD = (moviesRDD
		      .remove rated movies
		      .transform to required form)
Hard to explain without posting any code, but I'm not sure if it matters as this part isn't graded...

Anyway, the instructions say that it can be done with only one transformation, but I used two, plus another operation beforehand (taking elements from a list), any suggestions on how to improve it?
 This is by far the most exciting lab (although it was lot easier than the last one). I really enjoyed the whole course and especially this particular lab. I would like to thank the course staff for their invaluable input. Here is the list of the movies the recommendar system has suggested to me:
My highest rated movies as predicted (for movies with more than 75 reviews):
(4.013340999366559, u'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)', 278.0)
(4.002804205622631, u'Close Shave, A (1995)', 318.0)
(3.9928804120544807, u'Wrong Trousers, The (1993)', 425.0)
(3.9810190183370513, u'To Kill a Mockingbird (1962)', 451.0)
(3.9619543491150426, u'Raiders of the Lost Ark (1981)', 1195.0)
(3.9434016455662837, u'Star Wars: Episode IV - A New Hope (1977)', 1447.0)
(3.9179307368093235, u'Paths of Glory (1957)', 105.0)
(3.915710791806412, u'Rear Window (1954)', 503.0)
(3.9122457360514247, u'Bridge on the River Kwai, The (1957)', 481.0)
(3.911850019242983, u'Double Indemnity (1944)', 274.0)
(3.9046977928410853, u'Jean de Florette (1986)', 93.0)
(3.8952100151068025, u'North by Northwest (1959)', 700.0)
(3.888786315021378, u'12 Angry Men (1957)', 314.0)
(3.8800009251392344, u'Sixth Sense, The (1999)', 1110.0)
(3.8723695530552895, u'Maltese Falcon, The (1941)', 476.0)
(3.8701769266265944, u"One Flew Over the Cuckoo's Nest (1975)", 811.0)
(3.8654917901970602, u'Monty Python and the Holy Grail (1974)', 759.0)
(3.8632780713582875, u'Inherit the Wind (1960)', 133.0)
(3.8616810823429244, u'Wallace & Gromit: The Best of Aardman Animation (1996)', 202.0)
(3.8563415989051677, u'Great Escape, The (1963)', 360.0)
I have to say, its pretty close and I would love to check out the movies in the list that I haven't already seen.
Hope to learn more in CS190.1x (hopefully see some of you there) I tried to save the file to .py but the saved as .txt, tried to create a new file the result still the same. Anyone has any ideas for what happened.
I managed to submit and graded by autograder with .txt format Just want to say thank you for running this course. I have completed Lab 4. For question 3e, my predictions came back with the below data.

(5.615522768640832, u"Schindler's List (1993)", 1171)
(5.53277180639906, u'Mr. Smith Goes to Washington (1939)', 171)
(5.525497565868419, u'Anatomy of a Murder (1959)', 90)
(5.518193713937695, u'Toy Story 2 (1999)', 860)
(5.495846123594452, u'Sixth Sense, The (1999)', 1110)
(5.478569229947633, u"Singin' in the Rain (1952)", 356)
Why are these ratings above 5? My initial rating list for 10 movies had only 4 & 5.  amazonWeightsRDD = amazonFullRecToToken.map(lambda x: (x[0],tfidf(x[1], idfsFullBroadcast.value)))googleWeightsRDD = googleFullRecToToken.map(lambda x: (x[0],tfidf(x[1], idfsFullBroadcast.value)))

I tried the above method ,but I am getting following error message 
<ipython-input-46-6770aaed2ecb> in <module>()
     13 amazonWeightsRDD = amazonFullRecToToken.map(lambda x: (x[0],tfidf(x[1], idfsFullBroadcast.value)))
     14 googleWeightsRDD = googleFullRecToToken.map(lambda x: (x[0],tfidf(x[1], idfsFullBroadcast.value)))
---> 15 print 'There are %s Amazon weights and %s Google weights.' % (amazonWeightsRDD.count(),googleWeightsRDD.count())
     16 #print amazonWeightsRDD

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 
  536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 160.0 failed 1 times, most recent failure: Lost task 0.0 in stage 160.0 (TID 917, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 932, in <genexpr>
return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "<ipython-input-46-6770aaed2ecb>", line 13, in <lambda>
  File "<ipython-input-20-2ada708dc7d1>", line 11, in tfidf
  File "<ipython-input-20-2ada708dc7d1>", line 11, in <dictcomp>
TypeError: list indices must be integers, not str

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
  I am stuck at Lab 3 4b as I am facing following error:
ValueError: too many values to unpack
This error typically occurs after calling the count on the idfsFull RDD. I am wondering whether it is a problem with my idfs implementation. But the idfs implementation passes all the tests in the lab. sheet. I am not sure how to proceed with fixing this issue.StackTrace:
4 idfsFullCount = idfsFull.count()
      5 print 'There are %s unique tokens in the full datasets.' % idfsFullCount
      6 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 135.0 failed 1 times, most recent failure: Lost task 0.0 in stage 135.0 (TID 717, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 2252, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 282, in func
    return f(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1705, in combineLocally
    merger.mergeValues(iterator)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
  File "<ipython-input-51-3ede3e905831>", line 12, in <lambda>
ValueError: too many values to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

  but a little difficult (spécial lab3) ! What is the intuition behind "Another way to evaluate the model is to evaluate the error from a test set where every rating is the average rating for the training set."?Why is that a valid way to evaluate the model error? Hi,
Can anyoone help in solving issues of Lab 4
I m stuck in 1st i.e 1a.
I tried alot and wasted my 4-5 hours in it. This works in Python:

>>> list = (1,('a','b','c'))>>> list[0]1>>> list[1]('a', 'b', 'c')>>> list[1][0]'a'>>> (list[1][0],list[1][2])('a', 'c')

But in pySpark, when doing:

listRDD.map(lambda list: (list[1][0],list[1][2]))
... I am getting errors.

Why is it that native python works as expected but pySpark not? And... last but not least, how to achieve the same in pySpark? Hello all,

I am thinking of moving ahead and start using spark for some given set of images and perform some image analysis stuff (like: blob detection).

Any help from anyone would be appreciable. My code successfully constructed the RDDs and results as described:

movieIDsWithRatingsRDD: [(2, <pyspark.resultiterable.ResultIterable object at 0xb0e8d96c>), (4, <pyspark.resultiterable.ResultIterable object at 0xb0e8d92c>), (6, <pyspark.resultiterable.ResultIterable object at 0xb0e8d28c>)]

movieIDsWithAvgRatingsRDD: [(2, (129, 3.7131782945736433)), (4, (21, 4.190476190476191)), (6, (71, 3.9014084507042255))]

movieNameWithAvgRatingsRDD: [(4.114285714285714, u'Happiest Millionaire, The (1967)', 35), (3.9019607843137254, u'Grumpier Old Men (1995)', 51), (2.9, u'Hocus Pocus (1993)', 30)]

However - my code is failing your tests, e.g. my movie count is 2999.


1 test failed. incorrect movieIDsWithRatingsRDD.count() (expected 3615)
1 test failed. incorrect count of ratings for movieIDsWithRatingsTakeOrdered[0] (expected 993)
1 test failed. incorrect count of ratings for movieIDsWithRatingsTakeOrdered[1] (expected 332)
1 test failed. incorrect count of ratings for movieIDsWithRatingsTakeOrdered[2] (expected 299)
1 test failed. incorrect movieIDsWithAvgRatingsRDD.count() (expected 3615)
1 test failed. incorrect movieIDsWithAvgRatingsRDD.takeOrdered(3)
1 test failed. incorrect movieNameWithAvgRatingsRDD.count() (expected 3615)
1 test failed. incorrect movieNameWithAvgRatingsRDD.takeOrdered(3)
 Hi All,
 My internet search found one relevant match?
http://www.edureka.co/apache-spark-scala-training-certification

1. This edureka is a company based in India it seems. They also have training courses in Hadoop and big data stuff.
Anyone has used this company's training in the past? Can you share your experiences please(anonymously if you need)?

2. Does Databrick have plans to operate a Databrick University like Cloudera university in near future?
I went thru FAQ on Databrick company website, but I could not find any relevant info

Thanks in advance
 Hello, 
The first part in 1b I wrote like that
map(lambda x: (x[0],x[1::])).groupByKey()
But when it calls 

print 'movieIDsWithRatingsRDD: %s\n' % movieIDsWithRatingsRDD.take(3)

This error is throwing , can anyone help me?
---> 13 print 'movieIDsWithAvgRatingsRDD: %s\n' % movieIDsWithAvgRatingsRDD.take(3)
     14 
     15 # To `movieIDsWithAvgRatingsRDD`, apply RDD transformations that use `moviesRDD` to get the movie

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in take(self, num)
   1222 
   1223             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1224             res = self.context.runJob(self, takeUpToNumLeft, p, True)
   1225 
   1226             items += res

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)
    840         mappedRDD = rdd.mapPartitions(partitionFunc)
    841         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,
--> 842                                           allowLocal)
    843         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
    844 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 392.0 failed 1 times, most recent failure: Lost task 0.0 in stage 392.0 (TID 347, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1220, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-150-2f3eabb50c84>", line 12, in <lambda>
  File "<ipython-input-75-7f60b835c11c>", line 23, in getCountsAndAverages
TypeError: unsupported operand type(s) for +: 'float' and 'tuple'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745) Hi All,
 How much of HDFS we need to know to develop Spark applications?
I am talking about solving real-world business data problems.

Thanks <REDACTED> I think I figured it out. I needed a break and then to re-read the instructions.
 Dear instructors, 

Firstly, thank you very much for putting together such a wonderful course. Am new to python and definitely new to big data and spark. I tried my best amidst the various challenges and constraints but could not keep the pace. lab 3 broke me down and from there i just could not gather my self. 

I want to know what is the future course on the subject. I see a lot of opportunity for big data in my own organisation. How should i progress in spark. I know for sure i need to strengthen my python skills. 

Will this course be repeated. Or is there a sequel to it. Guidance on the matter would be of great help.

Thank you once again. Also a word of thanks to those special ppl who being a participant were also guiding and helping ppl like me...thanks 

Hoping to hear on the subject
Bye
 Hi,

When I am trying to submit the autograder, I am getting timeout error.
Please help!

I see the below error - (I checked and didn't use collect() at any place still its failing).

Timeout error happened during grading. Please review your code to be more efficient and submit the code again.

Your submission token ID is 1576694-4042f6fe48dbcf1a33b94b18b5c07e72:ip-172-31-24-73
Please include this submission token ID when you need support for your code submission.
Your anonymous student ID is XXXXXXXXXXXXXXX. Do not post this ID on Piazza.
 I would like to thank you course instructor Anthony D Joseph and the whole team for this course. I learned a lot and it was very well structured. Looking forward to Scalable Machine Learning course. 

Thanks again.

Happy 4th of July. I want to thank the instructor, the TAs, and my fellow classmates for this course! Definitely couldn't have done it without the forum. I started with a background in CS and work experience in Ruby (and two uni quarters of Python), and knew nothing about Spark or Big Data. I have learned a tremendous amount and already started on CS190.1x to learn more.

The labs were crafted with effort as they were relatively easy to follow along, though lab 3 was particularly challenging. Throughout the labs, I found myself sometimes jumping ahead (and straight into the <FILL IN> parts) because the comments explain so well what needs to done. As a result, I found myself having to go back and check formats of RDDs and/or the concepts behind that particular code block. I think this is great for learning new materials because of less confusion, and not so great because you can practically finish a problem and pass the tests without really knowing why/how you achieved it.

Despite that, I had a lot of fun learning about Spark and using iPython Notebooks. I will definitely delve deeper into Big Data, and possibly move my career toward that direction.

The end of Lab 4 gave me great suggestions for movies I haven't seen. As an avid Netflex user, it was fun to see a little of behind-the-scenes.

Jen Hi ,
Thanks for the great course!I learnt a bit about Python, PYSpark and bit about big data. (I guess) :)
Thanks to all the folks who helped put it together.
Regards
 What I am doing wrong here?

squaredErrorsRDD = <>
 I have been working on this lab for hours, but for no good. 
Why am I getting Py4JJavaError on doing 
print dayGroupedHosts.collect()
or
dayHostCount.sortByKey()
 <Dont post code> I very much enjoyed the course on many levels.

As a technical trainer for online courses, I realize what an incredible effort is required orchestrating this kind of hardware, putting together the content and dealing with all of the possible, unintended ways that learners can interpret the material. I'm sure you gathered a lot of great feedback that will make the next iteration better. 

My previous machine learning experience was in about 1990 in Sunnyvale, CA, at Systems Control, after going through the Lockheed AI center program and working with some very bright people in Palo Alto. We spent several years putting together a great application after investigating several approaches (neural networks/back prop, various decision tree, CHAID), but it didn't go too far due to political/funding issues. Getting a fire lit again and appreciating the progress made. LOVE IT!

Thank you,
Mary 4 am on a Sunday morning and I still can't figure this out. I initially had some difficulties calculating the sum
s = sum((amazonWeightsBroadcast.value[amazonRec][i]*googleWeightsBroadcast.value[googleRec][i] for i in tokens))
but I think I solved it. However I can't figure out how to calculate the 
value = s/((amazonNormsBroadcast.value[amazonRec])/(googleNormsBroadcast.value[googleRec]))
Here's the error code:
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-47-5c0cb5c60038> in <module>()
     23                        .cache())
     24 
---> 25 print similaritiesFullRDD.count()

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)
    930         3
    931         """
--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    933 
    934     def stats(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)
    921         6.0
    922         """
--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    924 
    925     def count(self):

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)
    737             yield reduce(f, iterator, initial)
    738 
--> 739         vals = self.mapPartitions(func).collect()
    740         if vals:
    741             return reduce(f, vals)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 121.0 failed 1 times, most recent failure: Lost task 0.0 in stage 121.0 (TID 496, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-47-5c0cb5c60038>", line 17, in fastCosineSimilarity
TypeError: list indices must be integers, not str

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
<p>Any help would be appreicated. Thanks.<br /><br />I do understand the error code. That amazonRec and googleRec are strings. But I don't know how to obtain the integer values so that they can be divide 's'</p> I was able to grab endpoints that don't have response_code 200. But when I tried to count how many are there and print it out, I got this error. What am I doing wrong?

Thanks!

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-57-911718c3f47f> in <module>()
      8 endpointCountPairTuple = not200.map(lambda log: (log.endpoint, 1))
      9 
---> 10 print endpointCountPairTuple.collect()
     11 
     12 endpointSum = endpointCountPairTuple.reduceByKey(lambda a, b : a + b)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError( I thought as a parting gift to the instructors, those who've been active in Piazza could collaboratively develop a set of FAQs that could either be used to shortcut some grief for students in future iterations or to improve the labs to avoid them in the first place. I've put a few below that come to mind, feel free to edit/add.

Q: I get an error -- (stack trace follows)
A: The really relevant part of the error stack trace is near the bottom -- all the stuff with chunks of code are not really necessary for debugging. You're looking for a couple of paragraphs at the bottom that have a specific error message. 

Q: I get an error in my lambda function -- "too many values to unpack" or "lambda() expects 1 parameter, 2 were given"
A: You need to understand what's being passed to your lambda function by the invoking transformation/action. lambda a,b : a+b and lambda (a,b) : (a+b) are very different. If in doubt, look to the enclosing RDD and do a take(3) on it so you see the exact structure that's being passed to your lambda.

Q: Do I have to use positional arguments with lambda functions, like a[1][0][1]?
A : No. It might be clearer to unpack your variables like lambda (rating, title, num_reviews) : (rating, title)

Q: The autograder runs slow. 
A: Look for inefficiencies in your code. Particular examples are cartesian(), join(), collect(). Most lab steps run in no more than 10 minutes on average machines. If your machine performs slowly, investigate getting a cloud-based account from Databricks.

Q: My movie recommendations look weird/include movies I've rated
A: The "Movies with average ratings RDD" that's used to show you the 50 most rated movies does NOT have the movie ID.  The best (but not quickest) way to fix this is to build a parallel RDD back in the early parts of the lab and carry an (id, title, rating, num_ratings) RDD forward for you to use later.  This will probably be fixed in a future version of the lab. Watch the sorting function, it uses positional parameters to do the sort and you might get weird values.

Q: Can I run this VM or native Spark on ... (OS)
A: Probably, but it's not going to be supported by the course staff, so debugging is on your own.

Q: I lost points on my lab after submitting...
A: For whatever reason, edx doesn't keep the highest score, just the latest. Until this is addressed, be very cognizant of the due dates or you could lose points.

Q: Is my score ever shown?
A: No -- only pass/fail.  Specific score is only seen by the student and instructors.

Q: How can I reduce lists without nesting them?
A: If you do lambda a,b : a+b with lists, it will create nested lists. An easy fix is to generate [a] and [b] from the map task, then a+b will be an unpacked [a b] list.

Q: I get the right transformation after mapping, but the structures are nested..
A: Look at flatmap instead

Q: I can't get past step.. (insert step here)
A: The best methodology is to take one step at a time, even when the result is a nested map-filter-reduce-sort command.  Do the map first, print take(3), then do the reduce, print take(3) and so on so you're familiar with the data structures as they get transformed.

Q: Why use take() instead of collect()? 
A: It's faster, just takes the first few records.

Q: I don't understand how sort() lambdas work.  
A: They specify a sorting function, not a transforming function. For these labs, it will normally be extracting a single value to sort by from the RDD and/or negating it to invert the sort order.  lambda row : -row[1] will sort in reverse order by the 2nd element in the row.

Q: I can't use my Broadcast variable, or get an error saying a Broadcast object has no attribute __getitem__
A: To send your RDD as a broadcast to other nodes, you should collectAsMap, then use sc.broadcast().  The resulting object is a wrapper, so you need to use [broadcastVar].value to grab your map again.

Q: I get an error saying can't find [Test, Broadcast, other object] ...
A: Ensure you run all cells above your location -- imports are frequently hiding in un-run cells.  If all else fails, do a Kernel - Restart from the Jupyter menu and re-run cells above your current point.


 Hi all,

in Lab 4, question 1, my movieIDsWithRatingsRDD looks fine (it passes the test also):

movieIDsWithRatingsRDD: [(1, <pyspark.resultiterable.ResultIterable object at 0xb1fabb0c>), (2, <pyspark.resultiterable.ResultIterable object at 0xb1fab2ec>), (3, <pyspark.resultiterable.ResultIterable object at 0xb0eb738c>)]

after converting the iterable results in a list I get the following:

movieIDsWithAvgRatingsRDD: [(1, [(5.0, u'Toy Story (1995)'), (4.0, u'Toy Story (1995)'), (5.0, u'Toy Story (1995)'), (3.0, u'Toy Story (1995)'), (4.0, u'Toy Story (1995)'), (5.0, u'Toy Story (1995)'), (5.0, u'Toy Story (1995)'), (4.0, u'Toy Story (1995)'), (4.0, u'Toy Story (1995)'), (5.0, u'Toy Story (1995)'), (3.0, u'Toy Story (1995)'), (3.0, u'Toy Story (1995)'), (4.0, u'Toy Story (1995)'), (3.0, u'Toy Story (1995)'), (3.0, u'Toy Story (1995)'), (3.0, u'Toy Story (1995)'), (5.0, u'Toy S
How can I eliminate/transform the title of the film from the tuple? I tried everything....If I use a lambda function like
.map(lambda x: (x[0],x[1]))

I get only the first list
[(1, [(5.0, u'Toy Story (1995)')]
If I use .map(lambda x: (x[0],x[1][0])I got an error. How can I keep the ratings and eliminate the titles?Thanks and best regards,Rodrigo. I'd like to thank all staff members for having put this course together and helped us here on the forum.

Special kudos to Anthony D. Joseph and Felix Cheung for being extra active on the forums!

Thank you! {'photoshop': 0.5, 'adobe': 0.5}
{'illustrator': 0.5, 'adobe': 0.5}
0.0761961264455 Why am I getting this error:
---------------------------------------------------------------------------Py4JJavaError                             Traceback (most recent call last)<ipython-input-57-152ccdbd39ec> in <module>()      7 dayHostCount = dayGroupedHosts.map(lambda a : (a[0], len(a[1])))      8 #dailyHosts = (dayHostCount.sortByKey()).cache()----> 9 dailyHosts = (dayHostCount.sortByKey().cache())     10 dailyHostsList = dailyHosts.take(30)     11 print 'Unique hosts per day: %s' % dailyHostsList/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sortByKey(self, ascending, numPartitions, keyfunc)    584         # the key-space into bins such that the bins have roughly the same    585         # number of (key, value) pairs falling into them--> 586         rddSize = self.count()    587         if not rddSize:    588             return self  # empty RDD/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in count(self)    930         3    931         """--> 932         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()    933     934     def stats(self):/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in sum(self)    921         6.0    922         """--> 923         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)    924     925     def count(self):/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in reduce(self, f)    737             yield reduce(f, iterator, initial)    738 --> 739         vals = self.mapPartitions(func).collect()    740         if vals:    741             return reduce(f, vals)/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)    711         """    712         with SCCallSiteSync(self.context) as css:--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())    714         return list(_load_from_socket(port, self._jrdd_deserializer))    715 /usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)    536         answer = self.gateway_client.send_command(command)    537         return_value = get_return_value(answer, self.gateway_client,--> 538                 self.target_id, self.name)    539     540         for temp_arg in temp_args:/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)    298                 raise Py4JJavaError(    299                     'An error occurred while calling {0}{1}{2}.\n'.--> 300                     format(target_id, '.', name), value)    301             else:    302                 raise Py4JError(Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 112.0 failed 1 times, most recent failure: Lost task 0.0 in stage 112.0 (TID 291, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main    process()  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process    serializer.dump_stream(func(split_index, iterator), outfile)  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 125, in dump_stream    for obj in iterator:  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py", line 1636, in add_shuffle_key    d = outputSerializer.dumps(buckets[split])  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 402, in dumps    return cPickle.dumps(obj, 2)TypeError: expected string or Unicode object, NoneType found	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)	at org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:914)
	at scala.collection.Iterator$GroupedIterator.go(Iterator.scala:929)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:968)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:972)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:210)	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)	at org.apache.spark.scheduler.Task.run(Task.scala:64)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:745)Driver stacktrace:	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) Hello Prof. Joseph and Instructors
 
Successfully completed the coursework with 100%. Thanks a lot for creating this coursework and the support throughout .I believe this is the first online course on Spark ever even  though Spark has been around for quite a few years. It was an exciting and a hassle free learning experience. It has motivated me to dig into spark further which seemed a daunting task otherwise .
 
Feedback 
 
1) Efficient Setup : The setup provided for running spark  on local PC was good especially for a person like me who is more from analysis background rather than core IT  background. Most of the times the hassle of setting up these new environments  is so time consuming that  we lose all the motivation to do the analysis which did not happen here.
 
2) Big-Data Feel : Although the spark is designed such  that if we write our code in functional programming paradigm, the code works identically for small and large datasets, I did not feel like doing a big data analysis.  You can  show  via a lecture or video  how the code works for big data as well .
 
3) Lab 3 and Lab 4 can be swapped ( for obvious reasons :)  )
 
4) Demonstration of Spark for feature extraction : Most of the big data problems start with extracting  a new feature from combination of one or more existing columns of the data set. It would be helpful to include such problems in the lab followed by analysis. 
 
5)  End to End Problems :  The labs can be fine tuned as a complete problem by itself  where it starts with data sets and a particular objective and ends with us generating a small report  of the analysis with the intermediate steps  of cleaning, processing ,transforming and modelling.  I again Thank you all  for making this possible.
Look forward to complete the "Scalable Machine Learning" as well  and a new advanced version of this course.
 
 
 
  I'm getting the following error:


crossSmall.take(2): [(('http://www.google.com/base/feeds/snippets/11448761432933644608', 'spanish vocabulary builder "expand your vocabulary! contains fun lessons that both teach and entertain you\'ll quickly find yourself mastering new terms. includes games and more!" '), ('b000jz4hqo', 'clickart 950 000 - premier image pack (dvd-rom)  "broderbund"')), (('http://www.google.com/base/feeds/snippets/11448761432933644608', 'spanish vocabulary builder "expand your vocabulary! contains fun lessons that both teach and entertain you\'ll quickly find yourself mastering new terms. includes games and more!" '), ('b0006zf55o', 'ca international - arcserve lap/desktop oem 30pk "oem arcserve backup v11.1 win 30u for laptops and desktops" "computer associates"'))]






---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-41-d27998464014> in <module>()
     73                 #computeSimilarity(cSmall[0])#(('http://www.google.com/base/feeds/snippets/11448761432933644608', 'clickart 950000 - premier image pack (dvd-rom) massive collection"expand  more!" '), ('b000jz4hqo', 'clickart 950 000 - premier image pack (dvd-rom)  "broderbund"')))
     74 
---> 75 similarities = (crossSmall.map(computeSimilarity).cache())
     76 
     77 #print '%s' % similarities.collect().take(1)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in cache(self)
    189         """
    190         self.is_cached = True
--> 191         self.persist(StorageLevel.MEMORY_ONLY_SER)
    192         return self
    193 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in persist(self, storageLevel)
    205         self.is_cached = True
    206         javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)
--> 207         self._jrdd.persist(javaStorageLevel)
    208         return self
    209 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _jrdd(self)
   2286         command = (self.func, profiler, self._prev_jrdd_deserializer,
   2287                    self._jrdd_deserializer)
-> 2288         pickled_cmd, bvars, env, includes = _prepare_for_python_RDD(self.ctx, command, self)
   2289         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(),
   2290                                              bytearray(pickled_cmd),

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in _prepare_for_python_RDD(sc, command, obj)
   2204     # the serialized command will be compressed by broadcast
   2205     ser = CloudPickleSerializer()
-> 2206     pickled_command = ser.dumps(command)
   2207     if len(pickled_command) > (1 << 20):  # 1M
   2208         broadcast = sc.broadcast(pickled_command)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py in dumps(self, obj)
    409 
    410     def dumps(self, obj):
--> 411         return cloudpickle.dumps(obj, 2)
    412 
    413 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dumps(obj, protocol)
    814 
    815     cp = CloudPickler(file,protocol)
--> 816     cp.dump(obj)
    817 
    818     #print 'cloud dumped', str(obj), str(cp.modules)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in dump(self, obj)
    131         self.inject_addons()
    132         try:
--> 133             return pickle.Pickler.dump(self, obj)
    134         except RuntimeError, e:
    135             if 'recursion' in e.args[0]:

/usr/lib/python2.7/pickle.pyc in dump(self, obj)
    222         if self.proto >= 2:
    223             self.write(PROTO + chr(self.proto))
--> 224         self.save(obj)
    225         self.write(STOP)
    226 

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    560         write(MARK)
    561         for element in obj:
--> 562             save(element)
    563 
    564         if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    252             klass = getattr(themodule, name, None)
    253             if klass is None or klass is not obj:
--> 254                 self.save_function_tuple(obj, [themodule])
    255                 return
    256 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    302         # create a skeleton function object and memoize it
    303         save(_make_skel_func)
--> 304         save((code, closure, base_globals))
    305         write(pickle.REDUCE)
    306         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    546         if n <= 3 and proto >= 2:
    547             for element in obj:
--> 548                 save(element)
    549             # Subtle.  Same as in the big comment below.
    550             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    598 
    599         self.memoize(obj)
--> 600         self._batch_appends(iter(obj))
    601 
    602     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    634                 write(APPENDS)
    635             elif n:
--> 636                 save(tmp[0])
    637                 write(APPEND)
    638             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    679                 for k, v in tmp:
    680                     save(k)
--> 681                     save(v)
    682                 write(SETITEMS)
    683             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    679                 for k, v in tmp:
    680                     save(k)
--> 681                     save(v)
    682                 write(SETITEMS)
    683             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function(self, obj, name, pack)
    247                     modList = list(mainmod.___pyc_forcedImports__)
    248                 self.savedForceImports = True
--> 249             self.save_function_tuple(obj, modList)
    250             return
    251         else:   # func is nested

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_function_tuple(self, func, forced_imports)
    307 
    308         # save the rest of the func data needed by _fill_function
--> 309         save(f_globals)
    310         save(defaults)
    311         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/cloudpickle.py in save_dict(self, obj)
    172             self.save_reduce(_get_module_builtins, (), obj=obj)
    173         else:
--> 174             pickle.Pickler.save_dict(self, obj)
    175     dispatch[pickle.DictionaryType] = save_dict
    176 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    647 
    648         self.memoize(obj)
--> 649         self._batch_setitems(obj.iteritems())
    650 
    651     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    679                 for k, v in tmp:
    680                     save(k)
--> 681                     save(v)
    682                 write(SETITEMS)
    683             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    304             reduce = getattr(obj, "__reduce_ex__", None)
    305             if reduce:
--> 306                 rv = reduce(self.proto)
    307             else:
    308                 reduce = getattr(obj, "__reduce__", None)

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py in __getnewargs__(self)
    242         # This method is called when attempting to pickle SparkContext, which is always an error:
    243         raise Exception(
--> 244             "It appears that you are attempting to reference SparkContext from a broadcast "
    245             "variable, action, or transforamtion. SparkContext can only be used on the driver, "
    246             "not in code that it run on workers. For more information, see SPARK-5063."

Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.

I printed for test crossSmall in the beginning. Any suggestions more than welcome.
Thank you

 how can i use reduceByKey() without arguments to get tuples of form (key, pyspark_iterable)?
I know i can use groupByKey() to get that but don't know how to use reduceBykey() for that as it take function as parameter.

Also, I want to know when using groupByKey() is good? Even after reading thru all the que  in lab4    i am not able to figure out I  am wondering if anyone can help me in this -

movieNameWithAvgRatingsRDD: [(2049, (u'Happiest Millionaire, The (1967)', <pyspark.resultiterable.ResultIterable object at 0xb0d712ec>)), (3, (u'Grumpier Old Men (1995)', <pyspark.resultiterable.ResultIterable object at 0xb0d715ec>))]

and for .map(lambda x:list(x[1][1])))  type -

movieNameWithAvgRatingsRDD: [[(5.0,), (4.0,), (3.0,), (5.0,), (3.0,), (4.0,), (4.0,), (3.0,), (5.0,), (2.0,), (2.0,), (5.0,), (3.0,), (4.0,), (4.0,), (4.0,), (3.0,), (5.0,), (3.0,), (4.0,), (2.0,), (4.0,)], [(2.0,), (2.0,), (5.0,), (1.0,), (5.0,), (3.0,), (4.0,), (1.0,), (4.0,), (3.0,), (4.0,), (4.0,), (2.0,), (3.0,), (3.0,), (3.0,), (2.0,), (3.0,), (4.0,), (3.0,), (4.0,), (2.0,), (3.0,), (5.0,), (1.0,), (3.0,), (1.0,), (3.0,), (5.0,), (5.0,), (3.0,), (3.0,), (2.0,), (3.0,), (3.0,), (2.0,), (2.0,), (4.0,), (3.0,), (3.0,), (2.0,), (1.0,), (4.0,), (2.0,), (2.0,), (1.0,), (4.0,), (1.0,), (3.0,), (4.0,), (3.0,), (3.0,), (3.0,), (3.0,), (3.0,), (2.0,), (4.0,), (2.0,), (2.0,), (3.0,), (4.0,), (3.0,), (2.0,), (3.0,), (3.0,), (2.0,), (4.0,), (3.0,), (4.0,), (4.0,), (3.0,), (5.0,), (5.0,), (2.0,), (3.0,), (3.0,), (3.0,), (3.0,), (3.0,), (2.0,), (3.0,), (3.0,), (2.0,), (2.0,), (3.0,), (3.0,), (3.0,), (4.0,), (3.0,), (2.0,), (1.0,), (3.0,), (3.0,), (1.0,), (4.0,), (4.0,), (4.0,), (3.0,), (5.0,), (4.0,), (5.0,), (4.0,), (4.0,), (3.0,), (3.0,), (2.0,), (2.0,), (3.0,), (3.0,), (3.0,), (3.0,), (4.0,), (3.0,), (3.0,), (3.0,), (3.0,), (1.0,), (4.0,), (5.0,), (2.0,), (4.0,), (3.0,), (3.0,), (3.0,), (1.0,), (5.0,), (1.0,), (1.0,), (2.0,), (4.0,), (3.0,), (2.0,), (3.0,), (3.0,), (4.0,), (4.0,), (4.0,), (3.0,), (2.0,), (3.0,), (2.0,), (3.0,), (1.0,), (3.0,), (3.0,), (5.0,), (3.0,), (3.0,), (5.0,), (3.0,), (1.0,), (4.0,), (3.0,), (4.0,), (3.0,), (5.0,), (1.0,), (3.0,), (3.0,), (3.0,), (4.0,), (3.0,), (5.0,), (3.0,), (3.0,), (4.0,), (4.0,), (3.0,), (3.0,), (4.0,), (3.0,), (3.0,), (3.0,), (4.0,), (4.0,), (3.0,), (2.0,), (3.0,), (2.0,), (1.0,), (3.0,), (4.0,), (3.0,), (4.0,), (2.0,), (1.0,), (1.0,), (3.0,), (4.0,), (3.0,), (4.0,), (4.0,), (4.0,), (4.0,), (5.0,), (2.0,), (1.0,), (2.0,), (4.0,), (4.0,), (5.0,), (3.0,), (4.0,), (3.0,), (3.0,), (4.0,), (2.0,), (5.0,), (4.0,), (2.0,), (5.0,), (3.0,), (1.0,), (3.0,), (2.0,), (4.0,), (4.0,), (4.0,), (3.0,), (1.0,), (2.0,), (2.0,), (3.0,), (5.0,), (2.0,), (3.0,), (3.0,), (3.0,), (2.0,), (2.0,), (3.0,), (3.0,), (1.0,), (1.0,), (3.0,), (4.0,), (3.0,), (2.0,), (2.0,), (4.0,), (2.0,), (2.0,), (2.0,), (4.0,), (3.0,), (2.0,), (3.0,), (2.0,), (2.0,), (2.0,), (3.0,), (3.0,), (4.0,), (3.0,), (3.0,), (3.0,), (4.0,), (3.0,), (1.0,), (3.0,), (3.0,), (3.0,), (5.0,), (2.0,), (2.0,), (4.0,), (3.0,), (5.0,), (2.0,), (5.0,), (4.0,), (5.0,), (5.0,), (5.0,), (5.0,), (3.0,), (4.0,), (4.0,), (4.0,), (5.0,), (3.0,), (3.0,), (3.0,), (3.0,), (3.0,), (2.0,), (5.0,), (2.0,), (2.0,), (1.0,), (2.0,), (5.0,), (3.0,), (3.0,), (3.0,), (4.0,), (3.0,), (4.0,), (2.0,)]]

 Hello 
I finish lab 4 and return back to lab 3. I'm in 3c. This code gives an error in their exist code (not my development) I need help to solve this error

here the code and here the error

# TODO: Replace <FILL IN> with appropriate codecrossSmall = (googleSmall .cartesian(amazonSmall) .cache())def computeSimilarity(record): """ Compute similarity on a combination record Args: record: a pair, (google record, amazon record) Returns: pair: a pair, (google URL, amazon ID, cosine similarity value) """ googleRec = record[0] amazonRec = record[1] googleURL = googleRec[0] amazonID = amazonRec[0] googleValue = googleRec[1] amazonValue = amazonRec[1] cs = cosineSimilarity(googleValue,amazonValue, idfsSmallWeights) return (googleURL, amazonID, cs)similarities = (crossSmall .map(lambda record: computeSimilarity(record)) .cache())def similar(amazonID, googleURL): """ Return similarity value Args: amazonID: amazon ID googleURL: google URL Returns: similar: cosine similarity value """ return (similarities .filter(lambda record: (record[0] == googleURL and (record[1] == amazonID))) .collect()[0][2])similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')print 'Requested similarity is %s.' % similarityAmazonGoogle

Can anyone help me to solve the problem?

Error :
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-28-399c027950a4> in <module>()
     32             .filter(lambda record: (record[0] == googleURL and (record[1] == amazonID)))
     33             .collect()[0][2])
---> 34 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')
     35 print 'Requested similarity is %s.' % similarityAmazonGoogle

<ipython-input-28-399c027950a4> in similar(amazonID, googleURL)
     30     """
     31     return (similarities
---> 32             .filter(lambda record: (record[0] == googleURL and (record[1] == amazonID)))
     33             .collect()[0][2])
     34 similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py in collect(self)
    711         """
    712         with SCCallSiteSync(self.context) as css:
--> 713             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    714         return list(_load_from_socket(port, self._jrdd_deserializer))
    715 

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 235, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 101, in main
    process()
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-28-399c027950a4>", line 21, in <lambda>
  File "<ipython-input-28-399c027950a4>", line 18, in computeSimilarity
  File "<ipython-input-24-f28a6991221d>", line 14, in cosineSimilarity
  File "<ipython-input-20-998b63fd80a2>", line 24, in tfidf
KeyError: 'and'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
</span></span></span></span></span></span></span></span></span></span></span>
<p><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansiyellow"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"><span class="ansicyan"><span class="ansigreen"> </span></span></span></span></span></span></span></span></span></span></span></p>
<p></p> I think I've seen this posted before, but no official comment on it.

3(a) uses movieLimitedAndSortedByRatingRDD, which is a filtered and sorted movieNameWithAvgRatingsRDD, which was specifically formed to get rid of IDs:

"

We want to see movie names, instead of movie IDs. ... convert that RDD into the form of (average rating, movie name, number of ratings). These transformations will yield an RDD that looks like: [(3.6818181818181817, u'Happiest Millionaire, The (1967)', 22)

"

So the last number in the 3a tuple is the number of ratings, and not the movie ID.  (The print statement correctly says this, but then the comment in the code block indicates the # of reviews is the ID).  So if you use these numbers, the code probably winds up working, you're just essentially adding in ~10 recommendations of random movies.